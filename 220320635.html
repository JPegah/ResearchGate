<!DOCTYPE html> <html lang="en" class="" id="rgw37_56ab1fcfbe0fb"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="6uVzkZ82bXKjriErrwH5B5odVkpmv8HzEa9W2PBkjFiQshiBxn0BARTazK44YpuR9LM7n3ao1NOjJipGM3O92oM2vWPoErRBb2UNaPyx7CvGnTEYD13M7MEdlEAaXuFkSwj3JT3LktoFoBT/cDTlqwQQRLXrQ9L1+XaOspC+TCIITwQveFKYjjT4Xwy6B8Pqz7hUTZSpCjny0IMEdhcaD5VaNro5v69Wqe6vp7rLN0oiOYb4HztIBoO648dsc0qvCfhSo2ayE9hver2A871Oim3LcYaWgDcf6pAVK/UBtb0="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-cc911ec5-4320-4803-9923-47ba5e6ab1ce",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Bayesian Gaussian Process Latent Variable Model." />
<meta property="og:description" content="We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model/links/0ffc98940cf255165fc9bf47/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model" />
<meta property="rg:id" content="PB:220320635" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Bayesian Gaussian Process Latent Variable Model." />
<meta name="citation_author" content="Michalis K. Titsias" />
<meta name="citation_author" content="Neil D. Lawrence" />
<meta name="citation_publication_date" content="2010/01/01" />
<meta name="citation_volume" content="9" />
<meta name="citation_firstpage" content="844" />
<meta name="citation_lastpage" content="851" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Bayesian Gaussian Process Latent Variable Model.</title>
<meta name="description" content="Bayesian Gaussian Process Latent Variable Model. on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1fcfbe0fb" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1fcfbe0fb" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1fcfbe0fb">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Bayesian%20Gaussian%20Process%20Latent%20Variable%20Model.&rft.title=Journal%20of%20Machine%20Learning%20Research%20-%20Proceedings%20Track&rft.jtitle=Journal%20of%20Machine%20Learning%20Research%20-%20Proceedings%20Track&rft.volume=9&rft.date=2010&rft.pages=844-851&rft.au=Michalis%20K.%20Titsias%2CNeil%20D.%20Lawrence&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Bayesian Gaussian Process Latent Variable Model.</h1> <meta itemprop="headline" content="Bayesian Gaussian Process Latent Variable Model.">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model/links/0ffc98940cf255165fc9bf47/smallpreview.png">  <div id="rgw7_56ab1fcfbe0fb" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1fcfbe0fb" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Michalis_Titsias" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Michalis K. Titsias" alt="Michalis K. Titsias" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michalis K. Titsias</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab1fcfbe0fb" data-account-key="Michalis_Titsias">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Michalis_Titsias"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Michalis K. Titsias" alt="Michalis K. Titsias" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Michalis_Titsias" class="display-name">Michalis K. Titsias</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/The_University_of_Manchester" title="The University of Manchester">The University of Manchester</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1fcfbe0fb"> <a href="researcher/39663468_Neil_D_Lawrence" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Neil D. Lawrence" alt="Neil D. Lawrence" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Neil D. Lawrence</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab1fcfbe0fb">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/39663468_Neil_D_Lawrence"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Neil D. Lawrence" alt="Neil D. Lawrence" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/39663468_Neil_D_Lawrence" class="display-name">Neil D. Lawrence</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     Journal of Machine Learning Research - Proceedings Track   <meta itemprop="datePublished" content="2010-01">  01/2010;  9:844-851.             <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/jmlr/jmlrp9.html#TitsiasL10" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw12_56ab1fcfbe0fb" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input vari- ables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maxi- mization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the di- mensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality re- duction problems, but the methodology is more general. For example, our algorithm is imme- diately applicable for training Gaussian process models in the presence of missing or uncertain inputs.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw25_56ab1fcfbe0fb">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw24_56ab1fcfbe0fb"  itemprop="articleBody">  <p>Page 1</p> <p>Bayesian Gaussian Process Latent Variable Model<br />Michalis K. Titsias<br />School of Computer Science<br />University of Manchester<br />Neil D. Lawrence<br />School of Computer Science<br />University of Manchester<br />Abstract<br />We introduce a variational inference framework<br />for training the Gaussian process latent variable<br />model and thus performing Bayesian nonlinear<br />dimensionality reduction. This method allows<br />us to variationally integrate out the input vari-<br />ables of the Gaussian process and compute a<br />lower bound on the exact marginal likelihood of<br />the nonlinear latent variable model. The maxi-<br />mization of the variational lower bound provides<br />a Bayesian training procedure that is robust to<br />overfitting and can automatically select the di-<br />mensionality of the nonlinear latent space. We<br />demonstrate our method on real world datasets.<br />The focus in this paper is on dimensionality re-<br />duction problems, but the methodology is more<br />general. For example, our algorithm is imme-<br />diately applicable for training Gaussian process<br />models in the presence of missing or uncertain<br />inputs.<br />1Introduction<br />Gaussian processes (GPs) (see e.g. Rasmussen and<br />Williams, 2006) are stochastic processes over real-valued<br />functions. GPs offer a Bayesian nonparametric framework<br />for inference of highly nonlinear latent functions from ob-<br />served data. They have become very popular in machine<br />learning for solving problems such as nonlinear regression<br />and classification.<br />The standard application of GP models is to supervised<br />learning tasks where both output and input data are as-<br />sumed to be given at training time. The application of<br />GPs to unsupervised learning tasks is more involved. One<br />approach to unsupervised learning with GPs is the Gaus-<br />sian process latent variable model (GP-LVM) proposed by<br />Appearing in Proceedings of the 13thInternational Conference<br />on Artificial Intelligence and Statistics (AISTATS) 2010, Chia La-<br />guna Resort, Sardinia, Italy. Volume 9 of JMLR: W&amp;CP 9. Copy-<br />right 2010 by the authors.<br />Lawrence (2004, 2005). GP-LVM can be considered as a<br />multiple-output GP regression model where only the out-<br />put data are given. The inputs are unobserved and are<br />treated as latent variables, however instead of integrating<br />out the latent variables, they are optimized.<br />makes the model tractable and some theoretical ground-<br />ing for the approach is given by the fact that the model<br />can be seen as a nonlinear extension of the linear prob-<br />abilistic PCA (PPCA). In PPCA (and in factor analysis<br />(FA)) Bayesian extensions of the model are straightforward<br />(Bishop, 1999b; Ghahramani and Beal, 2000) using varia-<br />tional algorithms based on mean field approximations. An<br />analogous variational method for the GP-LVM is a much<br />more challenging problem which had not been addressed<br />until this paper. The main difficulty is that to apply vari-<br />ational Bayes to GP-LVM we need to approximately inte-<br />grate out the latent/input variables that appear nonlinearly<br />in the inverse kernel matrix of the GP model. Standard<br />mean field variational methodologies do not lead to an an-<br />alytically tractable algorithm.<br />This trick<br />We introduce a framework that allows us to variationally<br />integrate out the latent variables in the GP-LVM and com-<br />pute a closed-form Jensen’s lower bound on the true log<br />marginal likelihood of the data. The key ingredient that<br />makes the variational Bayes approach tractable is the ap-<br />plication of variational inference in an expanded probabil-<br />ity model where the GP prior is augmented to include aux-<br />iliary inducing variables. Inducing variables were intro-<br />duced originally for computational speed ups in GP regres-<br />sion models (Csat´ o and Opper, 2002; Seeger et al., 2003;<br />Csat´ o, 2002; Snelson and Ghahramani, 2006; Qui˜ nonero<br />Candela and Rasmussen, 2005; Titsias, 2009). Our ap-<br />proach builds on, and significantly extends the variational<br />sparse GP method of Titsias (2009) so that a closed-form<br />variational lower bound of the GP-LVM marginal likeli-<br />hood is computed. This solves a key problem with the<br />GP-LVM: variational inference in the GP-LVM allows for<br />Bayesian training of the model that is robust to overfitting.<br />Furthermore, by using the automatic relevance determina-<br />tion (ARD) squared exponential kernel, the algorithm al-<br />lows us to automatically infer the dimensionality of the<br />nonlinear latent space without introducing explicit regular-<br />izers to enforce this constraint (Geiger et al., 2009).</p>  <p>Page 2</p> <p>Bayesian Gaussian Process Latent Variable Model<br />Although, in this paper, we focus on application of the vari-<br />ational approach tothe GP-LVM, the methodology we have<br />developed can be more widely applied to a variety of other<br />GP models. In particular, our algorithm is immediately ap-<br />plicable for training GPs with missing or uncertain inputs<br />(Girard et al., 2002). Other possible applications will be<br />discussed as future work.<br />In the remainder of the paper we first review the GP-LVM<br />and then we introduce our variational approximation. We<br />finish by demonstrating the ability of the new model to au-<br />tomatically determine dimensionality and resist overfitting<br />on real world datasets.<br />2 Gaussian process latent variable model<br />Let Y ∈<br />number of observations and D the dimensionality of each<br />data vector. These data are associated with latent variables<br />X ∈<br />ality reduction, Q ≪ D. The GP-LVM (Lawrence, 2005)<br />defines a forward (or generative) mapping from the latent<br />space to observation space that is governed by Gaussian<br />processes. If the GPs are taken to be independent across<br />the features then the likelihood function is written as<br />?N×Dbe the observed data where N is the<br />?N×Qwhere, for the purpose of doing dimension-<br />p(Y |X) =<br />D<br />?<br />d=1<br />p(yd|X),<br />(1)<br />where ydrepresents the dthcolumn of Y and<br />p(yd|X) = N(yd|0,KNN+ β−1IN).<br />(2)<br />Here, KNNis the N ×N covariance matrix defined by the<br />covariance (orkernel) function k(x,x′). Forthe purpose of<br />doing automatic model selection of the dimensionality of<br />latent space, this kernel can be chosen to follow the ARD<br />(see Rasmussen and Williams, 2006) squared exponential<br />form:<br />?<br />k(x,x′) = σ2<br />fexp<br />−1<br />2<br />Q<br />?<br />q=1<br />αq(xq− x′<br />q)2<br />?<br />.<br />(3)<br />Equation (1) can be viewed as the likelihood function of a<br />multiple-output GP regression model where the vectors of<br />different outputs are drawn independently from the same<br />Gaussian process prior which is evaluated at the inputs X.<br />Since X is a latent variable, we can assign it a prior density<br />given by the standard normal density. More precisely, the<br />prior for X is:<br />p(X) =<br />N<br />?<br />n=1<br />N(xn|0,IQ),<br />(4)<br />where each xnis the nthrow of X. The joint probability<br />model for the GP-LVM model is<br />p(Y,X) = p(Y |X)p(X).<br />(5)<br />The hyperparameters of the model are the kernel param-<br />eters θ = (σ2<br />rameter β. For the sake of clarity, these parameters are<br />omitted from the conditioning of the distribution1. Cur-<br />rently, the primary methodology for training the GP-LVM<br />model is to find the MAP estimate of X (Lawrence, 2005)<br />whilst jointly maximizing with respect to the hyperparam-<br />eters. Here, we develop a variational Bayesian approach to<br />marginalization of the latent variables, X, allowing us to<br />optimize the resulting lower bound on the marginal likeli-<br />hoodwithrespecttothehyperparameters. Thelowerbound<br />can also be used for model comparison and automatic se-<br />lection of the latent dimensionality.<br />f,α1,...,αQ) and the inverse variance pa-<br />3Variational inference<br />We wish to compute the marginal likelihood of the data:<br />?<br />However, this quantity is intractable as X appears nonlin-<br />early inside the inverse of the covariance matrix KNN+<br />β−1IN. Instead, we seek to apply an approximate varia-<br />tional inference procedure where we introduce a variational<br />distribution q(X) to approximate the true posterior distri-<br />bution p(X|Y ) over the latent variables. We take the varia-<br />tional distribution to have a factorized Gaussian form over<br />the latent variables,<br />p(Y ) =<br />p(Y |X)p(X)dX.<br />(6)<br />q(X) =<br />N<br />?<br />n=1<br />N(xn|µn,Sn),<br />(7)<br />where the variational parameters are {µn,Sn}N<br />simplicity, Sn is taken to be a diagonal covariance ma-<br />trix2. Using this variational distribution we can express a<br />Jensen’s lower bound on the logp(Y ) that takes the form:<br />?<br />=<br />q(X)logp(Y |X)dX −<br />n=1and, for<br />F(q) =<br />q(X)logp(Y |X)p(X)<br />q(X)<br />dX<br />??<br />q(X)logq(X)<br />p(X)dX<br />=?F(q) − KL(q||p),<br />tween the variational posterior distribution q(X) and the<br />prior distribution p(X) over the latent variables. This term<br />is computed analytically since both distributions are Gaus-<br />sians. Therefore, the difficult part when estimating the<br />above bound is the first term:<br />?<br />(8)<br />where the second term is the negative KL divergence be-<br />?F(q) =<br />1A<br />p(Y |X,β,θ)p(X).<br />2This can be extended to non-diagonal within our framework.<br />D<br />?<br />d=1<br />q(X)logp(yd|X)dX =<br />D<br />?<br />d=1<br />?Fd(q),<br />(9)<br />precise notationistowrite<br />p(Y,X|β,θ)=</p>  <p>Page 3</p> <p>Michalis K. Titsias, Neil D. Lawrence<br />where we have used (1). Thus, the computation of?F(q)<br />responding to the dthoutput. Notice that the computation of<br />?Fd(q) involves an analytically intractable integration. This<br />linear manner inside the inverse of the covariance matrix,<br />KNN+ β−1IN. Our main contribution is a mathematical<br />tool that allows us to compute a closed-form lower bound<br />for?Fd(q). As we will see, the key idea is to apply vari-<br />model.<br />breaks down to separate computations of each?Fd(q), cor-<br />arises because logp(yd|X) contains X in an highly non-<br />ational sparse GP regression in an augmented probability<br />3.1Lower bound by applying variational sparse GP<br />regression<br />The computation in?Fd(q) involves an expectation over the<br />compute a Jensen’s lower bound on logp(yd|X) by intro-<br />ducingtheGP latentfunctionvaluestogetherwithauxiliary<br />inducing variables as those used in sparse GP models.<br />intractable term logp(yd|X). To deal with this, we first<br />Sparse approximations have already been applied to speed<br />up the GP-LVM Lawrence (2007). The first step of our<br />approximation is equivalent to applying the new varia-<br />tional approximation of Titsias (2009) to the standard GP-<br />LVM. The likelihood function p(yd|X) is just the Gaussian<br />marginal likelihood of a GP regression model. We make<br />this explicit by introducing the GP latent function values<br />fd ∈<br />outputs yd(the dthcolumn of Y ). The “complete” likeli-<br />hood associated with the marginal likelihood p(yd|X) is:<br />?Nassociated with the vector of (noise corrupted)<br />p(yd,fd|X) = p(yd|fd)p(fd|X),<br />(10)<br />where p(yd|fd) = N(yd|fd,β−1IN) and p(fd|X) is the<br />zero-mean GP prior with covariance matrix KNN. Note<br />that the above joint model still contains X inside the in-<br />verse of KNN making expectations under distributions<br />over X difficult to compute. We finesse this intractability<br />by introducing auxiliary inducing variables and applying<br />the variational sparse GP formulation of Titsias (2009).<br />We follow the approach of Lawrence (2007): for each vec-<br />tor of latent function values fdwe introduce a separate set<br />of M inducing variables ud∈<br />ducing input locations given by Z ∈<br />ity, we assume that all uds, associated with different out-<br />puts, are evaluated at the same inducing locations, however<br />this could be relaxed. The udvariables are just function<br />points drawn from the GP prior. Using these inducing vari-<br />ables we augment the joint probability model in eq. (10):<br />?Mevaluated at a set of in-<br />?M×Q. For simplic-<br />p(yd,fd,ud|X,Z) = p(yd|fd)p(fd|ud,X,Z)p(ud|Z),<br />(11)<br />where we used the fact that the joint GP prior over function<br />values fdand udevaluated at inputs X and Z factorizes as<br />p(fd,ud|X,Z) = p(fd|ud,X,Z)p(ud|Z) where<br />p(fd|ud,X,Z) = N(fd|αd,KNN− KNMK−1<br />MMKMN)<br />is the conditional GP prior with αd = KNMK−1<br />Further, p(ud|Z) = N(ud|0,KMM) is the marginal GP<br />prior over the inducing variables. The likelihood p(yd|X)<br />can be equivalently computed from the above augmented<br />model by marginalizing out (fd,ud) and crucially this is<br />true for any value of the inducing inputs Z. This means<br />that, unlike X, the inducing inputs Z are not random vari-<br />ables. Neither are they model hyperparameters, they are<br />variational parameters. This interpretation of the induc-<br />ing inputs is key in developing our approximation, it arises<br />from the variational approach of Titsias (2009). Taking<br />advantage of this observation we now simplify our nota-<br />tion by dropping Z from our expressions. We can now<br />apply variational inference to approximate the true poste-<br />rior, p(fd,ud|yd,X) = p(fd|ud,yd,X)p(ud|yd,X), with<br />a sparse variational distribution that takes the form<br />MMud.<br />q(fd,ud) = p(fd|ud,X)φ(ud),<br />(12)<br />where p(fd|ud,X) is the conditional GP prior that appears<br />in the joint model in (11), while φ(ud) is a variational dis-<br />tribution over the inducing variables ud. Thus we obtain a<br />lower bound:<br />logp(yd|X) ≥<br />?<br />φ(ud)logp(ud)N(yd|αd,β−1IN)<br />φ(ud)<br />dud<br />−β<br />2Tr(KNN− KNMK−1<br />MMKMN),<br />(13)<br />where αd= KNMK−1<br />method (Titsias, 2009), the φ(ud) distribution is computed<br />in an optimal way. Such an optimal choice of this distribu-<br />tion depends on the latent variables X and is not useful in<br />our case. In order to obtain the bound for the GP-LVM we<br />need to take a mean field approach and force independence<br />of the distribution φ(ud) from the random variable X.<br />MMud. In the variational sparse GP<br />So far we have computed a lower bound on logp(yd|X)<br />which is the intractable term in?Fd(q). Using eq. (13) and<br />?Fd(q) ≥<br />−β<br />2Tr(K−1<br />the definition of?Fd(q) from (9) we have<br />q(X)<br />?<br />2Tr(KNN) +β<br />??<br />φ(ud)logp(ud)N(yd|αd,β−1IN)<br />φ(ud)<br />dud<br />MMKMNKNM)<br />?<br />dX,<br />where we used standard properties of the trace of a matrix.<br />Since (under our factorization assumption) φ(ud) does not<br />depend on the random variable X, we can swap the inte-<br />grations over X and udand perform firstly the integration<br />with respect to X:<br />?Fd(q) ≥<br />φ(ud)<br />?<br />−β<br />?<br />?logN(yd|αd,β−1IN)?q(X)+ logp(ud)<br />φ(ud)<br />?<br />dud<br />2Tr??KNN?q(X)<br />?+β<br />2Tr?K−1<br />MM?KMNKNM?q(X)<br />?,</p>  <p>Page 4</p> <p>Bayesian Gaussian Process Latent Variable Model<br />where ?·?q(X) denotes expectation under the distribu-<br />tion q(X).Now, we can analytically maximize the<br />above lower bound with respect to the distribution φ(ud).<br />The optimal setting of this distribution is φ(ud)<br />?logN(yd|αd,β−1IN)?q(X)p(ud) and the lower bound<br />that automatically incorporates such an optimal setting is<br />obtained easily by reversing Jensen’s inequality,<br />??<br />−β<br />∝<br />?Fd(q) ≥ log<br />e?log N(yd|αd,β−1IN)?q(X)p(ud)dud<br />?<br />2Tr??KNN?q(X)<br />The r.h.s. in this equation is a lower bound in which the<br />variational distribution φ(ud) has been eliminated opti-<br />mally. This quantity now can be computed in closed-<br />form since it boils down to computing the statistics<br />ψ0 = Tr??KNN?q(X)<br />ance functions, such as the ARD squared exponential from<br />(3), are computable analytically as discussed in section<br />3.2. Notice also that ?logN(yd|αd,β−1IN)?q(X)is just<br />a quadratic function of udthat depends on the statistics Ψ1<br />and Ψ2. Therefore, the integration involved in the above<br />equation is a standard Gaussian integral. The closed-form<br />of the lower bound on?Fd(q) is:<br />?Fd(q) ≥ log<br />−βψ0<br />2<br />where W = βIN− β2Ψ1(βΨ2+ KMM)−1ΨT<br />now compute the closed-from variational lower of the GP-<br />LVM according to equation (8). More precisely, by sum-<br />ming both sides of (14) over the D outputs we obtain on<br />the l.h.s. the term?F(q) (see equation (9)) and on the r.h.s.<br />(in place of?F(q)) in (8) we obtain the final GP-LVM lower<br />closely the corresponding sparse GP-LVM marginal likeli-<br />hood (where X is optimized, not integrated out) obtained<br />by applying the variational method of Titsias (2009). The<br />difference is that now (where X is variationally integrated<br />out) we obtain an extra regularization term, i.e. the term<br />KL(q||p) in (8), and also the kernel quantities Tr(KNN),<br />KNMand KMNKNMthat contain X are replaced by vari-<br />ational averages, which are the Ψ statistics defined above.<br />?+β<br />2Tr?K−1<br />MM?KMNKNM?q(X)<br />?.<br />?, Ψ1 = ?KNM?q(X)and Ψ2 =<br />?KMNKNM?q(X).These statistics for certain covari-<br />?<br />(β)<br />N<br />2|KMM|<br />1<br />2<br />(2π)<br />N<br />2|βΨ2+ KMM|<br />2Tr?K−1<br />1<br />2e−1<br />2yT<br />dWyd<br />?<br />+β<br />MMΨ2<br />?,<br />(14)<br />1. We can<br />a lower bound on?F(q). By substituting the latter quantity<br />bound. This bound has an elegant form since it resembles<br />The bound can be jointly maximized over the variational<br />parameters ({µn,Sn}N<br />eters (β,θ) by applying gradient-based optimization tech-<br />niques. The approach is similarto the MAP optimization of<br />the objective function employed in Lawrence (2005) with<br />the main difference that now we have an additional set of<br />variational parameters governing the approximate posterior<br />variances in the latent space.<br />n=1,Z) and the model hyperparam-<br />3.2 Computation of the Ψ statistics<br />To obtain an explicit evaluation of the variational lower<br />bound we need to compute the statistics (ψ0,Ψ1,Ψ2). We<br />can rewrite the ψ0statistic as ψ0=?N<br />ψn<br />0=<br />k(xn,xn)N(xn|µn,Sn)dxn.<br />n=1ψn<br />0where<br />?<br />(15)<br />Ψ1is an N × M matrix such that<br />?<br />Ψ2 is an M × M matrix which is written as Ψ2 =<br />?N<br />(Ψn<br />2)mm′ =<br />k(xn,zm)k(zm′,xn)N(xn|µn,Sn)dxn.<br />(Ψ1)nm=<br />k(xn,zm)N(xn|µn,Sn)dxn.<br />(16)<br />n=1Ψn<br />2where Ψn<br />?<br />2is such that<br />(17)<br />The above computations involve convolutions of the co-<br />variance function with a Gaussian density. For some stan-<br />dard kernels such the ARD squared exponential (SE) co-<br />variance and the linear covariance function these statistics<br />are obtained analytically. In particular for the ARD SE ker-<br />nel, ψ0= Nσ2<br />f,<br />(Ψ1)nm= σ2<br />f<br />Q<br />?<br />q=1<br />e−1<br />2<br />αq(µnq−zmq)2<br />αqSnq+1<br />(αqSnq+ 1)<br />1<br />2<br />and<br />(Ψn<br />2)mm′ = σ4<br />f<br />Q<br />?<br />q=1<br />e−<br />αq(zmq−zm′q)2<br />4<br />−<br />αq(µnq−¯ zq)2<br />2αqSnq+1<br />(2αqSnq+ 1)<br />1<br />2<br />,<br />where ¯ zq=<br />we need to compute the variational lower bound for the<br />ARD SE kernel. For the linear covariance function the in-<br />tegrals are also tractable. Suppose the kernel function fol-<br />lows the ARD linear form:<br />(zmq+zm′q)<br />2<br />. This gives us all the components<br />k(x,x′) = xTAx′,<br />(18)<br />where A is a positive definite diagonal covariance matrix.<br />Learning the diagonal elements of A will allow to perform<br />automatic model selection of the dimensionality of the lin-<br />ear latent space in a similar manner to ARD SE covari-<br />ance function. Thus, the framework provides an alternative<br />method to perform Bayesian probabilistic PCA (Bishop,<br />1999a;Minka,2001). Forthislinearkernelthestatisticsare<br />such that ψn<br />and (Ψn<br />0= Tr?A(µnµT<br />n+ Sn)?, (Ψ1)nm= µT<br />nAzm<br />2)mm′ = zT<br />mA(µnµT<br />n+ Sn)Azm′.<br />Finally, it is worth noticing that the Ψ statistics are com-<br />puted in a decomposable way which is useful when a new<br />data vector is inserted into the model. In particular, the<br />statistics ψ0and Ψ2are written as sums of independent<br />terms where each term is associated with a data point and<br />similarly each column of the matrix Ψ1is associated with<br />only one data point. These properties can help to speed up<br />computations during test time as discussed in section 4.</p>  <p>Page 5</p> <p>Michalis K. Titsias, Neil D. Lawrence<br />3.3Summary of the variational method<br />To summarize the above variational method allows to com-<br />pute a Jensen’s lower bound on the GP-LVM marginal like-<br />lihood and the key to obtaining this bound was to intro-<br />duce auxiliary variables into the model similar to those<br />used in sparse GP regression.<br />the method using a sequence of steps, we could also start<br />by writing the joint probability density over all variables<br />(Y,X,{fd,ud}D<br />distribution to approximate the model at once. This full<br />variational distribution that gives rise to the lower bound<br />obtained earlier is given by<br />Although, we explained<br />d=1) and then introduce the full variational<br />q({fd,ud}D<br />d=1,X) =<br />?D<br />d=1<br />?<br />p(fd|ud,X)φ(ud)<br />?<br />q(X).<br />This distribution is a mean field approximation with respect<br />to uds and X. However, it is not a mean field with respect<br />to fds since once uds and X are marginalized out, then fds<br />become coupled. In addition, the fact that the q(X) dis-<br />tribution is factorized over the latent variables is a conse-<br />quence of the mean field assumption between uds and X.<br />It does not need to be imposed in advance. To see this,<br />notice that q(X) appears only in the Ψ statistics which as<br />explained earlier are computed in a decomposable (across<br />data points/latent variables) way.<br />4Prediction and computation of<br />probabilities in test data<br />In this section, we discuss how we can use the proposed<br />model, from now on referred to as Bayesian GP-LVM,<br />in order to make predictions in unseen data. Firstly, we<br />explain how we can approximately compute the proba-<br />bility density p(y∗|Y ) of some observed test data vector<br />y∗ ∈<br />computation of this probability can allow us to use the<br />model as a density estimator which, for instance, can repre-<br />sent the class conditional distribution in a generative based<br />classification system. We will exploit such a use in section<br />5. Secondly we discuss how we can predict the function<br />values y∗given that we have an estimate of the variational<br />distribution q(x∗) forthe latent variable associated with the<br />observation y∗. This can be useful when we wish to predict<br />the missing values of some partially observed test output<br />y∗ = (yO<br />nents in the vector y∗and yU<br />we would like to predict. This second prediction task can<br />also be used to remove the noise of a fully observed output.<br />?D, which is allowed to have missing values. The<br />∗,yU<br />∗) ∈<br />?Dwhere yO<br />∗are observed compo-<br />∗are the missing values that<br />First we discuss how to approximate the density p(y∗|Y ).<br />By introducing the latent variables X (corresponding to the<br />training outputs Y ) and new test latent variables x∗, the<br />previous density is written as<br />p(y∗|Y ) =<br />?p(y∗,Y |X,x∗)p(X,x∗)dXdx∗<br />Note that this is a ratio of two marginal likelihoods. In the<br />denominator we have the marginal likelihood of the GP-<br />LVM for which we have already computed a variational<br />lower bound. The numerator is another marginal likelihood<br />that is obtained by augmenting the training data Y with<br />the test point y∗and integrating out both X and the newly<br />inserted latent variable x∗. To approximate the density<br />p(y∗|Y ), we construct a ratio of lower bounds as follows.<br />?p(Y |X)p(X)dX is approximated by the lower bound<br />the log marginal likelihood as computed in section 3. The<br />maximization of this lower bound specifies the variational<br />distribution q(X) over the latent variables in the training<br />data. Then, this distribution remains fixed during test time.<br />?p(y∗,Y |X,x∗)p(X,x∗)dXdx∗is approximated by the<br />timize with respect to the parameters (µ∗,S∗) of the Gaus-<br />sian variational distribution q(x∗). Such optimization is<br />subject to local minima. However, sensible initializations<br />of µ∗can be employed based on the mean of the variational<br />distributions associated with the nearest neighbours of y∗<br />in the training data Y . Furthermore, such optimization is<br />fast because we can perform several precomputations in<br />advance. In particular, notice that because the computa-<br />tion of the Ψ statistics decomposes across data, updating<br />these statistics to account for the insertion of the test point,<br />involves only averages overthe single-point variational dis-<br />tribution q(x∗). Finally, the approximation of p(y∗|Y ) is<br />given by<br />?p(Y |X)p(X)dX<br />.<br />(19)<br />eF(q(X))where F(q(X)) is the variational lower bound on<br />lower bound eF(q(X,x∗)). To compute this, we need to op-<br />q(y∗|Y ) = eF(q(X,x∗))−F(q(X)).<br />(20)<br />We now discuss the second prediction problem where a<br />partially observed test point y∗ = (yO<br />we wish to reconstruct the missing part yU<br />volves two steps. Firstly, we optimize the parameters of<br />the variational distribution q(x∗) by maximizing the varia-<br />tional lower bound on?p(yO<br />q(x∗); exactly as explained earlier. To predict now yU<br />take the standard GP prediction approach by taking also<br />into account the fact that the input x∗is uncertain since it<br />has the distribution q(x∗). Therefore, the problem takes<br />the form of GP prediction with uncertain inputs similar to<br />Girard et al. (2002). More precisely, to predict yU<br />predict its latent function values fU<br />???<br />=<br />q(fU<br />∗|x∗)q(x∗)dx∗,<br />∗,yU<br />∗) is given and<br />∗. This in-<br />∗,Y |X,x∗)p(X,x∗)dXdx∗<br />by keeping all the optimized quantities fixed apart from<br />∗, we<br />∗we first<br />∗according to<br />q(fU<br />∗) =<br />d∈U<br />?<br />p(fU<br />∗d|ud,x∗)φ(ud)dud<br />?<br />q(x∗)dx∗<br />?<br />(21)</p>  <p>Page 6</p> <p>Bayesian Gaussian Process Latent Variable Model<br />where q(fU<br />each factor takes the form of the projected process pre-<br />dictive distribution (Csat´ o and Opper, 2002; Seeger et al.,<br />2003; Rasmussen and Williams, 2006). The marginaliza-<br />tionofx∗couplesalldimensionsoffU<br />Gaussian fully dependent multivariate density. For squared<br />exponential kernels all moments of the density q(fU<br />analytically tractable. In practice, we will typically need<br />only the mean and covariance of fU<br />∗|x∗) is a factorized Gaussian distribution where<br />∗andproducesanon-<br />∗) are<br />∗. The mean is<br />?(fU<br />∗) = ΛTψ∗<br />1.<br />Here, Λ = β(KMM+ βΨ2)−1ΨT<br />matrix containing the columns of Y corresponding to the<br />missing values of y∗. Also, the vector ψ∗<br />by ψ∗<br />1YUwhere YUis the<br />1∈<br />?Mis defined<br />1= ?KM∗?q(x∗)where KM∗= k(Z,x∗). Similarly,<br />Cov(fU<br />+ ψ∗<br />∗) = ΛT?Ψ∗<br />2− ψ∗<br />MM− (KMM+ βΨ2)−1?Ψ∗<br />0= ?k(x∗,x∗)?q(x∗)and Ψ∗<br />Notice that the Ψ statistics (the terms (ψ∗<br />ing the test latent variable x∗appear naturally in these ex-<br />pressions. Using the above expressions, the predicted mean<br />of yU<br />equal to Cov(fU<br />1(ψ∗<br />1)T?Λ<br />0I − Tr??K−1<br />2<br />?I,<br />where ψ∗<br />2= ?KM∗K∗M?q(x∗).<br />0,ψ∗<br />1,Ψ∗<br />2)) involv-<br />∗is equal to<br />?(fU<br />∗) and the predicted covariance is<br />∗) + β−1I.<br />5 Experiments<br />To demonstrate the Bayesian GP-LVM we now consider<br />some standard machine learning data sets. Our aim is to<br />highlight several characteristics of the algorithm: the im-<br />proved quality of visualizations achieved by the model,<br />the utility of being able to access a lower bound on the<br />marginal likelihood of the data, and the ability of the model<br />to automatically determine the dimensionality of the data.<br />5.1Oil flow data<br />In the first experiment we illustrate the method in the multi-<br />phase oil flow data (Bishop and James, 1993) that consists<br />of 1000, 12 dimensional observations belonging to three<br />known classes corresponding to different phases of oil flow.<br />Figure 1 shows the results for these data obtained by ap-<br />plying the Bayesian GP-LVM with 10 latent dimensions<br />using the ARD SE kernel. The means of the variational<br />distribution were initialized based on PCA, while the vari-<br />ances in the variational distribution are initialized to neu-<br />tral values around 0.5. As shown in Figure 1(a), the al-<br />gorithm switches off automatically 7 out of 10 latent di-<br />mensions by making their inverse lengthscales zero. Fig-<br />ure 1(b) shows the visualization obtained by keeping only<br />the dominant latent directions (having the largest inverse<br />lengthscale) which are the dimensions 2 and 3. This is a<br />remarkably high quality two dimensional visualization of<br />this data. For comparison, Figure 1(c) shows the visualiza-<br />tion provided by the standard sparse GP-LVM that runs by<br />assuming only 2 latent dimensions. Both models use 50 in-<br />ducing variables, while the latent variables X optimized in<br />the standard GP-LVM are initialized based on PCA. Note<br />that if we were to run the standard GP-LVM with 10 latent<br />dimensions, the model would overfit the data, it would not<br />reduce the dimensionality in the manner achieved by the<br />Bayesian GP-LVM. In these two dimensions, the nearest<br />neighbour error for the different classes (phases of oil flow)<br />in the case of Bayesian GP-LVM is 3 errors from 1000 data<br />points. The number of the nearest neighbour errors made<br />when applying the standard GP-LVM was 26.<br />5.2Frey Faces Data<br />Here, we consider a dataset of faces (Roweis et al., 2002)<br />taken from a video sequence that consists of 1965 images<br />of size 28×20. In this dataset, we would like to exploit the<br />ability of the model to reconstruct partially observed test<br />data. Therefore, we train the model using a random selec-<br />tion of 1000 images and then we consider the remaining<br />965 images as test data. Furthermore, in each test image<br />we assume that only half of the image pixels are observed.<br />The missing pixels were chosen randomly for each test im-<br />age. After training on 1000 images, each partially observed<br />test image was processed separately (this involves the op-<br />timization of the corresponding variational distribution as<br />discussed in section 4) and the missing pixels were pre-<br />dicted. Figure 2 shows a few examples of reconstructed<br />test images. Each column in this figure corresponds to a<br />test image, where the top plot shows the true test image,<br />the middle one the partially observed image and the bottom<br />image shows the reconstructed image. We also measure<br />the mean absolute reconstruction error over all test images<br />and missing pixels and compare this errorwith the standard<br />sparse GP-LVM. This standard GP-LVM was applied using<br />several settings of the latent dimensionality: Q = 2,5,10<br />and 30. The Bayesian GP-LVM was trained once using<br />30 latent dimensions. The latent variables X in the stan-<br />dard GP-LVM and the means of the variational distribu-<br />tion in Bayesian GP-LVM were initialized through PCA.<br />The error for Bayesian GP-LVM was 7.4003. For the stan-<br />dard GP-LVM the error was 10.5748, 9.7284, 19.6949 and<br />19.6961 for 2,5,10 and 30 latent dimensions respectively.<br />Notice that the standard GP-LVM has poor performance<br />for large value of latent dimension and achieves the best<br />error when we consider 5 latent dimensions. Nevertheless,<br />this was still worse than the error from the Bayesian GP-<br />LVM. Finally, Figure 3 shows the values of the inverse<br />lengthscales obtained by the maximization of the varia-<br />tional lower bound. Although, in this case, the algorithm<br />does not shrink some of the dimensions completely to zero,<br />it does force many of them to obtain small values. Note that<br />one of the dimensions (the first from the left) seems to be<br />the most important in explaining the data.</p>  <p>Page 7</p> <p>Michalis K. Titsias, Neil D. Lawrence<br />12345678910<br />0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />−3−2−10123<br />−2<br />−1<br />0<br />1<br />2<br />3<br />−2−1012<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />(a)(b)(c)<br />Figure 1: Panel (a) shows the inverse lengthscales found by applying the Bayesian GP-LVM with ARD SE kernel on the<br />oil flow data. Panel (b) shows the visualization achieved by keeping the most dominant latent dimensions (2 and 3) which<br />have the largest inverse lengthscale value. Dimension 2 is plotted on the y-axis and 3 and on the x-axis. Plot (c) shows the<br />visualization found by standard sparse GP-LVM.<br />Figure 2: Examples of reconstruction of partially observed test images in Frey faces by applying the Bayesian GP-LVM.<br />Each column corresponds to a test image. In every column, the top panel shows the true test image, the middle panel the<br />partially observed image (where missing pixels are shown in black) and the bottom image is the reconstructed image.<br />Figure 3: This plot shows the values of the inverse length-<br />scales found by using the Bayesian GP-LVM with ARD SE<br />kernel in Frey faces.<br />5.3Digits Data<br />In the final experiment we use the Bayesian GP-LVM to<br />build a generative classifier for handwritten digit recogni-<br />tion. WeconsiderthewellknownUSPSdigitsdataset. This<br />dataset consists of 16 × 16 images for all 10 digits and it<br />is divided into 7291 training examples and 2007 test exam-<br />ples. We run 10 Bayesian GP-LVMs, one for each digit,<br />on the USPS data base. We used 10 latent dimensions and<br />50 inducing variables for each model. This allowed us to<br />build a probabilistic generative model for each digit so that<br />we can compute Bayesian class conditional densities in the<br />test data having the form p(y∗|Y,digit). These class condi-<br />tional densities are approximated through the ratio of lower<br />bounds in eq. (20) as described in section 4. The whole ap-<br />proach allows us to classify new digits by determining the<br />class labels for test data based on the highest class con-<br />ditional density value and using a uniform prior over class<br />labels. The test error made by the Bayesian GP-LVM in the<br />whole set of 2007 test points was 95 incorrectly classified<br />digits i.e. 4.73% error.<br />6Discussion<br />We have introduced an approximation to the marginal like-<br />lihood of the fully marginalized Gaussian process latent<br />variable model. Our approximation is in the form of a vari-<br />ational lower bound. With the fully marginalized model we<br />can automatically determine the latent dimensionality of a</p>  <p>Page 8</p> <p>Bayesian Gaussian Process Latent Variable Model<br />given data set. We demonstrated the utility of this rigorous<br />lower bound on a range of disparate real world data sets.<br />Our approach can immediately be applied to training Gaus-<br />sian processes with uncertain inputs where these inputs<br />have Gaussian prior densities. We also envisage several<br />other extensions that become computationally feasible us-<br />ing the same set of methodologies we espouse. Dynam-<br />ical models based on the GP-LVM have been proposed.<br />It would be straightforward to include a latent space prior<br />with a temporal component. This could be a Kalman filter,<br />a general Gaussian process (Lawrence and Moore, 2007)<br />or an auto regressive Gaussian process (Wang et al., 2006).<br />By using our approach to propagating the Gaussian noise<br />through the dynamics and the latent space a variational<br />lower bound on the likelihood of these models could be<br />derived. The importance of such nonlinear models is clear<br />from the success of unscented Kalman filters and the re-<br />lated ensemble Kalman filter.<br />The optimization procedure has a similar computational<br />cost to that of previously proposed sparse GP-LVMs. We<br />believe there is scope to improve the speed of the optimiza-<br />tion procedure by better exploiting the correlation present<br />in the parameters. A potential strategy would be to use the<br />control points idea used to speed up MCMC in GPs (Tit-<br />sias et al., 2009) in order to encode the variational poste-<br />rior, effectively decoupling these correlations and speeding<br />convergence of the optimizer.<br />Acknowledgements<br />We wish to thank Mauricio´Alvarez for his help with the<br />software implementation. This work is funded by EPSRC<br />Grant No EP/F005687/1 ”Gaussian Processes for Systems<br />Identification with Applications in Systems Biology”.<br />References<br />C. M. Bishop. Bayesian PCA. In M. J. Kearns, S. A. Solla,<br />and D. A. Cohn, editors, Advances in Neural Information Pro-<br />cessing Systems, volume 11, pages 482–388, Cambridge, MA,<br />1999a. MIT Press.<br />C. M. Bishop. Variational principal components. In Proceedings<br />Ninth International Conference on Artificial Neural Networks,<br />ICANN’99, volume 1, pages 509–514, 1999b.<br />C. M. Bishop and G. D. James. Analysis of multiphase flows<br />using dual-energy gamma densitometry and neural networks.<br />Nuclear Instruments and Methods in Physics Research, A327:<br />580–593, 1993.<br />L.Csat´ o. GaussianProcesses—IterativeSparseApproximations.<br />PhD thesis, Aston University, 2002.<br />L. Csat´ o and M. Opper. Sparse on-line Gaussian processes. Neu-<br />ral Computation, 14(3):641–668, 2002.<br />T. G. Dietterich, S. Becker, and Z. Ghahramani, editors. Advances<br />in Neural Information Processing Systems, volume 14, Cam-<br />bridge, MA, 2002. MIT Press.<br />A. Geiger, R. Urtasun, and T. Darrell. Rank priors for continu-<br />ous non-linear dimensionality reduction. In Proceedings of the<br />IEEE Computer Society Conference on Computer Vision and<br />Pattern Recognition, Miami, FL, 2009.<br />Z. Ghahramani and M. J. Beal. Variational inference for Bayesian<br />mixtures of factor analysers. In S. A. Solla, T. K. Leen, and K.-<br />R. M¨ uller, editors, Advances in Neural Information Processing<br />Systems, volume 12, pages 831–864, Cambridge, MA, 2000.<br />MIT Press.<br />A. Girard, C. E. Rasmussen, J. Qui˜ nonero Candela, and<br />R. Murray-Smith. Gaussian process priors with uncertain in-<br />puts - application to multiple-step ahead time series forecast-<br />ing. In Dietterich et al. (2002), pages 529–536.<br />N. D. Lawrence. Gaussian process models for visualisation of<br />high dimensional data. In S. Thrun, L. Saul, and B. Sch¨ olkopf,<br />editors, Advances in Neural Information Processing Systems,<br />volume 16, pages 329–336, Cambridge, MA, 2004. MIT Press.<br />N. D. Lawrence. Learning for larger datasets with the Gaussian<br />process latent variable model. In M. Meila and X. Shen, ed-<br />itors, Proceedings of the Eleventh International Workshop on<br />Artificial Intelligence and Statistics, pages 243–250, San Juan,<br />Puerto Rico, 21-24 March 2007. Omnipress.<br />N. D. Lawrence. Probabilistic non-linear principal component<br />analysis with Gaussian process latent variable models. Journal<br />of Machine Learning Research, 6:1783–1816, 11 2005.<br />N. D. Lawrence and A. J. Moore. Hierarchical Gaussian pro-<br />cess latent variable models. In Z. Ghahramani, editor, Pro-<br />ceedings of the International Conference in Machine Learning,<br />volume 24, pages 481–488. Omnipress, 2007.<br />T. P. Minka. Automatic choice of dimensionality for PCA. In<br />T. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances<br />in Neural Information Processing Systems, volume 13, pages<br />598–604, Cambridge, MA, 2001. MIT Press.<br />J. Qui˜ nonero Candela and C. E. Rasmussen. A unifying view of<br />sparse approximate Gaussian process regression. Journal of<br />Machine Learning Research, 6:1939–1959, 2005.<br />C. E. Rasmussen and C. K. I. Williams. Gaussian Processes for<br />Machine Learning. MIT Press, Cambridge, MA, 2006.<br />S. T. Roweis, L. K. Saul, and G. E. Hinton. Global coordination of<br />local linear models. In Dietterich et al. (2002), pages 889–896.<br />M. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward<br />selection to speed up sparse Gaussian process regression. In<br />C. M. Bishop and B. J. Frey, editors, Proceedings of the Ninth<br />International Workshop on Artificial Intelligence and Statis-<br />tics, Key West, FL, 3–6 Jan 2003.<br />E. Snelson and Z. Ghahramani. Sparse Gaussian processes using<br />pseudo-inputs. In Weiss et al. (2006).<br />M. Titsias, N. D. Lawrence, and M. Rattray.<br />pling for Gaussian process inference using control variables.<br />In D. Koller, Y. Bengio, D. Schuurmans, and L. Bottou, edi-<br />tors, Advances in Neural Information Processing Systems, vol-<br />ume 21, Cambridge, MA, 2009. MIT Press.<br />M. K. Titsias. Variational learning of inducing variables in sparse<br />Gaussian processes. In D. van Dyk and M. Welling, editors,<br />Proceedings of the Twelfth International Conference on Ar-<br />tificial Intelligence and Statistics, volume 5, pages 567–574,<br />Clearwater Beach, FL, 16-18 April 2009. JMLR W&amp;CP 5.<br />J. M. Wang, D. J. Fleet, and A. Hertzmann. Gaussian process<br />dynamical models. In Weiss et al. (2006).<br />Y. Weiss, B. Sch¨ olkopf, and J. C. Platt, editors. Advances in Neu-<br />ral Information Processing Systems, volume 18, Cambridge,<br />MA, 2006. MIT Press.<br />Efficient sam-</p>   </div> <div id="rgw17_56ab1fcfbe0fb" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw18_56ab1fcfbe0fb">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw19_56ab1fcfbe0fb"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://eprints.pascal-network.org/archive/00006343/01/vargplvm.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Bayesian Gaussian Process Latent Variable Model.">Bayesian Gaussian Process Latent Variable Model.</a> </div>  <div class="details">   Available from <a href="http://eprints.pascal-network.org/archive/00006343/01/vargplvm.pdf" target="_blank" rel="nofollow">pascal-network.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw26_56ab1fcfbe0fb" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (44) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw27_56ab1fcfbe0fb" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw28_56ab1fcfbe0fb" >  <div class="indent-left">  <div id="rgw29_56ab1fcfbe0fb" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Robert_Kopp" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Robert E. Kopp </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw30_56ab1fcfbe0fb">  <li class="citation-context-item"> "To address this challenge traditional sampling-based Monte Carlo Markov chain (MCMC) techniques are often employed, however they are time-consuming and will not be appropriate for large datasets. Explicit integrating out of the input uncertainty, when the input density is known, is intractable in common situations [2] , [3]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints"> <span class="publication-title js-publication-title">Gaussian Process for Noisy Inputs with Ordering Constraints</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2077228906_Cuong_Tran" class="authors js-author-name ga-publications-authors">Cuong Tran</a> &middot;     <a href="researcher/50848147_Vladimir_Pavlovic" class="authors js-author-name ga-publications-authors">Vladimir Pavlovic</a> &middot;     <a href="researcher/49598718_Robert_Kopp" class="authors js-author-name ga-publications-authors">Robert Kopp</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We study the Gaussian Process regression model in the context of training
data with noise in both input and output. The presence of two sources of noise
makes the task of learning accurate predictive models extremely challenging.
However, in some instances additional constraints may be available that can
reduce the uncertainty in the resulting predictive models. In particular, we
consider the case of monotonically ordered latent input, which occurs in many
application domains that deal with temporal data. We present a novel inference
and learning approach based on non-parametric Gaussian variational
approximation to learn the GP model while taking into account the new
constraints. The resulting strategy allows one to gain access to posterior
estimates of both the input and the output and results in improved predictive
performance. We compare our proposed models to state-of-the-art Noisy Input
Gaussian Process (NIGP) and other competing approaches on synthetic and real
sea-level rise data. Experimental results suggest that the proposed approach
consistently outperforms selected methods while, at the same time, reducing the
computational costs of learning and inference. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Robert_Kopp/publication/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints/links/55c8c5de08aeca747d670500.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw31_56ab1fcfbe0fb" >  <div class="indent-left">  <div id="rgw32_56ab1fcfbe0fb" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Zoltan_Szabo17" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Zoltan Szabo </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw33_56ab1fcfbe0fb">  <li class="citation-context-item"> "An established alternative is to construct an explicit probabilistic model of the functional relationship between low-dimensional manifold coordinates and each measured dimension of the data, assuming that the functions instantiate draws from Gaussian-process priors . The original Gaussian process latent variable model (GP-LVM) required optimisation of the lowdimensional coordinates, and thus still did not provide uncertainties on these locations or allow evaluation of the likelihood of a model over them [7]; however a recent extension exploits an auxilliary variable approach to optimise a more general variational bound, thus retaining approximate probabilistic semantics within the latent space [8]. The stochastic process model for the mapping functions also makes it straightforward to estimate the function at previously unobserved points, thus generalising out-of-sample with ease. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model"> <span class="publication-title js-publication-title">Bayesian Manifold Learning: The Locally Linear Latent Variable Model</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2056849184_Mijung_Park" class="authors js-author-name ga-publications-authors">Mijung Park</a> &middot;     <a href="researcher/2078956248_Wittawat_Jitkrittum" class="authors js-author-name ga-publications-authors">Wittawat Jitkrittum</a> &middot;     <a href="researcher/2056994783_Ahmad_Qamar" class="authors js-author-name ga-publications-authors">Ahmad Qamar</a> &middot;     <a href="researcher/69862689_Zoltan_Szabo" class="authors js-author-name ga-publications-authors">Zoltan Szabo</a> &middot;     <a href="researcher/36471253_Lars_Buesing" class="authors js-author-name ga-publications-authors">Lars Buesing</a> &middot;     <a href="researcher/38613445_Maneesh_Sahani" class="authors js-author-name ga-publications-authors">Maneesh Sahani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Zoltan_Szabo17/publication/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model/links/56394b5a08ae2da875c7a43f.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw34_56ab1fcfbe0fb" >  <div class="indent-left">  <div id="rgw35_56ab1fcfbe0fb" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/James_Hensman" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: James Hensman </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw36_56ab1fcfbe0fb">  <li class="citation-context-item"> "The approach has also been successfully used to perform scalable inference in more complex models such as the Gaussian process latent variable model (Titsias and Lawrence, 2010; Damianou et al., 2014) and the related Deep Gaussian process (Damianou and Lawrence, 2012; Hensman and Lawrence, 2014). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes"> <span class="publication-title js-publication-title">On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2048369253_Alexander_G_de_G_Matthews" class="authors js-author-name ga-publications-authors">Alexander G. de G. Matthews</a> &middot;     <a href="researcher/71410443_James_Hensman" class="authors js-author-name ga-publications-authors">James Hensman</a> &middot;     <a href="researcher/49549591_Richard_E_Turner" class="authors js-author-name ga-publications-authors">Richard E. Turner</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> The variational framework for learning inducing variables Titsias (2009) has
had a large impact on the Gaussian process literature. The framework may be
interpreted as minimizing a rigorously defined Kullback-Leibler divergence
between the approximate and posterior processes. To our knowledge this
connection has thus far gone unremarked in the literature. Many of the
technical requirements for such a result were derived in the pioneering work of
Seeger (2003,2003b). In this work we give a relatively gentle and largely
self-contained explanation of the result. The result is important in
understanding the variational inducing framework and could lead to principled
novel generalizations. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Apr 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/James_Hensman/publication/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes/links/55adfd8f08aed9b7dcdb08a0.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw21_56ab1fcfbe0fb" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56ab1fcfbe0fb">  </ul> </div> </div>   <div id="rgw13_56ab1fcfbe0fb" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw14_56ab1fcfbe0fb"> <div> <h5> <a href="publication/225042553_Supervised_Latent_Linear_Gaussian_Process_Latent_Variable_Model_for_Dimensionality_Reduction" class="color-inherit ga-similar-publication-title"><span class="publication-title">Supervised Latent Linear Gaussian Process Latent Variable Model for Dimensionality Reduction</span></a>  </h5>  <div class="authors"> <a href="researcher/71011413_Xinwei_Jiang" class="authors ga-similar-publication-author">Xinwei Jiang</a>, <a href="researcher/13018172_Junbin_Gao" class="authors ga-similar-publication-author">Junbin Gao</a>, <a href="researcher/69685994_Tianjiang_Wang" class="authors ga-similar-publication-author">Tianjiang Wang</a>, <a href="researcher/31016721_Lihong_Zheng" class="authors ga-similar-publication-author">Lihong Zheng</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw15_56ab1fcfbe0fb"> <div> <h5> <a href="publication/288756200_Supervised_latent_linear_Gaussian_process_latent_variable_model_based_classification" class="color-inherit ga-similar-publication-title"><span class="publication-title">Supervised latent linear Gaussian process latent variable model based classification</span></a>  </h5>  <div class="authors"> <a href="researcher/2091469721_Z_Hou" class="authors ga-similar-publication-author">Z. Hou</a>, <a href="researcher/2091508753_Q_Feng" class="authors ga-similar-publication-author">Q. Feng</a>, <a href="researcher/2091586138_X_Zuo" class="authors ga-similar-publication-author">X. Zuo</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1fcfbe0fb"> <div> <h5> <a href="publication/276528957_Gaussian_process_latent_variable_model_based_on_immune_clonal_selection_for_SAR_target_feature_extraction_and_recognition" class="color-inherit ga-similar-publication-title"><span class="publication-title">Gaussian process latent variable model based on immune clonal selection for SAR target feature extraction and recognition</span></a>  </h5>  <div class="authors"> <a href="researcher/2016774019_Xiang-Rong_ZHANG" class="authors ga-similar-publication-author">Xiang-Rong ZHANG</a>, <a href="researcher/2073907075_Li-Min_GOU" class="authors ga-similar-publication-author">Li-Min GOU</a>, <a href="researcher/2067985084_Yang-Yang_LI" class="authors ga-similar-publication-author">Yang-Yang LI</a>, <a href="researcher/69943699_Jie_FENG" class="authors ga-similar-publication-author">Jie FENG</a>, <a href="researcher/72874388_Li-Cheng_JIAO" class="authors ga-similar-publication-author">Li-Cheng JIAO</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw38_56ab1fcfbe0fb" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw39_56ab1fcfbe0fb">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw40_56ab1fcfbe0fb" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=D_4M2ekCpc4ADNJTvD1oL9BjKSJZrsQveE4oGefsT856lJo2Ndyj8IKPTz4ud6xT" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="m45TUI1oahrVyeNoVkC4NRUJNBoCmfurTpXqjKHZ20qmwkVf4pJCBFDejZCrfwGUO+3ckmStUftd69v5EvHh5t39PBz72jmn+kT0P5Yrzwhc8CkUYZWXv4qje7WvsGLdjRV26Zx4HktKik/mR42JcQqp5ewX7ofspWoEglTMfMI2yObiSF+3PT9jEc+7EkQ6YIAVnca0rJ35ii4K4i8041+xnXOYBbKLo8+OijErAdmb9GEHSpNACwclvy4QeqrQqOCX381g/epRbSMdS4iKq2qdP/njogxJPEsnRcsoN68="/> <input type="hidden" name="urlAfterLogin" value="publication/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNjM1X0JheWVzaWFuX0dhdXNzaWFuX1Byb2Nlc3NfTGF0ZW50X1ZhcmlhYmxlX01vZGVs"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNjM1X0JheWVzaWFuX0dhdXNzaWFuX1Byb2Nlc3NfTGF0ZW50X1ZhcmlhYmxlX01vZGVs"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNjM1X0JheWVzaWFuX0dhdXNzaWFuX1Byb2Nlc3NfTGF0ZW50X1ZhcmlhYmxlX01vZGVs"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw41_56ab1fcfbe0fb"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 869;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Michalis K. Titsias","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Michalis_Titsias","institution":"The University of Manchester","institutionUrl":false,"widgetId":"rgw4_56ab1fcfbe0fb"},"id":"rgw4_56ab1fcfbe0fb","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1977711","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1fcfbe0fb"},"id":"rgw3_56ab1fcfbe0fb","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220320635","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220320635,"title":"Bayesian Gaussian Process Latent Variable Model.","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"Journal of Machine Learning Research - Proceedings Track","publicationDate":"01\/2010;","publicationDateRobot":"2010-01","article":"9:844-851."}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmlr\/jmlrp9.html#TitsiasL10","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Bayesian Gaussian Process Latent Variable Model."},{"key":"rft.title","value":"Journal of Machine Learning Research - Proceedings Track"},{"key":"rft.jtitle","value":"Journal of Machine Learning Research - Proceedings Track"},{"key":"rft.volume","value":"9"},{"key":"rft.date","value":"2010"},{"key":"rft.pages","value":"844-851"},{"key":"rft.au","value":"Michalis K. Titsias,Neil D. Lawrence"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1fcfbe0fb"},"id":"rgw6_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220320635","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220320635,"peopleItems":[{"data":{"authorNameOnPublication":"Michalis K. Titsias","accountUrl":"profile\/Michalis_Titsias","accountKey":"Michalis_Titsias","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michalis K. Titsias","profile":{"professionalInstitution":{"professionalInstitutionName":"The University of Manchester","professionalInstitutionUrl":"institution\/The_University_of_Manchester"}},"professionalInstitutionName":"The University of Manchester","professionalInstitutionUrl":"institution\/The_University_of_Manchester","url":"profile\/Michalis_Titsias","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Michalis_Titsias","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab1fcfbe0fb"},"id":"rgw9_56ab1fcfbe0fb","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1977711&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"The University of Manchester","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":220320635,"widgetId":"rgw8_56ab1fcfbe0fb"},"id":"rgw8_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1977711&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=220320635","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/39663468_Neil_D_Lawrence","authorNameOnPublication":"Neil D. Lawrence","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Neil D. Lawrence","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/39663468_Neil_D_Lawrence","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab1fcfbe0fb"},"id":"rgw11_56ab1fcfbe0fb","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=39663468&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1fcfbe0fb"},"id":"rgw10_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=39663468&authorNameOnPublication=Neil%20D.%20Lawrence","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1fcfbe0fb"},"id":"rgw7_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220320635&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220320635,"abstract":"<noscript><\/noscript><div>We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input vari- ables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maxi- mization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the di- mensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality re- duction problems, but the methodology is more general. For example, our algorithm is imme- diately applicable for training Gaussian process models in the presence of missing or uncertain inputs.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw12_56ab1fcfbe0fb"},"id":"rgw12_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220320635","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model\/links\/0ffc98940cf255165fc9bf47\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab1fcfbe0fb"},"id":"rgw5_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220320635&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":71011413,"url":"researcher\/71011413_Xinwei_Jiang","fullname":"Xinwei Jiang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13018172,"url":"researcher\/13018172_Junbin_Gao","fullname":"Junbin Gao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69685994,"url":"researcher\/69685994_Tianjiang_Wang","fullname":"Tianjiang Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31016721,"url":"researcher\/31016721_Lihong_Zheng","fullname":"Lihong Zheng","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"May 2012","journal":"IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics: a publication of the IEEE Systems, Man, and Cybernetics Society","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/225042553_Supervised_Latent_Linear_Gaussian_Process_Latent_Variable_Model_for_Dimensionality_Reduction","usePlainButton":true,"publicationUid":225042553,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"6.22","url":"publication\/225042553_Supervised_Latent_Linear_Gaussian_Process_Latent_Variable_Model_for_Dimensionality_Reduction","title":"Supervised Latent Linear Gaussian Process Latent Variable Model for Dimensionality Reduction","displayTitleAsLink":true,"authors":[{"id":71011413,"url":"researcher\/71011413_Xinwei_Jiang","fullname":"Xinwei Jiang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13018172,"url":"researcher\/13018172_Junbin_Gao","fullname":"Junbin Gao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69685994,"url":"researcher\/69685994_Tianjiang_Wang","fullname":"Tianjiang Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":31016721,"url":"researcher\/31016721_Lihong_Zheng","fullname":"Lihong Zheng","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics: a publication of the IEEE Systems, Man, and Cybernetics Society 05\/2012; 42(6). DOI:10.1109\/TSMCB.2012.2196995"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/225042553_Supervised_Latent_Linear_Gaussian_Process_Latent_Variable_Model_for_Dimensionality_Reduction","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/225042553_Supervised_Latent_Linear_Gaussian_Process_Latent_Variable_Model_for_Dimensionality_Reduction\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw14_56ab1fcfbe0fb"},"id":"rgw14_56ab1fcfbe0fb","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=225042553","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2091469721,"url":"researcher\/2091469721_Z_Hou","fullname":"Z. Hou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091508753,"url":"researcher\/2091508753_Q_Feng","fullname":"Q. Feng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091586138,"url":"researcher\/2091586138_X_Zuo","fullname":"X. Zuo","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jul 2013","journal":"Journal of Computational Information Systems","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/288756200_Supervised_latent_linear_Gaussian_process_latent_variable_model_based_classification","usePlainButton":true,"publicationUid":288756200,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/288756200_Supervised_latent_linear_Gaussian_process_latent_variable_model_based_classification","title":"Supervised latent linear Gaussian process latent variable model based classification","displayTitleAsLink":true,"authors":[{"id":2091469721,"url":"researcher\/2091469721_Z_Hou","fullname":"Z. Hou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091508753,"url":"researcher\/2091508753_Q_Feng","fullname":"Q. Feng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2091586138,"url":"researcher\/2091586138_X_Zuo","fullname":"X. Zuo","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Computational Information Systems 07\/2013; 9(13):5085-5092. DOI:10.12733\/jcis6058"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/288756200_Supervised_latent_linear_Gaussian_process_latent_variable_model_based_classification","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/288756200_Supervised_latent_linear_Gaussian_process_latent_variable_model_based_classification\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab1fcfbe0fb"},"id":"rgw15_56ab1fcfbe0fb","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=288756200","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2016774019,"url":"researcher\/2016774019_Xiang-Rong_ZHANG","fullname":"Xiang-Rong ZHANG","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073907075,"url":"researcher\/2073907075_Li-Min_GOU","fullname":"Li-Min GOU","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2067985084,"url":"researcher\/2067985084_Yang-Yang_LI","fullname":"Yang-Yang LI","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":69943699,"url":"researcher\/69943699_Jie_FENG","fullname":"Jie FENG","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2013","journal":"JOURNAL OF INFRARED AND MILLIMETER WAVES","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/276528957_Gaussian_process_latent_variable_model_based_on_immune_clonal_selection_for_SAR_target_feature_extraction_and_recognition","usePlainButton":true,"publicationUid":276528957,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.30","url":"publication\/276528957_Gaussian_process_latent_variable_model_based_on_immune_clonal_selection_for_SAR_target_feature_extraction_and_recognition","title":"Gaussian process latent variable model based on immune clonal selection for SAR target feature extraction and recognition","displayTitleAsLink":true,"authors":[{"id":2016774019,"url":"researcher\/2016774019_Xiang-Rong_ZHANG","fullname":"Xiang-Rong ZHANG","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073907075,"url":"researcher\/2073907075_Li-Min_GOU","fullname":"Li-Min GOU","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2067985084,"url":"researcher\/2067985084_Yang-Yang_LI","fullname":"Yang-Yang LI","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69943699,"url":"researcher\/69943699_Jie_FENG","fullname":"Jie FENG","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":72874388,"url":"researcher\/72874388_Li-Cheng_JIAO","fullname":"Li-Cheng JIAO","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["JOURNAL OF INFRARED AND MILLIMETER WAVES 06\/2013; 32(3):231. DOI:10.3724\/SP.J.1010.2013.00231"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/276528957_Gaussian_process_latent_variable_model_based_on_immune_clonal_selection_for_SAR_target_feature_extraction_and_recognition","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/276528957_Gaussian_process_latent_variable_model_based_on_immune_clonal_selection_for_SAR_target_feature_extraction_and_recognition\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1fcfbe0fb"},"id":"rgw16_56ab1fcfbe0fb","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=276528957","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw13_56ab1fcfbe0fb"},"id":"rgw13_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220320635&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220320635,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220320635,"publicationType":"article","linkId":"0ffc98940cf255165fc9bf47","fileName":"Bayesian Gaussian Process Latent Variable Model.","fileUrl":"http:\/\/eprints.pascal-network.org\/archive\/00006343\/01\/vargplvm.pdf","name":"pascal-network.org","nameUrl":"http:\/\/eprints.pascal-network.org\/archive\/00006343\/01\/vargplvm.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw19_56ab1fcfbe0fb"},"id":"rgw19_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220320635&linkId=0ffc98940cf255165fc9bf47&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw18_56ab1fcfbe0fb"},"id":"rgw18_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320635&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":8,"valueFormatted":"8","widgetId":"rgw20_56ab1fcfbe0fb"},"id":"rgw20_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320635","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw17_56ab1fcfbe0fb"},"id":"rgw17_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320635&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220320635,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw22_56ab1fcfbe0fb"},"id":"rgw22_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320635&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":8,"valueFormatted":"8","widgetId":"rgw23_56ab1fcfbe0fb"},"id":"rgw23_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320635","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56ab1fcfbe0fb"},"id":"rgw21_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320635&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Bayesian Gaussian Process Latent Variable Model\nMichalis K. Titsias\nSchool of Computer Science\nUniversity of Manchester\nNeil D. Lawrence\nSchool of Computer Science\nUniversity of Manchester\nAbstract\nWe introduce a variational inference framework\nfor training the Gaussian process latent variable\nmodel and thus performing Bayesian nonlinear\ndimensionality reduction. This method allows\nus to variationally integrate out the input vari-\nables of the Gaussian process and compute a\nlower bound on the exact marginal likelihood of\nthe nonlinear latent variable model. The maxi-\nmization of the variational lower bound provides\na Bayesian training procedure that is robust to\noverfitting and can automatically select the di-\nmensionality of the nonlinear latent space. We\ndemonstrate our method on real world datasets.\nThe focus in this paper is on dimensionality re-\nduction problems, but the methodology is more\ngeneral. For example, our algorithm is imme-\ndiately applicable for training Gaussian process\nmodels in the presence of missing or uncertain\ninputs.\n1Introduction\nGaussian processes (GPs) (see e.g. Rasmussen and\nWilliams, 2006) are stochastic processes over real-valued\nfunctions. GPs offer a Bayesian nonparametric framework\nfor inference of highly nonlinear latent functions from ob-\nserved data. They have become very popular in machine\nlearning for solving problems such as nonlinear regression\nand classification.\nThe standard application of GP models is to supervised\nlearning tasks where both output and input data are as-\nsumed to be given at training time. The application of\nGPs to unsupervised learning tasks is more involved. One\napproach to unsupervised learning with GPs is the Gaus-\nsian process latent variable model (GP-LVM) proposed by\nAppearing in Proceedings of the 13thInternational Conference\non Artificial Intelligence and Statistics (AISTATS) 2010, Chia La-\nguna Resort, Sardinia, Italy. Volume 9 of JMLR: W&CP 9. Copy-\nright 2010 by the authors.\nLawrence (2004, 2005). GP-LVM can be considered as a\nmultiple-output GP regression model where only the out-\nput data are given. The inputs are unobserved and are\ntreated as latent variables, however instead of integrating\nout the latent variables, they are optimized.\nmakes the model tractable and some theoretical ground-\ning for the approach is given by the fact that the model\ncan be seen as a nonlinear extension of the linear prob-\nabilistic PCA (PPCA). In PPCA (and in factor analysis\n(FA)) Bayesian extensions of the model are straightforward\n(Bishop, 1999b; Ghahramani and Beal, 2000) using varia-\ntional algorithms based on mean field approximations. An\nanalogous variational method for the GP-LVM is a much\nmore challenging problem which had not been addressed\nuntil this paper. The main difficulty is that to apply vari-\national Bayes to GP-LVM we need to approximately inte-\ngrate out the latent\/input variables that appear nonlinearly\nin the inverse kernel matrix of the GP model. Standard\nmean field variational methodologies do not lead to an an-\nalytically tractable algorithm.\nThis trick\nWe introduce a framework that allows us to variationally\nintegrate out the latent variables in the GP-LVM and com-\npute a closed-form Jensen\u2019s lower bound on the true log\nmarginal likelihood of the data. The key ingredient that\nmakes the variational Bayes approach tractable is the ap-\nplication of variational inference in an expanded probabil-\nity model where the GP prior is augmented to include aux-\niliary inducing variables. Inducing variables were intro-\nduced originally for computational speed ups in GP regres-\nsion models (Csat\u00b4 o and Opper, 2002; Seeger et al., 2003;\nCsat\u00b4 o, 2002; Snelson and Ghahramani, 2006; Qui\u02dc nonero\nCandela and Rasmussen, 2005; Titsias, 2009). Our ap-\nproach builds on, and significantly extends the variational\nsparse GP method of Titsias (2009) so that a closed-form\nvariational lower bound of the GP-LVM marginal likeli-\nhood is computed. This solves a key problem with the\nGP-LVM: variational inference in the GP-LVM allows for\nBayesian training of the model that is robust to overfitting.\nFurthermore, by using the automatic relevance determina-\ntion (ARD) squared exponential kernel, the algorithm al-\nlows us to automatically infer the dimensionality of the\nnonlinear latent space without introducing explicit regular-\nizers to enforce this constraint (Geiger et al., 2009)."},{"page":2,"text":"Bayesian Gaussian Process Latent Variable Model\nAlthough, in this paper, we focus on application of the vari-\national approach tothe GP-LVM, the methodology we have\ndeveloped can be more widely applied to a variety of other\nGP models. In particular, our algorithm is immediately ap-\nplicable for training GPs with missing or uncertain inputs\n(Girard et al., 2002). Other possible applications will be\ndiscussed as future work.\nIn the remainder of the paper we first review the GP-LVM\nand then we introduce our variational approximation. We\nfinish by demonstrating the ability of the new model to au-\ntomatically determine dimensionality and resist overfitting\non real world datasets.\n2 Gaussian process latent variable model\nLet Y \u2208\nnumber of observations and D the dimensionality of each\ndata vector. These data are associated with latent variables\nX \u2208\nality reduction, Q \u226a D. The GP-LVM (Lawrence, 2005)\ndefines a forward (or generative) mapping from the latent\nspace to observation space that is governed by Gaussian\nprocesses. If the GPs are taken to be independent across\nthe features then the likelihood function is written as\n?N\u00d7Dbe the observed data where N is the\n?N\u00d7Qwhere, for the purpose of doing dimension-\np(Y |X) =\nD\n?\nd=1\np(yd|X),\n(1)\nwhere ydrepresents the dthcolumn of Y and\np(yd|X) = N(yd|0,KNN+ \u03b2\u22121IN).\n(2)\nHere, KNNis the N \u00d7N covariance matrix defined by the\ncovariance (orkernel) function k(x,x\u2032). Forthe purpose of\ndoing automatic model selection of the dimensionality of\nlatent space, this kernel can be chosen to follow the ARD\n(see Rasmussen and Williams, 2006) squared exponential\nform:\n?\nk(x,x\u2032) = \u03c32\nfexp\n\u22121\n2\nQ\n?\nq=1\n\u03b1q(xq\u2212 x\u2032\nq)2\n?\n.\n(3)\nEquation (1) can be viewed as the likelihood function of a\nmultiple-output GP regression model where the vectors of\ndifferent outputs are drawn independently from the same\nGaussian process prior which is evaluated at the inputs X.\nSince X is a latent variable, we can assign it a prior density\ngiven by the standard normal density. More precisely, the\nprior for X is:\np(X) =\nN\n?\nn=1\nN(xn|0,IQ),\n(4)\nwhere each xnis the nthrow of X. The joint probability\nmodel for the GP-LVM model is\np(Y,X) = p(Y |X)p(X).\n(5)\nThe hyperparameters of the model are the kernel param-\neters \u03b8 = (\u03c32\nrameter \u03b2. For the sake of clarity, these parameters are\nomitted from the conditioning of the distribution1. Cur-\nrently, the primary methodology for training the GP-LVM\nmodel is to find the MAP estimate of X (Lawrence, 2005)\nwhilst jointly maximizing with respect to the hyperparam-\neters. Here, we develop a variational Bayesian approach to\nmarginalization of the latent variables, X, allowing us to\noptimize the resulting lower bound on the marginal likeli-\nhoodwithrespecttothehyperparameters. Thelowerbound\ncan also be used for model comparison and automatic se-\nlection of the latent dimensionality.\nf,\u03b11,...,\u03b1Q) and the inverse variance pa-\n3Variational inference\nWe wish to compute the marginal likelihood of the data:\n?\nHowever, this quantity is intractable as X appears nonlin-\nearly inside the inverse of the covariance matrix KNN+\n\u03b2\u22121IN. Instead, we seek to apply an approximate varia-\ntional inference procedure where we introduce a variational\ndistribution q(X) to approximate the true posterior distri-\nbution p(X|Y ) over the latent variables. We take the varia-\ntional distribution to have a factorized Gaussian form over\nthe latent variables,\np(Y ) =\np(Y |X)p(X)dX.\n(6)\nq(X) =\nN\n?\nn=1\nN(xn|\u00b5n,Sn),\n(7)\nwhere the variational parameters are {\u00b5n,Sn}N\nsimplicity, Sn is taken to be a diagonal covariance ma-\ntrix2. Using this variational distribution we can express a\nJensen\u2019s lower bound on the logp(Y ) that takes the form:\n?\n=\nq(X)logp(Y |X)dX \u2212\nn=1and, for\nF(q) =\nq(X)logp(Y |X)p(X)\nq(X)\ndX\n??\nq(X)logq(X)\np(X)dX\n=?F(q) \u2212 KL(q||p),\ntween the variational posterior distribution q(X) and the\nprior distribution p(X) over the latent variables. This term\nis computed analytically since both distributions are Gaus-\nsians. Therefore, the difficult part when estimating the\nabove bound is the first term:\n?\n(8)\nwhere the second term is the negative KL divergence be-\n?F(q) =\n1A\np(Y |X,\u03b2,\u03b8)p(X).\n2This can be extended to non-diagonal within our framework.\nD\n?\nd=1\nq(X)logp(yd|X)dX =\nD\n?\nd=1\n?Fd(q),\n(9)\nprecise notationistowrite\np(Y,X|\u03b2,\u03b8)="},{"page":3,"text":"Michalis K. Titsias, Neil D. Lawrence\nwhere we have used (1). Thus, the computation of?F(q)\nresponding to the dthoutput. Notice that the computation of\n?Fd(q) involves an analytically intractable integration. This\nlinear manner inside the inverse of the covariance matrix,\nKNN+ \u03b2\u22121IN. Our main contribution is a mathematical\ntool that allows us to compute a closed-form lower bound\nfor?Fd(q). As we will see, the key idea is to apply vari-\nmodel.\nbreaks down to separate computations of each?Fd(q), cor-\narises because logp(yd|X) contains X in an highly non-\national sparse GP regression in an augmented probability\n3.1Lower bound by applying variational sparse GP\nregression\nThe computation in?Fd(q) involves an expectation over the\ncompute a Jensen\u2019s lower bound on logp(yd|X) by intro-\nducingtheGP latentfunctionvaluestogetherwithauxiliary\ninducing variables as those used in sparse GP models.\nintractable term logp(yd|X). To deal with this, we first\nSparse approximations have already been applied to speed\nup the GP-LVM Lawrence (2007). The first step of our\napproximation is equivalent to applying the new varia-\ntional approximation of Titsias (2009) to the standard GP-\nLVM. The likelihood function p(yd|X) is just the Gaussian\nmarginal likelihood of a GP regression model. We make\nthis explicit by introducing the GP latent function values\nfd \u2208\noutputs yd(the dthcolumn of Y ). The \u201ccomplete\u201d likeli-\nhood associated with the marginal likelihood p(yd|X) is:\n?Nassociated with the vector of (noise corrupted)\np(yd,fd|X) = p(yd|fd)p(fd|X),\n(10)\nwhere p(yd|fd) = N(yd|fd,\u03b2\u22121IN) and p(fd|X) is the\nzero-mean GP prior with covariance matrix KNN. Note\nthat the above joint model still contains X inside the in-\nverse of KNN making expectations under distributions\nover X difficult to compute. We finesse this intractability\nby introducing auxiliary inducing variables and applying\nthe variational sparse GP formulation of Titsias (2009).\nWe follow the approach of Lawrence (2007): for each vec-\ntor of latent function values fdwe introduce a separate set\nof M inducing variables ud\u2208\nducing input locations given by Z \u2208\nity, we assume that all uds, associated with different out-\nputs, are evaluated at the same inducing locations, however\nthis could be relaxed. The udvariables are just function\npoints drawn from the GP prior. Using these inducing vari-\nables we augment the joint probability model in eq. (10):\n?Mevaluated at a set of in-\n?M\u00d7Q. For simplic-\np(yd,fd,ud|X,Z) = p(yd|fd)p(fd|ud,X,Z)p(ud|Z),\n(11)\nwhere we used the fact that the joint GP prior over function\nvalues fdand udevaluated at inputs X and Z factorizes as\np(fd,ud|X,Z) = p(fd|ud,X,Z)p(ud|Z) where\np(fd|ud,X,Z) = N(fd|\u03b1d,KNN\u2212 KNMK\u22121\nMMKMN)\nis the conditional GP prior with \u03b1d = KNMK\u22121\nFurther, p(ud|Z) = N(ud|0,KMM) is the marginal GP\nprior over the inducing variables. The likelihood p(yd|X)\ncan be equivalently computed from the above augmented\nmodel by marginalizing out (fd,ud) and crucially this is\ntrue for any value of the inducing inputs Z. This means\nthat, unlike X, the inducing inputs Z are not random vari-\nables. Neither are they model hyperparameters, they are\nvariational parameters. This interpretation of the induc-\ning inputs is key in developing our approximation, it arises\nfrom the variational approach of Titsias (2009). Taking\nadvantage of this observation we now simplify our nota-\ntion by dropping Z from our expressions. We can now\napply variational inference to approximate the true poste-\nrior, p(fd,ud|yd,X) = p(fd|ud,yd,X)p(ud|yd,X), with\na sparse variational distribution that takes the form\nMMud.\nq(fd,ud) = p(fd|ud,X)\u03c6(ud),\n(12)\nwhere p(fd|ud,X) is the conditional GP prior that appears\nin the joint model in (11), while \u03c6(ud) is a variational dis-\ntribution over the inducing variables ud. Thus we obtain a\nlower bound:\nlogp(yd|X) \u2265\n?\n\u03c6(ud)logp(ud)N(yd|\u03b1d,\u03b2\u22121IN)\n\u03c6(ud)\ndud\n\u2212\u03b2\n2Tr(KNN\u2212 KNMK\u22121\nMMKMN),\n(13)\nwhere \u03b1d= KNMK\u22121\nmethod (Titsias, 2009), the \u03c6(ud) distribution is computed\nin an optimal way. Such an optimal choice of this distribu-\ntion depends on the latent variables X and is not useful in\nour case. In order to obtain the bound for the GP-LVM we\nneed to take a mean field approach and force independence\nof the distribution \u03c6(ud) from the random variable X.\nMMud. In the variational sparse GP\nSo far we have computed a lower bound on logp(yd|X)\nwhich is the intractable term in?Fd(q). Using eq. (13) and\n?Fd(q) \u2265\n\u2212\u03b2\n2Tr(K\u22121\nthe definition of?Fd(q) from (9) we have\nq(X)\n?\n2Tr(KNN) +\u03b2\n??\n\u03c6(ud)logp(ud)N(yd|\u03b1d,\u03b2\u22121IN)\n\u03c6(ud)\ndud\nMMKMNKNM)\n?\ndX,\nwhere we used standard properties of the trace of a matrix.\nSince (under our factorization assumption) \u03c6(ud) does not\ndepend on the random variable X, we can swap the inte-\ngrations over X and udand perform firstly the integration\nwith respect to X:\n?Fd(q) \u2265\n\u03c6(ud)\n?\n\u2212\u03b2\n?\n?logN(yd|\u03b1d,\u03b2\u22121IN)?q(X)+ logp(ud)\n\u03c6(ud)\n?\ndud\n2Tr??KNN?q(X)\n?+\u03b2\n2Tr?K\u22121\nMM?KMNKNM?q(X)\n?,"},{"page":4,"text":"Bayesian Gaussian Process Latent Variable Model\nwhere ?\u00b7?q(X) denotes expectation under the distribu-\ntion q(X).Now, we can analytically maximize the\nabove lower bound with respect to the distribution \u03c6(ud).\nThe optimal setting of this distribution is \u03c6(ud)\n?logN(yd|\u03b1d,\u03b2\u22121IN)?q(X)p(ud) and the lower bound\nthat automatically incorporates such an optimal setting is\nobtained easily by reversing Jensen\u2019s inequality,\n??\n\u2212\u03b2\n\u221d\n?Fd(q) \u2265 log\ne?log N(yd|\u03b1d,\u03b2\u22121IN)?q(X)p(ud)dud\n?\n2Tr??KNN?q(X)\nThe r.h.s. in this equation is a lower bound in which the\nvariational distribution \u03c6(ud) has been eliminated opti-\nmally. This quantity now can be computed in closed-\nform since it boils down to computing the statistics\n\u03c80 = Tr??KNN?q(X)\nance functions, such as the ARD squared exponential from\n(3), are computable analytically as discussed in section\n3.2. Notice also that ?logN(yd|\u03b1d,\u03b2\u22121IN)?q(X)is just\na quadratic function of udthat depends on the statistics \u03a81\nand \u03a82. Therefore, the integration involved in the above\nequation is a standard Gaussian integral. The closed-form\nof the lower bound on?Fd(q) is:\n?Fd(q) \u2265 log\n\u2212\u03b2\u03c80\n2\nwhere W = \u03b2IN\u2212 \u03b22\u03a81(\u03b2\u03a82+ KMM)\u22121\u03a8T\nnow compute the closed-from variational lower of the GP-\nLVM according to equation (8). More precisely, by sum-\nming both sides of (14) over the D outputs we obtain on\nthe l.h.s. the term?F(q) (see equation (9)) and on the r.h.s.\n(in place of?F(q)) in (8) we obtain the final GP-LVM lower\nclosely the corresponding sparse GP-LVM marginal likeli-\nhood (where X is optimized, not integrated out) obtained\nby applying the variational method of Titsias (2009). The\ndifference is that now (where X is variationally integrated\nout) we obtain an extra regularization term, i.e. the term\nKL(q||p) in (8), and also the kernel quantities Tr(KNN),\nKNMand KMNKNMthat contain X are replaced by vari-\national averages, which are the \u03a8 statistics defined above.\n?+\u03b2\n2Tr?K\u22121\nMM?KMNKNM?q(X)\n?.\n?, \u03a81 = ?KNM?q(X)and \u03a82 =\n?KMNKNM?q(X).These statistics for certain covari-\n?\n(\u03b2)\nN\n2|KMM|\n1\n2\n(2\u03c0)\nN\n2|\u03b2\u03a82+ KMM|\n2Tr?K\u22121\n1\n2e\u22121\n2yT\ndWyd\n?\n+\u03b2\nMM\u03a82\n?,\n(14)\n1. We can\na lower bound on?F(q). By substituting the latter quantity\nbound. This bound has an elegant form since it resembles\nThe bound can be jointly maximized over the variational\nparameters ({\u00b5n,Sn}N\neters (\u03b2,\u03b8) by applying gradient-based optimization tech-\nniques. The approach is similarto the MAP optimization of\nthe objective function employed in Lawrence (2005) with\nthe main difference that now we have an additional set of\nvariational parameters governing the approximate posterior\nvariances in the latent space.\nn=1,Z) and the model hyperparam-\n3.2 Computation of the \u03a8 statistics\nTo obtain an explicit evaluation of the variational lower\nbound we need to compute the statistics (\u03c80,\u03a81,\u03a82). We\ncan rewrite the \u03c80statistic as \u03c80=?N\n\u03c8n\n0=\nk(xn,xn)N(xn|\u00b5n,Sn)dxn.\nn=1\u03c8n\n0where\n?\n(15)\n\u03a81is an N \u00d7 M matrix such that\n?\n\u03a82 is an M \u00d7 M matrix which is written as \u03a82 =\n?N\n(\u03a8n\n2)mm\u2032 =\nk(xn,zm)k(zm\u2032,xn)N(xn|\u00b5n,Sn)dxn.\n(\u03a81)nm=\nk(xn,zm)N(xn|\u00b5n,Sn)dxn.\n(16)\nn=1\u03a8n\n2where \u03a8n\n?\n2is such that\n(17)\nThe above computations involve convolutions of the co-\nvariance function with a Gaussian density. For some stan-\ndard kernels such the ARD squared exponential (SE) co-\nvariance and the linear covariance function these statistics\nare obtained analytically. In particular for the ARD SE ker-\nnel, \u03c80= N\u03c32\nf,\n(\u03a81)nm= \u03c32\nf\nQ\n?\nq=1\ne\u22121\n2\n\u03b1q(\u00b5nq\u2212zmq)2\n\u03b1qSnq+1\n(\u03b1qSnq+ 1)\n1\n2\nand\n(\u03a8n\n2)mm\u2032 = \u03c34\nf\nQ\n?\nq=1\ne\u2212\n\u03b1q(zmq\u2212zm\u2032q)2\n4\n\u2212\n\u03b1q(\u00b5nq\u2212\u00af zq)2\n2\u03b1qSnq+1\n(2\u03b1qSnq+ 1)\n1\n2\n,\nwhere \u00af zq=\nwe need to compute the variational lower bound for the\nARD SE kernel. For the linear covariance function the in-\ntegrals are also tractable. Suppose the kernel function fol-\nlows the ARD linear form:\n(zmq+zm\u2032q)\n2\n. This gives us all the components\nk(x,x\u2032) = xTAx\u2032,\n(18)\nwhere A is a positive definite diagonal covariance matrix.\nLearning the diagonal elements of A will allow to perform\nautomatic model selection of the dimensionality of the lin-\near latent space in a similar manner to ARD SE covari-\nance function. Thus, the framework provides an alternative\nmethod to perform Bayesian probabilistic PCA (Bishop,\n1999a;Minka,2001). Forthislinearkernelthestatisticsare\nsuch that \u03c8n\nand (\u03a8n\n0= Tr?A(\u00b5n\u00b5T\nn+ Sn)?, (\u03a81)nm= \u00b5T\nnAzm\n2)mm\u2032 = zT\nmA(\u00b5n\u00b5T\nn+ Sn)Azm\u2032.\nFinally, it is worth noticing that the \u03a8 statistics are com-\nputed in a decomposable way which is useful when a new\ndata vector is inserted into the model. In particular, the\nstatistics \u03c80and \u03a82are written as sums of independent\nterms where each term is associated with a data point and\nsimilarly each column of the matrix \u03a81is associated with\nonly one data point. These properties can help to speed up\ncomputations during test time as discussed in section 4."},{"page":5,"text":"Michalis K. Titsias, Neil D. Lawrence\n3.3Summary of the variational method\nTo summarize the above variational method allows to com-\npute a Jensen\u2019s lower bound on the GP-LVM marginal like-\nlihood and the key to obtaining this bound was to intro-\nduce auxiliary variables into the model similar to those\nused in sparse GP regression.\nthe method using a sequence of steps, we could also start\nby writing the joint probability density over all variables\n(Y,X,{fd,ud}D\ndistribution to approximate the model at once. This full\nvariational distribution that gives rise to the lower bound\nobtained earlier is given by\nAlthough, we explained\nd=1) and then introduce the full variational\nq({fd,ud}D\nd=1,X) =\n?D\nd=1\n?\np(fd|ud,X)\u03c6(ud)\n?\nq(X).\nThis distribution is a mean field approximation with respect\nto uds and X. However, it is not a mean field with respect\nto fds since once uds and X are marginalized out, then fds\nbecome coupled. In addition, the fact that the q(X) dis-\ntribution is factorized over the latent variables is a conse-\nquence of the mean field assumption between uds and X.\nIt does not need to be imposed in advance. To see this,\nnotice that q(X) appears only in the \u03a8 statistics which as\nexplained earlier are computed in a decomposable (across\ndata points\/latent variables) way.\n4Prediction and computation of\nprobabilities in test data\nIn this section, we discuss how we can use the proposed\nmodel, from now on referred to as Bayesian GP-LVM,\nin order to make predictions in unseen data. Firstly, we\nexplain how we can approximately compute the proba-\nbility density p(y\u2217|Y ) of some observed test data vector\ny\u2217 \u2208\ncomputation of this probability can allow us to use the\nmodel as a density estimator which, for instance, can repre-\nsent the class conditional distribution in a generative based\nclassification system. We will exploit such a use in section\n5. Secondly we discuss how we can predict the function\nvalues y\u2217given that we have an estimate of the variational\ndistribution q(x\u2217) forthe latent variable associated with the\nobservation y\u2217. This can be useful when we wish to predict\nthe missing values of some partially observed test output\ny\u2217 = (yO\nnents in the vector y\u2217and yU\nwe would like to predict. This second prediction task can\nalso be used to remove the noise of a fully observed output.\n?D, which is allowed to have missing values. The\n\u2217,yU\n\u2217) \u2208\n?Dwhere yO\n\u2217are observed compo-\n\u2217are the missing values that\nFirst we discuss how to approximate the density p(y\u2217|Y ).\nBy introducing the latent variables X (corresponding to the\ntraining outputs Y ) and new test latent variables x\u2217, the\nprevious density is written as\np(y\u2217|Y ) =\n?p(y\u2217,Y |X,x\u2217)p(X,x\u2217)dXdx\u2217\nNote that this is a ratio of two marginal likelihoods. In the\ndenominator we have the marginal likelihood of the GP-\nLVM for which we have already computed a variational\nlower bound. The numerator is another marginal likelihood\nthat is obtained by augmenting the training data Y with\nthe test point y\u2217and integrating out both X and the newly\ninserted latent variable x\u2217. To approximate the density\np(y\u2217|Y ), we construct a ratio of lower bounds as follows.\n?p(Y |X)p(X)dX is approximated by the lower bound\nthe log marginal likelihood as computed in section 3. The\nmaximization of this lower bound specifies the variational\ndistribution q(X) over the latent variables in the training\ndata. Then, this distribution remains fixed during test time.\n?p(y\u2217,Y |X,x\u2217)p(X,x\u2217)dXdx\u2217is approximated by the\ntimize with respect to the parameters (\u00b5\u2217,S\u2217) of the Gaus-\nsian variational distribution q(x\u2217). Such optimization is\nsubject to local minima. However, sensible initializations\nof \u00b5\u2217can be employed based on the mean of the variational\ndistributions associated with the nearest neighbours of y\u2217\nin the training data Y . Furthermore, such optimization is\nfast because we can perform several precomputations in\nadvance. In particular, notice that because the computa-\ntion of the \u03a8 statistics decomposes across data, updating\nthese statistics to account for the insertion of the test point,\ninvolves only averages overthe single-point variational dis-\ntribution q(x\u2217). Finally, the approximation of p(y\u2217|Y ) is\ngiven by\n?p(Y |X)p(X)dX\n.\n(19)\neF(q(X))where F(q(X)) is the variational lower bound on\nlower bound eF(q(X,x\u2217)). To compute this, we need to op-\nq(y\u2217|Y ) = eF(q(X,x\u2217))\u2212F(q(X)).\n(20)\nWe now discuss the second prediction problem where a\npartially observed test point y\u2217 = (yO\nwe wish to reconstruct the missing part yU\nvolves two steps. Firstly, we optimize the parameters of\nthe variational distribution q(x\u2217) by maximizing the varia-\ntional lower bound on?p(yO\nq(x\u2217); exactly as explained earlier. To predict now yU\ntake the standard GP prediction approach by taking also\ninto account the fact that the input x\u2217is uncertain since it\nhas the distribution q(x\u2217). Therefore, the problem takes\nthe form of GP prediction with uncertain inputs similar to\nGirard et al. (2002). More precisely, to predict yU\npredict its latent function values fU\n???\n=\nq(fU\n\u2217|x\u2217)q(x\u2217)dx\u2217,\n\u2217,yU\n\u2217) is given and\n\u2217. This in-\n\u2217,Y |X,x\u2217)p(X,x\u2217)dXdx\u2217\nby keeping all the optimized quantities fixed apart from\n\u2217, we\n\u2217we first\n\u2217according to\nq(fU\n\u2217) =\nd\u2208U\n?\np(fU\n\u2217d|ud,x\u2217)\u03c6(ud)dud\n?\nq(x\u2217)dx\u2217\n?\n(21)"},{"page":6,"text":"Bayesian Gaussian Process Latent Variable Model\nwhere q(fU\neach factor takes the form of the projected process pre-\ndictive distribution (Csat\u00b4 o and Opper, 2002; Seeger et al.,\n2003; Rasmussen and Williams, 2006). The marginaliza-\ntionofx\u2217couplesalldimensionsoffU\nGaussian fully dependent multivariate density. For squared\nexponential kernels all moments of the density q(fU\nanalytically tractable. In practice, we will typically need\nonly the mean and covariance of fU\n\u2217|x\u2217) is a factorized Gaussian distribution where\n\u2217andproducesanon-\n\u2217) are\n\u2217. The mean is\n?(fU\n\u2217) = \u039bT\u03c8\u2217\n1.\nHere, \u039b = \u03b2(KMM+ \u03b2\u03a82)\u22121\u03a8T\nmatrix containing the columns of Y corresponding to the\nmissing values of y\u2217. Also, the vector \u03c8\u2217\nby \u03c8\u2217\n1YUwhere YUis the\n1\u2208\n?Mis defined\n1= ?KM\u2217?q(x\u2217)where KM\u2217= k(Z,x\u2217). Similarly,\nCov(fU\n+ \u03c8\u2217\n\u2217) = \u039bT?\u03a8\u2217\n2\u2212 \u03c8\u2217\nMM\u2212 (KMM+ \u03b2\u03a82)\u22121?\u03a8\u2217\n0= ?k(x\u2217,x\u2217)?q(x\u2217)and \u03a8\u2217\nNotice that the \u03a8 statistics (the terms (\u03c8\u2217\ning the test latent variable x\u2217appear naturally in these ex-\npressions. Using the above expressions, the predicted mean\nof yU\nequal to Cov(fU\n1(\u03c8\u2217\n1)T?\u039b\n0I \u2212 Tr??K\u22121\n2\n?I,\nwhere \u03c8\u2217\n2= ?KM\u2217K\u2217M?q(x\u2217).\n0,\u03c8\u2217\n1,\u03a8\u2217\n2)) involv-\n\u2217is equal to\n?(fU\n\u2217) and the predicted covariance is\n\u2217) + \u03b2\u22121I.\n5 Experiments\nTo demonstrate the Bayesian GP-LVM we now consider\nsome standard machine learning data sets. Our aim is to\nhighlight several characteristics of the algorithm: the im-\nproved quality of visualizations achieved by the model,\nthe utility of being able to access a lower bound on the\nmarginal likelihood of the data, and the ability of the model\nto automatically determine the dimensionality of the data.\n5.1Oil flow data\nIn the first experiment we illustrate the method in the multi-\nphase oil flow data (Bishop and James, 1993) that consists\nof 1000, 12 dimensional observations belonging to three\nknown classes corresponding to different phases of oil flow.\nFigure 1 shows the results for these data obtained by ap-\nplying the Bayesian GP-LVM with 10 latent dimensions\nusing the ARD SE kernel. The means of the variational\ndistribution were initialized based on PCA, while the vari-\nances in the variational distribution are initialized to neu-\ntral values around 0.5. As shown in Figure 1(a), the al-\ngorithm switches off automatically 7 out of 10 latent di-\nmensions by making their inverse lengthscales zero. Fig-\nure 1(b) shows the visualization obtained by keeping only\nthe dominant latent directions (having the largest inverse\nlengthscale) which are the dimensions 2 and 3. This is a\nremarkably high quality two dimensional visualization of\nthis data. For comparison, Figure 1(c) shows the visualiza-\ntion provided by the standard sparse GP-LVM that runs by\nassuming only 2 latent dimensions. Both models use 50 in-\nducing variables, while the latent variables X optimized in\nthe standard GP-LVM are initialized based on PCA. Note\nthat if we were to run the standard GP-LVM with 10 latent\ndimensions, the model would overfit the data, it would not\nreduce the dimensionality in the manner achieved by the\nBayesian GP-LVM. In these two dimensions, the nearest\nneighbour error for the different classes (phases of oil flow)\nin the case of Bayesian GP-LVM is 3 errors from 1000 data\npoints. The number of the nearest neighbour errors made\nwhen applying the standard GP-LVM was 26.\n5.2Frey Faces Data\nHere, we consider a dataset of faces (Roweis et al., 2002)\ntaken from a video sequence that consists of 1965 images\nof size 28\u00d720. In this dataset, we would like to exploit the\nability of the model to reconstruct partially observed test\ndata. Therefore, we train the model using a random selec-\ntion of 1000 images and then we consider the remaining\n965 images as test data. Furthermore, in each test image\nwe assume that only half of the image pixels are observed.\nThe missing pixels were chosen randomly for each test im-\nage. After training on 1000 images, each partially observed\ntest image was processed separately (this involves the op-\ntimization of the corresponding variational distribution as\ndiscussed in section 4) and the missing pixels were pre-\ndicted. Figure 2 shows a few examples of reconstructed\ntest images. Each column in this figure corresponds to a\ntest image, where the top plot shows the true test image,\nthe middle one the partially observed image and the bottom\nimage shows the reconstructed image. We also measure\nthe mean absolute reconstruction error over all test images\nand missing pixels and compare this errorwith the standard\nsparse GP-LVM. This standard GP-LVM was applied using\nseveral settings of the latent dimensionality: Q = 2,5,10\nand 30. The Bayesian GP-LVM was trained once using\n30 latent dimensions. The latent variables X in the stan-\ndard GP-LVM and the means of the variational distribu-\ntion in Bayesian GP-LVM were initialized through PCA.\nThe error for Bayesian GP-LVM was 7.4003. For the stan-\ndard GP-LVM the error was 10.5748, 9.7284, 19.6949 and\n19.6961 for 2,5,10 and 30 latent dimensions respectively.\nNotice that the standard GP-LVM has poor performance\nfor large value of latent dimension and achieves the best\nerror when we consider 5 latent dimensions. Nevertheless,\nthis was still worse than the error from the Bayesian GP-\nLVM. Finally, Figure 3 shows the values of the inverse\nlengthscales obtained by the maximization of the varia-\ntional lower bound. Although, in this case, the algorithm\ndoes not shrink some of the dimensions completely to zero,\nit does force many of them to obtain small values. Note that\none of the dimensions (the first from the left) seems to be\nthe most important in explaining the data."},{"page":7,"text":"Michalis K. Titsias, Neil D. Lawrence\n12345678910\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n\u22123\u22122\u221210123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22122\u22121012\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n(a)(b)(c)\nFigure 1: Panel (a) shows the inverse lengthscales found by applying the Bayesian GP-LVM with ARD SE kernel on the\noil flow data. Panel (b) shows the visualization achieved by keeping the most dominant latent dimensions (2 and 3) which\nhave the largest inverse lengthscale value. Dimension 2 is plotted on the y-axis and 3 and on the x-axis. Plot (c) shows the\nvisualization found by standard sparse GP-LVM.\nFigure 2: Examples of reconstruction of partially observed test images in Frey faces by applying the Bayesian GP-LVM.\nEach column corresponds to a test image. In every column, the top panel shows the true test image, the middle panel the\npartially observed image (where missing pixels are shown in black) and the bottom image is the reconstructed image.\nFigure 3: This plot shows the values of the inverse length-\nscales found by using the Bayesian GP-LVM with ARD SE\nkernel in Frey faces.\n5.3Digits Data\nIn the final experiment we use the Bayesian GP-LVM to\nbuild a generative classifier for handwritten digit recogni-\ntion. WeconsiderthewellknownUSPSdigitsdataset. This\ndataset consists of 16 \u00d7 16 images for all 10 digits and it\nis divided into 7291 training examples and 2007 test exam-\nples. We run 10 Bayesian GP-LVMs, one for each digit,\non the USPS data base. We used 10 latent dimensions and\n50 inducing variables for each model. This allowed us to\nbuild a probabilistic generative model for each digit so that\nwe can compute Bayesian class conditional densities in the\ntest data having the form p(y\u2217|Y,digit). These class condi-\ntional densities are approximated through the ratio of lower\nbounds in eq. (20) as described in section 4. The whole ap-\nproach allows us to classify new digits by determining the\nclass labels for test data based on the highest class con-\nditional density value and using a uniform prior over class\nlabels. The test error made by the Bayesian GP-LVM in the\nwhole set of 2007 test points was 95 incorrectly classified\ndigits i.e. 4.73% error.\n6Discussion\nWe have introduced an approximation to the marginal like-\nlihood of the fully marginalized Gaussian process latent\nvariable model. Our approximation is in the form of a vari-\national lower bound. With the fully marginalized model we\ncan automatically determine the latent dimensionality of a"},{"page":8,"text":"Bayesian Gaussian Process Latent Variable Model\ngiven data set. We demonstrated the utility of this rigorous\nlower bound on a range of disparate real world data sets.\nOur approach can immediately be applied to training Gaus-\nsian processes with uncertain inputs where these inputs\nhave Gaussian prior densities. We also envisage several\nother extensions that become computationally feasible us-\ning the same set of methodologies we espouse. Dynam-\nical models based on the GP-LVM have been proposed.\nIt would be straightforward to include a latent space prior\nwith a temporal component. This could be a Kalman filter,\na general Gaussian process (Lawrence and Moore, 2007)\nor an auto regressive Gaussian process (Wang et al., 2006).\nBy using our approach to propagating the Gaussian noise\nthrough the dynamics and the latent space a variational\nlower bound on the likelihood of these models could be\nderived. The importance of such nonlinear models is clear\nfrom the success of unscented Kalman filters and the re-\nlated ensemble Kalman filter.\nThe optimization procedure has a similar computational\ncost to that of previously proposed sparse GP-LVMs. We\nbelieve there is scope to improve the speed of the optimiza-\ntion procedure by better exploiting the correlation present\nin the parameters. A potential strategy would be to use the\ncontrol points idea used to speed up MCMC in GPs (Tit-\nsias et al., 2009) in order to encode the variational poste-\nrior, effectively decoupling these correlations and speeding\nconvergence of the optimizer.\nAcknowledgements\nWe wish to thank Mauricio\u00b4Alvarez for his help with the\nsoftware implementation. This work is funded by EPSRC\nGrant No EP\/F005687\/1 \u201dGaussian Processes for Systems\nIdentification with Applications in Systems Biology\u201d.\nReferences\nC. M. Bishop. Bayesian PCA. In M. J. Kearns, S. A. Solla,\nand D. A. Cohn, editors, Advances in Neural Information Pro-\ncessing Systems, volume 11, pages 482\u2013388, Cambridge, MA,\n1999a. MIT Press.\nC. M. Bishop. Variational principal components. In Proceedings\nNinth International Conference on Artificial Neural Networks,\nICANN\u201999, volume 1, pages 509\u2013514, 1999b.\nC. M. Bishop and G. D. James. Analysis of multiphase flows\nusing dual-energy gamma densitometry and neural networks.\nNuclear Instruments and Methods in Physics Research, A327:\n580\u2013593, 1993.\nL.Csat\u00b4 o. GaussianProcesses\u2014IterativeSparseApproximations.\nPhD thesis, Aston University, 2002.\nL. Csat\u00b4 o and M. Opper. Sparse on-line Gaussian processes. Neu-\nral Computation, 14(3):641\u2013668, 2002.\nT. G. Dietterich, S. Becker, and Z. Ghahramani, editors. Advances\nin Neural Information Processing Systems, volume 14, Cam-\nbridge, MA, 2002. MIT Press.\nA. Geiger, R. Urtasun, and T. Darrell. Rank priors for continu-\nous non-linear dimensionality reduction. In Proceedings of the\nIEEE Computer Society Conference on Computer Vision and\nPattern Recognition, Miami, FL, 2009.\nZ. Ghahramani and M. J. Beal. Variational inference for Bayesian\nmixtures of factor analysers. In S. A. Solla, T. K. Leen, and K.-\nR. M\u00a8 uller, editors, Advances in Neural Information Processing\nSystems, volume 12, pages 831\u2013864, Cambridge, MA, 2000.\nMIT Press.\nA. Girard, C. E. Rasmussen, J. Qui\u02dc nonero Candela, and\nR. Murray-Smith. Gaussian process priors with uncertain in-\nputs - application to multiple-step ahead time series forecast-\ning. In Dietterich et al. (2002), pages 529\u2013536.\nN. D. Lawrence. Gaussian process models for visualisation of\nhigh dimensional data. In S. Thrun, L. Saul, and B. Sch\u00a8 olkopf,\neditors, Advances in Neural Information Processing Systems,\nvolume 16, pages 329\u2013336, Cambridge, MA, 2004. MIT Press.\nN. D. Lawrence. Learning for larger datasets with the Gaussian\nprocess latent variable model. In M. Meila and X. Shen, ed-\nitors, Proceedings of the Eleventh International Workshop on\nArtificial Intelligence and Statistics, pages 243\u2013250, San Juan,\nPuerto Rico, 21-24 March 2007. Omnipress.\nN. D. Lawrence. Probabilistic non-linear principal component\nanalysis with Gaussian process latent variable models. Journal\nof Machine Learning Research, 6:1783\u20131816, 11 2005.\nN. D. Lawrence and A. J. Moore. Hierarchical Gaussian pro-\ncess latent variable models. In Z. Ghahramani, editor, Pro-\nceedings of the International Conference in Machine Learning,\nvolume 24, pages 481\u2013488. Omnipress, 2007.\nT. P. Minka. Automatic choice of dimensionality for PCA. In\nT. K. Leen, T. G. Dietterich, and V. Tresp, editors, Advances\nin Neural Information Processing Systems, volume 13, pages\n598\u2013604, Cambridge, MA, 2001. MIT Press.\nJ. Qui\u02dc nonero Candela and C. E. Rasmussen. A unifying view of\nsparse approximate Gaussian process regression. Journal of\nMachine Learning Research, 6:1939\u20131959, 2005.\nC. E. Rasmussen and C. K. I. Williams. Gaussian Processes for\nMachine Learning. MIT Press, Cambridge, MA, 2006.\nS. T. Roweis, L. K. Saul, and G. E. Hinton. Global coordination of\nlocal linear models. In Dietterich et al. (2002), pages 889\u2013896.\nM. Seeger, C. K. I. Williams, and N. D. Lawrence. Fast forward\nselection to speed up sparse Gaussian process regression. In\nC. M. Bishop and B. J. Frey, editors, Proceedings of the Ninth\nInternational Workshop on Artificial Intelligence and Statis-\ntics, Key West, FL, 3\u20136 Jan 2003.\nE. Snelson and Z. Ghahramani. Sparse Gaussian processes using\npseudo-inputs. In Weiss et al. (2006).\nM. Titsias, N. D. Lawrence, and M. Rattray.\npling for Gaussian process inference using control variables.\nIn D. Koller, Y. Bengio, D. Schuurmans, and L. Bottou, edi-\ntors, Advances in Neural Information Processing Systems, vol-\nume 21, Cambridge, MA, 2009. MIT Press.\nM. K. Titsias. Variational learning of inducing variables in sparse\nGaussian processes. In D. van Dyk and M. Welling, editors,\nProceedings of the Twelfth International Conference on Ar-\ntificial Intelligence and Statistics, volume 5, pages 567\u2013574,\nClearwater Beach, FL, 16-18 April 2009. JMLR W&CP 5.\nJ. M. Wang, D. J. Fleet, and A. Hertzmann. Gaussian process\ndynamical models. In Weiss et al. (2006).\nY. Weiss, B. Sch\u00a8 olkopf, and J. C. Platt, editors. Advances in Neu-\nral Information Processing Systems, volume 18, Cambridge,\nMA, 2006. MIT Press.\nEfficient sam-"}],"widgetId":"rgw24_56ab1fcfbe0fb"},"id":"rgw24_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220320635&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw25_56ab1fcfbe0fb"},"id":"rgw25_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220320635&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220320635,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":220320635,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2077228906,"url":"researcher\/2077228906_Cuong_Tran","fullname":"Cuong Tran","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":50848147,"url":"researcher\/50848147_Vladimir_Pavlovic","fullname":"Vladimir Pavlovic","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":49598718,"url":"researcher\/49598718_Robert_Kopp","fullname":"Robert Kopp","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272309860696093%401441935138728_m\/Robert_Kopp.png"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints","usePlainButton":true,"publicationUid":279633240,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints","title":"Gaussian Process for Noisy Inputs with Ordering Constraints","displayTitleAsLink":true,"authors":[{"id":2077228906,"url":"researcher\/2077228906_Cuong_Tran","fullname":"Cuong Tran","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":50848147,"url":"researcher\/50848147_Vladimir_Pavlovic","fullname":"Vladimir Pavlovic","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":49598718,"url":"researcher\/49598718_Robert_Kopp","fullname":"Robert Kopp","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272309860696093%401441935138728_m\/Robert_Kopp.png"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We study the Gaussian Process regression model in the context of training\ndata with noise in both input and output. The presence of two sources of noise\nmakes the task of learning accurate predictive models extremely challenging.\nHowever, in some instances additional constraints may be available that can\nreduce the uncertainty in the resulting predictive models. In particular, we\nconsider the case of monotonically ordered latent input, which occurs in many\napplication domains that deal with temporal data. We present a novel inference\nand learning approach based on non-parametric Gaussian variational\napproximation to learn the GP model while taking into account the new\nconstraints. The resulting strategy allows one to gain access to posterior\nestimates of both the input and the output and results in improved predictive\nperformance. We compare our proposed models to state-of-the-art Noisy Input\nGaussian Process (NIGP) and other competing approaches on synthetic and real\nsea-level rise data. Experimental results suggest that the proposed approach\nconsistently outperforms selected methods while, at the same time, reducing the\ncomputational costs of learning and inference.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Robert_Kopp\/publication\/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints\/links\/55c8c5de08aeca747d670500.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Robert_Kopp","sourceName":"Robert E. Kopp","hasSourceUrl":true},"publicationUid":279633240,"publicationUrl":"publication\/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints\/links\/55c8c5de08aeca747d670500\/smallpreview.png","linkId":"55c8c5de08aeca747d670500","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=279633240&reference=55c8c5de08aeca747d670500&eventCode=&origin=publication_list","widgetId":"rgw29_56ab1fcfbe0fb"},"id":"rgw29_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=279633240&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55c8c5de08aeca747d670500","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220320635,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/279633240_Gaussian_Process_for_Noisy_Inputs_with_Ordering_Constraints\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["To address this challenge traditional sampling-based Monte Carlo Markov chain (MCMC) techniques are often employed, however they are time-consuming and will not be appropriate for large datasets. Explicit integrating out of the input uncertainty, when the input density is known, is intractable in common situations [2] , [3]. "],"widgetId":"rgw30_56ab1fcfbe0fb"},"id":"rgw30_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw28_56ab1fcfbe0fb"},"id":"rgw28_56ab1fcfbe0fb","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=279633240&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2056849184,"url":"researcher\/2056849184_Mijung_Park","fullname":"Mijung Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2078956248,"url":"researcher\/2078956248_Wittawat_Jitkrittum","fullname":"Wittawat Jitkrittum","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278016631558145%401443295738585_m\/Wittawat_Jitkrittum.png"},{"id":2056994783,"url":"researcher\/2056994783_Ahmad_Qamar","fullname":"Ahmad Qamar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":69862689,"url":"researcher\/69862689_Zoltan_Szabo","fullname":"Zoltan Szabo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model","usePlainButton":true,"publicationUid":267393284,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model","title":"Bayesian Manifold Learning: The Locally Linear Latent Variable Model","displayTitleAsLink":true,"authors":[{"id":2056849184,"url":"researcher\/2056849184_Mijung_Park","fullname":"Mijung Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2078956248,"url":"researcher\/2078956248_Wittawat_Jitkrittum","fullname":"Wittawat Jitkrittum","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278016631558145%401443295738585_m\/Wittawat_Jitkrittum.png"},{"id":2056994783,"url":"researcher\/2056994783_Ahmad_Qamar","fullname":"Ahmad Qamar","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69862689,"url":"researcher\/69862689_Zoltan_Szabo","fullname":"Zoltan Szabo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":36471253,"url":"researcher\/36471253_Lars_Buesing","fullname":"Lars Buesing","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38613445,"url":"researcher\/38613445_Maneesh_Sahani","fullname":"Maneesh Sahani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Neural Information Processing Systems (NIPS-2015); 06\/2015"],"abstract":"We introduce the Locally Linear Latent Variable Model (LL-LVM), a probabilistic model for non-linear manifold discovery that describes a joint distribution over observations, their manifold coordinates and locally linear maps conditioned on a set of neighbourhood relationships. The model allows straightforward variational optimisation of the posterior distribution on coordinates and locally linear maps from the latent space to the observation space given the data. Thus, the LL-LVM encapsulates the local-geometry preserving intuitions that underlie non-probabilistic methods such as locally linear embedding (LLE). Its probabilistic semantics make it easy to evaluate the quality of hypothesised neighbourhood relationships, select the intrinsic dimensionality of the manifold, construct out-of-sample extensions and to combine the manifold model with additional probabilistic models that capture the structure of coordinates within the manifold.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Zoltan_Szabo17\/publication\/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model\/links\/56394b5a08ae2da875c7a43f.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Zoltan_Szabo17","sourceName":"Zoltan Szabo","hasSourceUrl":true},"publicationUid":267393284,"publicationUrl":"publication\/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model\/links\/56394b5a08ae2da875c7a43f\/smallpreview.png","linkId":"56394b5a08ae2da875c7a43f","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=267393284&reference=56394b5a08ae2da875c7a43f&eventCode=&origin=publication_list","widgetId":"rgw32_56ab1fcfbe0fb"},"id":"rgw32_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=267393284&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"56394b5a08ae2da875c7a43f","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220320635,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/267393284_Bayesian_Manifold_Learning_The_Locally_Linear_Latent_Variable_Model\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["An established alternative is to construct an explicit probabilistic model of the functional relationship between low-dimensional manifold coordinates and each measured dimension of the data, assuming that the functions instantiate draws from Gaussian-process priors . The original Gaussian process latent variable model (GP-LVM) required optimisation of the lowdimensional coordinates, and thus still did not provide uncertainties on these locations or allow evaluation of the likelihood of a model over them [7]; however a recent extension exploits an auxilliary variable approach to optimise a more general variational bound, thus retaining approximate probabilistic semantics within the latent space [8]. The stochastic process model for the mapping functions also makes it straightforward to estimate the function at previously unobserved points, thus generalising out-of-sample with ease. "],"widgetId":"rgw33_56ab1fcfbe0fb"},"id":"rgw33_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw31_56ab1fcfbe0fb"},"id":"rgw31_56ab1fcfbe0fb","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=267393284&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2048369253,"url":"researcher\/2048369253_Alexander_G_de_G_Matthews","fullname":"Alexander G. de G. Matthews","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71410443,"url":"researcher\/71410443_James_Hensman","fullname":"James Hensman","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m"},{"id":49549591,"url":"researcher\/49549591_Richard_E_Turner","fullname":"Richard E. Turner","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Apr 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes","usePlainButton":true,"publicationUid":275588150,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes","title":"On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes","displayTitleAsLink":true,"authors":[{"id":2048369253,"url":"researcher\/2048369253_Alexander_G_de_G_Matthews","fullname":"Alexander G. de G. Matthews","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71410443,"url":"researcher\/71410443_James_Hensman","fullname":"James Hensman","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A277099177889798%401443077000994_m"},{"id":49549591,"url":"researcher\/49549591_Richard_E_Turner","fullname":"Richard E. Turner","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"The variational framework for learning inducing variables Titsias (2009) has\nhad a large impact on the Gaussian process literature. The framework may be\ninterpreted as minimizing a rigorously defined Kullback-Leibler divergence\nbetween the approximate and posterior processes. To our knowledge this\nconnection has thus far gone unremarked in the literature. Many of the\ntechnical requirements for such a result were derived in the pioneering work of\nSeeger (2003,2003b). In this work we give a relatively gentle and largely\nself-contained explanation of the result. The result is important in\nunderstanding the variational inducing framework and could lead to principled\nnovel generalizations.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/James_Hensman\/publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes\/links\/55adfd8f08aed9b7dcdb08a0.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/James_Hensman","sourceName":"James Hensman","hasSourceUrl":true},"publicationUid":275588150,"publicationUrl":"publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes\/links\/55adfd8f08aed9b7dcdb08a0\/smallpreview.png","linkId":"55adfd8f08aed9b7dcdb08a0","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=275588150&reference=55adfd8f08aed9b7dcdb08a0&eventCode=&origin=publication_list","widgetId":"rgw35_56ab1fcfbe0fb"},"id":"rgw35_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=275588150&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"55adfd8f08aed9b7dcdb08a0","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220320635,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The approach has also been successfully used to perform scalable inference in more complex models such as the Gaussian process latent variable model (Titsias and Lawrence, 2010; Damianou et al., 2014) and the related Deep Gaussian process (Damianou and Lawrence, 2012; Hensman and Lawrence, 2014). "],"widgetId":"rgw36_56ab1fcfbe0fb"},"id":"rgw36_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw34_56ab1fcfbe0fb"},"id":"rgw34_56ab1fcfbe0fb","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=275588150&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":220320635,"publicationLink":"publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw27_56ab1fcfbe0fb"},"id":"rgw27_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=220320635&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=44","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":44,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw26_56ab1fcfbe0fb"},"id":"rgw26_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=220320635&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1fcfbe0fb"},"id":"rgw2_56ab1fcfbe0fb","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220320635},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220320635&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1fcfbe0fb"},"id":"rgw1_56ab1fcfbe0fb","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"6uVzkZ82bXKjriErrwH5B5odVkpmv8HzEa9W2PBkjFiQshiBxn0BARTazK44YpuR9LM7n3ao1NOjJipGM3O92oM2vWPoErRBb2UNaPyx7CvGnTEYD13M7MEdlEAaXuFkSwj3JT3LktoFoBT\/cDTlqwQQRLXrQ9L1+XaOspC+TCIITwQveFKYjjT4Xwy6B8Pqz7hUTZSpCjny0IMEdhcaD5VaNro5v69Wqe6vp7rLN0oiOYb4HztIBoO648dsc0qvCfhSo2ayE9hver2A871Oim3LcYaWgDcf6pAVK\/UBtb0=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Bayesian Gaussian Process Latent Variable Model.\" \/>\n<meta property=\"og:description\" content=\"We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model\/links\/0ffc98940cf255165fc9bf47\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model\" \/>\n<meta property=\"rg:id\" content=\"PB:220320635\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Bayesian Gaussian Process Latent Variable Model.\" \/>\n<meta name=\"citation_author\" content=\"Michalis K. Titsias\" \/>\n<meta name=\"citation_author\" content=\"Neil D. Lawrence\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/01\/01\" \/>\n<meta name=\"citation_volume\" content=\"9\" \/>\n<meta name=\"citation_firstpage\" content=\"844\" \/>\n<meta name=\"citation_lastpage\" content=\"851\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-cc911ec5-4320-4803-9923-47ba5e6ab1ce","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":844,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw37_56ab1fcfbe0fb"},"id":"rgw37_56ab1fcfbe0fb","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-cc911ec5-4320-4803-9923-47ba5e6ab1ce", "998b4fc384fe006cdc0f2a317de304181afc2e7b");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-cc911ec5-4320-4803-9923-47ba5e6ab1ce", "998b4fc384fe006cdc0f2a317de304181afc2e7b");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw38_56ab1fcfbe0fb"},"id":"rgw38_56ab1fcfbe0fb","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220320635_Bayesian_Gaussian_Process_Latent_Variable_Model","requestToken":"m45TUI1oahrVyeNoVkC4NRUJNBoCmfurTpXqjKHZ20qmwkVf4pJCBFDejZCrfwGUO+3ckmStUftd69v5EvHh5t39PBz72jmn+kT0P5Yrzwhc8CkUYZWXv4qje7WvsGLdjRV26Zx4HktKik\/mR42JcQqp5ewX7ofspWoEglTMfMI2yObiSF+3PT9jEc+7EkQ6YIAVnca0rJ35ii4K4i8041+xnXOYBbKLo8+OijErAdmb9GEHSpNACwclvy4QeqrQqOCX381g\/epRbSMdS4iKq2qdP\/njogxJPEsnRcsoN68=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=D_4M2ekCpc4ADNJTvD1oL9BjKSJZrsQveE4oGefsT856lJo2Ndyj8IKPTz4ud6xT","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwMzIwNjM1X0JheWVzaWFuX0dhdXNzaWFuX1Byb2Nlc3NfTGF0ZW50X1ZhcmlhYmxlX01vZGVs","signupCallToAction":"Join for free","widgetId":"rgw40_56ab1fcfbe0fb"},"id":"rgw40_56ab1fcfbe0fb","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw39_56ab1fcfbe0fb"},"id":"rgw39_56ab1fcfbe0fb","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw41_56ab1fcfbe0fb"},"id":"rgw41_56ab1fcfbe0fb","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
