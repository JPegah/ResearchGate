<!DOCTYPE html> <html lang="en" class="" id="rgw41_56ab9f50845fe"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="9MJZ1GGHbhlGMWNGG4CJAaxzdHUjUm5Yx4fbDEjK+yBQQ3WCZcD5XYFMT319sN5F9LrGTCXdwJbzBXfVRiQn0f9SRR5XdwRT9gnncAYV2otQTR7tn9/9JpYqQRfRZCR1znZ30nGsdGw4V3YZz0xRqb71SmFY3qnKIiml5C1umVuPMth5HgP8tboHmiWX9ar7ogITKdUCbuRKbm0c2vMEqNcEGCM/ISvwldGm+L+IUXQAI4QzgT1zyQIc++mO/Id9MXh6l1TDfsFkRRlINq9EsgxdPlzyw8ljh5pNd89nJEU="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-9d16b777-6463-4b5d-95d7-5901c2b13267",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Regression Density Estimation With Variational Methods and Stochastic Approximation" />
<meta property="og:description" content="Regression density estimation is the problem of flexibly estimating a response distribution as a function of covariates. An important approach to regression density estimation uses finite mixture..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation/links/00b7d53a84badac545000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation" />
<meta property="rg:id" content="PB:254295504" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1080/10618600.2012.679897" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Regression Density Estimation With Variational Methods and Stochastic Approximation" />
<meta name="citation_author" content="David J. Nott" />
<meta name="citation_author" content="Siew Li Tan" />
<meta name="citation_author" content="Mattias Villani" />
<meta name="citation_author" content="Robert Kohn" />
<meta name="citation_publication_date" content="2012/07/01" />
<meta name="citation_journal_title" content="Journal of Computational and Graphical Statistics" />
<meta name="citation_issn" content="1061-8600" />
<meta name="citation_volume" content="21" />
<meta name="citation_issue" content="3" />
<meta name="citation_doi" content="10.1080/10618600.2012.679897" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Robert_Kohn/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation/links/00b7d53a84badac545000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Regression Density Estimation With Variational Methods and Stochastic Approximation (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Regression Density Estimation With Variational Methods and Stochastic Approximation on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9f50845fe" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9f50845fe" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9f50845fe">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1080%2F10618600.2012.679897&rft.atitle=Regression%20Density%20Estimation%20With%20Variational%20Methods%20and%20Stochastic%20Approximation&rft.title=Journal%20of%20Computational%20and%20Graphical%20Statistics%20-%20J%20COMPUT%20GRAPH%20STAT&rft.jtitle=Journal%20of%20Computational%20and%20Graphical%20Statistics%20-%20J%20COMPUT%20GRAPH%20STAT&rft.volume=21&rft.issue=3&rft.date=2012&rft.issn=1061-8600&rft.au=David%20J.%20Nott%2CSiew%20Li%20Tan%2CMattias%20Villani%2CRobert%20Kohn&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Regression Density Estimation With Variational Methods and Stochastic Approximation</h1> <meta itemprop="headline" content="Regression Density Estimation With Variational Methods and Stochastic Approximation">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation/links/00b7d53a84badac545000000/smallpreview.png">  <div id="rgw8_56ab9f50845fe" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab9f50845fe"> <a href="researcher/8484000_David_J_Nott" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="David J. Nott" alt="David J. Nott" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David J. Nott</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw10_56ab9f50845fe">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8484000_David_J_Nott"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David J. Nott" alt="David J. Nott" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8484000_David_J_Nott" class="display-name">David J. Nott</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab9f50845fe"> <a href="researcher/59395451_Siew_Li_Tan" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Siew Li Tan" alt="Siew Li Tan" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Siew Li Tan</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab9f50845fe">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/59395451_Siew_Li_Tan"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Siew Li Tan" alt="Siew Li Tan" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/59395451_Siew_Li_Tan" class="display-name">Siew Li Tan</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab9f50845fe" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Mattias_Villani" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272308082311176%401441934714490_m/Mattias_Villani.png" title="Mattias Villani" alt="Mattias Villani" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Mattias Villani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw14_56ab9f50845fe" data-account-key="Mattias_Villani">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Mattias_Villani"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272308082311176%401441934714490_l/Mattias_Villani.png" title="Mattias Villani" alt="Mattias Villani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Mattias_Villani" class="display-name">Mattias Villani</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Linkoeping_University" title="Linköping University">Linköping University</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw15_56ab9f50845fe" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Robert_Kohn" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272445403824128%401441967454070_m/Robert_Kohn.png" title="Robert Kohn" alt="Robert Kohn" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Robert Kohn</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw16_56ab9f50845fe" data-account-key="Robert_Kohn">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Robert_Kohn"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272445403824128%401441967454070_l/Robert_Kohn.png" title="Robert Kohn" alt="Robert Kohn" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Robert_Kohn" class="display-name">Robert Kohn</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_New_South_Wales" title="University of New South Wales">University of New South Wales</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1061-8600_Journal_of_Computational_and_Graphical_Statistics"><span itemprop="name">Journal of Computational and Graphical Statistics</span></a> </span>    (Impact Factor: 1.22).     <meta itemprop="datePublished" content="2012-07">  07/2012;  21(3).    DOI:&nbsp;10.1080/10618600.2012.679897           </div> <div id="rgw17_56ab9f50845fe" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Regression density estimation is the problem of flexibly estimating a response distribution as a function of covariates. An important approach to regression density estimation uses finite mixture models and our article considers flexible mixtures of heteroscedastic regression (MHR) models where the response distribution is a normal mixture, with the component means, variances and mixture weights all varying as a function of covariates. Our article develops fast variational approximation methods for inference. Our motivation is that alternative computationally intensive MCMC methods for fitting mixture models are difficult to apply when it is desired to fit models repeatedly in exploratory analysis and model choice. Our article makes three contributions. First, a variational approximation for MHR models is described where the variational lower bound is in closed form. Second, the basic approximation can be improved by using stochastic approximation methods to perturb the initial solution to attain higher accuracy. Third, the advantages of our approach for model choice and evaluation compared to MCMC based approaches are illustrated. These advantages are particularly compelling for time series data where repeated refitting for one step ahead prediction in model choice and diagnostics and in rolling window computations is very common. Supplemental materials for the article are available online.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw18_56ab9f50845fe" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw32_56ab9f50845fe">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw33_56ab9f50845fe">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Robert_Kohn/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation/links/00b7d53a84badac545000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Robert_Kohn">Robert Kohn</a>, <span class="js-publication-date"> Jun 23, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw35_56ab9f50845fe" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw36_56ab9f50845fe" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw37_56ab9f50845fe" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw38_56ab9f50845fe" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw39_56ab9f50845fe" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw40_56ab9f50845fe" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw34_56ab9f50845fe" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FRobert_Kohn%2Fpublication%2F254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation%2Flinks%2F00b7d53a84badac545000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw31_56ab9f50845fe"  itemprop="articleBody">  <p>Page 1</p> <p>Regression density estimation with variational<br />methods and stochastic approximation<br />David J. Nott, Siew Li Tan, Mattias Villani and Robert Kohn∗<br />Abstract<br />Regression density estimation is the problem of flexibly estimating a response dis-<br />tribution as a function of covariates. An important approach to regression density<br />estimation uses mixtures of experts models and our article considers flexible mixtures<br />of heteroscedastic experts (MHE) regression models where the response distribution<br />is a normal mixture, with the component means, variances and mixture weights all<br />varying as a function of covariates. Our article develops fast variational approximation<br />methods for inference. Our motivation is that alternative computationally intensive<br />MCMC methods for fitting mixture models are difficult to apply when it is desired to<br />fit models repeatedly in exploratory analysis and model choice. Our article makes three<br />contributions. First, a variational approximation for MHE models is described where<br />the variational lower bound is in closed form. Second, the basic approximation can be<br />improved by using stochastic approximation methods to perturb the initial solution to<br />attain higher accuracy. Third, the advantages of our approach for model choice and<br />evaluation compared to MCMC based approaches are illustrated. These advantages<br />are particularly compelling for time series data where repeated refitting for one step<br />ahead prediction in model choice and diagnostics and in rolling window computations<br />is very common.<br />∗David J. Nott is Associate Professor, Department of Statistics and Applied Probability, National Uni-<br />versity of Singapore, Singapore 117546.(email standj@nus.edu.sg).<br />partment of Statistics and Applied Probability, National University of Singapore, Singapore 117546 (email<br />g0900760@nus.edu.sg). Mattias Villani is Professor, Division of Statistics, Dept. of Computer and Infor-<br />mation Science, Link¨ oping University, SE-58183 Link¨ oping, Sweden. (email: mattias.villani@liu.se). Robert<br />Kohn is Professor, Australian School of Business, University of New South Wales, Sydney 2052 Australia<br />(email r.kohn@unsw.edu.au).<br />Siew Li Tan is PhD student, De-<br />1</p>  <p>Page 2</p> <p>Keywords: Bayesian model selection, heteroscedasticity, mixtures of experts, stochastic ap-<br />proximation, variational approximation.<br />1 Introduction<br />Regression density estimation is the problem of flexibly estimating a response distribution<br />assuming that it varies smoothly as a function of covariates. An important approach to<br />regression density estimation uses mixtures of experts (ME) models and our article considers<br />flexible regression models where the response distribution is a normal mixture, with the<br />component means, variances and mixing weights all varying with covariates. The component<br />means and variances are described by a heteroscedastic linear model, and the component<br />weights by a multinomial logit model. These mixtures of heteroscedastic experts (MHE)<br />models extend conventional mixtures of experts models (Jacobs et al., 1991, Jordan and<br />Jacobs, 1994) by allowing the component models to be heteroscedastic. The terminology<br />mixtures of experts comes from machine learning, but it is frequently used in statistics as<br />well. The term “expert” refers to the individual mixture components and the term “gating<br />function” is used for the model for the mixing weights. Very often the experts are generalized<br />linear models, and such models (with or without covariate dependent mixing weights) are<br />sometimes referred to as mixtures of generalized linear models. In marketing, the mixture of<br />experts is sometimes referred to as the concomitant variable mixture regression model (see,<br />for example, Wedel, 2002).<br />The heteroscedastic extension of mixture of experts models is important in modern ap-<br />plications since simulations by Villani et al. (2009) showed that, when used to model het-<br />eroscedastic data, the performance of ME models with homoscedastic components deterio-<br />rates as the number of covariates increases, and there comes a point when their performance<br />cannot be improved simply by increasing the number of mixture components. They also<br />observed that with MHE models, fewer mixture components are required which makes the<br />estimation and interpretation of mixture models easier. In an analysis of the benchmark<br />LIDAR data set Li et al. (2010a) showed that an ME model with homoscedastic thin plate<br />components requires three components to achieve approximately the same performance as<br />an MHE model with a single thin plate component, providing further evidence that MHE<br />models help to reduce the number of mixture components required. Moreover, Villani et al.<br />(2009) demonstrated that the fit of an MHE model to homoscedastic data is comparable to<br />2</p>  <p>Page 3</p> <p>that of an ME model with homoscedastic components.<br />Our article makes three contributions. First, fast variational methods are developed<br />for fitting MHE models and a variational lower bound is obtained in closed form. Second,<br />stochastic approximation optimization techniques (see, for example, Spall, 2003) are used<br />to improve the variational approximation to attain higher accuracy. The computational<br />methods developed here can also be applied beyond the context of MHE models. Third, in<br />situations where it is necessary to fit a number of complex models to the data or if cross-<br />validation is used to select the best models, we demonstrate that variational approximation<br />methods provide an attractive alternative to MCMC methods, which may not be feasible due<br />to the computational complexity involved. For example, in model selection for complex time<br />series models, repeated one step ahead prediction is often carried out (Geweke and Amisano,<br />2010), with the parameters of each model re-estimated at each step. Another popular time<br />series procedure for studying model performance is to use a rolling window (Pesaran and<br />Timmermann, 2002). Suppose the data set consists of T observations, with each window of<br />fixed size M. In the first instance, each model is fitted to the first M observations. Next,<br />each model is fitted to observations 2 to M + 1, etc., with the final window consisting of<br />observations T −M +1 to T. Variational methods are ideally suited to this kind of repeated<br />refitting and can benefit from a “warm start” obtained from the previous fit. In Section<br />6.2 we quantify in an example the computational speed up that comes from initializing the<br />variational optimization based on the fit to the previous window rather than treating each<br />fit as an independent computation.<br />Use of variational approximations may be particularly compelling where the main focus<br />is predictive inference. Bayesian predictive inference is based on the predictive distribution<br />?<br />where y∗denotes a future response, y is the observed data and θ are parameters. There are<br />two components to our uncertainty in p(y∗|y). The first component is the inherent random-<br />ness in y∗that would occur even if θ were known: this is captured by the term p(y∗|θ,y) in<br />the integrand. The second component is related to parameter uncertainty and is captured<br />by the term p(θ|y). In general with large data sets the parameter uncertainty is small, and<br />an approximate treatment of p(θ|y) for predictive purposes may be attractive provided that<br />the approximate posterior provides good point estimation. In general, plugging in an ap-<br />proximate variational posterior instead of p(θ|y) in the expression for the predictive density<br />p(y∗|y) =p(y∗|θ,y)p(θ|y)dθ<br />3</p>  <p>Page 4</p> <p>can result in excellent predictive inference. Furthermore, this still accounts to some extent<br />for parameter uncertainty, hence improving on simple plug-in predictive density estimates.<br />Bayesian approaches to inference in mixtures of experts models were first considered in<br />Peng et al. (1996). Wood et al. (2002) and Wood et al. (2008) consider mixtures of experts<br />models with flexible terms for the covariates for continuous and binary responses respectively.<br />Geweke and Keane (2007) also take a Bayesian approach to inference in mixtures of experts<br />models with homoscedastic experts with a multinomial probit for the gating function which<br />allows a convenient Gibbs sampling MCMC scheme. MHE models have been considered<br />previously by Villani et al. (2009). They use a Bayesian approach to inference with MCMC<br />methods for computation and consider general smooth terms for the covariates and variable<br />selection in the mean and variance models and the gating function. Norets (2010) considers<br />approximation results for approximating quite general conditional densities in the Kullback-<br />Leibler sense using various kinds of normal mixtures.<br />derived and some interesting insights are obtained about when additional flexibility might<br />be most usefully employed in the mean, variance and gating functions. The approximation<br />results of Jiang and Tanner (1999) are concerned on the other hand with conditional densities<br />in a one parameter exponential family in which the expert components come from the same<br />exponential family. Their results are also useful for conditional densities which are discrete.<br />Methods related to mixtures of experts are currently under active development in the<br />area of Bayesian nonparametric approaches to regression density estimation. Rather than<br />considering a finite mixture of regressions it is possible to put a flexible prior on a mixing<br />distribution which varies over the space. For common priors the resulting models might be<br />considered to be mixtures of experts with an infinite number of components. There are both<br />advantages and disadvantages to this kind of approach. On the positive side, the difficult<br />question of model choice for the number of mixture components is avoided. However, a<br />finite mixture may be more interpretable and the nature of the model may be easier to<br />communicate to scientific practitioners. In addition, in the finite mixture framework it is<br />easier to incorporate some very natural extensions such as the expert components being<br />of qualitatively different types. We do not discuss these methods any further but refer to<br />MacEachern (1999), De Iorio et al. (2004), Griffin and Steel (2006) and Dunson et al. (2007)<br />and the references therein for a summary of relevant methodology and recent developments.<br />In terms of computational methodology, early approaches to fitting mixtures of experts<br />models (Jordan and Jacobs, 1994, for example) used maximum likelihood and the EM al-<br />gorithm.Modern Bayesian strategies for inference use MCMC computational methods,<br />Approximation error bounds are<br />4</p>  <p>Page 5</p> <p>although there are a number of authors who consider variational approaches similar to those<br />described in this paper (Waterhouse, MacKay and Robinson, 1996, Ueda and Ghahramani,<br />2002, Bishop and Svens´ en, 2003). However, these authors did not consider heteroscedastic<br />experts. Moreover, our paper also explores the advantages of variational methods in repeated<br />estimations of the model, as in model comparison by cross-validation or rolling window es-<br />timates to check for model stability. Outside the regression context, there are a number of<br />innovative approaches to model selection in fitting Gaussian mixture models which follow a<br />variational approach. Corduneanu and Bishop (2001) consider a variational lower bound on<br />the log marginal likelihood with all parameters, except the mixing coefficients, integrated<br />out, and estimate the mixing coefficients by maximizing the lower bound. This leads to some<br />of the coefficients being set to zero and an automated model selection approach. McGrory<br />and Titterington (2007) consider a variational extension of the DIC criterion (Spiegelhal-<br />ter et al., 2002) and a variational optimization technique where the algorithm is initialised<br />with a large number of components and mixture components whose weightings become suf-<br />ficiently small are dropped out as the optimization proceeds. Blei and Jordan (2006) have<br />considered variational approximation for Dirichlet process mixture models. Recently Wu et<br />al. (2011) have considered variational methods for fitting mixtures for data which require<br />models with narrow widely separated mixture components. They discuss sophisticated split<br />and merge algorithms, building on earlier related methods such as those of Ghahramani and<br />Beal (2000), Ueda and Ghahramani (2002) and Constantinopoulos and Likas (2007), for<br />simultaneous model selection and parameter estimation as well as novel criteria for model<br />evaluation.<br />One contribution of our paper is the development of broadly applicable stochastic approx-<br />imation correction methods for variational approximation. From our simulation studies, the<br />stochastic approximation correction is very helpful for getting an improved approximation<br />and requires less computation time than MCMC methods. Ji, Shen and West (2010) recently<br />proposed similar stochastic approximation methods for learning variational approximations<br />but we offer a number of improvements on their implementation. In particular, we are able<br />to suggest an improved gradient estimate compared to their approach and consider a strat-<br />egy of perturbing only the mean and scale in an initial variational approximation obtained<br />using closed form updates. Perturbing an existing solution allows us to keep the dimension<br />of the optimization low which is important for a fast and stable implementation. Ji et al.<br />(2010) suggest a number of other innovative ideas including the use of MCMC methods to<br />obtain computationally attractive upper and lower bounds on the marginal likelihood.<br />5</p>  <p>Page 6</p> <p>The paper is organized as follows. Section 2 introduces the mixture of heteroscedastic<br />experts model. Section 3 describes fast variational approximation methods for MHE models<br />and Section 4 uses of variational methods in model choice. Section 5 discusses improvements<br />on the basic approximation using a stochastic approximation correction which also integrates<br />out the mixture component indicators from the posterior. Section 6 considers examples<br />involving both real and simulated data and Section 7 concludes.<br />2 Heteroscedastic mixtures of experts models<br />Suppose that responses y1,...,ynare observed. They are modelled by a mixture of experts<br />model (Jacobs et al., 1991, Jordan and Jacobs, 1994) of the form<br />yi|δi,β,α ∼ N(xT<br />iβδi,exp(αT<br />δizi))<br />where δiis a categorical latent variable with k categories, δi∈ {1,...,k}, xi= (xi1,...,xip)T<br />and zi= (zi1,...,zim)Tare vectors of covariates for observation i, and βj= (βj1,...,βjp)Tand<br />αj= (αj1,...,αjm)T, j = 1,...,k are vectors of unknown parameters. The above model says<br />that conditional on δi = j, the response follows a heteroscedastic linear model where the<br />mean is xT<br />iβjand the log variance is zT<br />iαj. The prior for the latent variables δiis<br />P(δi= j|γ) = pij=<br />exp(γT<br />?k<br />jvi)<br />l=1exp(γT<br />lvi), j = 1,...,k<br />where vi = (vi1,...,vir)Tis a vector of covariates and γj = (γj1,...,γjr)Tare vectors of<br />unknown parameters, j = 2,...,k with γ1set to be identically zero for identifiability. With<br />this prior the responses are modelled as a mixture of heteroscedastic linear regressions,<br />where the mixture weights vary with the covariates. For Bayesian inference we require prior<br />distributions on the remaining unknown parameters in the model. Independent priors are<br />assumed for the βj, βj∼ N(µ0<br />for γ = (γT<br />in the above model. Variational inference for mixtures of experts models has been considered<br />before (Waterhouse, MacKay and Robinson, 1996, Ueda and Ghahramani, 2002, Bishop and<br />Svens´ en, 2003) but not for the case of heteroscedastic mixture components. In the case of<br />heteroscedastic expert components a variational lower bound can still be computed in closed<br />form, allowing fast computation.<br />βj,Σ0<br />γ,Σ0<br />βj), j = 1,...,k, for αj, αj∼ N(µ0<br />γ). We will describe fast methods for variational inference<br />αj,Σ0<br />αj), j = 1,...,k, and<br />2,...,γT<br />k)T, γ ∼ N(µ0<br />6</p>  <p>Page 7</p> <p>3Variational approximation<br />We consider a variational approximation to the joint posterior distribution of all the param-<br />eters θ of the form q(θ|λ) where λ is a set of variational parameters to be chosen. Variational<br />approximation methods originated in statistical physics, and they have been widely used<br />in the machine learning community for some time. Jordan et al. (1999) is an early refer-<br />ence and Bishop (2006, Chapter 10) gives a recent summary. Ormerod and Wand (2009)<br />give an introduction to variational approximation methods that is particularly accessible to<br />statisticians. Variational approximation is a very active area of research in both statistics<br />and machine learning. For additional discussion of the current state of the art in relation<br />to mixture models see the references in Section 1. Here a parametric form is chosen for<br />q(θ|λ) (described below) and we attempt to make q(θ|λ) a good approximation to p(θ|y) by<br />minimizing the Kullback-Leibler divergence between q(θ|λ) and p(θ|y), i.e.,<br />?<br />where p(y) is the marginal likelihood p(y) =?p(y|θ)p(θ)dθ. Note that since the Kullback-<br />Leibler divergence is positive, we have<br />?<br />logq(θ|λ)<br />p(θ|y)q(θ|λ)dθ =<br />?<br />log<br />q(θ|λ)<br />p(θ)p(y|θ)q(θ|λ)dθ + logp(y)(1)<br />logp(y) ≥<br />logp(θ)p(y|θ)<br />q(θ|λ)<br />q(θ|λ)dθ(2)<br />which gives a lower bound on the log marginal likelihood, and maximizing the lower bound is<br />equivalent to minimizing the Kullback-Leibler divergence between the posterior distribution<br />and the variational approximation. From (1), the difference between the lower bound and<br />the log marginal likelihood is the Kullback-Leibler divergence between the posterior and<br />variational approximation, and the lower bound is sometimes used as an approximation to<br />the log marginal likelihood for Bayesian model selection purposes. We discuss the role of the<br />marginal likelihood in Bayesian model comparison later.<br />The difference between our development of a variational approximation for the MHE<br />model and previous developments of variational methods for ME models with homoscedastic<br />experts lies both in the fact that a closed form derivation of the variational lower bound<br />is not obvious in the heteroscedastic case, as well as in the need to deal with the variance<br />parameters in the heteroscedastic linear expert models in the optimization. Optimization of<br />variational parameters for the variational posterior factors cannot be done in closed form,<br />and since numerical optimization over high-dimensional covariance matrices can be time<br />7</p>  <p>Page 8</p> <p>consuming we have developed an approximate method for dealing with these parameters that<br />is computationally efficient and effective in practice. For the model of Section 2, we write the<br />parameters as δ = (δ1,...,δn)T, β = (βT<br />that θ = (δT,βT,αT,γT)T. For convenience we write our variational approximation q(θ|λ) as<br />q(θ), suppressing dependence on λ in the notation. We consider a variational approximation<br />to the posterior of the form q(θ) = q(δ)q(β)q(α)q(γ), where<br />1,...βT<br />k)T, α = (αT<br />1,...αT<br />k)Tand γ = (γT<br />2,...,γT<br />k)Tso<br />q(δ) =<br />n<br />?<br />i=1<br />q(δi),q(β) =<br />k?<br />i=1<br />q(βi),q(α) =<br />k?<br />i=1<br />q(αi)<br />and q(βi) is normal, N(µq<br />i = 1,...,k, q(γ) is a delta function placing point mass of 1 on µq<br />j) = qij where?k<br />posterior that parameters for different expert components are independent and independent<br />of all other parameters, that mean and variance parameters are independent, and that the<br />latent variables δ are independent of each other and independent of all other parameters. We<br />assumed a degenerate point mass variational posterior for γ in order to make computation<br />of the lower bound tractable. However, following our description below of the variational<br />algorithm which uses the point mass form for q(γ) we also suggest a method for relaxing the<br />form of q(γ) to be a normal distribution. The assumption of independence between mean and<br />variance parameters can also be relaxed (John Ormerod, personal communication) but this<br />also makes the variational optimization slightly more complex. Although the independence<br />and distributional assumptions made in variational approximations are typically unrealistic,<br />it is often found that variational approaches give good point estimates, reasonable estimates<br />of marginal posterior distributions and excellent predictive inferences compared to other<br />approximations, particularly in high dimensions. For example, Blei and Jordan (2005),<br />Section 5, demonstrate in the context of Dirichlet process mixture models that predictive<br />inference based on a variational approximation is similar to fully Bayes predictive inference<br />implemented via MCMC. Braun and McAuliffe (2010), Section 4, report similar findings in<br />large-scale models of discrete choice.<br />Here we are considering γ as a fixed point estimate and if we write θ−γ for the rest of<br />the unknown parameters (that is, not including γ) then a lower bound on logp(y|γ) where<br />βi,Σq<br />βi) say for i = 1,...,k, q(αi) is normal, N(µq<br />αi,Σq<br />γ) and q(δi=<br />αi) say for<br />γ, q(γ) = δ(γ−µq<br />j=1qij = 1, i = 1,...,n, j = 1,...,k. We are assuming in the variational<br />8</p>  <p>Page 9</p> <p>p(y|γ) =?p(θ−γ|γ)p(y|θ)dθ−γis<br />?<br />logp(θ−γ|γ)p(y|θ)<br />q(θ−γ)<br />q(θ−γ)dθ−γ.<br />The lower bound can be computed in closed form, and this gives a lower bound on supγlogp(γ)p(y|γ)<br />of (see the supplementary materials)<br />L = −n<br />2log2π +(p + m)k<br />2<br />+ logp(µq<br />γ) −1<br />2<br />k<br />?<br />j=1<br />log|Σ0<br />βj| −1<br />2<br />k<br />?<br />j=1<br />tr(Σ0<br />βj<br />−1Σq<br />βj)<br />−1<br />2<br />k<br />?<br />k<br />?<br />n<br />?<br />j=1<br />(µq<br />βj− µ0<br />βj)TΣ0<br />βj<br />−1(µq<br />βj− µ0<br />βj) −1<br />2<br />k<br />?<br />n<br />?<br />j=1<br />log|Σ0<br />αj| −1<br />2<br />k<br />?<br />+1<br />j=1<br />tr(Σ0<br />αj<br />−1Σq<br />αj)<br />−1<br />2<br />j=1<br />(µq<br />αj− µ0<br />?<br />αj)TΣ0<br />αj<br />−1(µq<br />αj− µ0<br />αj) +<br />i=1<br />k<br />?<br />iΣq<br />iΣq<br />j=1<br />qijlogpij<br />qij<br />2<br />k<br />?<br />j=1<br />log|Σq<br />βj| +1<br />2<br />k<br />?<br />j=1<br />log|Σq<br />αj|<br />−<br />i=1<br />k<br />?<br />j=1<br />qij<br />1<br />2zT<br />iµq<br />αj+1<br />2<br />(yi− xT<br />exp(zT<br />iµq<br />iµq<br />βj)2+ xT<br />αj−1<br />βjxi<br />αjzi)<br />2zT<br />?<br />.(3)<br />Here p(µq<br />The variational parameters to be optimized consist of µq<br />qijfor i = 1,...,n, j = 1,...,k. We optimize the lower bound with respect to each of these<br />sets of parameters with the others held fixed in a gradient ascent algorithm. The updates<br />which are available in closed form are easy to derive using vector differential calculus (see,<br />for example, Wand, 2002). To initialize the algorithm, we first generate an initial clustering<br />of the data. Then for this initial clustering we do the following.<br />γ) is the prior distribution for γ evaluated at µq<br />γand pijis evaluated setting γ = µq<br />βj, Σq<br />γ.<br />βj, µq<br />αj, Σq<br />αj, j = 1,...,k, µq<br />γand<br />Algorithm 1:<br />Initialize: µq<br />j and 0 otherwise.<br />Do until the change in the lower bound between iterations is less than a tolerance:<br />?<br />?<br />αj= Σq<br />αj= 0 for j = 1,...,k and qijas 1 if the ith observation lies in cluster<br />• Σq<br />tor of responses and Djis the diagonal matrix with ith diagonal entry qij/exp?zT<br />• µq<br />βj←<br />XTDjX + Σ0<br />βj<br />−1?−1<br />where X is the design matrix with ith row xT<br />i, y is the vec-<br />iµq<br />αj− 1/2zT<br />iΣq<br />αjzi<br />?.<br />βj← Σq<br />βj<br />Σ0<br />βj<br />−1µ0<br />βj+ XTDjy<br />?<br />.<br />9</p>  <p>Page 10</p> <p>• Set µq<br />fixed at current values. As a function of µq<br />a generalized linear model with normal prior N(µ0<br />(yi−xT<br />is zT<br />mode has no closed form expression it is easily found by an iteratively weighted least<br />squares approach (McCullagh and Nelder, 1989, West, 1985) or some other numerical<br />optimization technique.<br />?<br />and Wj is diagonal with ith diagonal element qijwijexp(−zT<br />done provided that the replacement leads to an improvement in the lower bound.<br />αjto be the conditional mode of the lower bound with other variational parameters<br />αjthe lower bound is the log posterior for<br />αj,Σ0<br />αj), gamma responses wij =<br />iµq<br />αj− 1/2zT<br />βj)2+xT<br />iΣq<br />iΣq<br />βjxi, coefficients of variation?2/qijand where the log of the mean<br />iµq<br />αjziwhere the terms −1/2zT<br />iΣq<br />αjzidefine an offset. Although the<br />• Σq<br />αj←<br />ZTWjZ + Σ0<br />αj<br />−1?−1<br />where Z is the design matrix with ith row zT<br />i, i = 1,...,n<br />iµq<br />αj)/2. The update is<br />• For i = 1,...,n,<br />qij←<br />pijexp<br />?<br />−1<br />?<br />2zT<br />iµq<br />αj−1<br />2<br />(yi−xT<br />exp(zT<br />iµq<br />iµq<br />βj)2+xT<br />αj−1<br />iµq<br />iµq<br />iΣq<br />iΣq<br />βjxi<br />αjzi)<br />2zT<br />?<br />?k<br />l=1pilexp<br />−1<br />2zT<br />iµq<br />αl−1<br />2<br />(yi−xT<br />exp(zT<br />βl)2+xT<br />αl−1<br />iΣq<br />iΣq<br />βlxi<br />αlzi)<br />2zT<br />?<br />• Set µq<br />at their current values. As a function of µq<br />additive constants) logp(µq<br />in µq<br />prior on µq<br />regression only one component of this pseudo-response vector would be 1 with the<br />other terms 0 and although this is not the case here the usual iteratively weighted<br />least squares algorithm (or some other numerical optimization algorithm) can be used<br />for finding the mode.<br />γto be the conditional mode of the lower bound fixing other variational parameters<br />γ, the lower bound is (ignoring irrelevant<br />γ)+?n<br />i=1<br />?k<br />j=1qijlogpijwhere pijis computed here plugging<br />γfor γ. This is the log posterior for a Bayesian multinomial regression with normal<br />γand where the ith response is (qi1,...,qik)T.In a typical multinomial<br />Algorithm 1 requires an initial clustering in order to initialize the parameters. We con-<br />sider multiple clusterings in order to deal with the problem of multiple modes in the optimiza-<br />tion. We consider 20 random clusterings where the mixture component for each observation<br />is chosen uniformly at random. For these 20 clusterings we do short runs of Algorithm 1<br />with a very loose stopping criterion (we stop when the increase in the lower bound is less<br />than 1) and only follow the most promising solution with the highest attained value of the<br />lower bound to convergence. This strategy of “short runs” to identify a promising solution<br />10</p>  <p>Page 11</p> <p>to follow to full convergence is similar to one recommended for initialization of the EM<br />algorithm for maximum likelihood estimation of mixture models by Biernacki, Celeux and<br />Govaert (2003). Variational approaches to fitting mixture models such as those of McGrory<br />and Titterington (2007) have made use of the fact that mixture components tend to drop out<br />as fitting proceeds in order to do selection of the number of mixture components. This com-<br />ponent elimination feature is something that can happen with our algorithm also, although<br />this is dependent on the initial clustering used.<br />Unlike the ME model with homoscedastic components (Bishop and Svens´ en, 2003), not<br />all the parameters have closed form updates. In steps 1, 2 and 5 above we are able to<br />optimize the lower bound with respect to the parameter in closed form. However, in steps 3<br />and 6 we need to use an iterative method, and in step 4 we have used an approximation and<br />this update step is skipped if it does not improve the lower bound (3). Motivation for the<br />approximation at step 4 comes from the following: suppose the term q(αj) in our variational<br />posterior is not a normal distribution, but instead is not subject to any restriction. The<br />optimal choice for this term for maximizing the lower bound with other terms held fixed is<br />(see, for instance, Ormerod and Wand, 2009)<br />q(αj) ∝ exp{E(logp(θ)p(y|θ))}<br />(4)<br />where the expectation in the exponent is with respect to the variational posterior for all<br />parameters except αj. The expectation in (4) takes the form, apart from additive constants<br />not depending on αj, of the log posterior for a gamma generalized linear model (the same<br />model as considered in step 3 above, except that the offset term in the mean model is<br />omitted). If µq<br />the mean as µq<br />µq<br />with the ith diagonal element of Wjdefined as in step 4 above. Similar reasoning was used<br />by Waterhouse, Mackay and Robinson (1996) in approximating the posterior distribution<br />for the gating function parameters in the homoscedastic mixture of experts model. A referee<br />has pointed out that convergence of the variational Bayes algorithm can be very slow when<br />parameters are highly correlated between the blocks used in the variational factorization.<br />This might occur, for example, when there are two very similar mixture components. We<br />don’t see any easy solutions to this problem. One possible remedy consists of integrating<br />out the mixture indicators and using bigger blocks for the remaining parameters in the<br />blockwise gradient ascent, but this would involve a much greater computational burden and<br />αjis close to the mode, we can get a normal approximation to (4) by taking<br />αjand the covariance matrix as the negative inverse Hessian of the log of (4) at<br />αj. The inverse of the negative Hessian evaluated at µq<br />αjis of the form (ZTWjZ +Σ0<br />αj<br />−1)−1<br />11</p>  <p>Page 12</p> <p>the introduction of new approximations to the variational lower bound.<br />At convergence we also replace the delta function variational posterior for γ with a<br />normal approximation, in which the mean is µq<br />of the negative Hessian of the Bayesian multinomial log posterior considered in step 6 at<br />convergence. The justification for this is similar to our justification above for the update of<br />Σq<br />idea but they use such an approximation at every step of their iterative algorithm whereas<br />we use only a one-step approximation after first using a delta function approximation to the<br />posterior distribution for γ. Write Σq<br />for γ. With this variational posterior the variational lower bound on logp(y) is the same as<br />(3), except we need to replace?n<br />n<br />?<br />−1<br />γand the covariance matrix is the inverse<br />αjin step 4 of Algorithm 1. Waterhouse, Mackay and Robinson (1996) outline a similar<br />γfor the covariance matrix of the variational posterior<br />i=1<br />?k<br />j=1qijlogpij+ logp(µq<br />γ) with<br />i=1<br />k<br />?<br />j=1<br />qijE<br />?<br />log<br />?<br />exp(vT<br />?k<br />2tr<br />iγj)<br />l=1exp(vT<br />iγl)<br />?<br />??<br />−1<br />2log|Σ0<br />γ| −1<br />2(µq<br />γ− µ0<br />γ)TΣ0<br />γ<br />−1(µq<br />γ− µ0<br />γ)<br />?<br />Σ0<br />γ<br />−1Σq<br />γ<br />+1<br />2log|Σq<br />γ| +r(k − 1)<br />2<br />.<br />The only terms here not in closed form are the expectations in the first term. For the<br />purposes of defining a quantity which might be used as an approximation for logp(y) we<br />replace<br />?<br />E log<br />?<br />exp(vT<br />?k<br />iγj)<br />l=1exp(vT<br />iγl)<br />??<br />with log<br />?<br />exp(vT<br />?k<br />iµq<br />γj)<br />iµq<br />l=1exp(vT<br />γl)<br />?<br />where µq<br />observe that variational posterior approximations tend to “lock on” to a single mode of the<br />posterior distribution in mixture models where there are many equivalent modes. Since the<br />gap between the variational lower bound and the log marginal likelihood is the Kullback-<br />Leibler divergence between the variational posterior and the true posterior, the failure to<br />approximate all modes of the true posterior leads to underestimation of the log marginal<br />likelihood by the lower bound. See Bishop (2006) for further discussion. There are related<br />concerns with some MCMC methods for estimating the marginal likelihood (see, for example,<br />Fr¨ uhwirth-Schnatter, 2004, and the references therein). An adjustment to the optimized<br />lower bound to allow for the local nature of the posterior approximation when estimating<br />the marginal likelihood might be considered. If the k! different modes from relabelling are<br />well separated then adding logk! would be a reasonable adjustment. However our experience<br />with this approach when k is large is fairly negative, in the sense that the resulting adjusted<br />γjis the subvector of µq<br />γcorresponding to γj, j = 2,...,r.As a final note we<br />12</p>  <p>Page 13</p> <p>lower bound doesn’t provide a good approximation to the true log marginal likelihood useful<br />for model comparison: the logk! correction tends to be too large when modes overlap and<br />we usually don’t attempt any adjustment. In the examples of Section 6 we do not emphasize<br />the use of the marginal likelihood for model choice or attempt to support the claim that the<br />lower bound estimates the log marginal likelihood accurately.<br />We also note that for very large data sets, since the posterior is of the same form as the<br />prior for (β,α,γ), it is easy to implement our algorithm sequentially after splitting the data<br />set up into smaller chunks. One can learn a variational posterior approximation using the<br />data for the first chunk, and then this can be used as the prior for processing the next chunk<br />and so on. There may be difficulties with the naive implementation of this idea, however,<br />as the learning may get stuck in a local mode corresponding to the first reasonable solution<br />found. Honkela and Vapola (2003) present an on-line version of variational Bayesian learning<br />based on maintaining a decaying history of previous samples processed by the model which<br />ensures that the system is able to forget old solutions in favour of new better ones.<br />4Variational approximation and model choice<br />4.1 Cross-validation<br />Marginal likelihood is a popular approach to model selection in a Bayesian context. However,<br />we do not use this approach in our article because in our computations we only have upper<br />and lower approximations to the marginal likelihood, and the accuracy of such approxima-<br />tions may be very problem de- pendent. In this section we briefly outline how we carry out<br />model selection using likelihood cross-validation. In B-fold cross-validation we split the data<br />randomly into B roughly equal parts, and then B different training sets are constructed,<br />T1,...,TB by successively leaving out one of the B parts from the complete data set. The<br />corresponding test sets (the parts of the data which are left out in each case to construct<br />the training set) are denoted F1,...,FB. Then one useful measure of predictive performance<br />that can be used for model choice, the log predictive density score (LPDS), is<br />LPDS =1<br />B<br />B<br />?<br />i=1<br />logp(yFi|XFi,yTi).<br />Note that<br />logp(yF|XF,yT) = log<br />?<br />p(yF|XF,θ)p(θ|yT)dθ<br />13</p>  <p>Page 14</p> <p>where θ denotes all the unknown parameters. We have assumed here that yF and yT are<br />conditionally independent given θ. This usually does not hold for time series data and<br />modified approaches are appropriate for that case, as we discuss below. In the mixture of<br />experts context p(yF|XF,θ) is easy to write down as a normal mixture given the parameters,<br />and we replace p(θ|y) with our variational approximation q(θ). To approximate the integral<br />we generate Monte Carlo samples θi, i = 1,...,S, from q(θ) and then take the average of the<br />values p(yF|XF,θi). In later examples we use S = 1000.<br />4.2Model choice in time series<br />Later we consider autoregressive time series models in the form of MHE models, and in the<br />time series context the cross-validation approach described above is not very natural. Both<br />Geweke and Keane (2007) and Li et al. (2010b) consider a training set y≤T= (y1,...,yT) of<br />T initial observations and then measure predictive performance by the logarithmic score for<br />the subsequent T∗observations y&gt;T= (yT+1,...,yT+T∗). That is, predictive performance for<br />the purpose of model comparison is measured by<br />logp(y&gt;T|y≤T) =<br />T∗<br />?<br />i=1<br />logp(yT+i|y≤T+i−1)(5)<br />where<br />p(yT+i|y≤T+i−1) =<br />?<br />p(yT+i|θ,y≤T+i−1)p(θ|y≤T+i−1)dθ (6)<br />where p(θ|y≤T+i−1) denotes the posterior distribution for all unknowns θ based on data at<br />time T +i−1. Note that (5) contains T∗terms and that from (6) each of these terms involves<br />consideration of a different posterior distribution as successive points from the validation set<br />are added to the observed data. Geweke and Keane (2007) note that the most reliable and<br />efficient way to compute these T∗terms is to run an MCMC sampler separately for each of the<br />T∗terms to estimate the required posterior distribution. This is extremely computationally<br />demanding and if T∗is large, and if convergence of the MCMC scheme is slow, this may<br />be completely infeasible. While one might consider importance sampling ideas to reuse the<br />MCMC samples for successive terms such an idea is very difficult to implement reliably (see,<br />for example, Vehtari and Lampinen, 2002, for discussion). Li et al. (2010b) consider a<br />similar approach to Geweke and Keane (2007) for model choice and an approximation where<br />p(θ|y≤T) is used instead of p(θ|y≤T+i−1) in (6). They presented some empirical evidence for<br />14</p>  <p>Page 15</p> <p>the accuracy of this approach by comparison with a scheme where the posterior was updated<br />sequentially every 100th observation in a financial time series example.<br />We note that our variational approach is very efficient for implementing sequential updat-<br />ing. Apart from the fact that variational approximation is faster than MCMC to begin with,<br />in the time series context the result of the variational optimization from the last time step<br />can be used to initialize the optimization for the current time step so that the convergence<br />time of the variational scheme is generally small. This makes variational approaches ideally<br />suited to model choice based on one step ahead predictions and the logarithmic score for<br />time series data.<br />5Improving the basic approximation<br />It is well known that variational approximations can underestimate the variance of the pos-<br />terior in the context of mixture models. For example, Wang and Titterington (2005) showed<br />in the case of Gaussian mixtures that the covariance matrices from Variational Bayes approx-<br />imation are too small compared to those obtained by asymptotic arguments from maximum<br />likelihood estimation. Here, we propose a novel approach to improve the estimates obtained<br />from variational approximation using stochastic approximation (SA) which can result in im-<br />proved approximation of the posterior with reduced computational cost compared to MCMC.<br />We proceed as follows. First, we integrate out the latent variables δithereby relaxing the<br />assumption that the latent variables are independent of other parameters in the variational<br />approximation and allowing for an improvement. In approximating the marginal posterior<br />with the indicators integrated out we fix the posterior correlation structure to that obtained<br />from Algorithm 1, and consider a variational optimization over the choice of the mean and<br />variance in the variational posterior. Fixing the posterior correlation structure keeps the di-<br />mension of the optimization problem low. Ji et al. (2010) independently proposed a Monte<br />Carlo stochastic approximation for maximizing the lower bound numerically which uses a<br />similar approach. However, we offer a number of improvements on their implementation<br />which take the form of an improved gradient estimate in the SA procedure, and the idea of<br />perturbing only the mean and scale of an initial variational approximation obtained by the<br />approach of Algorithm 1.<br />Consider once more the general setting with an unknown θ to learn about, prior p(θ),<br />likelihood p(y|θ) and variational approximation q(θ|λ) that comes from some parametric<br />family where λ are parameters to be chosen. We will first develop a stochastic approximation<br />15</p>  <p>Page 16</p> <p>algorithm to maximize the lower bound on the log marginal likelihood. The ideas we describe<br />here are useful quite generally and are not specific to the mixtures context. From (2), the<br />lower bound is<br />L(λ) =<br />?<br />q(θ|λ)logp(θ)p(y|θ)<br />q(θ|λ)<br />dθ.<br />Maximizing this lower bound with respect to λ is equivalent to the problem of finding at<br />least one root λ∗such that g(λ) ≡<br />available, the root-finding SA algorithm introduced by Robbins and Monro (1951) may be<br />used for finding λ∗and one of the conditions for the algorithm to converge is for the noise<br />to have mean zero. Since the lower bound is an expectation with respect to q(θ|λ), unbiased<br />measurements of g(λ) at any λ may be computed provided it is valid to interchange the<br />derivative<br />∂<br />∂λL(λ) = 0. When noisy measurements of g(λ) are<br />∂<br />∂λand the integral. In this case,<br />g(λ) =<br />?<br />log<br />?p(θ)p(y|θ)<br />q(θ|λ)<br />?∂ logq(θ|λ)<br />∂λ<br />q(θ|λ)dθ<br />since<br />?<br />∂ logq(θ|λ)<br />∂λ<br />q(θ|λ)dθ = 0.<br />An unbiased estimate of the gradient g(λ) is thus<br />?<br />log<br />?p(θ?)p(y|θ?)<br />q(θ?|λ)<br />?<br />− c<br />?∂ logq(θ?|λ)<br />∂λ<br />(7)<br />where θ?∼ q(θ|λ) and c can be chosen arbitrarily. Now note that<br />logp(y) = logp(θ)p(y|θ)<br />p(θ|y)<br />,<br />for every θ. This suggests that if q(θ|λ) is a good approximation to p(θ|y), (as it might be<br />near the optimal value of λ) then the term<br />?<br />log<br />?p(θ?)p(y|θ?)<br />q(θ?|λ)<br />?<br />− c<br />?<br />in the gradient estimate is nearly constant and equal to logp(y)−c. In this case this would<br />make the variance of the gradient estimate, when λ is near the optimal value, roughly equal<br />to<br />(logp(y) − c)2Var<br />?∂ logq(θ?|λ)<br />∂λ<br />?<br />.<br />16</p>  <p>Page 17</p> <p>This suggests that when λ is close to the optimal value taking c close to logp(y) is a good<br />choice. In our application to mixtures, for the sequence of gradient estimates constructed<br />in the stochastic approximation procedure we initialize c to be the variational lower bound<br />obtained from Algorithm 1 and then update it as the algorithm proceeds. This is described<br />more precisely below. Ji et al. (2010) considered a similar approach but they effectively<br />use c = 1, obtained by differentiating directly under the integral sign. From our experience,<br />choosing c = 1 is usually suboptimal, but they counteract the variability in the gradient<br />estimate by using multiple simulations from q(θ|λ). Clearly from the above arguments if<br />logp(y) is large in magnitude, c = 1 could result in a gradient estimate with very high<br />variance (since the factor (logp(y) − 1)2is large), and this is supported by simulations we<br />have conducted (results not shown).<br />With an unbiased estimate of the gradient, a stochastic gradient algorithm can now be<br />used for optimizing the lower bound. Let λ(0)be some initial estimate of λ. We consider the<br />following algorithm:<br />Algorithm 2:<br />For k = 0,1,2,...<br />• Simulate θ(k)∼ q(θ|λ(k)).<br />• Set<br />λ(k+1)<br />= λ(k)+ akH(λ(k)) (8)<br />where H(λ(k)) is an unbiased estimate of the gradient g(λ(k)).<br />Under regularity conditions (Spall,2003), the λ(k)will converge to a local maximum of<br />the lower bound. The ak, k ≥ 0, are a sequence satisfying the conditions<br />?<br />ak→ 0<br />k<br />ak= ∞<br />?<br />k<br />a2<br />k&lt; ∞.<br />In particular, it is important to balance the gain sequence ak so that it goes to zero fast<br />enough to damp out noise effects when optimal λ is close, but not too fast to avoid false<br />convergence. The step (8) is a stochastic version of a gradient ascent algorithm where the<br />step sizes decrease according to the sequence ak.<br />An estimate of the lower bound on the log marginal likelihood from the stochastic ap-<br />17</p>  <p>Page 18</p> <p>proximation iterates is<br />1<br />N − N0<br />N<br />?<br />i=N0+1<br />logp(θ(i))p(y|θ(i))<br />q(θ(i)|λ(i))<br />which involves negligible additional computation after running the recursion (8). Here N<br />is the total number of iterations (this is often a fixed number based on our computational<br />budget but should be large enough to ensure convergence) and N0 is an initial number<br />of iterates to discard where we are not yet close to the optimal solution. The stochastic<br />approximation algorithm is easy to implement provided that q(θ|λ) is easy to simulate from.<br />In our gradient estimate there is a constant c that we have argued should be chosen to<br />be an estimate of the log marginal likelihood. We initialize c as the estimate of the log<br />marginal likelihood from Algorithm 1 and at iteration k &gt; 1 of Algorithm 2 we use the<br />above stochastic approximation log marginal likelihood estimate for c with N0 = 0 and<br />N = k − 1. The stochastic approximation algorithm for our mixture model is described in<br />more detail in the supplementary materials.<br />6 Examples<br />6.1Emulation of a rainfall runoff model<br />Our first example is concerned with emulation of a deterministic rainfall runoff model, the<br />Australian Water Balance Model (AWBM) (Boughton, 2004). The goal of model emulation<br />for a deterministic model is to develop a computationally cheap statistical surrogate (the<br />emulator) for the original model for some characteristic of the model output of interest. For<br />applications where the deterministic model is expensive to run and where we wish to run the<br />model many times (in model calibration for example) replacing the deterministic model by<br />the emulator may allow similar results to be achieved with an order of magnitude reduction<br />in computation time. Here we will be concerned with using a mixture of experts model to<br />emulate the AWBM streamflow response at a time of peak rainfall input (the response y)<br />as a function of the three AWBM model parameters (the covariates). For an overview of<br />the statistical analysis of computer models and model emulation see O’Hagan (2006). In the<br />statistical literature Gaussian process models which interpolate model output are often used<br />for construction of emulators, but it is often recommended to include an independent noise<br />term in such models (Pepelyshev, 2010).<br />The AWBM uses input time series of rainfall and evapotranspiration data to produce<br />18</p>  <p>Page 19</p> <p>estimates of catchment streamflow and is one of the most widely used rainfall-runoff mod-<br />els in Australia for applications such as estimating catchment water yield or design flood<br />estimation. The model has three parameters - the maximum storage capacity S, the base<br />flow index BFI and baseflow recession factor K. We have available model simulations for<br />approximately eleven years of average monthly potential evapotranspiration and daily rain-<br />fall data for the Barrington River catchment, located in north eastern New South Wales in<br />Australia. The model was run for 500 different values of the parameters (S,K,BFI) which<br />were generated according to a maximin Latin hypercube design. Our goal here is to emulate<br />the streamflow response of the AWBM at a fixed time (the time of peak input rainfall) as<br />a function of the parameters S and K (the model output at this time is fairly insensitive<br />to the value of the BFI). We use a mixture of experts model as an emulator where the<br />response is the AWBM output at the time of peak input rainfall (y), and the predictors are<br />S (x1) and K (x2). We have added a small amount of independent normal random noise<br />with a standard deviation 0.01 to the response y to avoid degeneracies in the variance model<br />in regions of the space where the response tends to be identically zero.<br />We considered fitting five models to the data. The first four models are MHE models<br />with both predictors in the mean and variance models. The models differ according to the<br />number of mixture components with models A, B, C and D in Table 1 having respectively 2,<br />3, 4 and 5 mixture components. Model E in Table 1 is a model with four mixture components<br />but with only an intercept in the variance model for the experts (a homoscedastic mixture<br />of experts). For our normal prior distributions we have used, in the notation of Section 2,<br />µ0<br />vectors and covariance matrices have the appropriate dimensions depending on the model<br />fitted. The estimated log marginal likelihoods (from the variational lower bound) for the<br />models fitted are given in Table 1, showing a clear preference for model C, the 4 component<br />MHE model. However, it is not our goal in this work to support model choice based on the<br />lower bound and we discuss further below model choice via the LPDS.<br />Figure 1 summarizes the fitted model with 4 heteroscedastic mixture components. Here<br />we have separated the observations into clusters according to which mixture component<br />each observation is most likely to belong to, and plotted observations for each cluster to-<br />gether with the fitted mean and standard deviation for each mixture component. Different<br />rows correspond to different mixture components. We have emphasized that an important<br />advantage of fast variational approaches to inference is the ability to fit many models for<br />model assessment and exploratory analysis. For instance, it is very difficult to use cross-<br />βj= 0, Σ0<br />βj= 10000 · I, µ0<br />αj= 0, Σαj = 100 · I, µ0<br />γ= 0 and Σ0<br />γ= 100 · I where mean<br />19</p>  <p>Page 20</p> <p>0.0 0.20.40.6 0.81.0<br />  0<br /> 20<br /> 40<br /> 60<br /> 80<br />100<br />120<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />X1<br />X2<br />y<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0 0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />0<br />50<br />100<br />X1<br />X2<br />Standard Deviation<br />0.00.2 0.4 0.60.81.0<br />  0<br /> 20<br /> 40<br /> 60<br /> 80<br />100<br />120<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />X1<br />X2<br />y<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0 0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />0<br />20<br />40<br />60<br />80<br />100<br />120<br />X1<br />X2<br />Standard Deviation<br />0.00.20.40.60.8 1.0<br />  0<br /> 20<br /> 40<br /> 60<br /> 80<br />100<br />120<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />X1<br />X2<br />y<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G GG GG G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G GG G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G GG G G G<br />G G<br />G G<br />G G G GG G G G<br />G G<br />G G<br />G G<br />G GG G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0 0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />0<br />50<br />100<br />X1<br />X2<br />Standard Deviation<br />0.00.2 0.4 0.60.81.0<br />  0<br /> 20<br /> 40<br /> 60<br /> 80<br />100<br />120<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />X1<br />X2<br />y<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />G G<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0 0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />1.0<br />0<br />50<br />100<br />X1<br />X2<br />Standard Deviation<br />Figure 1: Fitted component means (first column) and standard deviations (second column)<br />for four component MHE model for rainfall-runoff example.<br />20</p>  <p>Page 21</p> <p>Table 1: Marginal log likelihood estimated by variational lower bound (first row) and LPDS<br />with ten-fold cross validation estimated by variational approximation (second row) and<br />MCMC (third row) for inverse problem example.<br />Model A<br />-803.4<br />-65.9<br />-65.5<br />Model B<br />-688.4<br />-54.5<br />-54.2<br />Model C<br />-678.5<br />-51.5<br />-51.2<br />Model D<br />-682.8<br />-52.1<br />-51.4<br />Model E<br />-729<br />-57.2<br />-57.4<br />ML with VA<br />LPDS with VA<br />LPDS with MCMC<br />validatory approaches to model comparison if MCMC methods are used for computation<br />since we require repeated MCMC runs for model fits to different parts of the data and for<br />many models. Table 1 shows LPDS values obtained from both using the variational ap-<br />proximation and MCMC. The LPDS values computed by the variational approach compare<br />well with those obtained by MCMC, and the results suggest that a model with 4 mixture<br />components is adequate. We note that the results for MCMC for model D need to be treated<br />with some caution as there is very slow mixing in the MCMC scheme here due to the use<br />of too many mixture components and hence a poorly identified model. For the variational<br />approximation for this case, one of the mixture components effectively drops out with the<br />mixing weights being very small for all observations for one of the components.<br />Table 2 shows the CPU times taken for fitting the model to the full dataset and for<br />implementing cross-validation using both an MCMC approach and our variational method<br />without stochastic approximation correction. All code was written in the R language and<br />run on an Intel Core i5-2500 3.30Ghz processor workstation. Some difficulties in comparing<br />MCMC with variational approximation in this way need to be noted. First, the time taken<br />to use the variational approximation will depend on the way the algorithm is initialized,<br />the stopping rule, and the rate of convergence will also depend on the problem. Similarly,<br />for MCMC, the time taken depends on the number of sampling iterations, the number of<br />“burn in” iterations required to achieve convergence and the sampling algorithm - these<br />factors also tend to be problem specific. Here the method of intialization of the variational<br />method is the one described in Section 3 using short runs for multiple clusterings, and we<br />stop the variational algorithm when the relative change in the lower bound between succes-<br />sive iterations is less than 10−6. The MCMC algorithms were run for 10,000 iterations with<br />1,000 iterations burn in both for fitting the full data set and in the cross-validation calcu-<br />lations. Such short run times are only possible because our MCMC scheme actually uses a<br />very good proposal based on the variational approximation itself. We considered a random<br />21</p>  <p>Page 22</p> <p>walk Metropolis-Hastings algorithm with the mixture component indicators integrated out<br />where the proposal covariances are taken from the variational method and parameters are<br />updated in blocks that correspond to the variational factorization. This MCMC algorithm<br />generally mixes rapidly and initial values can also be based on the variational approxima-<br />tion so that 1000 “burn in” iterations was sufficient for all models fitted here. Looking at<br />the cross-validation calculations, there is a roughly 20 fold speed up by using variational<br />approximation in the computations when using just 10,000 iterations in the MCMC sam-<br />pling for all models. This is a fairly conservative estimate of the benefits and consistent<br />with other comparisons in the variational approximation literature. The difficulties of con-<br />vergence assessment in the MCMC approach are also avoided by the variational method.<br />For model C, we also compared posterior distributions obtained via MCMC with both our<br />Table 2: CPU times for variational approximation (VA) and MCMC for fitting inverse<br />problem example. Times (in seconds) are given for full data and cross-validation calculations,<br />for models 1-4.<br />Model A<br />88<br />330<br />121<br />2941<br />Model B<br />146<br />473<br />184<br />4409<br />Model C<br />215<br />650<br />281<br />5979<br />Model D<br />274<br />825<br />393<br />7626<br />Model E<br />254<br />659<br />276<br />5929<br />Full data VA<br />MCMC<br />VA<br />MCMC<br />Cross validation<br />simple variational approximation and the variational approximation incorporating stochas-<br />tic approximation correction. 10,000 iterations of stochastic approximation were used. Our<br />gain sequence was ak = 0.4/(k + 10000)0.9for the variance adjustment parameters and<br />ak= 0.4/(x+10000)0.8for the mean adjustment parameters. Computation of the stochastic<br />approximation correction took approximately 166 seconds of CPU time. Figure 2 shows the<br />marginal posterior distributions for the parameters in the gating function. We are looking<br />at just one of the modes here and there are no issues with label switching in the MCMC<br />as the modes corresponding to relabelling are well separated. The stochastic approximation<br />correction is helpful for obtaining an improved approximation for at least some of the pa-<br />rameters, with the estimated posterior marginals from stochastic approximation(dot-dashed<br />lines) generally being closer to the Monte Carlo estimated marginals (solid lines) than the<br />simple variational estimated marginals (dashed lines). There is little improvement in esti-<br />mation of the marginal posteriors for the mean and variance parameters or in predictive<br />inference by the stochastic approximation correction (results not shown). Similar benefits<br />22</p>  <p>Page 23</p> <p>in estimation of the gating function parameters have been observed in other examples that<br />we have considered. We also conducted a small simulation study to investigate model selec-<br />tion performance for the variational approach using ten fold cross-validation. In particular,<br />we simulated 50 data sets from the fitted four component heteroscedastic model that was<br />chosen as best in the analysis above. The parameters used for simulating the data were the<br />variational posterior mean values obtained from fitting to the real data. In our simulations<br />we compared heteroscedastic models with different numbers of mixture components with x1<br />and x2in both the mean and variance models (now model C is the “true” model). When<br />using cross-validation to select the best model the true model was chosen for 32 of the 50<br />simulated data sets, with model D (with one extra mixture component) being chosen in 17<br />cases and a six component MHE model chosen once.<br />6.2Time series example<br />Geweke and Keane (2007) consider a data set of returns to the S&amp;P500 stock market index.<br />We consider an analysis of the same data but follow Li et al. (2010b) and incorporate some<br />more recent observations. The data consists of 4646 daily returns from January 1, 1990 to<br />May 29, 2008 and our response ytis logpt/pt−1where ptis the closing S&amp;P500 index on day t.<br />Geweke and Keane (2007) and Li et al. (2010b) consider time series models for the data using<br />mixtures of experts where the covariates include functions of lagged response values. We<br />refer the reader to Li et al. (2010b) for a more comprehensive description of the data, and we<br />use their predictors Lastweek (average of return for last 5 trading days) LastMonth (average<br />of return for last 20 trading days) and MaxMin95 ((1−Φ)?<br />covariates were found to be significant in the dispersion model in Li et al. (2010b) in fitting<br />a certain one component skew t model with dispersion, skewness and degrees of freedom all<br />functions of covariates.<br />For the S&amp;P500 data, we follow Li et al. (2010b) and take T = 4646 training observations<br />and T∗= 199 validation observations. Li et al. (2010b) note that the choice of the last 199<br />observations in the series for validation is a difficult test for candidate models because this<br />period covers the recent financial crisis where unusually high volatility can be observed.<br />We consider mixtures of experts models with only an intercept term in the mean model<br />but an intercept and the covariates LastWeek, LastMonth and MaxMin95 in the variance<br />model and gating function and m = 1,2,3 and 4 experts. Table 3 shows the LPDS values<br />computed using the variational approximation with sequential updating of the posterior at<br />sφs(logp(h)<br />t−1−s−logp(l)<br />t−1−s) with<br />Φ = 0.95, p(h)<br />t−1−sand p(l)<br />t−1−sthe highest and lowest values of the index on day t). These<br />23</p>  <p>Page 24</p> <p>468 10 1418<br />0.0 0.1 0.2 0.3 0.4<br />Intercept−component 2<br />−60 −50−40 −30 −20<br />0.00<br />0.04<br />0.08<br />0.12<br />X1 coefficient−component 2<br />−50510<br />0.0<br />0.1<br />0.2<br />0.3<br />0.4<br />X2 coefficient−component 2<br />−35−25 −15<br />0.00<br />0.06<br />0.12<br />Intercept−component 3<br />2030405060<br />0.00<br />0.04<br />0.08<br />X1 coefficient−component 3<br />−202468<br />0.0<br />0.2<br />0.4<br />X2 coefficient−component 3<br />−6−4−2024<br />0.0<br />0.2<br />0.4<br />Intercept−component 4<br />−15−10 −505<br />0.00<br />0.10<br />0.20<br />X1 coefficient−component 4<br />02468 1014<br />0.0<br />0.2<br />0.4<br />X2 coefficient−component 4<br />Figure 2: Marginal posterior distributions for parameters in the rainfall runoff example<br />in the gating function estimated by Monte Carlo method (solid line), simple variational<br />approximation (dashed line) and variational approximation with stochastic approximation<br />correction (dot-dashed line). Rows are different components, and the left, middle and right<br />columns respectively shows the intercept terms and coefficients for x1and x2.<br />24</p>  <p>Page 25</p> <p>each time point as well as the LPDS computed using the approximation of Li et al. where<br />the posterior is not updated after the end of the training period (the latter for both the<br />variational method and using MCMC). Predictive density variates in the expression for<br />the LPDS are approximated averaging over 1000 Monte Carlo draws of the parameters for<br />each method. Based on the largest LPDS it seems reasonable to choose a two component<br />mixture as providing an adequate model. Computation times for MCMC and variational<br />Table 3: LPDS values for 1,2,3 and 4 mixture components and MCMC method with ap-<br />proximation of Li et al. without sequential updating (first line) variational method with<br />approximation of Li et al. (second line) and variational method with sequential updating<br />(last line)<br />Number of mixture components<br />12<br />-477.8-471.2<br />-478.0 -470.1<br />-477.7-470.0<br />34<br />Without sequential updating, MCMC<br />Without sequential updating, VA<br />With sequential updating, VA<br />-469.0<br />-470.1<br />-470.1<br />-470.6<br />-471.7<br />-473.3<br />approximation are shown in Table 4. The time for MCMC is just for the initial fit (based on<br />10,000 iterations for each of the models with 1000 burn in), but for variational approximation<br />we show the times for both the initial fit and initial fit plus sequential updating for validation.<br />The stopping criterion for the variational method is based on a relative tolerance of 10−6for<br />the lower bound. In this case there would be a roughly 200 fold speed up from employing<br />the variational method as the complete computations using the variational method (inital fit<br />plus validation) are similar to the computational requirements of just the initial fit for the<br />MCMC method and recall that the computational cost for the initial fit for MCMC needs<br />to be multiplied by approximately T∗= 199 to get the computational cost for the complete<br />computations.<br />Another application where MCMC methods may not be feasible at all for time series data<br />is where a model is fitted repeatedly within a rolling window of observations. We illustrate<br />this here for our two component mixture model where we examine parameter estimates for<br />the model within different windows to investigate the question of structural breaks and model<br />instability. For the S&amp;P500 data we consider windows of size M = 500. First we fit the model<br />to the first M observations. Next, we advance the rolling window by 50 observations (i.e. we<br />consider observations 51 to M +50) and refit the model. We continue in this way, advancing<br />the rolling window by 50 observations at each step. Figure 3 shows the estimated lower 1%<br />25</p>  <p>Page 26</p> <p>Table 4: Computation times (seconds) for LPDS calculations for models with 1,2,3 and 4<br />mixture components in the S&amp;P500 example. Rows 1-3 respectively are times for initial fit<br />for MCMC, initial fit for variational approximation, and initial fit plus sequential updating<br />for validation for variational approximation.<br />Number of mixture components<br />123<br />50424633427<br />1 7391022<br />25019022552<br />4<br />Initial fit MCMC<br />Initial fit VA<br />Initial fit + validation VA<br />4417<br />1442<br />4754<br />and 5% quantiles of the predictive densities for the covariate values for times t = 1000 and<br />t = 4000 versus the upper edge of the rolling window. There is some evidence of model<br />instability and structural change. Also shown in Figure 3 are estimated predictive densities<br />for the same covariates computed via MCMC (solid lines) and the variational approximation<br />(dashed lines). One can observe that the MCMC and variational predictive densities are<br />nearly indistinguishable, so that the variational approximation provides excellent predictive<br />inference here.<br />7 Discussion and conclusions<br />Our article describes fast variational approximation methods for mixtures of heteroscedastic<br />experts models and illustrates the benefits of this methodology in problems where repeated<br />refitting of models is required such as in exploratory analysis and cross-validation approaches<br />to model choice. There are a number of promising avenues for future research. One inter-<br />esting idea is to combine variational methods (particularly the stochastic approximation<br />approach of Section 4) with MCMC methods applied to a subset of the data. It might be<br />possible to get a rough idea of the correlation structure in the posterior from an MCMC<br />run for a subset and then to adjust means and variances using stochastic approximation<br />approaches similar to those that we have described. There are many issues to be addressed<br />in practice with such an approach however. MCMC methods and variational methods can<br />be complementary, in the sense that variational methods are able to provide good proposal<br />distributions for MCMC schemes, a strategy that is sometimes called variational MCMC (de<br />Freitas et al., 2001). The combination of variational methods with stochastic approximation<br />has the potential to broaden the applicability of such an approach. Another interesting ex-<br />26</p>  <p>Page 27</p> <p>1000200030004000<br />−14<br />−12<br />−10<br />−8<br />−6<br />−4<br />−2<br />Window location<br />Quantile<br />1000 2000 3000 4000<br />−8<br />−7<br />−6<br />−5<br />−4<br />−3<br />−2<br />−1<br />Window location<br />Quantile<br />−4−2024<br />0.0<br />0.2<br />0.4<br />0.6<br />0.8<br />y<br />Predictive density<br />−4−2024<br />0.0<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />y<br />Predictive density<br />Figure 3: Estimated 5% (solid line) and 1% (dashed line) quantiles of predictive densities<br />based on rolling window for S&amp;P 500 example and covariate values at t = 1000 (top left) and<br />t = 4000 (top right). Estimated quantiles are plotted versus the upper edge of the rolling<br />window. Also shown are the estimated predictive densities at covariate values for t = 1000<br />and t = 4000 (bottom left and right respectively) estimated based on the entire training<br />data set using MCMC (solid line) and variational approximation (dashed line).<br />27</p>  <p>Page 28</p> <p>tension which we have not pursued for MHE models is to allow some of the coefficients in<br />the expert components to be shared across components. This may be particularly useful in<br />the variance models for the expert components.<br />Acknowledgements<br />David Nott was supported by Singapore MOE grant R-155-000-068-133. Siew Li Tan was<br />partly supported as part of the SDWA’s tropical reservoir research programme. The authors<br />also wish to thank John Ormerod for comments and suggestions related to this work. Robert<br />Kohn’s research was partially supported by ARC grant DP0988579. We thank Lucy Marshall<br />for supplying the data for the example in Section 6.1.<br />References<br />Biernacki, C., Celeux, G. and Govaert, G. (2003). Choosing Starting Values for the EM<br />Algorithm for Getting the Highest Likelihood in Multivariate Gaussian Mixture Models.<br />Comp. Statist. Data Anal., 41, 561–575.<br />Bishop, C.M. (2006). Pattern Recognition and Machine Learning. New York: Springer.<br />Bishop, C. M. and Svens´ en, M. (2003). Bayesian hierarchical mixtures of experts. In:<br />U. Kjaerulff and C. Meek (Eds.), Proceedings Nineteenth Conference on Uncertainty in<br />Artificial Intelligence, pp. 5764, Morgan Kaufmann.<br />Blei, D. M. and Jordan, M.I. (2006). Variational inference for Dirichlet process mixtures.<br />Bayesian Analysis, 1, 121-144.<br />Braun, M. and McAuliffe, J. (2010). Variational Inference for Large-Scale Models of Discrete<br />Choice. J. .Amer. Statist. Assoc., 105, 324–335.<br />Boughton, W., 2004. The Australian water balance model. Environmental Modelling and<br />Software 19, 943–956.<br />Chib, S. and Jeliazkov, I. (2001). Marginal likelihood from the Metropolis-Hastings output.<br />J. Amer. Statist. Assoc., 96, 270–281.<br />28</p>  <p>Page 29</p> <p>Constantinopoulos, C. and Likas, A. (2007). Unsupervised learning of Gaussian mixtures<br />based on variational component splitting. IEEE Trans. Neural Netw., 18, 745-755.<br />Corduneanu, A., and Bishop, C.M. (2001). Variational Bayesian model selection for mix-<br />ture distributions. In: T. Jaakkola and T. Richardson (Eds), Artifcial Intelligence and<br />Statistics, Morgan Kaufmann, 27–34.<br />de Freitas, N., Højen-Sørensen, P., Jordan, M.I. and Russell, S. (2001). Variational MCMC.<br />In: J. Breese and D. Koller (Ed)., Uncertainty in Artificial Intelligence (UAI), Proceedings<br />of the Seventeenth Conference, 120–27.<br />De Iorio, M., M¨ uller, P., Rosner, G.L. &amp; MacEAchern, S.N. (2004). An ANOVA model<br />for dependent random measures. Journal of the American Statistical Association, 99,<br />205–15.<br />Dunson, D.B., Pillai, N. and Park, J-H. (2007). Bayesian density regression. Journal of the<br />Royal Statistical Society, Series B, 69, 163–83.<br />Fr¨ uhwirth-Schnatter, S. (2004). Estimating marginal likelihoods for mixture and Markov<br />switching models using bridge sampling techniques. The Econometrics Journal, 7, 143–<br />167.<br />Geweke, J. and Amisano, G. (2010). Comparing and evaluating Bayesian predictive distri-<br />butions of asset returns. International Journal of Forecasting, 26, 216–230.<br />Geweke, J. and Keane, M. (2007). Smoothly mixing regressions. Journal of Econometrics,<br />138, 252–291.<br />Ghahramani, Z. and Beal, M.J. (2000). Variational inference for Bayesian mixtures of factor<br />analysers. In: S. A. Solla, T. K. Leen, and K.- R. M¨ uller (Eds), Advances in Neural<br />Information Processing Systems, Volume 12, 831-864.<br />Griffin, J.E. and Steel, M.F.J. (2006). Order-based dependent Dirichlet processes. Journal<br />of the American Statistical Association, 101, 179–94.<br />Honkela, A. and Valpola, H. (2003) On-line Variational Bayesian Learning. In: Proceedings<br />of the 4th International Symposium on Independent Component Analysis and Blind Signal<br />Separation, ICA 2003, Nara, Japan, 803-808.<br />29</p>  <p>Page 30</p> <p>Jacobs, R., Jordan, M., Nowlan, S. and Hinton, G. (1991). Adaptive mixtures of local<br />experts. Neural Computation, 3, 79–87.<br />Jiang, W. and Tanner, M. (1999). Hierarchical mixtures-of-experts for exponential family<br />regression models: Approximation and maximum likelihood estimation. Ann. Statist.,<br />27, 987-1011.<br />Ji, C., Shen, H. and West, M. (2010). Bounded approximations for marginal likelihoods.<br />Technical report, Duke University ISDS, available at<br />http://ftp.stat.duke.edu/WorkingPapers/10-05.html<br />Jordan, M.I., Ghahramani, Z., Jaakkola, T.S., Saul, L.K., 1999. An introduction to varia-<br />tional methods for graphical models. In M. I. Jordan (Ed.), Learning in Graphical Models.<br />MIT Press, Cambridge.<br />Jordan, M.I. and Jacobs, R.A. (1994). Hierarchical mixtures of experts and the EM algo-<br />rithm. Neural Computation, 6, 181–214.<br />Li, F., Villani, M., and Kohn, R. (2010a). Modeling Conditional Densities using Finite<br />Smooth Mixtures, in Mixtures: Estimation and Applications (Mengersen, K.L., Robert,<br />C. P. and Titterington, D.M. eds), Wiley.<br />Li, F., Villani, M. and Kohn, R. (2010b). Flexible modeling of conditional distributions<br />using smooth mixtures of asymmetric student t densities. Journal of Statistical Planning<br />and Inference, 140, 3638–3654.<br />MacEachern, S.N. (1999). Dependent nonparametric processes. In ASA Proceedings of the<br />Section on Bayesian Statistical Science, Alexandria, VA: American Statistical Association.<br />McGrory, Clare A. and Titterington, D. M. (2007). Variational approximations in Bayesian<br />model selection for finite mixture distributions. Comp. Statist. Data Anal., 51, 5352–<br />5367.<br />Neal, R. M. (2001). Annealed importance sampling. Statistics and Computing, 11, 125–139.<br />Norets, A. (2010). Approximation of conditional densities by smooth mixtures of regressions.<br />Ann. Statist., 38, 1733-1766.<br />30</p>  <p>Page 31</p> <p>O’Hagan, A., 2006. Bayesian analysis of computer code outputs: a tutorial. Reliability<br />Engineering and System Safety 91, 1290–1300.<br />Ormerod, J.T. and Wand, M.P. (2009). Explaining variational approximations. Preprint<br />available at<br />http://www.uow.edu.au/~mwand/evapap.pdf<br />Pepelyshev, A. (2010) The role of the nugget term in the Gaussian process method. MODA<br />9—Advances in model-oriented design and analysis, Contrib. Statist. 149-156.<br />Pesaran, M.H. and Timmermann, A. (2002). Market timing and return prediction under<br />model instability. J. Empirical Finance, 9, 495–510.<br />Robbins, H. and Monro, S. (1951). A Stochastic Approximation Method. Ann. Math. Stat.<br />22, 400–407.<br />Smyth, G.K. (1989). Generalized linear models with varying dispersion. J. Roy. Statist.<br />Soc. Ser. B, 51, 47–60.<br />Spall, J.C. (2003). Introduction to Stochastic Search and Optimization: Estimation, Simu-<br />lation and Control. New Jersey: Wiley.<br />Spiegelhalter, D.J., Best, N.G., Carlin, B.P. and Van der Linde, A. (2002). Bayesian Mea-<br />sures of Model Complexity and Fit (with Discussion). J. Roy. Statist. Soc. Ser. B, 64,<br />583–616.<br />Ueda, N. and Ghahramani, Z. (2002). Bayesian model search for mixture models based on<br />optimizing variational bounds Neural Networks, 15, 1223-1241.<br />Vehtari, A. and Lampinen, J. (2002). Bayesian model assessment and comparison using<br />cross-validation predictive densities. Neural Computation, 14, 2439–2468.<br />Villani, M., Kohn, R. and Giordani, P. (2009). Regression Density Estimation using Smooth<br />Adaptive Gaussian Mixtures. Journal of Econometrics, 153, 155–173<br />Waterhouse, S., MacKay, D. and Robinson, T. (1996). Bayesian methods for mixtures of<br />experts. In: D.S. Touretzky, M.C. Mozer and M.E. Hasselmo (Eds.), Advances in Neural<br />Information Processing Systems 8, pp. 351–357, MIT Press.<br />31</p>  <p>Page 32</p> <p>Wand, M.P. (2002). Vector Differential Calculus in Statistics. The American Statistician,<br />56, 55–62.<br />Wang, B. and Titterington, D.M. (2005). Inadequacy of interval estimates corresponding<br />to variational Bayesian approximations. In: R.G. Cowell and Z. Ghahramani (Eds.),<br />Proceedings of the 10th International Workshop on Artificial Intelligence, pp. 373–380,<br />Society for Artificial Intelligence and Statistics.<br />Wedel, M. (2002). Concomitant variables in finite mixture models. Statistica Neerlandica,<br />56, 362–375.<br />West, M. (1985), “Generalized linear models: outlier accommodation, scale parameters and<br />prior distributions,” in Bayesian Statistics 2 (eds J.M. Bernardo et al.), pp. 531–538.<br />Amsterdam, North Holland.<br />Wood, S.A., Jiang, W., and Tanner, M.A. (2002). Bayesian Mixture of Splines for Spatially<br />Adaptive Nonparametric Regression. Biometrika, 89, 513-528.<br />Wood, S.A., Kohn, R., Cottet, R., Jiang, W. and Tanner, M. (2008). Locally adaptive<br />nonparametric binary regression. J. Comp. Graph. Statist., 17, 352–372.<br />Wu, B., McGrory, C.A. and Pettitt, A.N. (2011). A new variational Bayesian algorithm with<br />application to human mobility pattern modeling. Statistics and Computing, to appear.<br />32</p>  <a href="https://www.researchgate.net/profile/Robert_Kohn/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation/links/00b7d53a84badac545000000.pdf">Download full-text</a> </div> <div id="rgw23_56ab9f50845fe" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab9f50845fe">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab9f50845fe"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Robert_Kohn/publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation/links/00b7d53a84badac545000000.pdf" class="publication-viewer" title="00b7d53a84badac545000000.pdf">00b7d53a84badac545000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Robert_Kohn">Robert Kohn</a> &middot; Jul 3, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw26_56ab9f50845fe"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www.mattiasvillani.com/wp-content/uploads/2011/07/variational-heteroscedastic-moe-july-6-20114.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Regression Density Estimation With Variational Methods and Stochastic Approximation">Regression Density Estimation With Variational Met...</a> </div>  <div class="details">   Available from <a href="http://www.mattiasvillani.com/wp-content/uploads/2011/07/variational-heteroscedastic-moe-july-6-20114.pdf" target="_blank" rel="nofollow">mattiasvillani.com</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw28_56ab9f50845fe" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw29_56ab9f50845fe">  </ul> </div> </div>   <div id="rgw19_56ab9f50845fe" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw20_56ab9f50845fe"> <div> <h5> <a href="publication/284097225_Probabilistic_Segmentation_via_Total_Variation_Regularization" class="color-inherit ga-similar-publication-title"><span class="publication-title">Probabilistic Segmentation via Total Variation Regularization</span></a>  </h5>  <div class="authors"> <a href="researcher/2039943523_Matt_Wytock" class="authors ga-similar-publication-author">Matt Wytock</a>, <a href="researcher/71054384_J_Zico_Kolter" class="authors ga-similar-publication-author">J. Zico Kolter</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab9f50845fe"> <div> <h5> <a href="publication/283129397_Option-implied_risk_aversion_estimation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Option-implied risk aversion estimation</span></a>  </h5>  <div class="authors"> <a href="researcher/2083335584_Rihab_Bedoui" class="authors ga-similar-publication-author">Rihab Bedoui</a>, <a href="researcher/2083368493_Haykel_Hamdi" class="authors ga-similar-publication-author">Haykel Hamdi</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw22_56ab9f50845fe"> <div> <h5> <a href="publication/283654968_Simulation-Based_Density_Estimation_for_Time_Series_Using_Covariate_Data" class="color-inherit ga-similar-publication-title"><span class="publication-title">Simulation-Based Density Estimation for Time Series Using Covariate Data</span></a>  </h5>  <div class="authors"> <a href="researcher/81225812_Yin_Liao" class="authors ga-similar-publication-author">Yin Liao</a>, <a href="researcher/2084342226_John_Stachurski" class="authors ga-similar-publication-author">John Stachurski</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw42_56ab9f50845fe" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw43_56ab9f50845fe">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw44_56ab9f50845fe" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=l2ecvICoYiS1tjw4gHcNufgRiEjg5P9jOo4D4U8BksM5UJr1WzPxl9piSpwrboG1" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="C6p6fFSEuPqvV3JBPSwP2eXyWVzxq+d9NcQrxmtiPo5JJVGYISq3gVP58MFIn0Om6netkFE/w3G+pHHDwBwj461wHCphmHoMRX1H/QFg4B0rzM1Z+jxE3ady/dVD3H00roHmnAXvydHI2qQm+XcJgAvBs+ZM0aiXNlI3DnQ0JlPrWy1QSPR1XzVXK5knQePTuowR6Tz0+K+fbWj5PX+cc4vBliypqOkIMGtFyG9Z0br9v1m/uPZjhqTXVYq5Zsfqe6i06za421LTtabXxxECYMofRXujC1wTp9+P/zQFoRw="/> <input type="hidden" name="urlAfterLogin" value="publication/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjU0Mjk1NTA0X1JlZ3Jlc3Npb25fRGVuc2l0eV9Fc3RpbWF0aW9uX1dpdGhfVmFyaWF0aW9uYWxfTWV0aG9kc19hbmRfU3RvY2hhc3RpY19BcHByb3hpbWF0aW9u"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjU0Mjk1NTA0X1JlZ3Jlc3Npb25fRGVuc2l0eV9Fc3RpbWF0aW9uX1dpdGhfVmFyaWF0aW9uYWxfTWV0aG9kc19hbmRfU3RvY2hhc3RpY19BcHByb3hpbWF0aW9u"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjU0Mjk1NTA0X1JlZ3Jlc3Npb25fRGVuc2l0eV9Fc3RpbWF0aW9uX1dpdGhfVmFyaWF0aW9uYWxfTWV0aG9kc19hbmRfU3RvY2hhc3RpY19BcHByb3hpbWF0aW9u"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw45_56ab9f50845fe"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 495;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Mattias Villani","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272308082311176%401441934714490_m\/Mattias_Villani.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Mattias_Villani","institution":"Link\u00f6ping University","institutionUrl":false,"widgetId":"rgw4_56ab9f50845fe"},"id":"rgw4_56ab9f50845fe","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1397518","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9f50845fe"},"id":"rgw3_56ab9f50845fe","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=254295504","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":254295504,"title":"Regression Density Estimation With Variational Methods and Stochastic Approximation","journalTitle":"Journal of Computational and Graphical Statistics","journalDetailsTooltip":{"data":{"journalTitle":"Journal of Computational and Graphical Statistics","journalAbbrev":"J COMPUT GRAPH STAT","publisher":"American Statistical Association; Institute of Mathematical Statistics; Interface Foundation of North America, Taylor & Francis","issn":"1061-8600","impactFactor":"1.22","fiveYearImpactFactor":"1.81","citedHalfLife":">10.0","immediacyIndex":"0.27","eigenFactor":"0.01","articleInfluence":"1.88","widgetId":"rgw6_56ab9f50845fe"},"id":"rgw6_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1061-8600","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1080\/10618600.2012.679897","journalInfos":{"journal":"","publicationDate":"07\/2012;","publicationDateRobot":"2012-07","article":"21(3).","journalTitle":"Journal of Computational and Graphical Statistics","journalUrl":"journal\/1061-8600_Journal_of_Computational_and_Graphical_Statistics","impactFactor":1.22}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1080\/10618600.2012.679897"},{"key":"rft.atitle","value":"Regression Density Estimation With Variational Methods and Stochastic Approximation"},{"key":"rft.title","value":"Journal of Computational and Graphical Statistics - J COMPUT GRAPH STAT"},{"key":"rft.jtitle","value":"Journal of Computational and Graphical Statistics - J COMPUT GRAPH STAT"},{"key":"rft.volume","value":"21"},{"key":"rft.issue","value":"3"},{"key":"rft.date","value":"2012"},{"key":"rft.issn","value":"1061-8600"},{"key":"rft.au","value":"David J. Nott,Siew Li Tan,Mattias Villani,Robert Kohn"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab9f50845fe"},"id":"rgw7_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=254295504","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":254295504,"peopleItems":[{"data":{"authorUrl":"researcher\/8484000_David_J_Nott","authorNameOnPublication":"David J. Nott","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David J. Nott","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8484000_David_J_Nott","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw10_56ab9f50845fe"},"id":"rgw10_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8484000&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab9f50845fe"},"id":"rgw9_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8484000&authorNameOnPublication=David%20J.%20Nott","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/59395451_Siew_Li_Tan","authorNameOnPublication":"Siew Li Tan","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Siew Li Tan","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/59395451_Siew_Li_Tan","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab9f50845fe"},"id":"rgw12_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=59395451&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab9f50845fe"},"id":"rgw11_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=59395451&authorNameOnPublication=Siew%20Li%20Tan","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Mattias Villani","accountUrl":"profile\/Mattias_Villani","accountKey":"Mattias_Villani","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272308082311176%401441934714490_m\/Mattias_Villani.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Mattias Villani","profile":{"professionalInstitution":{"professionalInstitutionName":"Link\u00f6ping University","professionalInstitutionUrl":"institution\/Linkoeping_University"}},"professionalInstitutionName":"Link\u00f6ping University","professionalInstitutionUrl":"institution\/Linkoeping_University","url":"profile\/Mattias_Villani","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272308082311176%401441934714490_l\/Mattias_Villani.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Mattias_Villani","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw14_56ab9f50845fe"},"id":"rgw14_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1397518&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Link\u00f6ping University","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":2,"publicationUid":254295504,"widgetId":"rgw13_56ab9f50845fe"},"id":"rgw13_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1397518&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=2&publicationUid=254295504","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Robert Kohn","accountUrl":"profile\/Robert_Kohn","accountKey":"Robert_Kohn","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272445403824128%401441967454070_m\/Robert_Kohn.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Robert Kohn","profile":{"professionalInstitution":{"professionalInstitutionName":"University of New South Wales","professionalInstitutionUrl":"institution\/University_of_New_South_Wales"}},"professionalInstitutionName":"University of New South Wales","professionalInstitutionUrl":"institution\/University_of_New_South_Wales","url":"profile\/Robert_Kohn","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272445403824128%401441967454070_l\/Robert_Kohn.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Robert_Kohn","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw16_56ab9f50845fe"},"id":"rgw16_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1778020&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of New South Wales","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":2,"publicationUid":254295504,"widgetId":"rgw15_56ab9f50845fe"},"id":"rgw15_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1778020&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=2&publicationUid=254295504","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab9f50845fe"},"id":"rgw8_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=254295504&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":254295504,"abstract":"<noscript><\/noscript><div>Regression density estimation is the problem of flexibly estimating a response distribution as a function of covariates. An important approach to regression density estimation uses finite mixture models and our article considers flexible mixtures of heteroscedastic regression (MHR) models where the response distribution is a normal mixture, with the component means, variances and mixture weights all varying as a function of covariates. Our article develops fast variational approximation methods for inference. Our motivation is that alternative computationally intensive MCMC methods for fitting mixture models are difficult to apply when it is desired to fit models repeatedly in exploratory analysis and model choice. Our article makes three contributions. First, a variational approximation for MHR models is described where the variational lower bound is in closed form. Second, the basic approximation can be improved by using stochastic approximation methods to perturb the initial solution to attain higher accuracy. Third, the advantages of our approach for model choice and evaluation compared to MCMC based approaches are illustrated. These advantages are particularly compelling for time series data where repeated refitting for one step ahead prediction in model choice and diagnostics and in rolling window computations is very common. Supplemental materials for the article are available online.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw17_56ab9f50845fe"},"id":"rgw17_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=254295504","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw18_56ab9f50845fe"},"id":"rgw18_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9f50845fe"},"id":"rgw5_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=254295504&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2039943523,"url":"researcher\/2039943523_Matt_Wytock","fullname":"Matt Wytock","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71054384,"url":"researcher\/71054384_J_Zico_Kolter","fullname":"J. Zico Kolter","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284097225_Probabilistic_Segmentation_via_Total_Variation_Regularization","usePlainButton":true,"publicationUid":284097225,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284097225_Probabilistic_Segmentation_via_Total_Variation_Regularization","title":"Probabilistic Segmentation via Total Variation Regularization","displayTitleAsLink":true,"authors":[{"id":2039943523,"url":"researcher\/2039943523_Matt_Wytock","fullname":"Matt Wytock","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71054384,"url":"researcher\/71054384_J_Zico_Kolter","fullname":"J. Zico Kolter","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284097225_Probabilistic_Segmentation_via_Total_Variation_Regularization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284097225_Probabilistic_Segmentation_via_Total_Variation_Regularization\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab9f50845fe"},"id":"rgw20_56ab9f50845fe","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284097225","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2083335584,"url":"researcher\/2083335584_Rihab_Bedoui","fullname":"Rihab Bedoui","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083368493,"url":"researcher\/2083368493_Haykel_Hamdi","fullname":"Haykel Hamdi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2015","journal":"Journal of Economic Asymmetries","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283129397_Option-implied_risk_aversion_estimation","usePlainButton":true,"publicationUid":283129397,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283129397_Option-implied_risk_aversion_estimation","title":"Option-implied risk aversion estimation","displayTitleAsLink":true,"authors":[{"id":2083335584,"url":"researcher\/2083335584_Rihab_Bedoui","fullname":"Rihab Bedoui","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083368493,"url":"researcher\/2083368493_Haykel_Hamdi","fullname":"Haykel Hamdi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Economic Asymmetries 11\/2015; 12(2):142-152. DOI:10.1016\/j.jeca.2015.06.001"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283129397_Option-implied_risk_aversion_estimation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283129397_Option-implied_risk_aversion_estimation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab9f50845fe"},"id":"rgw21_56ab9f50845fe","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283129397","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":81225812,"url":"researcher\/81225812_Yin_Liao","fullname":"Yin Liao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084342226,"url":"researcher\/2084342226_John_Stachurski","fullname":"John Stachurski","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":"Journal of Business and Economic Statistics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283654968_Simulation-Based_Density_Estimation_for_Time_Series_Using_Covariate_Data","usePlainButton":true,"publicationUid":283654968,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.24","url":"publication\/283654968_Simulation-Based_Density_Estimation_for_Time_Series_Using_Covariate_Data","title":"Simulation-Based Density Estimation for Time Series Using Covariate Data","displayTitleAsLink":true,"authors":[{"id":81225812,"url":"researcher\/81225812_Yin_Liao","fullname":"Yin Liao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084342226,"url":"researcher\/2084342226_John_Stachurski","fullname":"John Stachurski","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Business and Economic Statistics 10\/2015; 33(4):595-606. DOI:10.1080\/07350015.2014.982247"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283654968_Simulation-Based_Density_Estimation_for_Time_Series_Using_Covariate_Data","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283654968_Simulation-Based_Density_Estimation_for_Time_Series_Using_Covariate_Data\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw22_56ab9f50845fe"},"id":"rgw22_56ab9f50845fe","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283654968","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw19_56ab9f50845fe"},"id":"rgw19_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=254295504&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":254295504,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":254295504,"publicationType":"article","linkId":"00b7d53a84badac545000000","fileName":"00b7d53a84badac545000000.pdf","fileUrl":"profile\/Robert_Kohn\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000.pdf","name":"Robert Kohn","nameUrl":"profile\/Robert_Kohn","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jul 3, 2014","fileSize":"1.14 MB","widgetId":"rgw25_56ab9f50845fe"},"id":"rgw25_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=254295504&linkId=00b7d53a84badac545000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":254295504,"publicationType":"article","linkId":"0376044a0cf2239b7c88b482","fileName":"Regression Density Estimation With Variational Methods and Stochastic Approximation","fileUrl":"http:\/\/www.mattiasvillani.com\/wp-content\/uploads\/2011\/07\/variational-heteroscedastic-moe-july-6-20114.pdf","name":"mattiasvillani.com","nameUrl":"http:\/\/www.mattiasvillani.com\/wp-content\/uploads\/2011\/07\/variational-heteroscedastic-moe-july-6-20114.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw26_56ab9f50845fe"},"id":"rgw26_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=254295504&linkId=0376044a0cf2239b7c88b482&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw24_56ab9f50845fe"},"id":"rgw24_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=254295504&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":33,"valueFormatted":"33","widgetId":"rgw27_56ab9f50845fe"},"id":"rgw27_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=254295504","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab9f50845fe"},"id":"rgw23_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=254295504&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":254295504,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw29_56ab9f50845fe"},"id":"rgw29_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=254295504&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":33,"valueFormatted":"33","widgetId":"rgw30_56ab9f50845fe"},"id":"rgw30_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=254295504","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw28_56ab9f50845fe"},"id":"rgw28_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=254295504&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Regression density estimation with variational\nmethods and stochastic approximation\nDavid J. Nott, Siew Li Tan, Mattias Villani and Robert Kohn\u2217\nAbstract\nRegression density estimation is the problem of flexibly estimating a response dis-\ntribution as a function of covariates. An important approach to regression density\nestimation uses mixtures of experts models and our article considers flexible mixtures\nof heteroscedastic experts (MHE) regression models where the response distribution\nis a normal mixture, with the component means, variances and mixture weights all\nvarying as a function of covariates. Our article develops fast variational approximation\nmethods for inference. Our motivation is that alternative computationally intensive\nMCMC methods for fitting mixture models are difficult to apply when it is desired to\nfit models repeatedly in exploratory analysis and model choice. Our article makes three\ncontributions. First, a variational approximation for MHE models is described where\nthe variational lower bound is in closed form. Second, the basic approximation can be\nimproved by using stochastic approximation methods to perturb the initial solution to\nattain higher accuracy. Third, the advantages of our approach for model choice and\nevaluation compared to MCMC based approaches are illustrated. These advantages\nare particularly compelling for time series data where repeated refitting for one step\nahead prediction in model choice and diagnostics and in rolling window computations\nis very common.\n\u2217David J. Nott is Associate Professor, Department of Statistics and Applied Probability, National Uni-\nversity of Singapore, Singapore 117546.(email standj@nus.edu.sg).\npartment of Statistics and Applied Probability, National University of Singapore, Singapore 117546 (email\ng0900760@nus.edu.sg). Mattias Villani is Professor, Division of Statistics, Dept. of Computer and Infor-\nmation Science, Link\u00a8 oping University, SE-58183 Link\u00a8 oping, Sweden. (email: mattias.villani@liu.se). Robert\nKohn is Professor, Australian School of Business, University of New South Wales, Sydney 2052 Australia\n(email r.kohn@unsw.edu.au).\nSiew Li Tan is PhD student, De-\n1"},{"page":2,"text":"Keywords: Bayesian model selection, heteroscedasticity, mixtures of experts, stochastic ap-\nproximation, variational approximation.\n1 Introduction\nRegression density estimation is the problem of flexibly estimating a response distribution\nassuming that it varies smoothly as a function of covariates. An important approach to\nregression density estimation uses mixtures of experts (ME) models and our article considers\nflexible regression models where the response distribution is a normal mixture, with the\ncomponent means, variances and mixing weights all varying with covariates. The component\nmeans and variances are described by a heteroscedastic linear model, and the component\nweights by a multinomial logit model. These mixtures of heteroscedastic experts (MHE)\nmodels extend conventional mixtures of experts models (Jacobs et al., 1991, Jordan and\nJacobs, 1994) by allowing the component models to be heteroscedastic. The terminology\nmixtures of experts comes from machine learning, but it is frequently used in statistics as\nwell. The term \u201cexpert\u201d refers to the individual mixture components and the term \u201cgating\nfunction\u201d is used for the model for the mixing weights. Very often the experts are generalized\nlinear models, and such models (with or without covariate dependent mixing weights) are\nsometimes referred to as mixtures of generalized linear models. In marketing, the mixture of\nexperts is sometimes referred to as the concomitant variable mixture regression model (see,\nfor example, Wedel, 2002).\nThe heteroscedastic extension of mixture of experts models is important in modern ap-\nplications since simulations by Villani et al. (2009) showed that, when used to model het-\neroscedastic data, the performance of ME models with homoscedastic components deterio-\nrates as the number of covariates increases, and there comes a point when their performance\ncannot be improved simply by increasing the number of mixture components. They also\nobserved that with MHE models, fewer mixture components are required which makes the\nestimation and interpretation of mixture models easier. In an analysis of the benchmark\nLIDAR data set Li et al. (2010a) showed that an ME model with homoscedastic thin plate\ncomponents requires three components to achieve approximately the same performance as\nan MHE model with a single thin plate component, providing further evidence that MHE\nmodels help to reduce the number of mixture components required. Moreover, Villani et al.\n(2009) demonstrated that the fit of an MHE model to homoscedastic data is comparable to\n2"},{"page":3,"text":"that of an ME model with homoscedastic components.\nOur article makes three contributions. First, fast variational methods are developed\nfor fitting MHE models and a variational lower bound is obtained in closed form. Second,\nstochastic approximation optimization techniques (see, for example, Spall, 2003) are used\nto improve the variational approximation to attain higher accuracy. The computational\nmethods developed here can also be applied beyond the context of MHE models. Third, in\nsituations where it is necessary to fit a number of complex models to the data or if cross-\nvalidation is used to select the best models, we demonstrate that variational approximation\nmethods provide an attractive alternative to MCMC methods, which may not be feasible due\nto the computational complexity involved. For example, in model selection for complex time\nseries models, repeated one step ahead prediction is often carried out (Geweke and Amisano,\n2010), with the parameters of each model re-estimated at each step. Another popular time\nseries procedure for studying model performance is to use a rolling window (Pesaran and\nTimmermann, 2002). Suppose the data set consists of T observations, with each window of\nfixed size M. In the first instance, each model is fitted to the first M observations. Next,\neach model is fitted to observations 2 to M + 1, etc., with the final window consisting of\nobservations T \u2212M +1 to T. Variational methods are ideally suited to this kind of repeated\nrefitting and can benefit from a \u201cwarm start\u201d obtained from the previous fit. In Section\n6.2 we quantify in an example the computational speed up that comes from initializing the\nvariational optimization based on the fit to the previous window rather than treating each\nfit as an independent computation.\nUse of variational approximations may be particularly compelling where the main focus\nis predictive inference. Bayesian predictive inference is based on the predictive distribution\n?\nwhere y\u2217denotes a future response, y is the observed data and \u03b8 are parameters. There are\ntwo components to our uncertainty in p(y\u2217|y). The first component is the inherent random-\nness in y\u2217that would occur even if \u03b8 were known: this is captured by the term p(y\u2217|\u03b8,y) in\nthe integrand. The second component is related to parameter uncertainty and is captured\nby the term p(\u03b8|y). In general with large data sets the parameter uncertainty is small, and\nan approximate treatment of p(\u03b8|y) for predictive purposes may be attractive provided that\nthe approximate posterior provides good point estimation. In general, plugging in an ap-\nproximate variational posterior instead of p(\u03b8|y) in the expression for the predictive density\np(y\u2217|y) =p(y\u2217|\u03b8,y)p(\u03b8|y)d\u03b8\n3"},{"page":4,"text":"can result in excellent predictive inference. Furthermore, this still accounts to some extent\nfor parameter uncertainty, hence improving on simple plug-in predictive density estimates.\nBayesian approaches to inference in mixtures of experts models were first considered in\nPeng et al. (1996). Wood et al. (2002) and Wood et al. (2008) consider mixtures of experts\nmodels with flexible terms for the covariates for continuous and binary responses respectively.\nGeweke and Keane (2007) also take a Bayesian approach to inference in mixtures of experts\nmodels with homoscedastic experts with a multinomial probit for the gating function which\nallows a convenient Gibbs sampling MCMC scheme. MHE models have been considered\npreviously by Villani et al. (2009). They use a Bayesian approach to inference with MCMC\nmethods for computation and consider general smooth terms for the covariates and variable\nselection in the mean and variance models and the gating function. Norets (2010) considers\napproximation results for approximating quite general conditional densities in the Kullback-\nLeibler sense using various kinds of normal mixtures.\nderived and some interesting insights are obtained about when additional flexibility might\nbe most usefully employed in the mean, variance and gating functions. The approximation\nresults of Jiang and Tanner (1999) are concerned on the other hand with conditional densities\nin a one parameter exponential family in which the expert components come from the same\nexponential family. Their results are also useful for conditional densities which are discrete.\nMethods related to mixtures of experts are currently under active development in the\narea of Bayesian nonparametric approaches to regression density estimation. Rather than\nconsidering a finite mixture of regressions it is possible to put a flexible prior on a mixing\ndistribution which varies over the space. For common priors the resulting models might be\nconsidered to be mixtures of experts with an infinite number of components. There are both\nadvantages and disadvantages to this kind of approach. On the positive side, the difficult\nquestion of model choice for the number of mixture components is avoided. However, a\nfinite mixture may be more interpretable and the nature of the model may be easier to\ncommunicate to scientific practitioners. In addition, in the finite mixture framework it is\neasier to incorporate some very natural extensions such as the expert components being\nof qualitatively different types. We do not discuss these methods any further but refer to\nMacEachern (1999), De Iorio et al. (2004), Griffin and Steel (2006) and Dunson et al. (2007)\nand the references therein for a summary of relevant methodology and recent developments.\nIn terms of computational methodology, early approaches to fitting mixtures of experts\nmodels (Jordan and Jacobs, 1994, for example) used maximum likelihood and the EM al-\ngorithm.Modern Bayesian strategies for inference use MCMC computational methods,\nApproximation error bounds are\n4"},{"page":5,"text":"although there are a number of authors who consider variational approaches similar to those\ndescribed in this paper (Waterhouse, MacKay and Robinson, 1996, Ueda and Ghahramani,\n2002, Bishop and Svens\u00b4 en, 2003). However, these authors did not consider heteroscedastic\nexperts. Moreover, our paper also explores the advantages of variational methods in repeated\nestimations of the model, as in model comparison by cross-validation or rolling window es-\ntimates to check for model stability. Outside the regression context, there are a number of\ninnovative approaches to model selection in fitting Gaussian mixture models which follow a\nvariational approach. Corduneanu and Bishop (2001) consider a variational lower bound on\nthe log marginal likelihood with all parameters, except the mixing coefficients, integrated\nout, and estimate the mixing coefficients by maximizing the lower bound. This leads to some\nof the coefficients being set to zero and an automated model selection approach. McGrory\nand Titterington (2007) consider a variational extension of the DIC criterion (Spiegelhal-\nter et al., 2002) and a variational optimization technique where the algorithm is initialised\nwith a large number of components and mixture components whose weightings become suf-\nficiently small are dropped out as the optimization proceeds. Blei and Jordan (2006) have\nconsidered variational approximation for Dirichlet process mixture models. Recently Wu et\nal. (2011) have considered variational methods for fitting mixtures for data which require\nmodels with narrow widely separated mixture components. They discuss sophisticated split\nand merge algorithms, building on earlier related methods such as those of Ghahramani and\nBeal (2000), Ueda and Ghahramani (2002) and Constantinopoulos and Likas (2007), for\nsimultaneous model selection and parameter estimation as well as novel criteria for model\nevaluation.\nOne contribution of our paper is the development of broadly applicable stochastic approx-\nimation correction methods for variational approximation. From our simulation studies, the\nstochastic approximation correction is very helpful for getting an improved approximation\nand requires less computation time than MCMC methods. Ji, Shen and West (2010) recently\nproposed similar stochastic approximation methods for learning variational approximations\nbut we offer a number of improvements on their implementation. In particular, we are able\nto suggest an improved gradient estimate compared to their approach and consider a strat-\negy of perturbing only the mean and scale in an initial variational approximation obtained\nusing closed form updates. Perturbing an existing solution allows us to keep the dimension\nof the optimization low which is important for a fast and stable implementation. Ji et al.\n(2010) suggest a number of other innovative ideas including the use of MCMC methods to\nobtain computationally attractive upper and lower bounds on the marginal likelihood.\n5"},{"page":6,"text":"The paper is organized as follows. Section 2 introduces the mixture of heteroscedastic\nexperts model. Section 3 describes fast variational approximation methods for MHE models\nand Section 4 uses of variational methods in model choice. Section 5 discusses improvements\non the basic approximation using a stochastic approximation correction which also integrates\nout the mixture component indicators from the posterior. Section 6 considers examples\ninvolving both real and simulated data and Section 7 concludes.\n2 Heteroscedastic mixtures of experts models\nSuppose that responses y1,...,ynare observed. They are modelled by a mixture of experts\nmodel (Jacobs et al., 1991, Jordan and Jacobs, 1994) of the form\nyi|\u03b4i,\u03b2,\u03b1 \u223c N(xT\ni\u03b2\u03b4i,exp(\u03b1T\n\u03b4izi))\nwhere \u03b4iis a categorical latent variable with k categories, \u03b4i\u2208 {1,...,k}, xi= (xi1,...,xip)T\nand zi= (zi1,...,zim)Tare vectors of covariates for observation i, and \u03b2j= (\u03b2j1,...,\u03b2jp)Tand\n\u03b1j= (\u03b1j1,...,\u03b1jm)T, j = 1,...,k are vectors of unknown parameters. The above model says\nthat conditional on \u03b4i = j, the response follows a heteroscedastic linear model where the\nmean is xT\ni\u03b2jand the log variance is zT\ni\u03b1j. The prior for the latent variables \u03b4iis\nP(\u03b4i= j|\u03b3) = pij=\nexp(\u03b3T\n?k\njvi)\nl=1exp(\u03b3T\nlvi), j = 1,...,k\nwhere vi = (vi1,...,vir)Tis a vector of covariates and \u03b3j = (\u03b3j1,...,\u03b3jr)Tare vectors of\nunknown parameters, j = 2,...,k with \u03b31set to be identically zero for identifiability. With\nthis prior the responses are modelled as a mixture of heteroscedastic linear regressions,\nwhere the mixture weights vary with the covariates. For Bayesian inference we require prior\ndistributions on the remaining unknown parameters in the model. Independent priors are\nassumed for the \u03b2j, \u03b2j\u223c N(\u00b50\nfor \u03b3 = (\u03b3T\nin the above model. Variational inference for mixtures of experts models has been considered\nbefore (Waterhouse, MacKay and Robinson, 1996, Ueda and Ghahramani, 2002, Bishop and\nSvens\u00b4 en, 2003) but not for the case of heteroscedastic mixture components. In the case of\nheteroscedastic expert components a variational lower bound can still be computed in closed\nform, allowing fast computation.\n\u03b2j,\u03a30\n\u03b3,\u03a30\n\u03b2j), j = 1,...,k, for \u03b1j, \u03b1j\u223c N(\u00b50\n\u03b3). We will describe fast methods for variational inference\n\u03b1j,\u03a30\n\u03b1j), j = 1,...,k, and\n2,...,\u03b3T\nk)T, \u03b3 \u223c N(\u00b50\n6"},{"page":7,"text":"3Variational approximation\nWe consider a variational approximation to the joint posterior distribution of all the param-\neters \u03b8 of the form q(\u03b8|\u03bb) where \u03bb is a set of variational parameters to be chosen. Variational\napproximation methods originated in statistical physics, and they have been widely used\nin the machine learning community for some time. Jordan et al. (1999) is an early refer-\nence and Bishop (2006, Chapter 10) gives a recent summary. Ormerod and Wand (2009)\ngive an introduction to variational approximation methods that is particularly accessible to\nstatisticians. Variational approximation is a very active area of research in both statistics\nand machine learning. For additional discussion of the current state of the art in relation\nto mixture models see the references in Section 1. Here a parametric form is chosen for\nq(\u03b8|\u03bb) (described below) and we attempt to make q(\u03b8|\u03bb) a good approximation to p(\u03b8|y) by\nminimizing the Kullback-Leibler divergence between q(\u03b8|\u03bb) and p(\u03b8|y), i.e.,\n?\nwhere p(y) is the marginal likelihood p(y) =?p(y|\u03b8)p(\u03b8)d\u03b8. Note that since the Kullback-\nLeibler divergence is positive, we have\n?\nlogq(\u03b8|\u03bb)\np(\u03b8|y)q(\u03b8|\u03bb)d\u03b8 =\n?\nlog\nq(\u03b8|\u03bb)\np(\u03b8)p(y|\u03b8)q(\u03b8|\u03bb)d\u03b8 + logp(y)(1)\nlogp(y) \u2265\nlogp(\u03b8)p(y|\u03b8)\nq(\u03b8|\u03bb)\nq(\u03b8|\u03bb)d\u03b8(2)\nwhich gives a lower bound on the log marginal likelihood, and maximizing the lower bound is\nequivalent to minimizing the Kullback-Leibler divergence between the posterior distribution\nand the variational approximation. From (1), the difference between the lower bound and\nthe log marginal likelihood is the Kullback-Leibler divergence between the posterior and\nvariational approximation, and the lower bound is sometimes used as an approximation to\nthe log marginal likelihood for Bayesian model selection purposes. We discuss the role of the\nmarginal likelihood in Bayesian model comparison later.\nThe difference between our development of a variational approximation for the MHE\nmodel and previous developments of variational methods for ME models with homoscedastic\nexperts lies both in the fact that a closed form derivation of the variational lower bound\nis not obvious in the heteroscedastic case, as well as in the need to deal with the variance\nparameters in the heteroscedastic linear expert models in the optimization. Optimization of\nvariational parameters for the variational posterior factors cannot be done in closed form,\nand since numerical optimization over high-dimensional covariance matrices can be time\n7"},{"page":8,"text":"consuming we have developed an approximate method for dealing with these parameters that\nis computationally efficient and effective in practice. For the model of Section 2, we write the\nparameters as \u03b4 = (\u03b41,...,\u03b4n)T, \u03b2 = (\u03b2T\nthat \u03b8 = (\u03b4T,\u03b2T,\u03b1T,\u03b3T)T. For convenience we write our variational approximation q(\u03b8|\u03bb) as\nq(\u03b8), suppressing dependence on \u03bb in the notation. We consider a variational approximation\nto the posterior of the form q(\u03b8) = q(\u03b4)q(\u03b2)q(\u03b1)q(\u03b3), where\n1,...\u03b2T\nk)T, \u03b1 = (\u03b1T\n1,...\u03b1T\nk)Tand \u03b3 = (\u03b3T\n2,...,\u03b3T\nk)Tso\nq(\u03b4) =\nn\n?\ni=1\nq(\u03b4i),q(\u03b2) =\nk?\ni=1\nq(\u03b2i),q(\u03b1) =\nk?\ni=1\nq(\u03b1i)\nand q(\u03b2i) is normal, N(\u00b5q\ni = 1,...,k, q(\u03b3) is a delta function placing point mass of 1 on \u00b5q\nj) = qij where?k\nposterior that parameters for different expert components are independent and independent\nof all other parameters, that mean and variance parameters are independent, and that the\nlatent variables \u03b4 are independent of each other and independent of all other parameters. We\nassumed a degenerate point mass variational posterior for \u03b3 in order to make computation\nof the lower bound tractable. However, following our description below of the variational\nalgorithm which uses the point mass form for q(\u03b3) we also suggest a method for relaxing the\nform of q(\u03b3) to be a normal distribution. The assumption of independence between mean and\nvariance parameters can also be relaxed (John Ormerod, personal communication) but this\nalso makes the variational optimization slightly more complex. Although the independence\nand distributional assumptions made in variational approximations are typically unrealistic,\nit is often found that variational approaches give good point estimates, reasonable estimates\nof marginal posterior distributions and excellent predictive inferences compared to other\napproximations, particularly in high dimensions. For example, Blei and Jordan (2005),\nSection 5, demonstrate in the context of Dirichlet process mixture models that predictive\ninference based on a variational approximation is similar to fully Bayes predictive inference\nimplemented via MCMC. Braun and McAuliffe (2010), Section 4, report similar findings in\nlarge-scale models of discrete choice.\nHere we are considering \u03b3 as a fixed point estimate and if we write \u03b8\u2212\u03b3 for the rest of\nthe unknown parameters (that is, not including \u03b3) then a lower bound on logp(y|\u03b3) where\n\u03b2i,\u03a3q\n\u03b2i) say for i = 1,...,k, q(\u03b1i) is normal, N(\u00b5q\n\u03b1i,\u03a3q\n\u03b3) and q(\u03b4i=\n\u03b1i) say for\n\u03b3, q(\u03b3) = \u03b4(\u03b3\u2212\u00b5q\nj=1qij = 1, i = 1,...,n, j = 1,...,k. We are assuming in the variational\n8"},{"page":9,"text":"p(y|\u03b3) =?p(\u03b8\u2212\u03b3|\u03b3)p(y|\u03b8)d\u03b8\u2212\u03b3is\n?\nlogp(\u03b8\u2212\u03b3|\u03b3)p(y|\u03b8)\nq(\u03b8\u2212\u03b3)\nq(\u03b8\u2212\u03b3)d\u03b8\u2212\u03b3.\nThe lower bound can be computed in closed form, and this gives a lower bound on sup\u03b3logp(\u03b3)p(y|\u03b3)\nof (see the supplementary materials)\nL = \u2212n\n2log2\u03c0 +(p + m)k\n2\n+ logp(\u00b5q\n\u03b3) \u22121\n2\nk\n?\nj=1\nlog|\u03a30\n\u03b2j| \u22121\n2\nk\n?\nj=1\ntr(\u03a30\n\u03b2j\n\u22121\u03a3q\n\u03b2j)\n\u22121\n2\nk\n?\nk\n?\nn\n?\nj=1\n(\u00b5q\n\u03b2j\u2212 \u00b50\n\u03b2j)T\u03a30\n\u03b2j\n\u22121(\u00b5q\n\u03b2j\u2212 \u00b50\n\u03b2j) \u22121\n2\nk\n?\nn\n?\nj=1\nlog|\u03a30\n\u03b1j| \u22121\n2\nk\n?\n+1\nj=1\ntr(\u03a30\n\u03b1j\n\u22121\u03a3q\n\u03b1j)\n\u22121\n2\nj=1\n(\u00b5q\n\u03b1j\u2212 \u00b50\n?\n\u03b1j)T\u03a30\n\u03b1j\n\u22121(\u00b5q\n\u03b1j\u2212 \u00b50\n\u03b1j) +\ni=1\nk\n?\ni\u03a3q\ni\u03a3q\nj=1\nqijlogpij\nqij\n2\nk\n?\nj=1\nlog|\u03a3q\n\u03b2j| +1\n2\nk\n?\nj=1\nlog|\u03a3q\n\u03b1j|\n\u2212\ni=1\nk\n?\nj=1\nqij\n1\n2zT\ni\u00b5q\n\u03b1j+1\n2\n(yi\u2212 xT\nexp(zT\ni\u00b5q\ni\u00b5q\n\u03b2j)2+ xT\n\u03b1j\u22121\n\u03b2jxi\n\u03b1jzi)\n2zT\n?\n.(3)\nHere p(\u00b5q\nThe variational parameters to be optimized consist of \u00b5q\nqijfor i = 1,...,n, j = 1,...,k. We optimize the lower bound with respect to each of these\nsets of parameters with the others held fixed in a gradient ascent algorithm. The updates\nwhich are available in closed form are easy to derive using vector differential calculus (see,\nfor example, Wand, 2002). To initialize the algorithm, we first generate an initial clustering\nof the data. Then for this initial clustering we do the following.\n\u03b3) is the prior distribution for \u03b3 evaluated at \u00b5q\n\u03b3and pijis evaluated setting \u03b3 = \u00b5q\n\u03b2j, \u03a3q\n\u03b3.\n\u03b2j, \u00b5q\n\u03b1j, \u03a3q\n\u03b1j, j = 1,...,k, \u00b5q\n\u03b3and\nAlgorithm 1:\nInitialize: \u00b5q\nj and 0 otherwise.\nDo until the change in the lower bound between iterations is less than a tolerance:\n?\n?\n\u03b1j= \u03a3q\n\u03b1j= 0 for j = 1,...,k and qijas 1 if the ith observation lies in cluster\n\u2022 \u03a3q\ntor of responses and Djis the diagonal matrix with ith diagonal entry qij\/exp?zT\n\u2022 \u00b5q\n\u03b2j\u2190\nXTDjX + \u03a30\n\u03b2j\n\u22121?\u22121\nwhere X is the design matrix with ith row xT\ni, y is the vec-\ni\u00b5q\n\u03b1j\u2212 1\/2zT\ni\u03a3q\n\u03b1jzi\n?.\n\u03b2j\u2190 \u03a3q\n\u03b2j\n\u03a30\n\u03b2j\n\u22121\u00b50\n\u03b2j+ XTDjy\n?\n.\n9"},{"page":10,"text":"\u2022 Set \u00b5q\nfixed at current values. As a function of \u00b5q\na generalized linear model with normal prior N(\u00b50\n(yi\u2212xT\nis zT\nmode has no closed form expression it is easily found by an iteratively weighted least\nsquares approach (McCullagh and Nelder, 1989, West, 1985) or some other numerical\noptimization technique.\n?\nand Wj is diagonal with ith diagonal element qijwijexp(\u2212zT\ndone provided that the replacement leads to an improvement in the lower bound.\n\u03b1jto be the conditional mode of the lower bound with other variational parameters\n\u03b1jthe lower bound is the log posterior for\n\u03b1j,\u03a30\n\u03b1j), gamma responses wij =\ni\u00b5q\n\u03b1j\u2212 1\/2zT\n\u03b2j)2+xT\ni\u03a3q\ni\u03a3q\n\u03b2jxi, coefficients of variation?2\/qijand where the log of the mean\ni\u00b5q\n\u03b1jziwhere the terms \u22121\/2zT\ni\u03a3q\n\u03b1jzidefine an offset. Although the\n\u2022 \u03a3q\n\u03b1j\u2190\nZTWjZ + \u03a30\n\u03b1j\n\u22121?\u22121\nwhere Z is the design matrix with ith row zT\ni, i = 1,...,n\ni\u00b5q\n\u03b1j)\/2. The update is\n\u2022 For i = 1,...,n,\nqij\u2190\npijexp\n?\n\u22121\n?\n2zT\ni\u00b5q\n\u03b1j\u22121\n2\n(yi\u2212xT\nexp(zT\ni\u00b5q\ni\u00b5q\n\u03b2j)2+xT\n\u03b1j\u22121\ni\u00b5q\ni\u00b5q\ni\u03a3q\ni\u03a3q\n\u03b2jxi\n\u03b1jzi)\n2zT\n?\n?k\nl=1pilexp\n\u22121\n2zT\ni\u00b5q\n\u03b1l\u22121\n2\n(yi\u2212xT\nexp(zT\n\u03b2l)2+xT\n\u03b1l\u22121\ni\u03a3q\ni\u03a3q\n\u03b2lxi\n\u03b1lzi)\n2zT\n?\n\u2022 Set \u00b5q\nat their current values. As a function of \u00b5q\nadditive constants) logp(\u00b5q\nin \u00b5q\nprior on \u00b5q\nregression only one component of this pseudo-response vector would be 1 with the\nother terms 0 and although this is not the case here the usual iteratively weighted\nleast squares algorithm (or some other numerical optimization algorithm) can be used\nfor finding the mode.\n\u03b3to be the conditional mode of the lower bound fixing other variational parameters\n\u03b3, the lower bound is (ignoring irrelevant\n\u03b3)+?n\ni=1\n?k\nj=1qijlogpijwhere pijis computed here plugging\n\u03b3for \u03b3. This is the log posterior for a Bayesian multinomial regression with normal\n\u03b3and where the ith response is (qi1,...,qik)T.In a typical multinomial\nAlgorithm 1 requires an initial clustering in order to initialize the parameters. We con-\nsider multiple clusterings in order to deal with the problem of multiple modes in the optimiza-\ntion. We consider 20 random clusterings where the mixture component for each observation\nis chosen uniformly at random. For these 20 clusterings we do short runs of Algorithm 1\nwith a very loose stopping criterion (we stop when the increase in the lower bound is less\nthan 1) and only follow the most promising solution with the highest attained value of the\nlower bound to convergence. This strategy of \u201cshort runs\u201d to identify a promising solution\n10"},{"page":11,"text":"to follow to full convergence is similar to one recommended for initialization of the EM\nalgorithm for maximum likelihood estimation of mixture models by Biernacki, Celeux and\nGovaert (2003). Variational approaches to fitting mixture models such as those of McGrory\nand Titterington (2007) have made use of the fact that mixture components tend to drop out\nas fitting proceeds in order to do selection of the number of mixture components. This com-\nponent elimination feature is something that can happen with our algorithm also, although\nthis is dependent on the initial clustering used.\nUnlike the ME model with homoscedastic components (Bishop and Svens\u00b4 en, 2003), not\nall the parameters have closed form updates. In steps 1, 2 and 5 above we are able to\noptimize the lower bound with respect to the parameter in closed form. However, in steps 3\nand 6 we need to use an iterative method, and in step 4 we have used an approximation and\nthis update step is skipped if it does not improve the lower bound (3). Motivation for the\napproximation at step 4 comes from the following: suppose the term q(\u03b1j) in our variational\nposterior is not a normal distribution, but instead is not subject to any restriction. The\noptimal choice for this term for maximizing the lower bound with other terms held fixed is\n(see, for instance, Ormerod and Wand, 2009)\nq(\u03b1j) \u221d exp{E(logp(\u03b8)p(y|\u03b8))}\n(4)\nwhere the expectation in the exponent is with respect to the variational posterior for all\nparameters except \u03b1j. The expectation in (4) takes the form, apart from additive constants\nnot depending on \u03b1j, of the log posterior for a gamma generalized linear model (the same\nmodel as considered in step 3 above, except that the offset term in the mean model is\nomitted). If \u00b5q\nthe mean as \u00b5q\n\u00b5q\nwith the ith diagonal element of Wjdefined as in step 4 above. Similar reasoning was used\nby Waterhouse, Mackay and Robinson (1996) in approximating the posterior distribution\nfor the gating function parameters in the homoscedastic mixture of experts model. A referee\nhas pointed out that convergence of the variational Bayes algorithm can be very slow when\nparameters are highly correlated between the blocks used in the variational factorization.\nThis might occur, for example, when there are two very similar mixture components. We\ndon\u2019t see any easy solutions to this problem. One possible remedy consists of integrating\nout the mixture indicators and using bigger blocks for the remaining parameters in the\nblockwise gradient ascent, but this would involve a much greater computational burden and\n\u03b1jis close to the mode, we can get a normal approximation to (4) by taking\n\u03b1jand the covariance matrix as the negative inverse Hessian of the log of (4) at\n\u03b1j. The inverse of the negative Hessian evaluated at \u00b5q\n\u03b1jis of the form (ZTWjZ +\u03a30\n\u03b1j\n\u22121)\u22121\n11"},{"page":12,"text":"the introduction of new approximations to the variational lower bound.\nAt convergence we also replace the delta function variational posterior for \u03b3 with a\nnormal approximation, in which the mean is \u00b5q\nof the negative Hessian of the Bayesian multinomial log posterior considered in step 6 at\nconvergence. The justification for this is similar to our justification above for the update of\n\u03a3q\nidea but they use such an approximation at every step of their iterative algorithm whereas\nwe use only a one-step approximation after first using a delta function approximation to the\nposterior distribution for \u03b3. Write \u03a3q\nfor \u03b3. With this variational posterior the variational lower bound on logp(y) is the same as\n(3), except we need to replace?n\nn\n?\n\u22121\n\u03b3and the covariance matrix is the inverse\n\u03b1jin step 4 of Algorithm 1. Waterhouse, Mackay and Robinson (1996) outline a similar\n\u03b3for the covariance matrix of the variational posterior\ni=1\n?k\nj=1qijlogpij+ logp(\u00b5q\n\u03b3) with\ni=1\nk\n?\nj=1\nqijE\n?\nlog\n?\nexp(vT\n?k\n2tr\ni\u03b3j)\nl=1exp(vT\ni\u03b3l)\n?\n??\n\u22121\n2log|\u03a30\n\u03b3| \u22121\n2(\u00b5q\n\u03b3\u2212 \u00b50\n\u03b3)T\u03a30\n\u03b3\n\u22121(\u00b5q\n\u03b3\u2212 \u00b50\n\u03b3)\n?\n\u03a30\n\u03b3\n\u22121\u03a3q\n\u03b3\n+1\n2log|\u03a3q\n\u03b3| +r(k \u2212 1)\n2\n.\nThe only terms here not in closed form are the expectations in the first term. For the\npurposes of defining a quantity which might be used as an approximation for logp(y) we\nreplace\n?\nE log\n?\nexp(vT\n?k\ni\u03b3j)\nl=1exp(vT\ni\u03b3l)\n??\nwith log\n?\nexp(vT\n?k\ni\u00b5q\n\u03b3j)\ni\u00b5q\nl=1exp(vT\n\u03b3l)\n?\nwhere \u00b5q\nobserve that variational posterior approximations tend to \u201clock on\u201d to a single mode of the\nposterior distribution in mixture models where there are many equivalent modes. Since the\ngap between the variational lower bound and the log marginal likelihood is the Kullback-\nLeibler divergence between the variational posterior and the true posterior, the failure to\napproximate all modes of the true posterior leads to underestimation of the log marginal\nlikelihood by the lower bound. See Bishop (2006) for further discussion. There are related\nconcerns with some MCMC methods for estimating the marginal likelihood (see, for example,\nFr\u00a8 uhwirth-Schnatter, 2004, and the references therein). An adjustment to the optimized\nlower bound to allow for the local nature of the posterior approximation when estimating\nthe marginal likelihood might be considered. If the k! different modes from relabelling are\nwell separated then adding logk! would be a reasonable adjustment. However our experience\nwith this approach when k is large is fairly negative, in the sense that the resulting adjusted\n\u03b3jis the subvector of \u00b5q\n\u03b3corresponding to \u03b3j, j = 2,...,r.As a final note we\n12"},{"page":13,"text":"lower bound doesn\u2019t provide a good approximation to the true log marginal likelihood useful\nfor model comparison: the logk! correction tends to be too large when modes overlap and\nwe usually don\u2019t attempt any adjustment. In the examples of Section 6 we do not emphasize\nthe use of the marginal likelihood for model choice or attempt to support the claim that the\nlower bound estimates the log marginal likelihood accurately.\nWe also note that for very large data sets, since the posterior is of the same form as the\nprior for (\u03b2,\u03b1,\u03b3), it is easy to implement our algorithm sequentially after splitting the data\nset up into smaller chunks. One can learn a variational posterior approximation using the\ndata for the first chunk, and then this can be used as the prior for processing the next chunk\nand so on. There may be difficulties with the naive implementation of this idea, however,\nas the learning may get stuck in a local mode corresponding to the first reasonable solution\nfound. Honkela and Vapola (2003) present an on-line version of variational Bayesian learning\nbased on maintaining a decaying history of previous samples processed by the model which\nensures that the system is able to forget old solutions in favour of new better ones.\n4Variational approximation and model choice\n4.1 Cross-validation\nMarginal likelihood is a popular approach to model selection in a Bayesian context. However,\nwe do not use this approach in our article because in our computations we only have upper\nand lower approximations to the marginal likelihood, and the accuracy of such approxima-\ntions may be very problem de- pendent. In this section we briefly outline how we carry out\nmodel selection using likelihood cross-validation. In B-fold cross-validation we split the data\nrandomly into B roughly equal parts, and then B different training sets are constructed,\nT1,...,TB by successively leaving out one of the B parts from the complete data set. The\ncorresponding test sets (the parts of the data which are left out in each case to construct\nthe training set) are denoted F1,...,FB. Then one useful measure of predictive performance\nthat can be used for model choice, the log predictive density score (LPDS), is\nLPDS =1\nB\nB\n?\ni=1\nlogp(yFi|XFi,yTi).\nNote that\nlogp(yF|XF,yT) = log\n?\np(yF|XF,\u03b8)p(\u03b8|yT)d\u03b8\n13"},{"page":14,"text":"where \u03b8 denotes all the unknown parameters. We have assumed here that yF and yT are\nconditionally independent given \u03b8. This usually does not hold for time series data and\nmodified approaches are appropriate for that case, as we discuss below. In the mixture of\nexperts context p(yF|XF,\u03b8) is easy to write down as a normal mixture given the parameters,\nand we replace p(\u03b8|y) with our variational approximation q(\u03b8). To approximate the integral\nwe generate Monte Carlo samples \u03b8i, i = 1,...,S, from q(\u03b8) and then take the average of the\nvalues p(yF|XF,\u03b8i). In later examples we use S = 1000.\n4.2Model choice in time series\nLater we consider autoregressive time series models in the form of MHE models, and in the\ntime series context the cross-validation approach described above is not very natural. Both\nGeweke and Keane (2007) and Li et al. (2010b) consider a training set y\u2264T= (y1,...,yT) of\nT initial observations and then measure predictive performance by the logarithmic score for\nthe subsequent T\u2217observations y>T= (yT+1,...,yT+T\u2217). That is, predictive performance for\nthe purpose of model comparison is measured by\nlogp(y>T|y\u2264T) =\nT\u2217\n?\ni=1\nlogp(yT+i|y\u2264T+i\u22121)(5)\nwhere\np(yT+i|y\u2264T+i\u22121) =\n?\np(yT+i|\u03b8,y\u2264T+i\u22121)p(\u03b8|y\u2264T+i\u22121)d\u03b8 (6)\nwhere p(\u03b8|y\u2264T+i\u22121) denotes the posterior distribution for all unknowns \u03b8 based on data at\ntime T +i\u22121. Note that (5) contains T\u2217terms and that from (6) each of these terms involves\nconsideration of a different posterior distribution as successive points from the validation set\nare added to the observed data. Geweke and Keane (2007) note that the most reliable and\nefficient way to compute these T\u2217terms is to run an MCMC sampler separately for each of the\nT\u2217terms to estimate the required posterior distribution. This is extremely computationally\ndemanding and if T\u2217is large, and if convergence of the MCMC scheme is slow, this may\nbe completely infeasible. While one might consider importance sampling ideas to reuse the\nMCMC samples for successive terms such an idea is very difficult to implement reliably (see,\nfor example, Vehtari and Lampinen, 2002, for discussion). Li et al. (2010b) consider a\nsimilar approach to Geweke and Keane (2007) for model choice and an approximation where\np(\u03b8|y\u2264T) is used instead of p(\u03b8|y\u2264T+i\u22121) in (6). They presented some empirical evidence for\n14"},{"page":15,"text":"the accuracy of this approach by comparison with a scheme where the posterior was updated\nsequentially every 100th observation in a financial time series example.\nWe note that our variational approach is very efficient for implementing sequential updat-\ning. Apart from the fact that variational approximation is faster than MCMC to begin with,\nin the time series context the result of the variational optimization from the last time step\ncan be used to initialize the optimization for the current time step so that the convergence\ntime of the variational scheme is generally small. This makes variational approaches ideally\nsuited to model choice based on one step ahead predictions and the logarithmic score for\ntime series data.\n5Improving the basic approximation\nIt is well known that variational approximations can underestimate the variance of the pos-\nterior in the context of mixture models. For example, Wang and Titterington (2005) showed\nin the case of Gaussian mixtures that the covariance matrices from Variational Bayes approx-\nimation are too small compared to those obtained by asymptotic arguments from maximum\nlikelihood estimation. Here, we propose a novel approach to improve the estimates obtained\nfrom variational approximation using stochastic approximation (SA) which can result in im-\nproved approximation of the posterior with reduced computational cost compared to MCMC.\nWe proceed as follows. First, we integrate out the latent variables \u03b4ithereby relaxing the\nassumption that the latent variables are independent of other parameters in the variational\napproximation and allowing for an improvement. In approximating the marginal posterior\nwith the indicators integrated out we fix the posterior correlation structure to that obtained\nfrom Algorithm 1, and consider a variational optimization over the choice of the mean and\nvariance in the variational posterior. Fixing the posterior correlation structure keeps the di-\nmension of the optimization problem low. Ji et al. (2010) independently proposed a Monte\nCarlo stochastic approximation for maximizing the lower bound numerically which uses a\nsimilar approach. However, we offer a number of improvements on their implementation\nwhich take the form of an improved gradient estimate in the SA procedure, and the idea of\nperturbing only the mean and scale of an initial variational approximation obtained by the\napproach of Algorithm 1.\nConsider once more the general setting with an unknown \u03b8 to learn about, prior p(\u03b8),\nlikelihood p(y|\u03b8) and variational approximation q(\u03b8|\u03bb) that comes from some parametric\nfamily where \u03bb are parameters to be chosen. We will first develop a stochastic approximation\n15"},{"page":16,"text":"algorithm to maximize the lower bound on the log marginal likelihood. The ideas we describe\nhere are useful quite generally and are not specific to the mixtures context. From (2), the\nlower bound is\nL(\u03bb) =\n?\nq(\u03b8|\u03bb)logp(\u03b8)p(y|\u03b8)\nq(\u03b8|\u03bb)\nd\u03b8.\nMaximizing this lower bound with respect to \u03bb is equivalent to the problem of finding at\nleast one root \u03bb\u2217such that g(\u03bb) \u2261\navailable, the root-finding SA algorithm introduced by Robbins and Monro (1951) may be\nused for finding \u03bb\u2217and one of the conditions for the algorithm to converge is for the noise\nto have mean zero. Since the lower bound is an expectation with respect to q(\u03b8|\u03bb), unbiased\nmeasurements of g(\u03bb) at any \u03bb may be computed provided it is valid to interchange the\nderivative\n\u2202\n\u2202\u03bbL(\u03bb) = 0. When noisy measurements of g(\u03bb) are\n\u2202\n\u2202\u03bband the integral. In this case,\ng(\u03bb) =\n?\nlog\n?p(\u03b8)p(y|\u03b8)\nq(\u03b8|\u03bb)\n?\u2202 logq(\u03b8|\u03bb)\n\u2202\u03bb\nq(\u03b8|\u03bb)d\u03b8\nsince\n?\n\u2202 logq(\u03b8|\u03bb)\n\u2202\u03bb\nq(\u03b8|\u03bb)d\u03b8 = 0.\nAn unbiased estimate of the gradient g(\u03bb) is thus\n?\nlog\n?p(\u03b8?)p(y|\u03b8?)\nq(\u03b8?|\u03bb)\n?\n\u2212 c\n?\u2202 logq(\u03b8?|\u03bb)\n\u2202\u03bb\n(7)\nwhere \u03b8?\u223c q(\u03b8|\u03bb) and c can be chosen arbitrarily. Now note that\nlogp(y) = logp(\u03b8)p(y|\u03b8)\np(\u03b8|y)\n,\nfor every \u03b8. This suggests that if q(\u03b8|\u03bb) is a good approximation to p(\u03b8|y), (as it might be\nnear the optimal value of \u03bb) then the term\n?\nlog\n?p(\u03b8?)p(y|\u03b8?)\nq(\u03b8?|\u03bb)\n?\n\u2212 c\n?\nin the gradient estimate is nearly constant and equal to logp(y)\u2212c. In this case this would\nmake the variance of the gradient estimate, when \u03bb is near the optimal value, roughly equal\nto\n(logp(y) \u2212 c)2Var\n?\u2202 logq(\u03b8?|\u03bb)\n\u2202\u03bb\n?\n.\n16"},{"page":17,"text":"This suggests that when \u03bb is close to the optimal value taking c close to logp(y) is a good\nchoice. In our application to mixtures, for the sequence of gradient estimates constructed\nin the stochastic approximation procedure we initialize c to be the variational lower bound\nobtained from Algorithm 1 and then update it as the algorithm proceeds. This is described\nmore precisely below. Ji et al. (2010) considered a similar approach but they effectively\nuse c = 1, obtained by differentiating directly under the integral sign. From our experience,\nchoosing c = 1 is usually suboptimal, but they counteract the variability in the gradient\nestimate by using multiple simulations from q(\u03b8|\u03bb). Clearly from the above arguments if\nlogp(y) is large in magnitude, c = 1 could result in a gradient estimate with very high\nvariance (since the factor (logp(y) \u2212 1)2is large), and this is supported by simulations we\nhave conducted (results not shown).\nWith an unbiased estimate of the gradient, a stochastic gradient algorithm can now be\nused for optimizing the lower bound. Let \u03bb(0)be some initial estimate of \u03bb. We consider the\nfollowing algorithm:\nAlgorithm 2:\nFor k = 0,1,2,...\n\u2022 Simulate \u03b8(k)\u223c q(\u03b8|\u03bb(k)).\n\u2022 Set\n\u03bb(k+1)\n= \u03bb(k)+ akH(\u03bb(k)) (8)\nwhere H(\u03bb(k)) is an unbiased estimate of the gradient g(\u03bb(k)).\nUnder regularity conditions (Spall,2003), the \u03bb(k)will converge to a local maximum of\nthe lower bound. The ak, k \u2265 0, are a sequence satisfying the conditions\n?\nak\u2192 0\nk\nak= \u221e\n?\nk\na2\nk< \u221e.\nIn particular, it is important to balance the gain sequence ak so that it goes to zero fast\nenough to damp out noise effects when optimal \u03bb is close, but not too fast to avoid false\nconvergence. The step (8) is a stochastic version of a gradient ascent algorithm where the\nstep sizes decrease according to the sequence ak.\nAn estimate of the lower bound on the log marginal likelihood from the stochastic ap-\n17"},{"page":18,"text":"proximation iterates is\n1\nN \u2212 N0\nN\n?\ni=N0+1\nlogp(\u03b8(i))p(y|\u03b8(i))\nq(\u03b8(i)|\u03bb(i))\nwhich involves negligible additional computation after running the recursion (8). Here N\nis the total number of iterations (this is often a fixed number based on our computational\nbudget but should be large enough to ensure convergence) and N0 is an initial number\nof iterates to discard where we are not yet close to the optimal solution. The stochastic\napproximation algorithm is easy to implement provided that q(\u03b8|\u03bb) is easy to simulate from.\nIn our gradient estimate there is a constant c that we have argued should be chosen to\nbe an estimate of the log marginal likelihood. We initialize c as the estimate of the log\nmarginal likelihood from Algorithm 1 and at iteration k > 1 of Algorithm 2 we use the\nabove stochastic approximation log marginal likelihood estimate for c with N0 = 0 and\nN = k \u2212 1. The stochastic approximation algorithm for our mixture model is described in\nmore detail in the supplementary materials.\n6 Examples\n6.1Emulation of a rainfall runoff model\nOur first example is concerned with emulation of a deterministic rainfall runoff model, the\nAustralian Water Balance Model (AWBM) (Boughton, 2004). The goal of model emulation\nfor a deterministic model is to develop a computationally cheap statistical surrogate (the\nemulator) for the original model for some characteristic of the model output of interest. For\napplications where the deterministic model is expensive to run and where we wish to run the\nmodel many times (in model calibration for example) replacing the deterministic model by\nthe emulator may allow similar results to be achieved with an order of magnitude reduction\nin computation time. Here we will be concerned with using a mixture of experts model to\nemulate the AWBM streamflow response at a time of peak rainfall input (the response y)\nas a function of the three AWBM model parameters (the covariates). For an overview of\nthe statistical analysis of computer models and model emulation see O\u2019Hagan (2006). In the\nstatistical literature Gaussian process models which interpolate model output are often used\nfor construction of emulators, but it is often recommended to include an independent noise\nterm in such models (Pepelyshev, 2010).\nThe AWBM uses input time series of rainfall and evapotranspiration data to produce\n18"},{"page":19,"text":"estimates of catchment streamflow and is one of the most widely used rainfall-runoff mod-\nels in Australia for applications such as estimating catchment water yield or design flood\nestimation. The model has three parameters - the maximum storage capacity S, the base\nflow index BFI and baseflow recession factor K. We have available model simulations for\napproximately eleven years of average monthly potential evapotranspiration and daily rain-\nfall data for the Barrington River catchment, located in north eastern New South Wales in\nAustralia. The model was run for 500 different values of the parameters (S,K,BFI) which\nwere generated according to a maximin Latin hypercube design. Our goal here is to emulate\nthe streamflow response of the AWBM at a fixed time (the time of peak input rainfall) as\na function of the parameters S and K (the model output at this time is fairly insensitive\nto the value of the BFI). We use a mixture of experts model as an emulator where the\nresponse is the AWBM output at the time of peak input rainfall (y), and the predictors are\nS (x1) and K (x2). We have added a small amount of independent normal random noise\nwith a standard deviation 0.01 to the response y to avoid degeneracies in the variance model\nin regions of the space where the response tends to be identically zero.\nWe considered fitting five models to the data. The first four models are MHE models\nwith both predictors in the mean and variance models. The models differ according to the\nnumber of mixture components with models A, B, C and D in Table 1 having respectively 2,\n3, 4 and 5 mixture components. Model E in Table 1 is a model with four mixture components\nbut with only an intercept in the variance model for the experts (a homoscedastic mixture\nof experts). For our normal prior distributions we have used, in the notation of Section 2,\n\u00b50\nvectors and covariance matrices have the appropriate dimensions depending on the model\nfitted. The estimated log marginal likelihoods (from the variational lower bound) for the\nmodels fitted are given in Table 1, showing a clear preference for model C, the 4 component\nMHE model. However, it is not our goal in this work to support model choice based on the\nlower bound and we discuss further below model choice via the LPDS.\nFigure 1 summarizes the fitted model with 4 heteroscedastic mixture components. Here\nwe have separated the observations into clusters according to which mixture component\neach observation is most likely to belong to, and plotted observations for each cluster to-\ngether with the fitted mean and standard deviation for each mixture component. Different\nrows correspond to different mixture components. We have emphasized that an important\nadvantage of fast variational approaches to inference is the ability to fit many models for\nmodel assessment and exploratory analysis. For instance, it is very difficult to use cross-\n\u03b2j= 0, \u03a30\n\u03b2j= 10000 \u00b7 I, \u00b50\n\u03b1j= 0, \u03a3\u03b1j = 100 \u00b7 I, \u00b50\n\u03b3= 0 and \u03a30\n\u03b3= 100 \u00b7 I where mean\n19"},{"page":20,"text":"0.0 0.20.40.6 0.81.0\n  0\n 20\n 40\n 60\n 80\n100\n120\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nX1\nX2\ny\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n50\n100\nX1\nX2\nStandard Deviation\n0.00.2 0.4 0.60.81.0\n  0\n 20\n 40\n 60\n 80\n100\n120\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nX1\nX2\ny\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G G G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n20\n40\n60\n80\n100\n120\nX1\nX2\nStandard Deviation\n0.00.20.40.60.8 1.0\n  0\n 20\n 40\n 60\n 80\n100\n120\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nX1\nX2\ny\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG GG GG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG GG G\nG GG G\nG G\nG G\nG G\nG G\nG GG G G G\nG G\nG G\nG G G GG G G G\nG G\nG G\nG G\nG GG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n50\n100\nX1\nX2\nStandard Deviation\n0.00.2 0.4 0.60.81.0\n  0\n 20\n 40\n 60\n 80\n100\n120\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nX1\nX2\ny\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\nG G\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n50\n100\nX1\nX2\nStandard Deviation\nFigure 1: Fitted component means (first column) and standard deviations (second column)\nfor four component MHE model for rainfall-runoff example.\n20"},{"page":21,"text":"Table 1: Marginal log likelihood estimated by variational lower bound (first row) and LPDS\nwith ten-fold cross validation estimated by variational approximation (second row) and\nMCMC (third row) for inverse problem example.\nModel A\n-803.4\n-65.9\n-65.5\nModel B\n-688.4\n-54.5\n-54.2\nModel C\n-678.5\n-51.5\n-51.2\nModel D\n-682.8\n-52.1\n-51.4\nModel E\n-729\n-57.2\n-57.4\nML with VA\nLPDS with VA\nLPDS with MCMC\nvalidatory approaches to model comparison if MCMC methods are used for computation\nsince we require repeated MCMC runs for model fits to different parts of the data and for\nmany models. Table 1 shows LPDS values obtained from both using the variational ap-\nproximation and MCMC. The LPDS values computed by the variational approach compare\nwell with those obtained by MCMC, and the results suggest that a model with 4 mixture\ncomponents is adequate. We note that the results for MCMC for model D need to be treated\nwith some caution as there is very slow mixing in the MCMC scheme here due to the use\nof too many mixture components and hence a poorly identified model. For the variational\napproximation for this case, one of the mixture components effectively drops out with the\nmixing weights being very small for all observations for one of the components.\nTable 2 shows the CPU times taken for fitting the model to the full dataset and for\nimplementing cross-validation using both an MCMC approach and our variational method\nwithout stochastic approximation correction. All code was written in the R language and\nrun on an Intel Core i5-2500 3.30Ghz processor workstation. Some difficulties in comparing\nMCMC with variational approximation in this way need to be noted. First, the time taken\nto use the variational approximation will depend on the way the algorithm is initialized,\nthe stopping rule, and the rate of convergence will also depend on the problem. Similarly,\nfor MCMC, the time taken depends on the number of sampling iterations, the number of\n\u201cburn in\u201d iterations required to achieve convergence and the sampling algorithm - these\nfactors also tend to be problem specific. Here the method of intialization of the variational\nmethod is the one described in Section 3 using short runs for multiple clusterings, and we\nstop the variational algorithm when the relative change in the lower bound between succes-\nsive iterations is less than 10\u22126. The MCMC algorithms were run for 10,000 iterations with\n1,000 iterations burn in both for fitting the full data set and in the cross-validation calcu-\nlations. Such short run times are only possible because our MCMC scheme actually uses a\nvery good proposal based on the variational approximation itself. We considered a random\n21"},{"page":22,"text":"walk Metropolis-Hastings algorithm with the mixture component indicators integrated out\nwhere the proposal covariances are taken from the variational method and parameters are\nupdated in blocks that correspond to the variational factorization. This MCMC algorithm\ngenerally mixes rapidly and initial values can also be based on the variational approxima-\ntion so that 1000 \u201cburn in\u201d iterations was sufficient for all models fitted here. Looking at\nthe cross-validation calculations, there is a roughly 20 fold speed up by using variational\napproximation in the computations when using just 10,000 iterations in the MCMC sam-\npling for all models. This is a fairly conservative estimate of the benefits and consistent\nwith other comparisons in the variational approximation literature. The difficulties of con-\nvergence assessment in the MCMC approach are also avoided by the variational method.\nFor model C, we also compared posterior distributions obtained via MCMC with both our\nTable 2: CPU times for variational approximation (VA) and MCMC for fitting inverse\nproblem example. Times (in seconds) are given for full data and cross-validation calculations,\nfor models 1-4.\nModel A\n88\n330\n121\n2941\nModel B\n146\n473\n184\n4409\nModel C\n215\n650\n281\n5979\nModel D\n274\n825\n393\n7626\nModel E\n254\n659\n276\n5929\nFull data VA\nMCMC\nVA\nMCMC\nCross validation\nsimple variational approximation and the variational approximation incorporating stochas-\ntic approximation correction. 10,000 iterations of stochastic approximation were used. Our\ngain sequence was ak = 0.4\/(k + 10000)0.9for the variance adjustment parameters and\nak= 0.4\/(x+10000)0.8for the mean adjustment parameters. Computation of the stochastic\napproximation correction took approximately 166 seconds of CPU time. Figure 2 shows the\nmarginal posterior distributions for the parameters in the gating function. We are looking\nat just one of the modes here and there are no issues with label switching in the MCMC\nas the modes corresponding to relabelling are well separated. The stochastic approximation\ncorrection is helpful for obtaining an improved approximation for at least some of the pa-\nrameters, with the estimated posterior marginals from stochastic approximation(dot-dashed\nlines) generally being closer to the Monte Carlo estimated marginals (solid lines) than the\nsimple variational estimated marginals (dashed lines). There is little improvement in esti-\nmation of the marginal posteriors for the mean and variance parameters or in predictive\ninference by the stochastic approximation correction (results not shown). Similar benefits\n22"},{"page":23,"text":"in estimation of the gating function parameters have been observed in other examples that\nwe have considered. We also conducted a small simulation study to investigate model selec-\ntion performance for the variational approach using ten fold cross-validation. In particular,\nwe simulated 50 data sets from the fitted four component heteroscedastic model that was\nchosen as best in the analysis above. The parameters used for simulating the data were the\nvariational posterior mean values obtained from fitting to the real data. In our simulations\nwe compared heteroscedastic models with different numbers of mixture components with x1\nand x2in both the mean and variance models (now model C is the \u201ctrue\u201d model). When\nusing cross-validation to select the best model the true model was chosen for 32 of the 50\nsimulated data sets, with model D (with one extra mixture component) being chosen in 17\ncases and a six component MHE model chosen once.\n6.2Time series example\nGeweke and Keane (2007) consider a data set of returns to the S&P500 stock market index.\nWe consider an analysis of the same data but follow Li et al. (2010b) and incorporate some\nmore recent observations. The data consists of 4646 daily returns from January 1, 1990 to\nMay 29, 2008 and our response ytis logpt\/pt\u22121where ptis the closing S&P500 index on day t.\nGeweke and Keane (2007) and Li et al. (2010b) consider time series models for the data using\nmixtures of experts where the covariates include functions of lagged response values. We\nrefer the reader to Li et al. (2010b) for a more comprehensive description of the data, and we\nuse their predictors Lastweek (average of return for last 5 trading days) LastMonth (average\nof return for last 20 trading days) and MaxMin95 ((1\u2212\u03a6)?\ncovariates were found to be significant in the dispersion model in Li et al. (2010b) in fitting\na certain one component skew t model with dispersion, skewness and degrees of freedom all\nfunctions of covariates.\nFor the S&P500 data, we follow Li et al. (2010b) and take T = 4646 training observations\nand T\u2217= 199 validation observations. Li et al. (2010b) note that the choice of the last 199\nobservations in the series for validation is a difficult test for candidate models because this\nperiod covers the recent financial crisis where unusually high volatility can be observed.\nWe consider mixtures of experts models with only an intercept term in the mean model\nbut an intercept and the covariates LastWeek, LastMonth and MaxMin95 in the variance\nmodel and gating function and m = 1,2,3 and 4 experts. Table 3 shows the LPDS values\ncomputed using the variational approximation with sequential updating of the posterior at\ns\u03c6s(logp(h)\nt\u22121\u2212s\u2212logp(l)\nt\u22121\u2212s) with\n\u03a6 = 0.95, p(h)\nt\u22121\u2212sand p(l)\nt\u22121\u2212sthe highest and lowest values of the index on day t). These\n23"},{"page":24,"text":"468 10 1418\n0.0 0.1 0.2 0.3 0.4\nIntercept\u2212component 2\n\u221260 \u221250\u221240 \u221230 \u221220\n0.00\n0.04\n0.08\n0.12\nX1 coefficient\u2212component 2\n\u221250510\n0.0\n0.1\n0.2\n0.3\n0.4\nX2 coefficient\u2212component 2\n\u221235\u221225 \u221215\n0.00\n0.06\n0.12\nIntercept\u2212component 3\n2030405060\n0.00\n0.04\n0.08\nX1 coefficient\u2212component 3\n\u2212202468\n0.0\n0.2\n0.4\nX2 coefficient\u2212component 3\n\u22126\u22124\u22122024\n0.0\n0.2\n0.4\nIntercept\u2212component 4\n\u221215\u221210 \u2212505\n0.00\n0.10\n0.20\nX1 coefficient\u2212component 4\n02468 1014\n0.0\n0.2\n0.4\nX2 coefficient\u2212component 4\nFigure 2: Marginal posterior distributions for parameters in the rainfall runoff example\nin the gating function estimated by Monte Carlo method (solid line), simple variational\napproximation (dashed line) and variational approximation with stochastic approximation\ncorrection (dot-dashed line). Rows are different components, and the left, middle and right\ncolumns respectively shows the intercept terms and coefficients for x1and x2.\n24"},{"page":25,"text":"each time point as well as the LPDS computed using the approximation of Li et al. where\nthe posterior is not updated after the end of the training period (the latter for both the\nvariational method and using MCMC). Predictive density variates in the expression for\nthe LPDS are approximated averaging over 1000 Monte Carlo draws of the parameters for\neach method. Based on the largest LPDS it seems reasonable to choose a two component\nmixture as providing an adequate model. Computation times for MCMC and variational\nTable 3: LPDS values for 1,2,3 and 4 mixture components and MCMC method with ap-\nproximation of Li et al. without sequential updating (first line) variational method with\napproximation of Li et al. (second line) and variational method with sequential updating\n(last line)\nNumber of mixture components\n12\n-477.8-471.2\n-478.0 -470.1\n-477.7-470.0\n34\nWithout sequential updating, MCMC\nWithout sequential updating, VA\nWith sequential updating, VA\n-469.0\n-470.1\n-470.1\n-470.6\n-471.7\n-473.3\napproximation are shown in Table 4. The time for MCMC is just for the initial fit (based on\n10,000 iterations for each of the models with 1000 burn in), but for variational approximation\nwe show the times for both the initial fit and initial fit plus sequential updating for validation.\nThe stopping criterion for the variational method is based on a relative tolerance of 10\u22126for\nthe lower bound. In this case there would be a roughly 200 fold speed up from employing\nthe variational method as the complete computations using the variational method (inital fit\nplus validation) are similar to the computational requirements of just the initial fit for the\nMCMC method and recall that the computational cost for the initial fit for MCMC needs\nto be multiplied by approximately T\u2217= 199 to get the computational cost for the complete\ncomputations.\nAnother application where MCMC methods may not be feasible at all for time series data\nis where a model is fitted repeatedly within a rolling window of observations. We illustrate\nthis here for our two component mixture model where we examine parameter estimates for\nthe model within different windows to investigate the question of structural breaks and model\ninstability. For the S&P500 data we consider windows of size M = 500. First we fit the model\nto the first M observations. Next, we advance the rolling window by 50 observations (i.e. we\nconsider observations 51 to M +50) and refit the model. We continue in this way, advancing\nthe rolling window by 50 observations at each step. Figure 3 shows the estimated lower 1%\n25"},{"page":26,"text":"Table 4: Computation times (seconds) for LPDS calculations for models with 1,2,3 and 4\nmixture components in the S&P500 example. Rows 1-3 respectively are times for initial fit\nfor MCMC, initial fit for variational approximation, and initial fit plus sequential updating\nfor validation for variational approximation.\nNumber of mixture components\n123\n50424633427\n1 7391022\n25019022552\n4\nInitial fit MCMC\nInitial fit VA\nInitial fit + validation VA\n4417\n1442\n4754\nand 5% quantiles of the predictive densities for the covariate values for times t = 1000 and\nt = 4000 versus the upper edge of the rolling window. There is some evidence of model\ninstability and structural change. Also shown in Figure 3 are estimated predictive densities\nfor the same covariates computed via MCMC (solid lines) and the variational approximation\n(dashed lines). One can observe that the MCMC and variational predictive densities are\nnearly indistinguishable, so that the variational approximation provides excellent predictive\ninference here.\n7 Discussion and conclusions\nOur article describes fast variational approximation methods for mixtures of heteroscedastic\nexperts models and illustrates the benefits of this methodology in problems where repeated\nrefitting of models is required such as in exploratory analysis and cross-validation approaches\nto model choice. There are a number of promising avenues for future research. One inter-\nesting idea is to combine variational methods (particularly the stochastic approximation\napproach of Section 4) with MCMC methods applied to a subset of the data. It might be\npossible to get a rough idea of the correlation structure in the posterior from an MCMC\nrun for a subset and then to adjust means and variances using stochastic approximation\napproaches similar to those that we have described. There are many issues to be addressed\nin practice with such an approach however. MCMC methods and variational methods can\nbe complementary, in the sense that variational methods are able to provide good proposal\ndistributions for MCMC schemes, a strategy that is sometimes called variational MCMC (de\nFreitas et al., 2001). The combination of variational methods with stochastic approximation\nhas the potential to broaden the applicability of such an approach. Another interesting ex-\n26"},{"page":27,"text":"1000200030004000\n\u221214\n\u221212\n\u221210\n\u22128\n\u22126\n\u22124\n\u22122\nWindow location\nQuantile\n1000 2000 3000 4000\n\u22128\n\u22127\n\u22126\n\u22125\n\u22124\n\u22123\n\u22122\n\u22121\nWindow location\nQuantile\n\u22124\u22122024\n0.0\n0.2\n0.4\n0.6\n0.8\ny\nPredictive density\n\u22124\u22122024\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\ny\nPredictive density\nFigure 3: Estimated 5% (solid line) and 1% (dashed line) quantiles of predictive densities\nbased on rolling window for S&P 500 example and covariate values at t = 1000 (top left) and\nt = 4000 (top right). Estimated quantiles are plotted versus the upper edge of the rolling\nwindow. Also shown are the estimated predictive densities at covariate values for t = 1000\nand t = 4000 (bottom left and right respectively) estimated based on the entire training\ndata set using MCMC (solid line) and variational approximation (dashed line).\n27"},{"page":28,"text":"tension which we have not pursued for MHE models is to allow some of the coefficients in\nthe expert components to be shared across components. This may be particularly useful in\nthe variance models for the expert components.\nAcknowledgements\nDavid Nott was supported by Singapore MOE grant R-155-000-068-133. Siew Li Tan was\npartly supported as part of the SDWA\u2019s tropical reservoir research programme. The authors\nalso wish to thank John Ormerod for comments and suggestions related to this work. Robert\nKohn\u2019s research was partially supported by ARC grant DP0988579. We thank Lucy Marshall\nfor supplying the data for the example in Section 6.1.\nReferences\nBiernacki, C., Celeux, G. and Govaert, G. (2003). Choosing Starting Values for the EM\nAlgorithm for Getting the Highest Likelihood in Multivariate Gaussian Mixture Models.\nComp. Statist. Data Anal., 41, 561\u2013575.\nBishop, C.M. (2006). Pattern Recognition and Machine Learning. New York: Springer.\nBishop, C. M. and Svens\u00b4 en, M. (2003). Bayesian hierarchical mixtures of experts. In:\nU. Kjaerulff and C. Meek (Eds.), Proceedings Nineteenth Conference on Uncertainty in\nArtificial Intelligence, pp. 5764, Morgan Kaufmann.\nBlei, D. M. and Jordan, M.I. (2006). Variational inference for Dirichlet process mixtures.\nBayesian Analysis, 1, 121-144.\nBraun, M. and McAuliffe, J. (2010). Variational Inference for Large-Scale Models of Discrete\nChoice. J. .Amer. Statist. Assoc., 105, 324\u2013335.\nBoughton, W., 2004. The Australian water balance model. Environmental Modelling and\nSoftware 19, 943\u2013956.\nChib, S. and Jeliazkov, I. (2001). Marginal likelihood from the Metropolis-Hastings output.\nJ. Amer. Statist. Assoc., 96, 270\u2013281.\n28"},{"page":29,"text":"Constantinopoulos, C. and Likas, A. (2007). Unsupervised learning of Gaussian mixtures\nbased on variational component splitting. IEEE Trans. Neural Netw., 18, 745-755.\nCorduneanu, A., and Bishop, C.M. (2001). Variational Bayesian model selection for mix-\nture distributions. In: T. Jaakkola and T. Richardson (Eds), Artifcial Intelligence and\nStatistics, Morgan Kaufmann, 27\u201334.\nde Freitas, N., H\u00f8jen-S\u00f8rensen, P., Jordan, M.I. and Russell, S. (2001). Variational MCMC.\nIn: J. Breese and D. Koller (Ed)., Uncertainty in Artificial Intelligence (UAI), Proceedings\nof the Seventeenth Conference, 120\u201327.\nDe Iorio, M., M\u00a8 uller, P., Rosner, G.L. & MacEAchern, S.N. (2004). An ANOVA model\nfor dependent random measures. Journal of the American Statistical Association, 99,\n205\u201315.\nDunson, D.B., Pillai, N. and Park, J-H. (2007). Bayesian density regression. Journal of the\nRoyal Statistical Society, Series B, 69, 163\u201383.\nFr\u00a8 uhwirth-Schnatter, S. (2004). Estimating marginal likelihoods for mixture and Markov\nswitching models using bridge sampling techniques. The Econometrics Journal, 7, 143\u2013\n167.\nGeweke, J. and Amisano, G. (2010). Comparing and evaluating Bayesian predictive distri-\nbutions of asset returns. International Journal of Forecasting, 26, 216\u2013230.\nGeweke, J. and Keane, M. (2007). Smoothly mixing regressions. Journal of Econometrics,\n138, 252\u2013291.\nGhahramani, Z. and Beal, M.J. (2000). Variational inference for Bayesian mixtures of factor\nanalysers. In: S. A. Solla, T. K. Leen, and K.- R. M\u00a8 uller (Eds), Advances in Neural\nInformation Processing Systems, Volume 12, 831-864.\nGriffin, J.E. and Steel, M.F.J. (2006). Order-based dependent Dirichlet processes. Journal\nof the American Statistical Association, 101, 179\u201394.\nHonkela, A. and Valpola, H. (2003) On-line Variational Bayesian Learning. In: Proceedings\nof the 4th International Symposium on Independent Component Analysis and Blind Signal\nSeparation, ICA 2003, Nara, Japan, 803-808.\n29"},{"page":30,"text":"Jacobs, R., Jordan, M., Nowlan, S. and Hinton, G. (1991). Adaptive mixtures of local\nexperts. Neural Computation, 3, 79\u201387.\nJiang, W. and Tanner, M. (1999). Hierarchical mixtures-of-experts for exponential family\nregression models: Approximation and maximum likelihood estimation. Ann. Statist.,\n27, 987-1011.\nJi, C., Shen, H. and West, M. (2010). Bounded approximations for marginal likelihoods.\nTechnical report, Duke University ISDS, available at\nhttp:\/\/ftp.stat.duke.edu\/WorkingPapers\/10-05.html\nJordan, M.I., Ghahramani, Z., Jaakkola, T.S., Saul, L.K., 1999. An introduction to varia-\ntional methods for graphical models. In M. I. Jordan (Ed.), Learning in Graphical Models.\nMIT Press, Cambridge.\nJordan, M.I. and Jacobs, R.A. (1994). Hierarchical mixtures of experts and the EM algo-\nrithm. Neural Computation, 6, 181\u2013214.\nLi, F., Villani, M., and Kohn, R. (2010a). Modeling Conditional Densities using Finite\nSmooth Mixtures, in Mixtures: Estimation and Applications (Mengersen, K.L., Robert,\nC. P. and Titterington, D.M. eds), Wiley.\nLi, F., Villani, M. and Kohn, R. (2010b). Flexible modeling of conditional distributions\nusing smooth mixtures of asymmetric student t densities. Journal of Statistical Planning\nand Inference, 140, 3638\u20133654.\nMacEachern, S.N. (1999). Dependent nonparametric processes. In ASA Proceedings of the\nSection on Bayesian Statistical Science, Alexandria, VA: American Statistical Association.\nMcGrory, Clare A. and Titterington, D. M. (2007). Variational approximations in Bayesian\nmodel selection for finite mixture distributions. Comp. Statist. Data Anal., 51, 5352\u2013\n5367.\nNeal, R. M. (2001). Annealed importance sampling. Statistics and Computing, 11, 125\u2013139.\nNorets, A. (2010). Approximation of conditional densities by smooth mixtures of regressions.\nAnn. Statist., 38, 1733-1766.\n30"},{"page":31,"text":"O\u2019Hagan, A., 2006. Bayesian analysis of computer code outputs: a tutorial. Reliability\nEngineering and System Safety 91, 1290\u20131300.\nOrmerod, J.T. and Wand, M.P. (2009). Explaining variational approximations. Preprint\navailable at\nhttp:\/\/www.uow.edu.au\/~mwand\/evapap.pdf\nPepelyshev, A. (2010) The role of the nugget term in the Gaussian process method. MODA\n9\u2014Advances in model-oriented design and analysis, Contrib. Statist. 149-156.\nPesaran, M.H. and Timmermann, A. (2002). Market timing and return prediction under\nmodel instability. J. Empirical Finance, 9, 495\u2013510.\nRobbins, H. and Monro, S. (1951). A Stochastic Approximation Method. Ann. Math. Stat.\n22, 400\u2013407.\nSmyth, G.K. (1989). Generalized linear models with varying dispersion. J. Roy. Statist.\nSoc. Ser. B, 51, 47\u201360.\nSpall, J.C. (2003). Introduction to Stochastic Search and Optimization: Estimation, Simu-\nlation and Control. New Jersey: Wiley.\nSpiegelhalter, D.J., Best, N.G., Carlin, B.P. and Van der Linde, A. (2002). Bayesian Mea-\nsures of Model Complexity and Fit (with Discussion). J. Roy. Statist. Soc. Ser. B, 64,\n583\u2013616.\nUeda, N. and Ghahramani, Z. (2002). Bayesian model search for mixture models based on\noptimizing variational bounds Neural Networks, 15, 1223-1241.\nVehtari, A. and Lampinen, J. (2002). Bayesian model assessment and comparison using\ncross-validation predictive densities. Neural Computation, 14, 2439\u20132468.\nVillani, M., Kohn, R. and Giordani, P. (2009). Regression Density Estimation using Smooth\nAdaptive Gaussian Mixtures. Journal of Econometrics, 153, 155\u2013173\nWaterhouse, S., MacKay, D. and Robinson, T. (1996). Bayesian methods for mixtures of\nexperts. In: D.S. Touretzky, M.C. Mozer and M.E. Hasselmo (Eds.), Advances in Neural\nInformation Processing Systems 8, pp. 351\u2013357, MIT Press.\n31"},{"page":32,"text":"Wand, M.P. (2002). Vector Differential Calculus in Statistics. The American Statistician,\n56, 55\u201362.\nWang, B. and Titterington, D.M. (2005). Inadequacy of interval estimates corresponding\nto variational Bayesian approximations. In: R.G. Cowell and Z. Ghahramani (Eds.),\nProceedings of the 10th International Workshop on Artificial Intelligence, pp. 373\u2013380,\nSociety for Artificial Intelligence and Statistics.\nWedel, M. (2002). Concomitant variables in finite mixture models. Statistica Neerlandica,\n56, 362\u2013375.\nWest, M. (1985), \u201cGeneralized linear models: outlier accommodation, scale parameters and\nprior distributions,\u201d in Bayesian Statistics 2 (eds J.M. Bernardo et al.), pp. 531\u2013538.\nAmsterdam, North Holland.\nWood, S.A., Jiang, W., and Tanner, M.A. (2002). Bayesian Mixture of Splines for Spatially\nAdaptive Nonparametric Regression. Biometrika, 89, 513-528.\nWood, S.A., Kohn, R., Cottet, R., Jiang, W. and Tanner, M. (2008). Locally adaptive\nnonparametric binary regression. J. Comp. Graph. Statist., 17, 352\u2013372.\nWu, B., McGrory, C.A. and Pettitt, A.N. (2011). A new variational Bayesian algorithm with\napplication to human mobility pattern modeling. Statistics and Computing, to appear.\n32"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Robert_Kohn\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000.pdf","widgetId":"rgw31_56ab9f50845fe"},"id":"rgw31_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=254295504&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw32_56ab9f50845fe"},"id":"rgw32_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=254295504&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":254295504,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00b7d53a84badac545000000","name":"Robert Kohn","date":"Jun 23, 2014 ","nameLink":"profile\/Robert_Kohn","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Robert_Kohn\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Robert_Kohn\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"b02580c47a25b0dd96779f057e964a08","showFileSizeNote":false,"fileSize":"1.14 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00b7d53a84badac545000000","name":"Robert Kohn","date":"Jun 23, 2014 ","nameLink":"profile\/Robert_Kohn","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Robert_Kohn\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Robert_Kohn\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"b02580c47a25b0dd96779f057e964a08","showFileSizeNote":false,"fileSize":"1.14 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=eeAhjsPFqVdJ2sdNkMdN0YLf8ph1ZdCt0ZWTnlgoo7JHzZVtHzRYHYcjY4uoBLvrB0uHDyNWUe1mxXCam1YM3Q.AL7-LcOabHLhI2w-0A44yo8E4e4Nvx7Yf2IF0kC8JaKNBRO_qsm9gjN4YYRW7fn_E4itzS3IDacHLKZy-GRXqA","clickOnPill":"publication.PublicationFigures.html?_sg=HVrja3zOjvtJAP69zOgRqAXuqd3ZZ9lNiFTDI3YdxNGp3OFYMRsSaLoV2zorwDgF1ngoDVRTCIV_9QBhtEiaxw.-mPAUbhIBaNSXQUFlFap6cEAqf55Ylx4vsOWmKmp1mKED92hHGh6pjx5Oc8g2EgUgv1b08Q2nqomOObkutjInQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FRobert_Kohn%2Fpublication%2F254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation%2Flinks%2F00b7d53a84badac545000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=mCBNzKZrqxen5GxPmpGGTavtiDIs2EBBeofSn16imTs_oMR92Lzy_IuLJBc4DIvf_M3HY1P9AWtwzuRalgWe7g","urlHash":"ce28304fd804ecd0fac1e9df65b9ef63","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=jiiRQSjSQvkPpo2cxGkvhRBhfCiZWrN0AbSvDInzJtF3auWkXgGWkOD-tC3mTAf9V36r4HFWaA_57rEgIVH85sG0Gj49TnRIoLQLLyewByw.UCIHyYnlfLs_ERAnaSq5GWfp59X2NJxMRLRVN-_lVZEjmVTfFEPJ0GWEky67tHeshcAmt4pmjkZKZduwelAhKQ.YdBlSEE9tf4V4bwa0hWMNijXQRs_uH-tAZlOIQ3sukT9RMaKlnYP1jlx7XljACtbY0x52ieFo5ZP6Nzv--JoEw","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00b7d53a84badac545000000","trackedDownloads":{"00b7d53a84badac545000000":{"v":false,"d":false}},"assetId":"AS:114973388840964@1404423201027","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":254295504,"commentCursorPromo":null,"widgetId":"rgw34_56ab9f50845fe"},"id":"rgw34_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FRobert_Kohn%2Fpublication%2F254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation%2Flinks%2F00b7d53a84badac545000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A114973388840964%401404423201027&publicationUid=254295504&linkId=00b7d53a84badac545000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Regression Density Estimation With Variational Methods and Stochastic Approximation","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=gaR2p9zt8R51C6OGce_UXFfDUQrTXZGzwy2ppP-VHB_32YIrDyIPGwqKZKvI7wilvDni8xOrNrU_-3VFBBgGxkeMpVKJjY2HHI1DZcM9P3g.3BXbgISO-4szFawjPphcleBzamlT9syhDcbWbU05Xz4c4o4p8gInTJOG0r0-M-5vQqzkeUN4nEAAQECQ5mA75A.qndMFPSmnUzIrlqpOE8GseLvisbkp3NFo9XOn0HDf2KOwpfapQB9n7mRiiaB7AAWyqBPG-eba9bLw1TH6Th8mQ","publicationUid":254295504,"trackedDownloads":{"00b7d53a84badac545000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab9f50845fe"},"id":"rgw36_56ab9f50845fe","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab9f50845fe"},"id":"rgw37_56ab9f50845fe","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw38_56ab9f50845fe"},"id":"rgw38_56ab9f50845fe","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw39_56ab9f50845fe"},"id":"rgw39_56ab9f50845fe","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw40_56ab9f50845fe"},"id":"rgw40_56ab9f50845fe","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw35_56ab9f50845fe"},"id":"rgw35_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw33_56ab9f50845fe"},"id":"rgw33_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9f50845fe"},"id":"rgw2_56ab9f50845fe","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":254295504},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=254295504&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9f50845fe"},"id":"rgw1_56ab9f50845fe","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"9MJZ1GGHbhlGMWNGG4CJAaxzdHUjUm5Yx4fbDEjK+yBQQ3WCZcD5XYFMT319sN5F9LrGTCXdwJbzBXfVRiQn0f9SRR5XdwRT9gnncAYV2otQTR7tn9\/9JpYqQRfRZCR1znZ30nGsdGw4V3YZz0xRqb71SmFY3qnKIiml5C1umVuPMth5HgP8tboHmiWX9ar7ogITKdUCbuRKbm0c2vMEqNcEGCM\/ISvwldGm+L+IUXQAI4QzgT1zyQIc++mO\/Id9MXh6l1TDfsFkRRlINq9EsgxdPlzyw8ljh5pNd89nJEU=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Regression Density Estimation With Variational Methods and Stochastic Approximation\" \/>\n<meta property=\"og:description\" content=\"Regression density estimation is the problem of flexibly estimating a response distribution as a function of covariates. An important approach to regression density estimation uses finite mixture...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\" \/>\n<meta property=\"rg:id\" content=\"PB:254295504\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1080\/10618600.2012.679897\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Regression Density Estimation With Variational Methods and Stochastic Approximation\" \/>\n<meta name=\"citation_author\" content=\"David J. Nott\" \/>\n<meta name=\"citation_author\" content=\"Siew Li Tan\" \/>\n<meta name=\"citation_author\" content=\"Mattias Villani\" \/>\n<meta name=\"citation_author\" content=\"Robert Kohn\" \/>\n<meta name=\"citation_publication_date\" content=\"2012\/07\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Journal of Computational and Graphical Statistics\" \/>\n<meta name=\"citation_issn\" content=\"1061-8600\" \/>\n<meta name=\"citation_volume\" content=\"21\" \/>\n<meta name=\"citation_issue\" content=\"3\" \/>\n<meta name=\"citation_doi\" content=\"10.1080\/10618600.2012.679897\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Robert_Kohn\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\/links\/00b7d53a84badac545000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-9d16b777-6463-4b5d-95d7-5901c2b13267","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":480,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw41_56ab9f50845fe"},"id":"rgw41_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-9d16b777-6463-4b5d-95d7-5901c2b13267", "e114a7813413d76fa9714410dc408d33e575dc51");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-9d16b777-6463-4b5d-95d7-5901c2b13267", "e114a7813413d76fa9714410dc408d33e575dc51");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw42_56ab9f50845fe"},"id":"rgw42_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/254295504_Regression_Density_Estimation_With_Variational_Methods_and_Stochastic_Approximation","requestToken":"C6p6fFSEuPqvV3JBPSwP2eXyWVzxq+d9NcQrxmtiPo5JJVGYISq3gVP58MFIn0Om6netkFE\/w3G+pHHDwBwj461wHCphmHoMRX1H\/QFg4B0rzM1Z+jxE3ady\/dVD3H00roHmnAXvydHI2qQm+XcJgAvBs+ZM0aiXNlI3DnQ0JlPrWy1QSPR1XzVXK5knQePTuowR6Tz0+K+fbWj5PX+cc4vBliypqOkIMGtFyG9Z0br9v1m\/uPZjhqTXVYq5Zsfqe6i06za421LTtabXxxECYMofRXujC1wTp9+P\/zQFoRw=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=l2ecvICoYiS1tjw4gHcNufgRiEjg5P9jOo4D4U8BksM5UJr1WzPxl9piSpwrboG1","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjU0Mjk1NTA0X1JlZ3Jlc3Npb25fRGVuc2l0eV9Fc3RpbWF0aW9uX1dpdGhfVmFyaWF0aW9uYWxfTWV0aG9kc19hbmRfU3RvY2hhc3RpY19BcHByb3hpbWF0aW9u","signupCallToAction":"Join for free","widgetId":"rgw44_56ab9f50845fe"},"id":"rgw44_56ab9f50845fe","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw43_56ab9f50845fe"},"id":"rgw43_56ab9f50845fe","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw45_56ab9f50845fe"},"id":"rgw45_56ab9f50845fe","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
