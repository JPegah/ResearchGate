<!DOCTYPE html> <html lang="en" class="" id="rgw52_56ab1b42d0e37"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="k6d8jsrnyms1RmvxQNzZmoQ6jRKYL3n+UCiB+DRSSQVIy3JrQNWzoWnS1ZiTEkVms30g2RWEVHIxYzV9AVmKKyadXnqvuh7TIYLZi5kNpiJnzYVMhCg+4Oj2rBuTOgMuoymrEHPEwU2gczstS91+TjCpDWfZ4JU9gP5V9m/kOKI9Lpj3yedEr3yub8JotRyNKGme0P4NHisvKBlGCjSZM0+yQDHtPU9YDJi0AQBVRPr8vv/s/lzAwcqYVI6gWMpUroP/ksWd+o9qlu2xnLDphYqcH4dizXg7s4yaDUNnF1w="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-44206215-fa9a-4e6d-9ca3-468f07ca00d9",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/220320734_Distributed_Algorithms_for_Topic_Models" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Distributed Algorithms for Topic Models" />
<meta property="og:description" content="We describedistributedalgorithmsfortwowidely-usedtopicmodels,namelytheLatentDirich- let Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In ourdistributed algorithmsthe..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/220320734_Distributed_Algorithms_for_Topic_Models/links/0912f5107f7dc2714e000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/220320734_Distributed_Algorithms_for_Topic_Models" />
<meta property="rg:id" content="PB:220320734" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1145/1577069.1755845" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Distributed Algorithms for Topic Models" />
<meta name="citation_author" content="David Newman" />
<meta name="citation_author" content="Arthur U. Asuncion" />
<meta name="citation_author" content="Padhraic Smyth" />
<meta name="citation_author" content="Max Welling" />
<meta name="citation_publication_date" content="2009/08/01" />
<meta name="citation_journal_title" content="Journal of Machine Learning Research" />
<meta name="citation_issn" content="1532-4435" />
<meta name="citation_volume" content="10" />
<meta name="citation_firstpage" content="1801" />
<meta name="citation_lastpage" content="1828" />
<meta name="citation_doi" content="10.1145/1577069.1755845" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/David_Newman6/publication/220320734_Distributed_Algorithms_for_Topic_Models/links/0912f5107f7dc2714e000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/220320734_Distributed_Algorithms_for_Topic_Models" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/220320734_Distributed_Algorithms_for_Topic_Models" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Distributed Algorithms for Topic Models (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Distributed Algorithms for Topic Models on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1b42d0e37" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1b42d0e37" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1b42d0e37">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1145%2F1577069.1755845&rft.atitle=Distributed%20Algorithms%20for%20Topic%20Models&rft.title=Journal%20of%20Machine%20Learning%20Research&rft.jtitle=Journal%20of%20Machine%20Learning%20Research&rft.volume=10&rft.date=2009&rft.pages=1801-1828&rft.issn=1532-4435&rft.au=David%20Newman%2CArthur%20U.%20Asuncion%2CPadhraic%20Smyth%2CMax%20Welling&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Distributed Algorithms for Topic Models</h1> <meta itemprop="headline" content="Distributed Algorithms for Topic Models">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/220320734_Distributed_Algorithms_for_Topic_Models/links/0912f5107f7dc2714e000000/smallpreview.png">  <div id="rgw8_56ab1b42d0e37" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab1b42d0e37" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/David_Newman6" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272452597055500%401441969169380_m" title="David Newman" alt="David Newman" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Newman</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab1b42d0e37" data-account-key="David_Newman6">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/David_Newman6"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272452597055500%401441969169380_l" title="David Newman" alt="David Newman" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/David_Newman6" class="display-name">David Newman</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_California_Irvine" title="University of California, Irvine">University of California, Irvine</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab1b42d0e37"> <a href="researcher/69884878_Arthur_U_Asuncion" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Arthur U. Asuncion" alt="Arthur U. Asuncion" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Arthur U. Asuncion</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1b42d0e37">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69884878_Arthur_U_Asuncion"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Arthur U. Asuncion" alt="Arthur U. Asuncion" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69884878_Arthur_U_Asuncion" class="display-name">Arthur U. Asuncion</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab1b42d0e37"> <a href="researcher/66646470_Padhraic_Smyth" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Padhraic Smyth" alt="Padhraic Smyth" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Padhraic Smyth</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab1b42d0e37">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/66646470_Padhraic_Smyth"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Padhraic Smyth" alt="Padhraic Smyth" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/66646470_Padhraic_Smyth" class="display-name">Padhraic Smyth</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw15_56ab1b42d0e37"> <a href="researcher/69847505_Max_Welling" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Max Welling" alt="Max Welling" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Max Welling</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw16_56ab1b42d0e37">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69847505_Max_Welling"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Max Welling" alt="Max Welling" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69847505_Max_Welling" class="display-name">Max Welling</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1532-4435_Journal_of_Machine_Learning_Research"><span itemprop="name">Journal of Machine Learning Research</span></a> </span>    (Impact Factor: 2.47).     <meta itemprop="datePublished" content="2009-08">  08/2009;  10:1801-1828.    DOI:&nbsp;10.1145/1577069.1755845           <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/journals/jmlr/jmlr10.html#NewmanASW09" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw17_56ab1b42d0e37" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>We describedistributedalgorithmsfortwowidely-usedtopicmodels,namelytheLatentDirich- let Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In ourdistributed algorithmsthe data is partitioned across separate processors and inference is done in a parallel, dis- tributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straight- forward mapping of LDA to a distributed processor setting. In this algorithm processors concur- rently perform Gibbs sampling over local data followed by a global update of topic counts. The al- gorithmissimpletoimplementandcanbeviewedasanapproximationtoGibbs-sampledLDA.The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world textcorporawe show that distributed learning works well in practice. For both LDA and HDP, we show that the convergedtest-data log probability for distributed learning is indistinguish- able from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw18_56ab1b42d0e37" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw32_56ab1b42d0e37">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw44_56ab1b42d0e37">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/David_Newman6/publication/220320734_Distributed_Algorithms_for_Topic_Models/links/0912f5107f7dc2714e000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/David_Newman6">David Newman</a>   </span>  </div>  <div class="social-share-container"><div id="rgw46_56ab1b42d0e37" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw47_56ab1b42d0e37" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw48_56ab1b42d0e37" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw49_56ab1b42d0e37" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw50_56ab1b42d0e37" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw51_56ab1b42d0e37" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw45_56ab1b42d0e37" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Newman6%2Fpublication%2F220320734_Distributed_Algorithms_for_Topic_Models%2Flinks%2F0912f5107f7dc2714e000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw31_56ab1b42d0e37"  itemprop="articleBody">  <p>Page 1</p> <p>Journal of Machine Learning Research x (2009) x-xxSubmitted 6/08; Published xx/xx<br />Distributed Algorithms for Topic Models<br />David Newman<br />Department of Computer Science<br />University of California, Irvine<br />Irvine, CA 92697, USA<br />Arthur Asuncion<br />Department of Computer Science<br />University of California, Irvine<br />Irvine, CA 92697, USA<br />Padhraic Smyth<br />Department of Computer Science<br />University of California, Irvine<br />Irvine, CA 92697, USA<br />NEWMAN@UCI.EDU<br />ASUNCION@ICS.UCI.EDU<br />SMYTH@ICS.UCI.EDU<br />Max Welling<br />Department of Computer Science<br />University of California, Irvine<br />Irvine, CA 92697, USA<br />WELLING@ICS.UCI.EDU<br />Editor:<br />Abstract<br />We describedistributedalgorithmsfortwowidely-usedtopicmodels,namelytheLatentDirich-<br />let Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In ourdistributed<br />algorithmsthe data is partitioned across separate processors and inference is done in a parallel, dis-<br />tributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straight-<br />forward mapping of LDA to a distributed processor setting. In this algorithm processors concur-<br />rently perform Gibbs sampling over local data followed by a global update of topic counts. The al-<br />gorithmissimpletoimplementandcanbeviewedas anapproximationtoGibbs-sampledLDA.The<br />second version is a model that uses a hierarchical Bayesian extension of LDA to directly account<br />for distributed data. This model has a theoretical guarantee of convergence but is more complex<br />to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward<br />mapping approach, and merges newly-created topics either by matching or by topic-id. Using five<br />real-world text corporawe show that distributed learning works well in practice. For both LDA and<br />HDP, we show that the convergedtest-data log probability for distributed learning is indistinguish-<br />able from that obtained with single-processor learning. Our extensive experimental results include<br />learning topic models for two multi-million document collections using a 1024-processor parallel<br />computer.<br />Keywords:<br />Topic Models, Latent Dirichlet Allocation, Hierarchical Dirichlet Processes, Dis-<br />tributed Parallel Computation<br />1. Introduction<br />Very large data sets, such as collections of images or text documents, are becoming increasingly<br />common, with examples ranging from collections of online books at Google and Amazon, to the<br />c 2009 David Newman, Arthur Asuncion, Padhraic Smyth and Max Welling.</p>  <p>Page 2</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />large collection of images at Flickr. These data sets present major opportunities for machine learn-<br />ing, such as the ability to explore richer and more expressive models than previously possible, and<br />provide new and interesting domains for the application of learning algorithms.<br />However, the scale of these data sets also brings significant challenges for machine learning,<br />particularly in terms of computation time and memory requirements. For example, a text corpus<br />with one million documents, each containing one thousand words, will require approximately eight<br />GBytes of memory to store the billion words. Adding the memory required for parameters, which<br />usually exceeds memory for the data, creates a total memory requirement that exceeds that available<br />on a typical desktop computer. If one were to assume that a simple operation, such as computing a<br />probability vector over categories using Bayes rule, takes on the order of<br />then a full pass through the billion words would take 1000 seconds. Thus, algorithms that make<br />multiple passes through the data, for example clustering and classification algorithms, will have run<br />times in days for this sized corpus. Furthermore, for small to moderate sized document sets where<br />memory is not an issue, it would be useful to have algorithms that could take advantage of desktop<br />multiprocessor/multicore technology to learn models in near real-time.<br />An obvious approach for addressing these time and memory issues is to distribute the learning<br />algorithm over multiple processors. Inparticular, with<br />the memory issue by distributing<br />of the total data to each processor. However, the computation<br />problem remains non-trivial for a fairly large class of learning algorithms, namely how to combine<br />local processing on each processor to arrive at a useful global solution.<br />In this general context we investigate distributed algorithms for two widely-used unsupervised<br />learning models: the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Pro-<br />cess (HDP) model. LDA and HDP models are arguably among the most successful recent learning<br />algorithms for analyzing discrete data such as bags of words from a collection of text documents.<br />However, they can take days to learn for large corpora, and thus, distributed learning would be<br />particularly useful.<br />The rest of the paper is organized as follows: In Section 2 we review the standard derivation<br />of LDA and HDP. Section 3 presents our two distributed algorithms for LDA and one distributed<br />algorithm for HDP. Empirical results are provided in Section 4. Scalability results are presented<br />in Section 5, and further analysis of distributed LDA is provided in Section 6. A comparison with<br />related models is given in Section 7. Finally, Section 8 concludes the paper.<br />seconds per word,<br />processors, itis somewhat trivial toaddress<br />2. Latent Dirichlet Allocation and Hierarchical Dirichlet Process Model<br />We start by reviewing the LDA and HDP models. Both LDA and HDP are generative probabilistic<br />models for discrete data such as bags of words from text documents – in this context these models<br />are often referred to as topic models. To illustrate the notation, we refer the reader to the graphical<br />models for LDA and HDP shown in Figure 1.<br />LDA models each of<br />documents in a collection as a mixture over<br />topic being a multinomial distribution over a vocabulary of<br />draw a mixing proportion from a Dirichlet with parameter . For the<br />topic is drawn with probability. Word<br />on valuewith probability . A Dirichlet prior with parameter<br />distributions.<br />latent topics, with each<br />words. For document , we first<br />word in the document, a<br />is then drawn from topic<br />is placed on the word-topic<br />, withtaking<br />2</p>  <p>Page 3</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />α<br />ij<br />Z<br />ij<br />X<br />k φ<br />j θ<br />k<br />α<br />ij<br />Z<br />ij<br />X<br />j θ<br />k φ<br />K<br />D<br />Nj<br />∞<br />Nj<br />D<br />ββ<br />γ<br />η<br />Figure 1: Graphical models for LDA(left) and HDP(right). Observed variables (words) are shaded,<br />and hyperparameters are shown in squares.<br />Thus, the generative process for LDA is given by<br />(1)<br />Toavoid clutterwedenotesampling fromaDirichlet<br />, and likewise for . In this paper, we use symmetric Dirichlet priors for simplicity, un-<br />less specified otherwise. The full joint distribution over all parameters and variables is<br />asshorthand for<br />(2)<br />where<br />summed out.<br />computations, representing the number of words assigned to topic<br />of times word is assigned to topic<br />commonly used variables in Table 1.<br />Given the observed words<br />posterior distribution over the latent topic assignments<br />topics . Approximate inference for LDA can be performed either using variational methods (Blei<br />et al., 2003) or Markov chain Monte Carlo methods (Griffiths and Steyvers, 2004). In this paper we<br />focus on Markov chain Monte Carlo algorithms for approximate inference. MCMC is widely used<br />as an inference method for a variety of topic models, for example Rosen-Zvi et al. (2004), Li and<br />McCallum (2006), and Chemudugunta et al. (2007) all use MCMC for inference. In the MCMC<br />context, the usual procedure is to integrate out the mixtures<br />called collapsing – and just sample the latent variables . Given the current state of all but one<br />, and we use the convention that missing indices are<br />are the two primary count arrays used in<br />in document , and the number<br />in the corpus, respectively. For ease of reading we list<br />and<br />, the task of Bayesian inference for LDA is to compute the<br />, the mixing proportions, and the<br />and topics in (2) – a procedure<br />3</p>  <p>Page 4</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />Description<br />Number of documents in collection<br />Number of distinct words in vocabulary<br />Total number of words in collection<br />Number of topics<br />observed word in document<br />Topic assigned to<br />Count of word assigned to topic<br />Count of topic assigned in document<br />Probability of word given topic<br />Probability of topic given document<br />Table 1: Description of commonly used variables.<br />variable, the conditional probability of is<br />(3)<br />where the superscript<br />HDP is a collection of Dirichlet Processes which share the same topic distributions and can be<br />viewed as the non-parametric extension of LDA. The advantage of HDP is that the number of topics<br />is determined by the data. The HDP model is obtained by taking the following model in the limit as<br />goes to infinity. Let be top level Dirichlet variables sampled from a Dirichlet with parameter<br />. The topic mixture for each document,, is drawn from a Dirichlet with parameters<br />The word-topic distributions are drawn from a base Dirichlet distribution with parameter<br />in LDA,is sampled from, and word is sampled from the corresponding topic<br />generative process is given by<br />means that the corresponding word is excluded in the counts.<br />.<br />. As<br />. The<br />(4)<br />The posterior distribution is sampled using the direct assignment sampler for HDP described<br />in Teh et al. (2006). As was done for LDA, both<br />the following conditional distribution:<br />andare integrated out, andis sampled from<br />if previously used<br />new<br />ifis new.<br />(5)<br />The sampling scheme for<br />probability mass proportional to<br />defined to have infinitely many topics, the sampling algorithm only instantiates topics as needed.<br />is also detailed in Teh et al. (2006). Note that a small amount of<br />newis reserved for the instantiation of new topics. While HDP is<br />4</p>  <p>Page 5</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />0 200400 6008001000<br />1800<br />1850<br />1900<br />1950<br />2000<br />2050<br />2100<br />2150<br />2200<br />Iteration<br />Perplexity<br />Non−Collapsed<br />θ Collapsed<br />θ, φ Collapsed<br />Figure 2: On theNIPS datasetusing<br />faster than the partially collapsed (circles) and non-collapsed (triangles) samplers.<br />topics,the fullycollapsedGibbssampler(solidline)converges<br />Need for Distributed Algorithms: One could argue that it is trivial to distribute non-collapsed<br />Gibbs sampling, because sampling of can happen independently given<br />can be done concurrently. In the non-collapsed Gibbs sampler, one samples<br />and then samplesandgiven . Furthermore, if individual documents are not spread across<br />different processors, one can marginalize over just<br />collapsed scheme, the latent variableson each processor can be concurrently sampled, where the<br />concurrency is over processors.<br />Unfortunately, the non-collapsed and partially collapsed Gibbs samplers exhibit slow conver-<br />gence due to the strong dependencies between the parameters and latent variables. Generally, we<br />expect faster mixing as more variables are collapsed (Liu et al., 1994; Casella and Robert, 1996).<br />Figure 2 shows, using one of the data sets used throughout our paper, that the log probability of<br />test data (measured as perplexity, which is defined in Section 4) of the non-collapsed and partially<br />collapsed samplers converges more slowly than the fully collapsed sampler.<br />The slow convergence of partially collapsed and non-collapsed Gibbs samplers motivates the<br />need to devise distributed algorithms for fully collapsed Gibbs samplers. In the following section<br />we introduce distributed topic modeling algorithms that take advantage of the benefits of collapsing<br />both and .<br />and , and therefore<br />givenand,<br />, since is processor-specific. In this partially<br />3. Distributed Algorithms for Topic Models<br />We introduce algorithms for LDA and HDP where the data, parameters, and computation are dis-<br />tributed over distinct processors. We distribute the<br />imately documents on each processor. Documents are randomly assigned to processors,<br />although as we will see later, the assignment of documents to processors – ranging from random to<br />highly non-random or adversarial – appears to have little influence on the results. This indifference<br />documents overprocessors, with approx-<br />5</p>  <p>Page 6</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />Algorithm 1 AD-LDA<br />repeat<br />for each processor<br />Copy global counts:<br />Sample<br />end for<br />Synchronize<br />Update global counts:<br />until termination criterion satisfied<br />in parallel do<br />locally: LDA-Gibbs-Iteration(,,, , , )<br />is somewhat understandable given that converged results from Gibbs sampling are independent of<br />sampling order.<br />We partition the words from the<br />documents into<br />sponding topic assignments into<br />from documents<br />Topic-document counts are likewise distributed as<br />distributed, with each processor keeping a separate local copy<br />and the corre-<br />, the words, where processor<br />, the corresponding topic assignments.<br />. The word-topic counts<br />.<br />stores<br />, and<br />are also<br />3.1 Approximate Distributed Latent Dirichlet Allocation<br />The difficulty of distributing and parallelizing over Gibbs sampling updates (3) lies in the fact that<br />Gibbs sampling is a strictly sequential process. To asymptotically sample from the posterior distri-<br />bution, the update of any topic assignmentcan not be performed concurrently with the update<br />of any other topic assignment . But given the typically large number of word tokens compared<br />to the number of processors, to what extent will the update of one topic assignment<br />the update of any other topic assignment ? Our hypothesis is that this dependence is weak, and<br />therefore we should be able to relax the requirement of sequential sampling of topic assignments<br />and still learn a useful model. One can see this weak dependence in the following common situation.<br />If two processors are concurrently sampling, but sampling different words in different documents<br />(i.e.), then concurrent sampling will be very close to sequential sampling because the<br />only term affecting the order of operations is the total count of topics<br />of (3).<br />The pseudocode for our Approximate Distributed LDA (AD-LDA) algorithm is shown in Al-<br />gorithm 1. After distributing the data and parameters across processors, AD-LDA performs simul-<br />taneous LDA Gibbs sampling on each of theprocessors. After processor<br />its local data and updated topic assignments, the processor has modified count arrays<br />. The topic-document counts are distinct because of the document index, , and will<br />be consistent with the topic assignments . However, the word-topic counts<br />be different on each processor, and not globally consistent with . To merge back to a single and<br />consistent set of word-topic counts, we perform a reduce operation on<br />update the global counts. After the synchronization and update operations, each processor has the<br />same values in thearray which are consistent with the global vector of topic assignments .<br />Note thatis not the result ofseparate LDA models running on separate data. In particular,<br />each word-topic count array reflects all the counts, not just those local to that processor, so for every<br />processor<br />, whereis the total number of words in the corpus. As in LDA, the<br />depend on<br />in the denominator<br />has swept through<br />and<br />will in general<br />across all processors to<br />6</p>  <p>Page 7</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />jp<br />θ<br />γ<br />, a b<br />, c d<br />k<br />β<br />p<br />α<br />ijp<br />Z<br />ijp<br />X<br />k<br />Φ<br />kp<br />ϕ<br />K<br />P<br />Njp<br />Dp<br />P<br />Figure 3: Graphical model for Hierarchical Distributed Latent Dirichlet Allocation.<br />algorithm can terminate either after a fixed number of iterations, or based on some suitable MCMC<br />convergence metric.<br />We chose the name Approximate Distributed LDA because in this algorithm we are no longer<br />asymptotically sampling from the true posterior, but to an approximation of the true posterior.<br />Nonetheless, we will show in our experimental results that the approximation made by Approxi-<br />mate Distributed LDA works very well.<br />3.2 Hierarchical Distributed Latent Dirichlet Allocation<br />In AD-LDAwe constructed an algorithm where each processor is independently computing an LDA<br />model, but at the end of each sweep through a processor’s data, a consistent global array of topic<br />counts<br />is reconstructed. This global array of topic counts could be thought of as a parent topic<br />distribution, from which each processor draws its own local topic distribution.<br />Using this intuition, we created a Bayesian model reflecting this structure, as shown in Fig-<br />ure 3. Our Hierarchical Distributed LDA model (HD-LDA) places a hierarchy over word-topic<br />distributions, withbeing the global or parent word-topic distribution and<br />topic distributions on each processor. The local word-topic distributions<br />according to a Dirichlet distribution with a topic-dependent strength parameter<br />. The model on each processor is simply an LDA model. The generative process is<br />given by:<br />the local word-<br />are drawn from<br />, for each topic<br />(6)<br />From this generative process, we derive Gibbs sampling equations for HD-LDA. The derivation<br />is based on the Teh et al. (2006) sampling schemes for Hierarchical Dirichlet Processes. As was<br />done for LDA, we start by integrating out<br />and . The collapsed distribution of and on<br />7</p>  <p>Page 8</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />processoris given by:<br />(7)<br />From this we derive the conditional probability for sampling a topic assignment<br />AD-LDA, the topic assignments on any processor are now conditionally independent of the topic<br />assignments on the other processors given, thus allowing each processor to sample<br />rently. The conditional probability ofis<br />. Unlike<br />concur-<br />(8)<br />The full derivation of the Gibbs sampling equations for HD-LDA is provided in Appendix A,<br />which lists the complete set of sampling equations for<br />The pseudocode for our Hierarchical Distributed LDA algorithm is given in Algorithm 2. Each<br />variable in this model is either local or global, depending on whether inference for the variable<br />is computed locally on a processor or globally, requiring information from all processors. Local<br />variables include<br />, , , , and . Global variables include<br />sampling to sample its local variables concurrently. After each sweep through the processor’s data,<br />the global variables are sampled. Note that, unlike AD-LDA,HD-LDAis performing strictly correct<br />sampling for its model.<br />HD-LDAcan beviewed asamixturemodel with<br />weights. In this view the data have been hard-assigned to their respective clusters (i.e. processors),<br />and the parameters of the clusters are generated from a shared prior distribution.<br />,and.<br />and . Each processor uses Gibbs<br />LDAmixturecomponents withequal mixing<br />Algorithm 2 HD-LDA<br />repeat<br />for each processor<br />Sample<br />Sample<br />end for<br />Synchronize<br />Sample:<br />Broadcast:<br />until termination criterion satisfied<br />in parallel do<br />locally: LDA-Gibbs-Iteration(<br />locally<br />,,,,,)<br />,<br />,<br />3.3 Approximate Distributed Hierarchical Dirichlet Processes<br />Our third distributed algorithm, Approximate Distributed HDP, takes the same approach as AD-<br />LDA. Processors concurrently run HDP for a single sweep through their local data. After all of<br />the processors sweep through their data, a synchronization and update step is performed to create a<br />8</p>  <p>Page 9</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />Algorithm 3 AD-HDP<br />repeat<br />for each processor<br />Sample<br />Report<br />end for<br />Synchronize<br />Update global counts (and merge new topics):<br />in parallel do<br />locally: HDP-Gibbs-Iteration(<br />, to master node<br />,,,,, , , )<br />Sample: ,<br />Broadcast:<br />until termination criterion satisfied<br />,<br />, , ,<br />Processor 1<br />Processor 2<br />Processor 3<br />++++++<br />++++++<br />+<br />========<br />Merged Topics<br />New Topics<br />T1 T2 T3 T4 T5 T6 T7 T8<br />Figure 4: The simplest method to merge new topics in AD-HDP is by integer topic label.<br />single set of globally-consistent word-topic counts<br />as AD-HDP, and provide the pseudocode in Algorithm 3.<br />Unlike AD-LDA, which uses a fixed number of topics, individual processors in AD-HDP may<br />instantiate new topics during the sampling phase, according to the HDP sampling equation (5). Dur-<br />ing the synchronization and update step, instead of treating each processor’s new topics as distinct,<br />we merge new topics that were instantiated on different processors. Merging new topics helps limit<br />unnecessary growth in the total number of topics and allows AD-HDP to produce more of a global<br />model.<br />There are several ways to merge newly created topics on each processor. A simple way –<br />inspired byAD-LDA–istomerge newtopics based ontheir integer topic label. Amore complicated<br />way is to match new topics across processors based on topic similarity.<br />In the first merging scheme, new topics are merged based on their integer topic label. For exam-<br />ple, assume that we have three processors, and at the end of a sweep through the data, processor one<br />has 8 new topics, processor two has 6 new topics, and processor three has 7 new topics. Then dur-<br />ing synchronization, all these new topics would be aligned by topic label and their counts summed,<br />producing 8 new global topics, as shown in Figure 4.<br />While this merging of new topics by topic-id may seem suboptimal, it is computationally simple<br />and efficient. We will show in the next section that this merging generally works well in practice,<br />even when processors only have a small amount of data. We suggest that even if the merging by<br />. We refer to the distributed version of HDP<br />9</p>  <p>Page 10</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />Algorithm 4 Greedy Matching of New Topics for AD-HDP<br />Initialize global set of new topics,<br />for<br />= 2 to P do<br />for topic in processor p’s set of new topics do<br />Initialize score array<br />for topic in<br />do<br />score[ ] = symmetric-KL-divergence( , )<br />end for<br />if min(score) threshold then<br />Add ’s counts to the topic in<br />else<br />Augmentwith the new topic<br />end if<br />end for<br />end for<br />, to be processor 1’s set of new topics<br />corresponding to min(score)<br />topic-id is initially quite random, the subsequent dynamics align the topics in a sensible manner. We<br />will also show that AD-HDP ultimately learns models with similar perplexity to HDP, irrespective<br />of how new topics are merged.<br />We also investigate more complicated schemes for merging new topics in AD-HDP, beyond the<br />simple approach of merging by topic-id. Instead of aligning new topics based topic-id it is possible<br />to align new topics using a similarity metric such as symmetric Kullback-Leibler divergence. How-<br />ever, finding the optimal matching of topics in the case where<br />1999). Thus, we consider approximate schemes: bipartite matching using a reference processor,<br />and greedy matching.<br />Inthe bipartite matching scheme, weselect areference processor and perform bipartite matching<br />between every processor’s new topics and the set of new topics of the reference processor. The<br />bipartite match is computed using the Hungarian algorithm, which runs in<br />overall complexity ofwhere is the maximum number of new topics on a processor. We<br />implemented this scheme but did not find any improvement over AD-HDPwith merging by topic-id.<br />In the greedy matching scheme, new topics on each processor are sequentially compared to a<br />global set of new topics. This global set is initialized to the first processor’s set of new topics. If<br />a new topic is sufficiently different from every topic in the global set, the number of topics in the<br />global set is incremented; otherwise, the counts for that new topic are added to those from the closest<br />match in the global set. A threshold is used to determine whether a new topic is sufficiently different<br />from another topic. The worst case complexity of this algorithm is<br />every new topic is found to be different from every other new topic in the global set. Increasing this<br />threshold will make it more likely for new topics to merge with the topics already in the global set<br />(instead of incrementing the set), causing the expected running time of this merging algorithm to<br />be linear in the number of processors. The pseudocode of this greedy matching scheme is shown in<br />Algorithm 4. This algorithm is run after each iteration of AD-HDP to produce a global set of new<br />topics. We show in the next section that this greedy matching scheme significantly improves the<br />rate of convergence for AD-HDP.<br />is NP-hard (Burkard and C ¸ela,<br />, producing an<br />– this is the case where<br />10</p>  <p>Page 11</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />KOS<br />3,000<br />6,906<br />467,714<br />NIPS<br />1,500<br />12,419<br />2,166,058<br />WIKIPEDIA<br />2,051,929<br />120,927<br />344,941,756<br />PUBMED<br />8,200,000<br />141,043<br />737,869,083<br />NEWSGROUPS<br />train<br />19500<br />27,059<br />2,057,207<br />test<br />430184--498<br />Table 2: Characteristics of data sets used in experiments.<br />4. Experiments<br />The purpose of our experiments is to investigate how our distributed topic model algorithms, AD-<br />LDA, HD-LDA and AD-HDP, perform when compared to their sequential counterparts, LDA and<br />HDP. We are interested in two aspects of performance: the quality of the model learned, measured<br />by log probability of test data; and the time taken to learn the model. Our primary data sets for these<br />experiments were KOS blog entries, from dailykos.com, and NIPS papers, from books.nips.cc.<br />We chose these relatively small data sets to allow us to perform a large number of experiments.<br />Both data sets were split into a training set and a test set. Size parameters for these data sets are<br />shown in Table 2. For each corpus,<br />is the number of documents,<br />andis the total number of words. Two larger data sets WIKIPEDIA, from en.wikipedia.org,<br />and PUBMED, from pubmed.gov were used for speedup experiments, described in Section 5. For<br />precision-recall experiments we used the NEWSGROUPS data set, taken from the UCI Machine<br />Learning Repository. All the data sets used in this paper can be downloaded from the UCI Machine<br />Learning Repository (Asuncion and Newman, 2007).<br />Using the KOSand NIPSdata sets, wecomputed test set perplexities for arange oftopics<br />for numbers of processors,, ranging from 1to 3000. The distributed algorithms wereinitialized by<br />first randomly assigning topics to words in , then counting topics in documents,<br />topics,, for each processor. Foreach run of LDA,AD-LDA,and HD-LDA,a sample was taken<br />at 500 iterations of the Gibbs sampler, which is well after the typical burn-in period of the initial<br />200-300 iterations. For each run of HDP and AD-HDP, we allow the Gibbs sampler to run for 3000<br />iterations, to allow the number of topics to grow. In our perplexity experiments, multiple processors<br />were simulated in software by separating data, running sequentially through each processor, and<br />simulating the global synchronization and update steps. For the speedup experiments, computations<br />were run on 64 to 1024 processors on a 2000+ processor parallel supercomputer.<br />The following set of hyperparameters was used for the experiments, where hyperparameters are<br />shown as variables in squares in the graphical models in Figures 1 and 3. For AD-LDA we set<br />and . For AD-HDP we set,<br />andcould have also been fixed, resampling these hyperparameters allows for more robust topic<br />growth, as described by Teh et al. (2006). For LDA and AD-LDA we fixed the hyperparameters<br />and , but these priors could also be learned using sampling.<br />Selection of hyperparameters for HD-LDA was guided by our experience with AD-LDA. For<br />AD-LDA,, but for HD-LDA<br />mode ofto simulate the inclusion of global counts in<br />set, because it is important to scale by the number of topics to prevent oversmoothing<br />when the counts are spread thinly among many topics. Finally, we choose and<br />is the vocabulary size<br />, and<br />, and words in<br />Gammaand Gamma. While<br />, so we chooseandto make the<br />as is done in AD-LDA.We<br />to make the mode<br />11</p>  <p>Page 12</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />of<br />set:<br />, matching the value of<br />,<br />To systematically evaluate our distributed topic model algorithms, AD-LDA, HD-LDA and<br />AD-HDP, we measured performance using test set perplexity, which is computed as Perp<br />test<br />. For every test document, half the words at random are designated for fold-<br />in, and the remaining words are used as test. The document mixture<br />part, and log probability of the test words is computed using this mixture, ensuring that the test<br />words are not used in estimation of model parameters. For AD-LDA, the perplexity computation<br />exactly follows that of LDA, since a single set of topic counts<br />taken. In contrast, allcopies ofare required to compute perplexity for HD-LDA. Except<br />where stated, perplexities are computed for all algorithms using<br />from ten independent chains using<br />used in our LDA and AD-LDA experiments. Specifically, we<br />and.,<br />test<br />test<br />is learned using the fold-in<br />are saved when a sample is<br />samples from the posterior<br />testtest<br />(9)<br />This perplexity computation follows the standard practice of averaging over multiple chains when<br />making predictions with LDA models trained via Gibbs sampling, as discussed in Griffiths and<br />Steyvers (2004). Averaging over ten samples significantly reduces perplexity compared to using a<br />single sample from one chain. While we perform averaging over multiple samples to improve the<br />estimate of perplexity, we have also observed similar relative results across our algorithms when we<br />use a single sample to compute perplexity.<br />Analogous perplexity calculations are used for HD-LDA and AD-HDP. With HD-LDA we ad-<br />ditionally compute processor-specific responsibilities, since test documents do not belong to any<br />particular processor, unlike the training documents. Each processor learns a document mixture<br />using the fold-in part for each test document. For each processor, the likelihood is calculated over<br />the words in the fold-in part in a manner analogous to (9), and these likelihoods are normalized to<br />form the responsibilities,. To compute perplexity, we compute the likelihood over the test words,<br />using a responsibility-weighted average of probabilities over all processors:<br />testtest<br />(10)<br />where<br />Computing perplexity in this manner prevents the possibility of seeing or using test words during<br />the training and fold-in phases.<br />4.1 Perplexity<br />The perplexity results for KOS and NIPS in Figure 5 clearly show that the model perplexity is<br />essentially the same for the distributed models AD-LDA and AD-HDP at<br />their single-processor versions at. The figures show the test set perplexity, versus number of<br />processors,, for different numbers of topics<br />models which learn the number of topics. The<br />HDP (triangles), and we use our distributed algorithms – AD-LDA (crosses), HD-LDA (squares),<br />andas<br />for the LDA-type models, and also for the HDP-<br />perplexity is computed by LDA (circles) and<br />12</p>  <p>Page 13</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />and AD-HDP (stars) – to compute the<br />perplexity as a function of the number of topics is much greater than the variability due to the<br />number of processors. Note that there is essentially no perplexity difference between AD-LDA and<br />HD-LDA.<br />andperplexities. The variability in<br />P=1P=10P=100<br />1200<br />1300<br />1400<br />1500<br />1600<br />1700<br />1800<br />Number of Processors<br />Perplexity<br />KOS Data Set<br />K=8<br />K=16<br />K=32<br />K=64<br />HDP<br />LDA<br />AD−LDA<br />HD−LDA<br />HDP<br />AD−HDP<br />P=1P=10P=100<br />1100<br />1200<br />1300<br />1400<br />1500<br />1600<br />1700<br />1800<br />1900<br />2000<br />Number of Processors<br />Perplexity<br />NIPS Data Set<br />K=10<br />K=20<br />K=40<br />K=80<br />HDP<br />LDA<br />AD−LDA<br />HD−LDA<br />HDP<br />AD−HDP<br />Figure 5: Test perplexity on KOS (left) and NIPS (right) data versus number of processors P.<br />corresponds to LDA and HDP. At<br />and AD-HDP.<br />and we show AD-LDA, HD-LDA<br />10<br />0<br />10<br />1<br />10<br />2<br />10<br />3<br />10<br />4<br />1350<br />1400<br />1450<br />1500<br />1550<br />1600<br />1650<br />1700<br />1750<br />T=8<br />T=16<br />T=32<br />T=64<br />Number of processors P<br />Perplexity<br />KOS Data Set<br />10<br />0<br />10<br />1<br />10<br />2<br />10<br />3<br />10<br />4<br />1400<br />1500<br />1600<br />1700<br />1800<br />1900<br />2000<br />T=10<br />T=20<br />T=40<br />T=80<br />Number of processors P<br />Perplexity<br />NIPS Data Set<br />Figure 6: AD-LDA test perplexity versus number of processors up to the limiting case of number<br />of processors equal to number of documents in collection. Left plot shows perplexity for<br />KOS and right plot shows perplexity for NIPS.<br />13</p>  <p>Page 14</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />Even in the limit of a large number of processors, the perplexity for the distributed algorithms<br />matches that for the sequential version. In fact, in the limiting case of just one document per<br />processor,for KOS andfor NIPS, we see that the perplexities of AD-LDA are<br />generally no different to those of LDA, as shown in the rightmost point in each curve in Figure 6.<br />AD-HDP instantiates fewer topics but produces a similar perplexity to HDP. The average num-<br />ber of topics instantiated by HDP on KOS was 669 while the average number of topics instantiated<br />by AD-HDP was 490 () and 471 (<br />AD-HDP instantiated 569 () and 569 (<br />because of the merging across processors of newly-created topics. The similar perplexity results for<br />AD-HDP compared to HDP, despite the fewer topics, is partly due to the relatively small probability<br />mass in many of the topics.<br />Despite no formal convergence guarantees, the approximate distributed algorithms, AD-LDA<br />and AD-HDP, converged to good solutions in every single experiment (of the more than one hun-<br />dred) we conducted using multiple real-world data sets. We also tested both our distributed LDA<br />algorithms with adversarial/non-random distributions of topics across processors using synthesized<br />data. One example of an adversarial distribution of documents is where each document only uses a<br />single topic, and these documents are distributed such that processor<br />about topic . In this case the distributed topic models have to learn the correct set of<br />though each processor only sees local documents that pertain to just one of the topics. We ran multi-<br />ple experiments, starting with 1000 documents that were hard-assigned to<br />document is only about one topic), and distributing the 1000 documents over<br />where each processor contained documents belonging to the same topic (an analogy is one proces-<br />sor only having documents about sports, the next processor only having documents about arts, and<br />so on). The perplexity performance of AD-LDA and HD-LDA under these adversarial/non-random<br />distribution of documents was as good as the performance when the documents were distributed<br />randomly, and as good as the performance of single-processor LDA.<br />To demonstrate that the low perplexities obtained from the distributed algorithms with<br />processors are not just due to averaging effects, we split the NIPS corpus into one hundred<br />15-document collections, and ran LDA separately on each of these hundred collections. The test<br />perplexity atcomputed by averaging 100-separate LDA models was 2117, significantly<br />higher than thetest perplexity of 1575 for AD-LDA and HD-LDA. This shows that a<br />baseline approach of simple averaging of results from separate processors performs much worse<br />than the distributed coordinated learning algorithms that we propose in this paper.<br />). For NIPS, HDP instantiated 687 topics while<br />) topics. AD-HDP instantiates fewer topics<br />only has documents that are<br />topics, even<br />topics (i.e. each<br />processors,<br />4.2 Convergence<br />One could imagine that distributed algorithms, where each processor only sees its own local data,<br />may converge more slowly than single-processor algorithms where the data is global. Consequently,<br />we performed experiments to see whether our distributed algorithms were converging at the same<br />rate as their sequential counterparts. If the distributed algorithms were converging slower, the com-<br />putational gains of parallelization would be reduced. Our experiments consistently showed that the<br />convergence rate for the distributed LDA algorithms was just as fast as those for the single processor<br />case. As an example, Figure 7 shows test perplexity versus iteration of the Gibbs sampler for the<br />NIPS data attopics. During burn-in, up to iteration 200, the distributed algorithms are ac-<br />tually converging slightly faster than single processor LDA. Note that one iteration of AD-LDA or<br />14</p>  <p>Page 15</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />HD-LDA on a parallel multi-processor computer only takes a fraction (at best<br />time of one iteration of LDA on a single processor computer.<br />) of the wall-clock<br />50 100150 200250 300350 400<br />1700<br />1800<br />1900<br />2000<br />2100<br />2200<br />Iteration<br />Perplexity<br /> <br /> <br />LDA<br />AD−LDA P=10<br />AD−LDA P=100<br />HD−LDA P=10<br />HD−LDA P=100<br />Figure 7: Convergence of test perplexity versus iteration for the distributed algorithms AD-LDA<br />and HD-LDA using the NIPS data set andtopics.<br />We see slightly different convergence behavior in the non-parametric topic models. AD-HDP<br />converges more slowly than HDP, as shown in Figure 8, due to AD-HDP’s heavy averaging of new<br />topics resulting from merging by topic-id (i.e. no matching). This slower convergence may partially<br />be a result of the lower number of topics instantiated. The number of new topics instantiated in one<br />pass of AD-HDPis limited to the maximum number of new topics instantiated on any one processor.<br />For example, in the right plot, after 500 iterations, HDP has instantiated 360 topics, whereas AD-<br />HDP has instantiated 210 () and 250 (<br />the perplexity of HDP is lower than the perplexity of AD-HDP. After three thousand iterations, AD-<br />HDP produces the same perplexity as HDP, which is reassuring because it indicates that AD-HDP<br />is ultimately producing a model that has the same predictive ability as HDP. We observe a similar<br />result for the NIPS data set.<br />One way to accelerate the rate of convergence for AD-HDP is to match newly generated topics<br />by similarity instead of by topic-id. Figure 9 shows that performing the greedy matching scheme for<br />new topics as described in Algorithm 4 significantly improves the rate of convergence for AD-HDP.<br />In this experiment, we used a threshold of 2 for determining topic similarity. The number of topics<br />increases at a faster rate for AD-HDP with matching, since the greedy matching scheme is more<br />flexible in that the number of new topics at each iteration is not limited to the maximum number of<br />new topics instantiated on any one processor. The results show that the greedy matching scheme<br />enables AD-HDPto converge almost as quickly as HDP.In practice, only a few new topics<br />are generated locally on each processor each iteration, and so the computational overhead of this<br />heuristic matching scheme is minimal relative to the time for Gibbs sampling.<br />) topics. Correspondingly, at 500 iterations,<br />15</p>  <p>Page 16</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />0 50010001500<br />Iteration<br />200025003000<br />1200<br />1300<br />1400<br />1500<br />1600<br />1700<br />1800<br />Perplexity<br />HDP<br />AD−HDP P=10<br />AD−HDP P=100<br />050010001500<br />Iteration<br />200025003000<br />0<br />100<br />200<br />300<br />400<br />500<br />600<br />700<br />800<br />900<br />1000<br />Number of Topics<br />HDP<br />AD−HDP P=10<br />AD−HDP P=100<br />Figure 8: Results forHDPversus AD-HDPwithno matching. Leftplot showstest perplexity versus<br />iteration for HDP and AD-HDP. Right plot shows number of topics versus iteration for<br />HDP and AD-HDP. Results are for the KOS data set.<br />050010001500<br />Iteration<br />20002500 3000<br />1200<br />1300<br />1400<br />1500<br />1600<br />1700<br />1800<br />Perplexity<br />HDP<br />AD−HDP P=100<br />AD−HDP P=100 (with matching)<br />0 50010001500<br />Iteration<br />200025003000<br />0<br />100<br />200<br />300<br />400<br />500<br />600<br />700<br />800<br />900<br />1000<br />Number of Topics<br />HDP<br />AD−HDP P=100<br />AD−HDP P=100 (with matching)<br />Figure 9: Results for HDP versus AD-HDP with greedy matching. Left plot shows test perplexity<br />versus iteration for HDPand AD-HDP.Right plot shows number of topics versus iteration<br />for HDP and AD-HDP. Results are for the KOS data set.<br />To further check that the distributed algorithms were performing comparably to their single<br />processor counterparts, we ran experiments to investigate whether the results were sensitive to the<br />number of topics used in the models, in case the distributed algorithms’ performance worsens when<br />the number of topics becomes very large. Figure 10 shows the test perplexity computed on the<br />NIPS data set, as a function of the number of topics, for the LDA algorithms and a fixed number of<br />processors(the results forthe KOSdata setwerequite similar and therefore not shown). The<br />perplexities of the different algorithms closely track each other as number of topics,<br />In fact, in some cases HD-LDA produces slightly lower perplexities than those of single processor<br />, increases.<br />16</p>  <p>Page 17</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />LDA. This lower perplexity may be due to the fact that in HD-LDA test perplexity is computed<br />using P sets of topic parameters, thus it has more parameters than AD-LDA to better fit the data.<br />0100200300400 500600 700<br />1000<br />1100<br />1200<br />1300<br />1400<br />1500<br />1600<br />1700<br />1800<br />1900<br />2000<br />Number of Topics<br />Perplexity<br />LDA<br />AD−LDA P=10<br />HD−LDA P=10<br />Figure 10: Test perplexity versus number of topics using the NIPS data set (S=5).<br />4.3 Precision and Recall<br />In addition to our experiments measuring perplexity, wealso performed precision/recall calculations<br />using the NEWSGROUPS data set, where each document’s corresponding newsgroup is the class<br />label. In this experiment we use LDA and AD-LDA to learn topic models on the training data. Once<br />the model is learned, each test document can be treated as a ”query”, where the goal is to retrieve<br />relevant documents from the training set. For each test document, the training documents are ranked<br />according to how probable the test document is under each training document’s mixture<br />set of topics. From this ranking, one can calculate mean average precision and area under the<br />ROC curve.<br />and the<br />0 1020304050<br />0.04<br />0.06<br />0.08<br />0.1<br />0.12<br />0.14<br />0.16<br />Iteration<br />Mean Average Precision<br />LDA<br />AD−LDA P=10<br />AD−LDA P=100<br />0 10203040 50<br />0.5<br />0.55<br />0.6<br />0.65<br />0.7<br />0.75<br />0.8<br />0.85<br />0.9<br />Iteration<br />Mean Area Under ROC Curve<br />LDA<br />AD−LDA P=10<br />AD−LDA P=100<br />Figure 11: Precision/recall results: (left) Mean average precision for LDA/AD-LDA. (right) Area<br />under the ROC curve for LDA/AD-LDA.<br />17</p>  <p>Page 18</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />Figure 11 shows the mean average precision and the area under the ROC curve achieved by<br />LDA and AD-LDA, plotted versus iteration. LDA performs slightly better than AD-LDA for the<br />first 20 iterations, but AD-LDA catches up and converges to the same mean average precision and<br />area under the ROC curve as LDA. This again shows that our distributed/parallel version of LDA<br />produces a very similar result to the single-processor version.<br />5. Scalability<br />The primary motivation for developing distributed algorithms for LDA and HDP is to have highly<br />scalable algorithms, in terms of memory and computation time. Memory requirements depend on<br />both memory for data and memory for model parameters. The memory for the data scales with<br />the total number of words in the corpus. The memory for the parameters is linear in the number<br />of topics , which is either fixed for the LDA models or learned for the HDP models. The per-<br />processor per-iteration time and space complexity of LDA and AD-LDA are shown in Table 3.<br />AD-LDA’s memory requirement scales well as collection sizes grow, because while corpus size (<br />and) can get arbitrarily large, which can be offset by increasing the number of processors,<br />the vocabulary sizewill tend to asymptote, or at least grow more slowly. Similarly the time<br />complexity scales well since the leading order term<br />The communication cost of the reduce operation, denoted by<br />taken to perform the global sum of the count difference<br />stages and can be implemented efficiently in standard language/protocols such as MPI, the<br />Message Passing Interface. Because of the additional<br />, with increasing efficiency as this ratio increases. Space and time complexity of HD-LDA are<br />similar to that of AD-LDA, but HD-LDA has bigger constants. For a given number of topics,<br />we argue that AD-HDP has similar time complexity as AD-LDA.<br />We performed large-scale speedup experiments with just AD-LDA instead of all three of our<br />distributed topic modeling algorithms because AD-LDA produces very similar results to HD-LDA,<br />but with significantly less computation. We expect that relative speedup performance for HD-LDA<br />and AD-HDP should follow that for AD-LDA.<br />,<br />,<br />is divided by.<br />in the table, represents the time<br />. This is executed in<br />term, parallel efficiency will depend on<br />,<br />LDA AD-LDA<br />Space<br />Time<br />Table 3: Space and time complexity of LDA and AD-LDA.<br />We used two multi-million document data sets, WIKIPEDIAand PUBMED,for speedup exper-<br />iments on a large-scale supercomputer. The supercomputer used was DataStar, a 15.6 TFlop teras-<br />cale machine at San Diego Supercomputer Center built from 265 IBM P655 8-way compute nodes.<br />We implemented a parallel version of AD-LDA using the Message Passing Interface protocol. We<br />ran AD-LDA on WIKIPEDIA usingtopics and PUBMED using<br />tributed overand 1024 processors. The speedup results, shown in Figure 12,<br />show relatively high parallel efficiency, with approximately 700 times speedup for WIKIPEDIA<br />and 800 times speedup for PUBMED when using<br />efficiencies of approximately 0.7 and 0.8 respectively. This speedup is computed relative to the<br />topics dis-<br />processors, corresponding to parallel<br />18</p>  <p>Page 19</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />0200 400<br />Number of processors<br />600 8001000<br />0<br />100<br />200<br />300<br />400<br />500<br />600<br />700<br />800<br />900<br />1000<br />Speedup<br />Perfect<br />AD−LDA (PUBMED)<br />AD−LDA (WIKIPEDIA)<br />Figure 12: Parallel speedup results for 64 to 1024 processors on multi-million document datasets<br />WIKIPEDIA and PUBMED.<br />time per iteration when using<br />it is not possible, due to memory limitations, to run these models on a single processor. Multiple<br />runs were timed for both WIKIPEDIA and PUBMED, and the resulting variation in timing was less<br />than 1%, so error bars are not shown in the figure. We see slightly higher parallel efficiency for<br />PUBMED versus WIKIPEDIAbecause PUBMED has a larger amount of computation per unit data<br />communicated,.<br />processors (i.e. atprocessors speedup=64), since<br />This speedup dramatically reduces the learning time for large topic models. If we were to learn<br />topic model for PUBMED using LDA on a single processor, it would require over 300<br />days instead of the 10 hours required to learn the same model using AD-LDAon 1024 processors. In<br />our speedup experiments on these large data sets, we did not directly investigate latency or commu-<br />nication bandwidth effects. Nevertheless, one could expect that if the communication time becomes<br />very long compared to the computation time, then it may be worth doing multiple Gibbs sampling<br />sweeps on a processor’s local data before performing the synchronization and global update step.<br />In Section 6 we further examine this question of frequency of synchronizations. The relative time<br />for communication versus computation also effects the weak scaling of parallelization, where the<br />problem size increases linearly with the number of processors. We expect that parallel efficiency<br />will be relatively constant for weak scaling since<br />a<br />is constant.<br />In addition to the large-scale speedup experiments run on the 1024-processor parallel super-<br />computer, we also performed small-scale speedup experiments for AD-HDP on an 8-node parallel<br />cluster running MPI. Using the NIPS data set we measured parallel efficiencies of 0.75 and 0.5 for<br />and. The latter result on 8 processors means that the HDP model for NIPS can be<br />learned four times faster than on a single processor.<br />19</p>  <p>Page 20</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />6. Analysis of Approximate Distributed LDA<br />Finally, we investigate the dynamics of AD-LDA learning using toy data to get further insight<br />into how AD-LDA is working. While we have shown experimental results showing that AD-LDA<br />produces models with similar perplexity and similar convergence rates to LDA, it is not obvious<br />why this algorithm works so well in practice. Our toy example has<br />topics. We generated document collections according to the LDA generative process given by (1).<br />We chose a low dimension vocabulary, , so that we could plot the evolution of the Gibbs sampler<br />on a two-dimensional word-topic simplex. We first generated data, then learned models using LDA<br />and AD-LDA.<br />The left plot of Figure 13 shows thedistance between the model’s estimate of a particular<br />topic-word distribution and the true distribution, as a function of Gibbs iteration, for both single-<br />processor LDA and AD-LDA with . LDA and AD-LDA have qualitatively the same three-<br />phase learning dynamics. The first four or so iterations (labeled initialize) correspond to somewhat<br />random movement close to the randomly initialized starting point. In the next phase (labeled burn-<br />in) both algorithms rapidly move in parameter space toward the posterior mode. And finally after<br />burn-in (labeled stationary) both are sampling around the mode. In the right plot we show the sim-<br />ilarity between AD-LDA and LDA samples taken from the equilibrium distribution – here plotted<br />on the two-dimensional planar simplex corresponding to the three-word topic distribution.<br />words and<br />0204060 80100<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0.25<br />0.3<br />0.35<br />0.4<br />Iteration<br />L1 norm<br />initialize<br />burn−in<br />stationary<br />LDA<br />AD−LDA proc1<br />AD−LDA proc2<br />0.8 0.810.820.830.840.85<br />0.445<br />0.45<br />0.455<br />0.46<br />0.465<br />0.47<br />0.475<br /> <br /> <br />topic mode<br />LDA<br />AD−LDA<br />Figure 13: (Left)distance to the mode for LDA and for<br />samples of(projected onto the topic simplex) taken from the equilibrium distribution,<br />showing the similarity between LDA and<br />this figure.<br />AD-LDA. (Right) Closeup of 50<br />AD-LDA. Note the zoomed scale in<br />The left plot of Figure 14 depicts the same trajectory shown in Figure 13 left, projected onto the<br />topic simplex. This plot shows the paths in parameter space of each model, and the same three-phase<br />learning dynamics: taking a few small steps near the starting point, moving up to the true solution,<br />and then sampling near the posterior mode for the rest of the iterations. For each Gibbs iteration,<br />the parameters corresponding to each of the two individual processors, and those parameters after<br />merging, are shown for AD-LDA. One can see the alternating pattern of two separate (but close)<br />20</p>  <p>Page 21</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />parameter estimates on each processor, followed by a merged estimate. We observed that after the<br />initial few iterations, the individual processor steps and the merge step each resulted ina movecloser<br />to the mode. One might worry that the AD-LDA algorithm would get trapped close to the initial<br />starting point, e.g., due to repeated label switching or oscillatory behavior of topic labeling across<br />processors. In practice we have consistently observed that the algorithm quickly discards such<br />configurations due to the stochastic nature of the moves and latches onto a consistent and stable<br />labeling that rapidly moves it toward the posterior mode. The figure clearly illustrates that LDA and<br />AD-LDA have qualitatively similar learning dynamics. The right plot in Figure 14 illustrates the<br />same qualitative behavior as in the left plot, but now for<br />Interestingly, across a wide range of experiments, we observed that the variance in the AD-<br />LDA word-topic distribution samples is typically only about 70% of the variance in LDA topic<br />samples. Since the samplers are not the same it makes sense that the posterior variance differs<br />(i.e. is underestimated) by the parallel sampler. We expect less variance because AD-LDA ignores<br />fluctuations in the bulk of . Nonetheless, all of our experiments indicate that the posterior mode<br />and means found by the parallel sampler are essentially the same as those found by the sequential<br />sampler.<br />processors.<br />0.650.70.750.80.85 0.9<br />0.36<br />0.38<br />0.4<br />0.42<br />0.44<br />0.46<br />0.48<br />0.5<br />start<br />topic mode<br />LDA<br />AD−LDA proc1<br />AD−LDA proc2<br />0.650.7 0.750.80.85 0.9<br />0.36<br />0.38<br />0.4<br />0.42<br />0.44<br />0.46<br />0.48<br />0.5<br />start<br />topic mode<br />LDA<br />AD−LDA proc1<br />AD−LDA proc2<br />AD−LDA proc3<br />...etc...<br />AD−LDA proc10<br />Figure 14: (Left) Projection of topics onto simplex, showing convergence to mode for<br />(Right) Same as left plot, but with<br />.<br />.<br />Another insight can be gained by thinking of LDA as an approximation to stochastic descent in<br />the space of assignment variables . On a single processor, one can view Gibbs sampling during<br />burn-in as a stochastic algorithm to move up the likelihood surface. With multiple processors, each<br />processor computes an upward direction in its own subspace, keeping all other directions fixed.<br />The global update step then recombines these directions by vector-addition, in the same way as one<br />would compute a gradient using finite differences. This is expected to be accurate as long as the<br />surface is locally convex or concave, but will break down at saddle-points. We conjecture AD-LDA<br />works reliably because saddle points are unstable and rare because the posterior is usually highly<br />peaked for LDA models and high-dimensional count data sets.<br />While we see similar perplexities for AD-LDA compared to LDA, we could further ask if the<br />AD-LDA algorithm is producing any bias in its estimates of the model parameters. To test this, we<br />21</p>  <p>Page 22</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />10<br />0<br />10<br />1<br />10<br />2<br />10<br />3<br />10<br />4<br />0<br />0.01<br />0.02<br />0.03<br />0.04<br />0.05<br />0.06<br />0.07<br />0.08<br />0.09<br />Number of processors, P<br />||E(φ)−φref||<br />reference = LDA<br />reference = true<br />Figure 15: Averageerror in word-topic distribution versus P for AD-LDA.<br />performed a series of experiments where we generated synthetic data sets according to the LDA<br />generative process, with known word-topic distributions<br />models fromeach ofthesimulated data sets. Wecomputed theexpected value ofthe AD-LDAtopics<br />and compared this to two reference values,<br />the other based on multiple LDA samples,<br />ref<br />closer to the LDA topics<br />variation in learning LDA models from finite data sets is much greater than the variation between<br />LDA and AD-LDA on the same data sets.<br />. We then learned LDA and AD-LDA<br />refone based on the true distribution,<br />LDA. Figure 15 shows that AD-LDA is much<br />LDA than either are to the true topics<br />ref<br />,<br />, telling us that the sampling<br />When Does AD-LDA Fail? In all of our experiments thus far, we have seen that our distributed al-<br />gorithms learn models with equivalent predictive power as their non-distributed counterparts. How-<br />ever, when global synchronizations are done less frequently (i.e., when the synchronization step is<br />performed after multiple Gibbs sampling sweeps through local data), the distributed algorithms may<br />converge to suboptimal solutions.<br />When the synchronization interval is increased dramatically, it is possible for AD-LDA to con-<br />verge to a suboptimal solution. This can happen because the topics (with the same integer label) on<br />each processor can drift far apart, so that topicon one processor diverges from topic<br />processor. In Figure 16, we show the results of an experiment on KOS where synchronizations only<br />occur once every 100 iterations. For processors, AD-LDA performs significantly worse than<br />LDA. Theprocessor case is the worst case for AD-LDA, since one half of the total words on<br />each processor have the freedom to drift. In contrast, when<br />only locally modify 1/100of the topic assignments, and so the topics on each processor can not<br />drift far from the global set of topic counts at the previous iteration. Bipartite matching significantly<br />on another<br />processors, each processor can<br />22</p>  <p>Page 23</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />improves the perplexity in the<br />has indeed caused the topics to drift apart. Fortunately, topic drifting becomes less of a problem as<br />more processors are used, and can be eliminated by frequent synchronization. It is also important<br />to note that AD-LDA, where processors synchronize after every iteration, gives essentially<br />identical results as LDA. Our recommendation in practice is to perform the synchronization and<br />count updates after each iteration of the Gibbs sampler. As shown earlier in the paper, this leads<br />to performance that is essentially indistinguishable from LDA. Since most multi-processor comput-<br />ing hardware will tend to have communication bandwidth matched to processor speed (i.e. faster<br />and/or more processors usually come with a faster communication network), synchronizing after<br />each iteration of the Gibbs sampler will usually be the optimal strategy.<br />processor case, suggesting that the lack of communication<br />010002000 300040005000<br />1700<br />1800<br />1900<br />2000<br />2100<br />2200<br />2300<br />2400<br />2500<br />Iteration<br />Perplexity<br />LDA<br />AD−LDA P=2<br />AD−LDA P=2, Sync=100<br />AD−LDA P=2, Sync=100 (with matching)<br />AD−LDA P=100, Sync=100<br />Figure 16: Test perplexity versus iteration where synchronizations between processors only occur<br />every 100 iterations, KOS,.<br />7. Related Work<br />Approximate inference for topic models such as LDA and HDP can be carried out using a variety<br />of methods, the most common being variational methods and Markov chain Monte Carlo methods.<br />Previous efforts to parallelize these algorithms have focused on variational methods, which are often<br />straightforward to cast in a distributed framework. For example, Blei et al. (2002) and Nallapati<br />et al. (2007) describe distributed variational EM methods for LDA. In their distributed variational<br />approach, the computationally expensive E-step is easily parallelized because the document-specific<br />variational parameters are independent. Wolfe et al. (2008) investigate the parallelization of both<br />the E and M-steps of variational EM for LDA, under a variety of computer network topologies. In<br />these cases the distributed version of LDA produces identical results to the sequential version of<br />the algorithm. However, memory for variational inference in LDA scales as<br />number of distinct document-word pairs in the corpus. For typical English-language corpora, the<br />total number of words in the corpus is less than twice the number of distinct document-word pairs<br />, whereis the<br />23</p>  <p>Page 24</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />(<br />number of documents,<br />memory requirement of<br />), socan be considered on the order of<br />, this memory requirement of<br />for MCMC methods.<br />. Sinceis usually much larger than the<br />is not nearly as scalable as that the<br />Parallelized versions of various machine learning algorithms have also been developed. Forman<br />and Zhang (2000) describe a parallel k-means algorithm, and W. Kowalczyk and N. Vlassis (2005)<br />describe an asynchronous parallel EM algorithm for Gaussian mixture learning. A parallel EM<br />algorithm for Probabilistic Latent Semantic Analysis, implemented using Google’s MapReduce<br />framework, was described in Das et al. (2007). A review of how to parallelize an array of standard<br />machine learning algorithms using MapReduce was presented by Chu et al. (2007). Rossini et al.<br />(2007) presents a framework for statisticians that allows for the parallel computing of independent<br />tasks within the R language.<br />While many of these EM algorithms are readily parallelizable, Gibbs sampling of dependent<br />variables (such as topic assignments) is fundamentally sequential and therefore difficult to paral-<br />lelize. One way to parallelize Gibbs sampling is to run multiple independent chains in parallel to<br />obtain multiple samples; however, this multiple-chain approach does not address the fact that the<br />burn-in within each chain may take a long time. Furthermore, for some applications, one is not in-<br />terested in multiple samples from independent chains. For example, if we wish to learn topics for a<br />very large document collection, one is usually satisfied with mean values of word-topic distributions<br />taken from a single chain.<br />One can parallelize a single MCMC chain by decomposing the variables into independent non-<br />interacting blocks that can be sampled concurrently (Kontoghiorghes, 2005). However, when the<br />variables are not independent, sampling variables in parallel is not possible. Brockwell (2006)<br />presents ageneral parallel MCMCalgorithm basedon pre-fetching, but itisnotpractical forlearning<br />topic models because it discards most of its computations which makes it relatively inefficient. It<br />is possible to construct partially parallel Gibbs samplers, in which the samples are independently<br />accepted with some probability. In the limit as this probability goes to zero, this sampler will<br />approach the sequential Gibbs sampler, as explained in P.Ferrari et al.(1993). However, this method<br />is also not practical when learning topic models because it is computationally inefficient. Younes<br />(1998) shows the existence of exact parallel samplers that make use of periodic synchronous random<br />fields. However there is no known method for constructing such a sampler.<br />Our HD-LDA model is similar to the DCM-LDA model presented by Mimno and McCallum<br />(2007). There the authors perform topic modeling on a collection of books by learning a different<br />topic model for each book and then clustering these learned topics together to find global topics. In<br />this model, the concept of a book is directly analogous to our concept of a processor. DCM-LDA<br />uses Stochastic EM along with agglomerative clustering to learn topics, while our HD-LDA follows<br />a fully Bayesian approach for inference. HD-LDA also differs from other topic hierarchies found<br />in the literature. The Hierarchical Dirichlet Process model of Teh et al. (2006) places a deeper<br />hierarchical prior on the topic mixture, instead of on the word-topic distributions. The Pachinko<br />Allocation Model presented by Li and McCallum (2006) deals with a document-specific hierarchy<br />of topic-assignments. These types of hierarchies do not directly facilitate proper parallel Gibbs<br />sampling as is done in HD-LDA.<br />24</p>  <p>Page 25</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />8. Conclusions<br />We have proposed three different algorithms for distributing across multiple processors Gibbs sam-<br />pling for LDA and HDP. With our approximate distributed algorithm, AD-LDA, we sample from an<br />approximation to the posterior distribution by allowing different processors to concurrently sample<br />topic assignments on their local subsets of the data. Despite having no formal convergence guar-<br />antees, AD-LDA works very well empirically and is easy to implement. With our hierarchical dis-<br />tributed model, HD-LDA, we adapt the underlying LDA model to map to the distributed processor<br />architecture. This model is more complicated than AD-LDA, but it inherits the usual convergence<br />properties of Markov chain Monte Carlo. We discovered that careful selection of hyperparameters<br />was critical to making HD-LDA work well, but this selection was clearly informed by AD-LDA.<br />Our distributed algorithm AD-HDP followed the same approach as AD-LDA, but with an additional<br />step to merge newly instantiated topics.<br />Our proposed distributed algorithms learn LDA models with predictive performance that is no<br />different than single-processor LDA. On each processor they burn-in and converge at the same rate<br />as LDA, yielding significant speedups in practice. For HDP, our distributed algorithm eventually<br />produced the same perplexity as the single-processor version of HDP. Prior to reaching the con-<br />verged perplexity result, AD-HDP had higher perplexity than HDP since the merging of new topics<br />by label slows the rate of topic growth. We also discovered that matching new topics by similarity<br />significantly improves AD-HDP’s rate of convergence.<br />The space and time complexity of these distributed algorithms make them scalable to run very<br />large data sets, for example, collections with billions to trillions of words. Using two multi-million<br />document datasets, and running computations on a 1024-processor parallel supercomputer, we<br />showed how one can achieve a 700-800 times reduction in wall-clock time by using our distributed<br />approach.<br />There are several potentially interesting research directions that can be pursued using the algo-<br />rithms proposed here as a starting point. One research direction is to use more complex schemes that<br />allow data to adaptively move from one processor to another. The distributed schemes presented in<br />this paper can also be used to parallelize topic models that are based on or derived from LDA and<br />HDP, and beyond that a potentially larger class of graphical models.<br />Acknowledgments<br />This material is based upon work supported by the National Science Foundation. DN and PS were<br />supported byNSFgrants SCI-0225642, CNS-0551510, and IIS-0083489. DNwasalso supported by<br />a Google Research Award. PS was also supported by ONR grant N00014-18-1-101 and a different<br />Google Research Award. AA was supported by an NSF graduate fellowship. MW was supported<br />by NSF grants IIS-0535278 and IIS-0447903, and ONR grant 00014-06-1-073. Computations were<br />performed at SDSC under an MRAC Allocation.<br />25</p>  <p>Page 26</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />Appendix A.<br />The auxiliary variable method explained in Escobar and West (1995) and Teh et al. (2006) is used<br />to sample, , and. To derive Gibbs sampling equations, we use the following expansions:<br />(11)<br />(S is Stirling number of first kind) (12)<br />The first expansion follows from the definition of the Beta function, and the second expansion<br />makes use of the Stirling number of the first kind to rewrite the factorial (see (Abramowitz and<br />Stegun, 1964)).<br />Now we derive the sampling equation for. Combining the collapsed distribution (7) with the<br />prior on(6) gives the posterior distribution for<br />1:<br />(13)<br />Using the expansions (11,12) we introduce the auxiliary variables<br />and :<br />(14)<br />The joint distribution above allows us to create sampling equations for, , and :<br />Gamma(15)<br />Beta (16)<br />Antoniak(17)<br />1. To avoid notational clutter, we denote conditioned-upon variables and parameters by a dash. These variables can be<br />inferred from context.<br />26</p>  <p>Page 27</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />The Antoniak distribution is the distribution of the number of occupied tables if<br />are sent into a restaurant that follows the Chinese restaurant process with strength parameter<br />Sampling from the Antoniak distribution is done by sampling<br />customers<br />.<br />Bernoulli variables:<br />Bernoulli(18)<br />(19)<br />Using the same auxiliary variable techniques, we derive sampling equations for<br />variables are sampled jointly because they are dependent. The posterior distribution for<br />and the joint distribution with the auxiliary variables<br />and. These<br />and<br />andare given by:<br />(20)<br />(21)<br />Note that the set of variables ( and ) is unrelated to the set of auxiliary variables introduced<br />. The sampling equations for ,, , andare:for<br />Gamma (22)<br />Dirichlet(23)<br />Beta(24)<br />Antoniak (25)<br />27</p>  <p>Page 28</p> <p>NEWMAN, ASUNCION, SMYTH, WELLING<br />References<br />M. Abramowitz and I. Stegun. Handbook of Mathematical Functions with Formulas, Graphs, and<br />Mathematical Tables. Dover, New York, 1964.<br />A. Asuncion and D. Newman.<br />http://www.ics.uci.edu/ mlearn/MLRepository.html.<br />UCI machine learning repository,2007.URL<br />D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. In Advances in Neural Information<br />Processing Systems, volume 14, pages 601–608, Cambridge, MA, 2002. MIT Press.<br />D. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research,<br />3:993–1022, 2003.<br />A. Brockwell. Parallel Markov chain Monte Carlo simulation by pre-fetching. Journal of Compu-<br />tational &amp; Graphical Statistics, 15, No. 1:246–261, 2006.<br />R. Burkard and E. C ¸ela. Linear assignment problems and extensions. In P. Pardalos and D. Du, ed-<br />itors, Handbook of Combinatorial Optimization, Supplement Volume A. Kluwer Academic Pub-<br />lishers, 1999.<br />G. Casella and C. Robert. Rao-Blackwellisation of sampling schemes. Biometrika, 83(1):81–94,<br />1996.<br />C. Chemudugunta, P. Smyth, and M. Steyvers. Modeling general and specific aspects of documents<br />with a probabilistic topic model. In Advances in Neural Information Processing Systems 19,<br />pages 241–248. MIT Press, Cambridge, MA, 2007.<br />C. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Olukotun. Map-Reduce for machine<br />learning on multicore. In Advances in Neural Information Processing Systems 19, pages 281–<br />288. MIT Press, Cambridge, MA, 2007.<br />A. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: Scalable online collabo-<br />rative filtering. In WWW ’07: Proceedings of the 16th International Conference on World Wide<br />Web, pages 271–280, New York, NY, 2007. ACM.<br />M. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the<br />American Statistical Association, 90(430):577–588, 1995.<br />G. Forman and B. Zhang. Distributed data clustering can be efficient and exact. In ACM KDD<br />Explorations, volume 2, pages 34–38, New York, NY, 2000. ACM.<br />T. Griffiths and M. Steyvers. Finding scientific topics. In Proceedings of the National Academy of<br />Sciences, volume 101, pages 5228–5235, 2004.<br />E. Kontoghiorghes.<br />Monographs). Chapman &amp; Hall / CRC, 2005.<br />Handbook of Parallel Computing and Statistics (Statistics, Textbooks and<br />W.Liand A.McCallum. Pachinko allocation: DAG-structured mixture models of topic correlations.<br />InProceedings oftheInternational Conference onMachine Learning, volume 23, pages 577–584,<br />New York, NY, 2006. ACM.<br />28</p>  <p>Page 29</p> <p>DISTRIBUTED ALGORITHMS FOR TOPIC MODELS<br />J. Liu, W. Wong, and A. Kong. Covariance structure of the Gibbs sampler with applications to the<br />comparisons of estimators and augmentation schemes. Biometrika, 81(1):27–40, 1994.<br />D. Mimno and A. McCallum. Organizing the OCA: Learning faceted subjects from a library of<br />digital books. In JCDL ’07: Proceedings of the 2007 conference on digital libraries, pages<br />376–385, New York, NY, 2007. ACM.<br />R. Nallapati, W. Cohen, and J. Lafferty. Parallelized variational EM for latent Dirichlet allocation:<br />An experimental evaluation of speed and scalability. In ICDMW ’07: Proceedings of the Seventh<br />IEEE International Conference on Data Mining Workshops, pages 349–354, Washington, DC,<br />2007. IEEE Computer Society.<br />P. Ferrari, A. Frigessi, and R. Schonmann. Convergence of some partially parallel Gibbs samplers<br />with annealing. In Annals of Applied Probability, volume 3, pages 137–153. 1993.<br />M. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The author-topic model for authors and<br />documents. InProceedings oftheConference onUncertainty inArtificial Intelligence, volume 20,<br />pages 487–494, Arlington, VA, 2004. AUAI Press.<br />A. Rossini, L. Tierney, and N. Li. Simple parallel statistical computing in R. Journal of Computa-<br />tional &amp; Graphical Statistics, 16(2):399, 2007.<br />Y. W. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical Dirichlet processes. Journal of the<br />American Statistical Association, 101(476):1566–1581, 2006.<br />W. Kowalczyk and N. Vlassis. Newscast EM. In Advances in Neural Information Processing<br />Systems 17, pages 713–720. MIT Press, Cambridge, MA, 2005.<br />J. Wolfe, A. Haghighi, and D. Klein. Fully distributed EM for very large datasets. In Proceedings<br />of the International Conference on Machine Learning, pages 1184–1191. ACM, New York, NY,<br />2008.<br />L. Younes. Synchronous random fields and image restoration. IEEE Transactions on Pattern Anal-<br />ysis and Machine Intelligence, 20(4):380–390, 1998.<br />29</p>  <a href="https://www.researchgate.net/profile/David_Newman6/publication/220320734_Distributed_Algorithms_for_Topic_Models/links/0912f5107f7dc2714e000000.pdf">Download full-text</a> </div> <div id="rgw23_56ab1b42d0e37" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab1b42d0e37">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab1b42d0e37"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/David_Newman6/publication/220320734_Distributed_Algorithms_for_Topic_Models/links/0912f5107f7dc2714e000000.pdf" class="publication-viewer" title="0912f5107f7dc2714e000000.pdf">0912f5107f7dc2714e000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/David_Newman6">David Newman</a> &middot; May 16, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw26_56ab1b42d0e37"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.2280&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Distributed Algorithms for Topic Models">Distributed Algorithms for Topic Models</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.158.2280&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw33_56ab1b42d0e37" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (91) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw34_56ab1b42d0e37" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab1b42d0e37" >  <div class="indent-left">  <div id="rgw36_56ab1b42d0e37" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Dat_Quoc_Nguyen" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Dat Quoc Nguyen </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab1b42d0e37">  <li class="citation-context-item"> "For variational inference LDA, we use Blei&#39;s implementation . 5 For Gibbs sampling LDA, we use the jLDADMM package 6 (Nguyen, 2015) with common hyper-parameters β = 0.01 and α = 0.1 (Newman et al., 2009; Hu et al., 2011; Xie and Xing, 2013). We ran Gibbs sampling LDA for 2000 iterations and evaluated the topics assigned to words in the last sample. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling"> <span class="publication-title js-publication-title">Improving Topic Coherence with Latent Feature Word Representations in MAP Estimation for Topic Modeling</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2077564610_Dat_Quoc_Nguyen" class="authors js-author-name ga-publications-authors">Dat Quoc Nguyen</a> &middot;     <a href="researcher/82182307_Kairit_Sirts" class="authors js-author-name ga-publications-authors">Kairit Sirts</a> &middot;     <a href="researcher/2077644564_Mark_Johnson" class="authors js-author-name ga-publications-authors">Mark Johnson</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature word vectors have been used to obtain high performance in many natural language processing (NLP) tasks. In this paper, we present a new approach by incorporating word vectors to directly optimize the maximum a posteriori (MAP) estimation in a topic model. Preliminary results show that the word vectors induced from the experimental corpus can be used to improve the assignments of topics to words. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Conference Paper &middot; Dec 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Dat_Quoc_Nguyen/publication/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling/links/563a161f08ae337ef298384e.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw38_56ab1b42d0e37" >  <div class="indent-left">  <div id="rgw39_56ab1b42d0e37" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1511.00440" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw40_56ab1b42d0e37">  <li class="citation-context-item"> "However, an efficient and scalable solution should combine the innovations from both algorithm side and system side. Contemporary topic modelling systems either focus on algorithm side [31] [32] [15] [20] that has different sampling methods, or focus on system side [27] [19] [21]. These separation efforts makes it hard to port new algorithms on old systems. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform"> <span class="publication-title js-publication-title">ZenLDA: An Efficient and Scalable Topic Model Training System on Distributed Data-Parallel Platform</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2083969722_Bo_Zhao" class="authors js-author-name ga-publications-authors">Bo Zhao</a> &middot;     <a href="researcher/2083940763_Hucheng_Zhou" class="authors js-author-name ga-publications-authors">Hucheng Zhou</a> &middot;     <a href="researcher/2083964488_Guoqiang_Li" class="authors js-author-name ga-publications-authors">Guoqiang Li</a> &middot;     <a href="researcher/2083966503_Yihua_Huang" class="authors js-author-name ga-publications-authors">Yihua Huang</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> This paper presents our recent efforts, zenLDA, an efficient and scalable
Collapsed Gibbs Sampling system for Latent Dirichlet Allocation training, which
is thought to be challenging that both data parallelism and model parallelism
are required because of the Big sampling data with up to billions of documents
and Big model size with up to trillions of parameters. zenLDA combines both
algorithm level improvements and system level optimizations. It first presents
a novel CGS algorithm that balances the time complexity, model accuracy and
parallelization flexibility. The input corpus in zenLDA is represented as a
directed graph and model parameters are annotated as the corresponding vertex
attributes. The distributed training is parallelized by partitioning the graph
that in each iteration it first applies CGS step for all partitions in
parallel, followed by synchronizing the computed model each other. In this way,
both data parallelism and model parallelism are achieved by converting them to
graph parallelism. We revisited the tradeoff between system efficiency and
model accuracy and presented approximations such as unsynchronized model,
sparse model initialization and &quot;converged&quot; token exclusion. zenLDA is built on
GraphX in Spark that provides distributed data abstraction (RDD) and expressive
APIs to simplify the programming efforts and simultaneously hides the system
complexities. This enables us to implement other CGS algorithm with a few lines
of code change. To better fit in distributed data-parallel framework and
achieve comparable performance with contemporary systems, we also presented
several system level optimizations to push the performance limit. zenLDA was
evaluated it against web-scale corpus, and the result indicates that zenLDA can
achieve about much better performance than other CGS algorithm we implemented,
and simultaneously achieve better model accuracy. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Nov 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw41_56ab1b42d0e37" >  <div class="indent-left">  <div id="rgw42_56ab1b42d0e37" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Jaan_Altosaar" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Jaan Altosaar </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw43_56ab1b42d0e37">  <li class="citation-context-item"> "than O(1), and 2) while the document alias table samples z and u simultaneously, after sampling z from the word alias table u must be sampled using t lc /n lz (Chen et al., 2011). Parallelizing C-HDP requires an additional empirical method of merging new topics between threads (Newman et al., 2009), which is outside of the scope of this work. Our implementation of both models, C-LDA and C-HDP, are open-sourced online 1 . " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections"> <span class="publication-title js-publication-title">Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2079808098_Jingwei_Zhang" class="authors js-author-name ga-publications-authors">Jingwei Zhang</a> &middot;     <a href="researcher/2079817210_Aaron_Gerow" class="authors js-author-name ga-publications-authors">Aaron Gerow</a> &middot;     <a href="researcher/2037263079_Jaan_Altosaar" class="authors js-author-name ga-publications-authors">Jaan Altosaar</a> &middot;     <a href="researcher/2079820121_James_Evans" class="authors js-author-name ga-publications-authors">James Evans</a> &middot;     <a href="researcher/2079797438_Richard_Jean_So" class="authors js-author-name ga-publications-authors">Richard Jean So</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Weak topic correlation across document collections with different numbers of
topics in individual collections presents challenges for existing
cross-collection topic models. This paper introduces two probabilistic topic
models, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address
problems that can arise when analyzing large, asymmetric, and potentially
weakly-related collections. Topic correlations in weakly-related collections
typically lie in the tail of the topic distribution, where they would be
overlooked by models unable to fit large numbers of topics. To efficiently
model this long tail for large-scale analysis, our models implement a parallel
sampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et
al., 2015). The models are first evaluated on synthetic data, generated to
simulate various collection-level asymmetries. We then present a case study of
modeling over 300k documents in collections of sciences and humanities research
from JSTOR. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Jaan_Altosaar/publication/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections/links/5678e9a708aebcdda0ec48ec.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw28_56ab1b42d0e37" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw29_56ab1b42d0e37">  </ul> </div> </div>   <div id="rgw19_56ab1b42d0e37" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw20_56ab1b42d0e37"> <div> <h5> <a href="publication/283109008_FSMBUS_A_frequent_subgraph_mining_algorithm_in_single_large-scale_graph_using_spark" class="color-inherit ga-similar-publication-title"><span class="publication-title">FSMBUS: A frequent subgraph mining algorithm in single large-scale graph using spark</span></a>  </h5>  <div class="authors"> <a href="researcher/2083287402_Y_Yan" class="authors ga-similar-publication-author">Y. Yan</a>, <a href="researcher/2083293820_Y_Dong" class="authors ga-similar-publication-author">Y. Dong</a>, <a href="researcher/2083268665_X_He" class="authors ga-similar-publication-author">X. He</a>, <a href="researcher/2083283723_W_Wang" class="authors ga-similar-publication-author">W. Wang</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab1b42d0e37"> <div> <h5> <a href="publication/282950275_Predicting_the_Performance_of_Parallel_Computing_Models_Using_Queuing_System" class="color-inherit ga-similar-publication-title"><span class="publication-title">Predicting the Performance of Parallel Computing Models Using Queuing System</span></a>  </h5>  <div class="authors"> <a href="researcher/2046323160_Shen_Chao" class="authors ga-similar-publication-author">Shen Chao</a>, <a href="researcher/2083047874_Tong_Weiqin" class="authors ga-similar-publication-author">Tong Weiqin</a>, <a href="researcher/2083035109_Samina_Kausar" class="authors ga-similar-publication-author">Samina Kausar</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw22_56ab1b42d0e37"> <div> <h5> <a href="publication/276455483_Efficient_distributed_subgraph_similarity_matching" class="color-inherit ga-similar-publication-title"><span class="publication-title">Efficient distributed subgraph similarity matching</span></a>  </h5>  <div class="authors"> <a href="researcher/2032632587_Ye_Yuan" class="authors ga-similar-publication-author">Ye Yuan</a>, <a href="researcher/7468771_Guoren_Wang" class="authors ga-similar-publication-author">Guoren Wang</a>, <a href="researcher/2073854184_Jeffery_Yu_Xu" class="authors ga-similar-publication-author">Jeffery Yu Xu</a>, <a href="researcher/76405502_Lei_Chen" class="authors ga-similar-publication-author">Lei Chen</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw53_56ab1b42d0e37" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw54_56ab1b42d0e37">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw55_56ab1b42d0e37" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=abTIiIlehDiPBiJPmqQ-qAMqAl-TtW9kStZKMclcBP7L7wYWOU_NOl_poYCa9pvF" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="J37fLLdLgzu8uFLwTB2q4CtpA1DNfhZAa3wR8Hy/yPqPgSVEYiMCShwvlWE/QdfhB/FPFPztqvuv5u6FzyzrA2ivjA/GRnl5NVLqHyvugdL5HIdkURDpdkokV1txd4AVSyOgZT3behqsDEzYoW+Oiwv7hB8PGcHUbq1D3y+C2ignlXpKVil92FvKnQ9Hd19neWUL8ZOVGXTMMB6iz6S79XS6ugReIMx3qTHpLxPCjXVtT/A7JduhNZbveGnAS50ThOc/f5FIXCeAAaHBO7ge9TPwoqtSKKbwsMmB93epaf0="/> <input type="hidden" name="urlAfterLogin" value="publication/220320734_Distributed_Algorithms_for_Topic_Models"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNzM0X0Rpc3RyaWJ1dGVkX0FsZ29yaXRobXNfZm9yX1RvcGljX01vZGVscw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNzM0X0Rpc3RyaWJ1dGVkX0FsZ29yaXRobXNfZm9yX1RvcGljX01vZGVscw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIwMzIwNzM0X0Rpc3RyaWJ1dGVkX0FsZ29yaXRobXNfZm9yX1RvcGljX01vZGVscw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw56_56ab1b42d0e37"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 734;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"David Newman","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272452597055500%401441969169380_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/David_Newman6","institution":"University of California, Irvine","institutionUrl":false,"widgetId":"rgw4_56ab1b42d0e37"},"id":"rgw4_56ab1b42d0e37","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1844890","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1b42d0e37"},"id":"rgw3_56ab1b42d0e37","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=220320734","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":220320734,"title":"Distributed Algorithms for Topic Models","journalTitle":"Journal of Machine Learning Research","journalDetailsTooltip":{"data":{"journalTitle":"Journal of Machine Learning Research","journalAbbrev":"J MACH LEARN RES","publisher":false,"issn":"1532-4435","impactFactor":"2.47","fiveYearImpactFactor":"4.77","citedHalfLife":"8.30","immediacyIndex":"0.31","eigenFactor":"0.03","articleInfluence":"3.23","widgetId":"rgw6_56ab1b42d0e37"},"id":"rgw6_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1532-4435","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1145\/1577069.1755845","journalInfos":{"journal":"","publicationDate":"08\/2009;","publicationDateRobot":"2009-08","article":"10:1801-1828.","journalTitle":"Journal of Machine Learning Research","journalUrl":"journal\/1532-4435_Journal_of_Machine_Learning_Research","impactFactor":2.47}},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/journals\/jmlr\/jmlr10.html#NewmanASW09","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1145\/1577069.1755845"},{"key":"rft.atitle","value":"Distributed Algorithms for Topic Models"},{"key":"rft.title","value":"Journal of Machine Learning Research"},{"key":"rft.jtitle","value":"Journal of Machine Learning Research"},{"key":"rft.volume","value":"10"},{"key":"rft.date","value":"2009"},{"key":"rft.pages","value":"1801-1828"},{"key":"rft.issn","value":"1532-4435"},{"key":"rft.au","value":"David Newman,Arthur U. Asuncion,Padhraic Smyth,Max Welling"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab1b42d0e37"},"id":"rgw7_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=220320734","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":220320734,"peopleItems":[{"data":{"authorNameOnPublication":"David Newman","accountUrl":"profile\/David_Newman6","accountKey":"David_Newman6","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272452597055500%401441969169380_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Newman","profile":{"professionalInstitution":{"professionalInstitutionName":"University of California, Irvine","professionalInstitutionUrl":"institution\/University_of_California_Irvine"}},"professionalInstitutionName":"University of California, Irvine","professionalInstitutionUrl":"institution\/University_of_California_Irvine","url":"profile\/David_Newman6","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272452597055500%401441969169380_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"David_Newman6","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab1b42d0e37"},"id":"rgw10_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1844890&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of California, Irvine","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":1,"publicationUid":220320734,"widgetId":"rgw9_56ab1b42d0e37"},"id":"rgw9_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1844890&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=1&publicationUid=220320734","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69884878_Arthur_U_Asuncion","authorNameOnPublication":"Arthur U. Asuncion","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Arthur U. Asuncion","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69884878_Arthur_U_Asuncion","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1b42d0e37"},"id":"rgw12_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69884878&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1b42d0e37"},"id":"rgw11_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69884878&authorNameOnPublication=Arthur%20U.%20Asuncion","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/66646470_Padhraic_Smyth","authorNameOnPublication":"Padhraic Smyth","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Padhraic Smyth","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/66646470_Padhraic_Smyth","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab1b42d0e37"},"id":"rgw14_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=66646470&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab1b42d0e37"},"id":"rgw13_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=66646470&authorNameOnPublication=Padhraic%20Smyth","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69847505_Max_Welling","authorNameOnPublication":"Max Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Max Welling","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69847505_Max_Welling","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw16_56ab1b42d0e37"},"id":"rgw16_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69847505&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw15_56ab1b42d0e37"},"id":"rgw15_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69847505&authorNameOnPublication=Max%20Welling","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1b42d0e37"},"id":"rgw8_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=220320734&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":220320734,"abstract":"<noscript><\/noscript><div>We describedistributedalgorithmsfortwowidely-usedtopicmodels,namelytheLatentDirich- let Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In ourdistributed algorithmsthe data is partitioned across separate processors and inference is done in a parallel, dis- tributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straight- forward mapping of LDA to a distributed processor setting. In this algorithm processors concur- rently perform Gibbs sampling over local data followed by a global update of topic counts. The al- gorithmissimpletoimplementandcanbeviewedasanapproximationtoGibbs-sampledLDA.The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world textcorporawe show that distributed learning works well in practice. For both LDA and HDP, we show that the convergedtest-data log probability for distributed learning is indistinguish- able from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw17_56ab1b42d0e37"},"id":"rgw17_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=220320734","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw18_56ab1b42d0e37"},"id":"rgw18_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1b42d0e37"},"id":"rgw5_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=220320734&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2083287402,"url":"researcher\/2083287402_Y_Yan","fullname":"Y. Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083293820,"url":"researcher\/2083293820_Y_Dong","fullname":"Y. Dong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083268665,"url":"researcher\/2083268665_X_He","fullname":"X. He","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083283723,"url":"researcher\/2083283723_W_Wang","fullname":"W. Wang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283109008_FSMBUS_A_frequent_subgraph_mining_algorithm_in_single_large-scale_graph_using_spark","usePlainButton":true,"publicationUid":283109008,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283109008_FSMBUS_A_frequent_subgraph_mining_algorithm_in_single_large-scale_graph_using_spark","title":"FSMBUS: A frequent subgraph mining algorithm in single large-scale graph using spark","displayTitleAsLink":true,"authors":[{"id":2083287402,"url":"researcher\/2083287402_Y_Yan","fullname":"Y. Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083293820,"url":"researcher\/2083293820_Y_Dong","fullname":"Y. Dong","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083268665,"url":"researcher\/2083268665_X_He","fullname":"X. He","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083283723,"url":"researcher\/2083283723_W_Wang","fullname":"W. Wang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283109008_FSMBUS_A_frequent_subgraph_mining_algorithm_in_single_large-scale_graph_using_spark","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283109008_FSMBUS_A_frequent_subgraph_mining_algorithm_in_single_large-scale_graph_using_spark\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab1b42d0e37"},"id":"rgw20_56ab1b42d0e37","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283109008","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2046323160,"url":"researcher\/2046323160_Shen_Chao","fullname":"Shen Chao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083047874,"url":"researcher\/2083047874_Tong_Weiqin","fullname":"Tong Weiqin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083035109,"url":"researcher\/2083035109_Samina_Kausar","fullname":"Samina Kausar","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jul 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282950275_Predicting_the_Performance_of_Parallel_Computing_Models_Using_Queuing_System","usePlainButton":true,"publicationUid":282950275,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282950275_Predicting_the_Performance_of_Parallel_Computing_Models_Using_Queuing_System","title":"Predicting the Performance of Parallel Computing Models Using Queuing System","displayTitleAsLink":true,"authors":[{"id":2046323160,"url":"researcher\/2046323160_Shen_Chao","fullname":"Shen Chao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083047874,"url":"researcher\/2083047874_Tong_Weiqin","fullname":"Tong Weiqin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083035109,"url":"researcher\/2083035109_Samina_Kausar","fullname":"Samina Kausar","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282950275_Predicting_the_Performance_of_Parallel_Computing_Models_Using_Queuing_System","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282950275_Predicting_the_Performance_of_Parallel_Computing_Models_Using_Queuing_System\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab1b42d0e37"},"id":"rgw21_56ab1b42d0e37","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282950275","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2032632587,"url":"researcher\/2032632587_Ye_Yuan","fullname":"Ye Yuan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7468771,"url":"researcher\/7468771_Guoren_Wang","fullname":"Guoren Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073854184,"url":"researcher\/2073854184_Jeffery_Yu_Xu","fullname":"Jeffery Yu Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":76405502,"url":"researcher\/76405502_Lei_Chen","fullname":"Lei Chen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2015","journal":"The VLDB Journal","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/276455483_Efficient_distributed_subgraph_similarity_matching","usePlainButton":true,"publicationUid":276455483,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.57","url":"publication\/276455483_Efficient_distributed_subgraph_similarity_matching","title":"Efficient distributed subgraph similarity matching","displayTitleAsLink":true,"authors":[{"id":2032632587,"url":"researcher\/2032632587_Ye_Yuan","fullname":"Ye Yuan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7468771,"url":"researcher\/7468771_Guoren_Wang","fullname":"Guoren Wang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073854184,"url":"researcher\/2073854184_Jeffery_Yu_Xu","fullname":"Jeffery Yu Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":76405502,"url":"researcher\/76405502_Lei_Chen","fullname":"Lei Chen","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["The VLDB Journal 06\/2015; 24(3):369-394. DOI:10.1007\/s00778-015-0381-6"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/276455483_Efficient_distributed_subgraph_similarity_matching","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/276455483_Efficient_distributed_subgraph_similarity_matching\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw22_56ab1b42d0e37"},"id":"rgw22_56ab1b42d0e37","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=276455483","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw19_56ab1b42d0e37"},"id":"rgw19_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=220320734&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":220320734,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":220320734,"publicationType":"article","linkId":"0912f5107f7dc2714e000000","fileName":"0912f5107f7dc2714e000000.pdf","fileUrl":"profile\/David_Newman6\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000.pdf","name":"David Newman","nameUrl":"profile\/David_Newman6","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"May 16, 2014","fileSize":"1.84 MB","widgetId":"rgw25_56ab1b42d0e37"},"id":"rgw25_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220320734&linkId=0912f5107f7dc2714e000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":220320734,"publicationType":"article","linkId":"0ff775070cf25dfdcf51885f","fileName":"Distributed Algorithms for Topic Models","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.158.2280&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.158.2280&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw26_56ab1b42d0e37"},"id":"rgw26_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=220320734&linkId=0ff775070cf25dfdcf51885f&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw24_56ab1b42d0e37"},"id":"rgw24_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320734&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":83,"valueFormatted":"83","widgetId":"rgw27_56ab1b42d0e37"},"id":"rgw27_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320734","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab1b42d0e37"},"id":"rgw23_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320734&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":220320734,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw29_56ab1b42d0e37"},"id":"rgw29_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=220320734&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":83,"valueFormatted":"83","widgetId":"rgw30_56ab1b42d0e37"},"id":"rgw30_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=220320734","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw28_56ab1b42d0e37"},"id":"rgw28_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=220320734&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Journal of Machine Learning Research x (2009) x-xxSubmitted 6\/08; Published xx\/xx\nDistributed Algorithms for Topic Models\nDavid Newman\nDepartment of Computer Science\nUniversity of California, Irvine\nIrvine, CA 92697, USA\nArthur Asuncion\nDepartment of Computer Science\nUniversity of California, Irvine\nIrvine, CA 92697, USA\nPadhraic Smyth\nDepartment of Computer Science\nUniversity of California, Irvine\nIrvine, CA 92697, USA\nNEWMAN@UCI.EDU\nASUNCION@ICS.UCI.EDU\nSMYTH@ICS.UCI.EDU\nMax Welling\nDepartment of Computer Science\nUniversity of California, Irvine\nIrvine, CA 92697, USA\nWELLING@ICS.UCI.EDU\nEditor:\nAbstract\nWe describedistributedalgorithmsfortwowidely-usedtopicmodels,namelytheLatentDirich-\nlet Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In ourdistributed\nalgorithmsthe data is partitioned across separate processors and inference is done in a parallel, dis-\ntributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straight-\nforward mapping of LDA to a distributed processor setting. In this algorithm processors concur-\nrently perform Gibbs sampling over local data followed by a global update of topic counts. The al-\ngorithmissimpletoimplementandcanbeviewedas anapproximationtoGibbs-sampledLDA.The\nsecond version is a model that uses a hierarchical Bayesian extension of LDA to directly account\nfor distributed data. This model has a theoretical guarantee of convergence but is more complex\nto implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward\nmapping approach, and merges newly-created topics either by matching or by topic-id. Using five\nreal-world text corporawe show that distributed learning works well in practice. For both LDA and\nHDP, we show that the convergedtest-data log probability for distributed learning is indistinguish-\nable from that obtained with single-processor learning. Our extensive experimental results include\nlearning topic models for two multi-million document collections using a 1024-processor parallel\ncomputer.\nKeywords:\nTopic Models, Latent Dirichlet Allocation, Hierarchical Dirichlet Processes, Dis-\ntributed Parallel Computation\n1. Introduction\nVery large data sets, such as collections of images or text documents, are becoming increasingly\ncommon, with examples ranging from collections of online books at Google and Amazon, to the\nc 2009 David Newman, Arthur Asuncion, Padhraic Smyth and Max Welling."},{"page":2,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nlarge collection of images at Flickr. These data sets present major opportunities for machine learn-\ning, such as the ability to explore richer and more expressive models than previously possible, and\nprovide new and interesting domains for the application of learning algorithms.\nHowever, the scale of these data sets also brings significant challenges for machine learning,\nparticularly in terms of computation time and memory requirements. For example, a text corpus\nwith one million documents, each containing one thousand words, will require approximately eight\nGBytes of memory to store the billion words. Adding the memory required for parameters, which\nusually exceeds memory for the data, creates a total memory requirement that exceeds that available\non a typical desktop computer. If one were to assume that a simple operation, such as computing a\nprobability vector over categories using Bayes rule, takes on the order of\nthen a full pass through the billion words would take 1000 seconds. Thus, algorithms that make\nmultiple passes through the data, for example clustering and classification algorithms, will have run\ntimes in days for this sized corpus. Furthermore, for small to moderate sized document sets where\nmemory is not an issue, it would be useful to have algorithms that could take advantage of desktop\nmultiprocessor\/multicore technology to learn models in near real-time.\nAn obvious approach for addressing these time and memory issues is to distribute the learning\nalgorithm over multiple processors. Inparticular, with\nthe memory issue by distributing\nof the total data to each processor. However, the computation\nproblem remains non-trivial for a fairly large class of learning algorithms, namely how to combine\nlocal processing on each processor to arrive at a useful global solution.\nIn this general context we investigate distributed algorithms for two widely-used unsupervised\nlearning models: the Latent Dirichlet Allocation (LDA) model, and the Hierarchical Dirichet Pro-\ncess (HDP) model. LDA and HDP models are arguably among the most successful recent learning\nalgorithms for analyzing discrete data such as bags of words from a collection of text documents.\nHowever, they can take days to learn for large corpora, and thus, distributed learning would be\nparticularly useful.\nThe rest of the paper is organized as follows: In Section 2 we review the standard derivation\nof LDA and HDP. Section 3 presents our two distributed algorithms for LDA and one distributed\nalgorithm for HDP. Empirical results are provided in Section 4. Scalability results are presented\nin Section 5, and further analysis of distributed LDA is provided in Section 6. A comparison with\nrelated models is given in Section 7. Finally, Section 8 concludes the paper.\nseconds per word,\nprocessors, itis somewhat trivial toaddress\n2. Latent Dirichlet Allocation and Hierarchical Dirichlet Process Model\nWe start by reviewing the LDA and HDP models. Both LDA and HDP are generative probabilistic\nmodels for discrete data such as bags of words from text documents \u2013 in this context these models\nare often referred to as topic models. To illustrate the notation, we refer the reader to the graphical\nmodels for LDA and HDP shown in Figure 1.\nLDA models each of\ndocuments in a collection as a mixture over\ntopic being a multinomial distribution over a vocabulary of\ndraw a mixing proportion from a Dirichlet with parameter . For the\ntopic is drawn with probability. Word\non valuewith probability . A Dirichlet prior with parameter\ndistributions.\nlatent topics, with each\nwords. For document , we first\nword in the document, a\nis then drawn from topic\nis placed on the word-topic\n, withtaking\n2"},{"page":3,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\n\u03b1\nij\nZ\nij\nX\nk \u03c6\nj \u03b8\nk\n\u03b1\nij\nZ\nij\nX\nj \u03b8\nk \u03c6\nK\nD\nNj\n\u221e\nNj\nD\n\u03b2\u03b2\n\u03b3\n\u03b7\nFigure 1: Graphical models for LDA(left) and HDP(right). Observed variables (words) are shaded,\nand hyperparameters are shown in squares.\nThus, the generative process for LDA is given by\n(1)\nToavoid clutterwedenotesampling fromaDirichlet\n, and likewise for . In this paper, we use symmetric Dirichlet priors for simplicity, un-\nless specified otherwise. The full joint distribution over all parameters and variables is\nasshorthand for\n(2)\nwhere\nsummed out.\ncomputations, representing the number of words assigned to topic\nof times word is assigned to topic\ncommonly used variables in Table 1.\nGiven the observed words\nposterior distribution over the latent topic assignments\ntopics . Approximate inference for LDA can be performed either using variational methods (Blei\net al., 2003) or Markov chain Monte Carlo methods (Griffiths and Steyvers, 2004). In this paper we\nfocus on Markov chain Monte Carlo algorithms for approximate inference. MCMC is widely used\nas an inference method for a variety of topic models, for example Rosen-Zvi et al. (2004), Li and\nMcCallum (2006), and Chemudugunta et al. (2007) all use MCMC for inference. In the MCMC\ncontext, the usual procedure is to integrate out the mixtures\ncalled collapsing \u2013 and just sample the latent variables . Given the current state of all but one\n, and we use the convention that missing indices are\nare the two primary count arrays used in\nin document , and the number\nin the corpus, respectively. For ease of reading we list\nand\n, the task of Bayesian inference for LDA is to compute the\n, the mixing proportions, and the\nand topics in (2) \u2013 a procedure\n3"},{"page":4,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nDescription\nNumber of documents in collection\nNumber of distinct words in vocabulary\nTotal number of words in collection\nNumber of topics\nobserved word in document\nTopic assigned to\nCount of word assigned to topic\nCount of topic assigned in document\nProbability of word given topic\nProbability of topic given document\nTable 1: Description of commonly used variables.\nvariable, the conditional probability of is\n(3)\nwhere the superscript\nHDP is a collection of Dirichlet Processes which share the same topic distributions and can be\nviewed as the non-parametric extension of LDA. The advantage of HDP is that the number of topics\nis determined by the data. The HDP model is obtained by taking the following model in the limit as\ngoes to infinity. Let be top level Dirichlet variables sampled from a Dirichlet with parameter\n. The topic mixture for each document,, is drawn from a Dirichlet with parameters\nThe word-topic distributions are drawn from a base Dirichlet distribution with parameter\nin LDA,is sampled from, and word is sampled from the corresponding topic\ngenerative process is given by\nmeans that the corresponding word is excluded in the counts.\n.\n. As\n. The\n(4)\nThe posterior distribution is sampled using the direct assignment sampler for HDP described\nin Teh et al. (2006). As was done for LDA, both\nthe following conditional distribution:\nandare integrated out, andis sampled from\nif previously used\nnew\nifis new.\n(5)\nThe sampling scheme for\nprobability mass proportional to\ndefined to have infinitely many topics, the sampling algorithm only instantiates topics as needed.\nis also detailed in Teh et al. (2006). Note that a small amount of\nnewis reserved for the instantiation of new topics. While HDP is\n4"},{"page":5,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\n0 200400 6008001000\n1800\n1850\n1900\n1950\n2000\n2050\n2100\n2150\n2200\nIteration\nPerplexity\nNon\u2212Collapsed\n\u03b8 Collapsed\n\u03b8, \u03c6 Collapsed\nFigure 2: On theNIPS datasetusing\nfaster than the partially collapsed (circles) and non-collapsed (triangles) samplers.\ntopics,the fullycollapsedGibbssampler(solidline)converges\nNeed for Distributed Algorithms: One could argue that it is trivial to distribute non-collapsed\nGibbs sampling, because sampling of can happen independently given\ncan be done concurrently. In the non-collapsed Gibbs sampler, one samples\nand then samplesandgiven . Furthermore, if individual documents are not spread across\ndifferent processors, one can marginalize over just\ncollapsed scheme, the latent variableson each processor can be concurrently sampled, where the\nconcurrency is over processors.\nUnfortunately, the non-collapsed and partially collapsed Gibbs samplers exhibit slow conver-\ngence due to the strong dependencies between the parameters and latent variables. Generally, we\nexpect faster mixing as more variables are collapsed (Liu et al., 1994; Casella and Robert, 1996).\nFigure 2 shows, using one of the data sets used throughout our paper, that the log probability of\ntest data (measured as perplexity, which is defined in Section 4) of the non-collapsed and partially\ncollapsed samplers converges more slowly than the fully collapsed sampler.\nThe slow convergence of partially collapsed and non-collapsed Gibbs samplers motivates the\nneed to devise distributed algorithms for fully collapsed Gibbs samplers. In the following section\nwe introduce distributed topic modeling algorithms that take advantage of the benefits of collapsing\nboth and .\nand , and therefore\ngivenand,\n, since is processor-specific. In this partially\n3. Distributed Algorithms for Topic Models\nWe introduce algorithms for LDA and HDP where the data, parameters, and computation are dis-\ntributed over distinct processors. We distribute the\nimately documents on each processor. Documents are randomly assigned to processors,\nalthough as we will see later, the assignment of documents to processors \u2013 ranging from random to\nhighly non-random or adversarial \u2013 appears to have little influence on the results. This indifference\ndocuments overprocessors, with approx-\n5"},{"page":6,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nAlgorithm 1 AD-LDA\nrepeat\nfor each processor\nCopy global counts:\nSample\nend for\nSynchronize\nUpdate global counts:\nuntil termination criterion satisfied\nin parallel do\nlocally: LDA-Gibbs-Iteration(,,, , , )\nis somewhat understandable given that converged results from Gibbs sampling are independent of\nsampling order.\nWe partition the words from the\ndocuments into\nsponding topic assignments into\nfrom documents\nTopic-document counts are likewise distributed as\ndistributed, with each processor keeping a separate local copy\nand the corre-\n, the words, where processor\n, the corresponding topic assignments.\n. The word-topic counts\n.\nstores\n, and\nare also\n3.1 Approximate Distributed Latent Dirichlet Allocation\nThe difficulty of distributing and parallelizing over Gibbs sampling updates (3) lies in the fact that\nGibbs sampling is a strictly sequential process. To asymptotically sample from the posterior distri-\nbution, the update of any topic assignmentcan not be performed concurrently with the update\nof any other topic assignment . But given the typically large number of word tokens compared\nto the number of processors, to what extent will the update of one topic assignment\nthe update of any other topic assignment ? Our hypothesis is that this dependence is weak, and\ntherefore we should be able to relax the requirement of sequential sampling of topic assignments\nand still learn a useful model. One can see this weak dependence in the following common situation.\nIf two processors are concurrently sampling, but sampling different words in different documents\n(i.e.), then concurrent sampling will be very close to sequential sampling because the\nonly term affecting the order of operations is the total count of topics\nof (3).\nThe pseudocode for our Approximate Distributed LDA (AD-LDA) algorithm is shown in Al-\ngorithm 1. After distributing the data and parameters across processors, AD-LDA performs simul-\ntaneous LDA Gibbs sampling on each of theprocessors. After processor\nits local data and updated topic assignments, the processor has modified count arrays\n. The topic-document counts are distinct because of the document index, , and will\nbe consistent with the topic assignments . However, the word-topic counts\nbe different on each processor, and not globally consistent with . To merge back to a single and\nconsistent set of word-topic counts, we perform a reduce operation on\nupdate the global counts. After the synchronization and update operations, each processor has the\nsame values in thearray which are consistent with the global vector of topic assignments .\nNote thatis not the result ofseparate LDA models running on separate data. In particular,\neach word-topic count array reflects all the counts, not just those local to that processor, so for every\nprocessor\n, whereis the total number of words in the corpus. As in LDA, the\ndepend on\nin the denominator\nhas swept through\nand\nwill in general\nacross all processors to\n6"},{"page":7,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\njp\n\u03b8\n\u03b3\n, a b\n, c d\nk\n\u03b2\np\n\u03b1\nijp\nZ\nijp\nX\nk\n\u03a6\nkp\n\u03d5\nK\nP\nNjp\nDp\nP\nFigure 3: Graphical model for Hierarchical Distributed Latent Dirichlet Allocation.\nalgorithm can terminate either after a fixed number of iterations, or based on some suitable MCMC\nconvergence metric.\nWe chose the name Approximate Distributed LDA because in this algorithm we are no longer\nasymptotically sampling from the true posterior, but to an approximation of the true posterior.\nNonetheless, we will show in our experimental results that the approximation made by Approxi-\nmate Distributed LDA works very well.\n3.2 Hierarchical Distributed Latent Dirichlet Allocation\nIn AD-LDAwe constructed an algorithm where each processor is independently computing an LDA\nmodel, but at the end of each sweep through a processor\u2019s data, a consistent global array of topic\ncounts\nis reconstructed. This global array of topic counts could be thought of as a parent topic\ndistribution, from which each processor draws its own local topic distribution.\nUsing this intuition, we created a Bayesian model reflecting this structure, as shown in Fig-\nure 3. Our Hierarchical Distributed LDA model (HD-LDA) places a hierarchy over word-topic\ndistributions, withbeing the global or parent word-topic distribution and\ntopic distributions on each processor. The local word-topic distributions\naccording to a Dirichlet distribution with a topic-dependent strength parameter\n. The model on each processor is simply an LDA model. The generative process is\ngiven by:\nthe local word-\nare drawn from\n, for each topic\n(6)\nFrom this generative process, we derive Gibbs sampling equations for HD-LDA. The derivation\nis based on the Teh et al. (2006) sampling schemes for Hierarchical Dirichlet Processes. As was\ndone for LDA, we start by integrating out\nand . The collapsed distribution of and on\n7"},{"page":8,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nprocessoris given by:\n(7)\nFrom this we derive the conditional probability for sampling a topic assignment\nAD-LDA, the topic assignments on any processor are now conditionally independent of the topic\nassignments on the other processors given, thus allowing each processor to sample\nrently. The conditional probability ofis\n. Unlike\nconcur-\n(8)\nThe full derivation of the Gibbs sampling equations for HD-LDA is provided in Appendix A,\nwhich lists the complete set of sampling equations for\nThe pseudocode for our Hierarchical Distributed LDA algorithm is given in Algorithm 2. Each\nvariable in this model is either local or global, depending on whether inference for the variable\nis computed locally on a processor or globally, requiring information from all processors. Local\nvariables include\n, , , , and . Global variables include\nsampling to sample its local variables concurrently. After each sweep through the processor\u2019s data,\nthe global variables are sampled. Note that, unlike AD-LDA,HD-LDAis performing strictly correct\nsampling for its model.\nHD-LDAcan beviewed asamixturemodel with\nweights. In this view the data have been hard-assigned to their respective clusters (i.e. processors),\nand the parameters of the clusters are generated from a shared prior distribution.\n,and.\nand . Each processor uses Gibbs\nLDAmixturecomponents withequal mixing\nAlgorithm 2 HD-LDA\nrepeat\nfor each processor\nSample\nSample\nend for\nSynchronize\nSample:\nBroadcast:\nuntil termination criterion satisfied\nin parallel do\nlocally: LDA-Gibbs-Iteration(\nlocally\n,,,,,)\n,\n,\n3.3 Approximate Distributed Hierarchical Dirichlet Processes\nOur third distributed algorithm, Approximate Distributed HDP, takes the same approach as AD-\nLDA. Processors concurrently run HDP for a single sweep through their local data. After all of\nthe processors sweep through their data, a synchronization and update step is performed to create a\n8"},{"page":9,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nAlgorithm 3 AD-HDP\nrepeat\nfor each processor\nSample\nReport\nend for\nSynchronize\nUpdate global counts (and merge new topics):\nin parallel do\nlocally: HDP-Gibbs-Iteration(\n, to master node\n,,,,, , , )\nSample: ,\nBroadcast:\nuntil termination criterion satisfied\n,\n, , ,\nProcessor 1\nProcessor 2\nProcessor 3\n++++++\n++++++\n+\n========\nMerged Topics\nNew Topics\nT1 T2 T3 T4 T5 T6 T7 T8\nFigure 4: The simplest method to merge new topics in AD-HDP is by integer topic label.\nsingle set of globally-consistent word-topic counts\nas AD-HDP, and provide the pseudocode in Algorithm 3.\nUnlike AD-LDA, which uses a fixed number of topics, individual processors in AD-HDP may\ninstantiate new topics during the sampling phase, according to the HDP sampling equation (5). Dur-\ning the synchronization and update step, instead of treating each processor\u2019s new topics as distinct,\nwe merge new topics that were instantiated on different processors. Merging new topics helps limit\nunnecessary growth in the total number of topics and allows AD-HDP to produce more of a global\nmodel.\nThere are several ways to merge newly created topics on each processor. A simple way \u2013\ninspired byAD-LDA\u2013istomerge newtopics based ontheir integer topic label. Amore complicated\nway is to match new topics across processors based on topic similarity.\nIn the first merging scheme, new topics are merged based on their integer topic label. For exam-\nple, assume that we have three processors, and at the end of a sweep through the data, processor one\nhas 8 new topics, processor two has 6 new topics, and processor three has 7 new topics. Then dur-\ning synchronization, all these new topics would be aligned by topic label and their counts summed,\nproducing 8 new global topics, as shown in Figure 4.\nWhile this merging of new topics by topic-id may seem suboptimal, it is computationally simple\nand efficient. We will show in the next section that this merging generally works well in practice,\neven when processors only have a small amount of data. We suggest that even if the merging by\n. We refer to the distributed version of HDP\n9"},{"page":10,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nAlgorithm 4 Greedy Matching of New Topics for AD-HDP\nInitialize global set of new topics,\nfor\n= 2 to P do\nfor topic in processor p\u2019s set of new topics do\nInitialize score array\nfor topic in\ndo\nscore[ ] = symmetric-KL-divergence( , )\nend for\nif min(score) threshold then\nAdd \u2019s counts to the topic in\nelse\nAugmentwith the new topic\nend if\nend for\nend for\n, to be processor 1\u2019s set of new topics\ncorresponding to min(score)\ntopic-id is initially quite random, the subsequent dynamics align the topics in a sensible manner. We\nwill also show that AD-HDP ultimately learns models with similar perplexity to HDP, irrespective\nof how new topics are merged.\nWe also investigate more complicated schemes for merging new topics in AD-HDP, beyond the\nsimple approach of merging by topic-id. Instead of aligning new topics based topic-id it is possible\nto align new topics using a similarity metric such as symmetric Kullback-Leibler divergence. How-\never, finding the optimal matching of topics in the case where\n1999). Thus, we consider approximate schemes: bipartite matching using a reference processor,\nand greedy matching.\nInthe bipartite matching scheme, weselect areference processor and perform bipartite matching\nbetween every processor\u2019s new topics and the set of new topics of the reference processor. The\nbipartite match is computed using the Hungarian algorithm, which runs in\noverall complexity ofwhere is the maximum number of new topics on a processor. We\nimplemented this scheme but did not find any improvement over AD-HDPwith merging by topic-id.\nIn the greedy matching scheme, new topics on each processor are sequentially compared to a\nglobal set of new topics. This global set is initialized to the first processor\u2019s set of new topics. If\na new topic is sufficiently different from every topic in the global set, the number of topics in the\nglobal set is incremented; otherwise, the counts for that new topic are added to those from the closest\nmatch in the global set. A threshold is used to determine whether a new topic is sufficiently different\nfrom another topic. The worst case complexity of this algorithm is\nevery new topic is found to be different from every other new topic in the global set. Increasing this\nthreshold will make it more likely for new topics to merge with the topics already in the global set\n(instead of incrementing the set), causing the expected running time of this merging algorithm to\nbe linear in the number of processors. The pseudocode of this greedy matching scheme is shown in\nAlgorithm 4. This algorithm is run after each iteration of AD-HDP to produce a global set of new\ntopics. We show in the next section that this greedy matching scheme significantly improves the\nrate of convergence for AD-HDP.\nis NP-hard (Burkard and C \u00b8ela,\n, producing an\n\u2013 this is the case where\n10"},{"page":11,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nKOS\n3,000\n6,906\n467,714\nNIPS\n1,500\n12,419\n2,166,058\nWIKIPEDIA\n2,051,929\n120,927\n344,941,756\nPUBMED\n8,200,000\n141,043\n737,869,083\nNEWSGROUPS\ntrain\n19500\n27,059\n2,057,207\ntest\n430184--498\nTable 2: Characteristics of data sets used in experiments.\n4. Experiments\nThe purpose of our experiments is to investigate how our distributed topic model algorithms, AD-\nLDA, HD-LDA and AD-HDP, perform when compared to their sequential counterparts, LDA and\nHDP. We are interested in two aspects of performance: the quality of the model learned, measured\nby log probability of test data; and the time taken to learn the model. Our primary data sets for these\nexperiments were KOS blog entries, from dailykos.com, and NIPS papers, from books.nips.cc.\nWe chose these relatively small data sets to allow us to perform a large number of experiments.\nBoth data sets were split into a training set and a test set. Size parameters for these data sets are\nshown in Table 2. For each corpus,\nis the number of documents,\nandis the total number of words. Two larger data sets WIKIPEDIA, from en.wikipedia.org,\nand PUBMED, from pubmed.gov were used for speedup experiments, described in Section 5. For\nprecision-recall experiments we used the NEWSGROUPS data set, taken from the UCI Machine\nLearning Repository. All the data sets used in this paper can be downloaded from the UCI Machine\nLearning Repository (Asuncion and Newman, 2007).\nUsing the KOSand NIPSdata sets, wecomputed test set perplexities for arange oftopics\nfor numbers of processors,, ranging from 1to 3000. The distributed algorithms wereinitialized by\nfirst randomly assigning topics to words in , then counting topics in documents,\ntopics,, for each processor. Foreach run of LDA,AD-LDA,and HD-LDA,a sample was taken\nat 500 iterations of the Gibbs sampler, which is well after the typical burn-in period of the initial\n200-300 iterations. For each run of HDP and AD-HDP, we allow the Gibbs sampler to run for 3000\niterations, to allow the number of topics to grow. In our perplexity experiments, multiple processors\nwere simulated in software by separating data, running sequentially through each processor, and\nsimulating the global synchronization and update steps. For the speedup experiments, computations\nwere run on 64 to 1024 processors on a 2000+ processor parallel supercomputer.\nThe following set of hyperparameters was used for the experiments, where hyperparameters are\nshown as variables in squares in the graphical models in Figures 1 and 3. For AD-LDA we set\nand . For AD-HDP we set,\nandcould have also been fixed, resampling these hyperparameters allows for more robust topic\ngrowth, as described by Teh et al. (2006). For LDA and AD-LDA we fixed the hyperparameters\nand , but these priors could also be learned using sampling.\nSelection of hyperparameters for HD-LDA was guided by our experience with AD-LDA. For\nAD-LDA,, but for HD-LDA\nmode ofto simulate the inclusion of global counts in\nset, because it is important to scale by the number of topics to prevent oversmoothing\nwhen the counts are spread thinly among many topics. Finally, we choose and\nis the vocabulary size\n, and\n, and words in\nGammaand Gamma. While\n, so we chooseandto make the\nas is done in AD-LDA.We\nto make the mode\n11"},{"page":12,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nof\nset:\n, matching the value of\n,\nTo systematically evaluate our distributed topic model algorithms, AD-LDA, HD-LDA and\nAD-HDP, we measured performance using test set perplexity, which is computed as Perp\ntest\n. For every test document, half the words at random are designated for fold-\nin, and the remaining words are used as test. The document mixture\npart, and log probability of the test words is computed using this mixture, ensuring that the test\nwords are not used in estimation of model parameters. For AD-LDA, the perplexity computation\nexactly follows that of LDA, since a single set of topic counts\ntaken. In contrast, allcopies ofare required to compute perplexity for HD-LDA. Except\nwhere stated, perplexities are computed for all algorithms using\nfrom ten independent chains using\nused in our LDA and AD-LDA experiments. Specifically, we\nand.,\ntest\ntest\nis learned using the fold-in\nare saved when a sample is\nsamples from the posterior\ntesttest\n(9)\nThis perplexity computation follows the standard practice of averaging over multiple chains when\nmaking predictions with LDA models trained via Gibbs sampling, as discussed in Griffiths and\nSteyvers (2004). Averaging over ten samples significantly reduces perplexity compared to using a\nsingle sample from one chain. While we perform averaging over multiple samples to improve the\nestimate of perplexity, we have also observed similar relative results across our algorithms when we\nuse a single sample to compute perplexity.\nAnalogous perplexity calculations are used for HD-LDA and AD-HDP. With HD-LDA we ad-\nditionally compute processor-specific responsibilities, since test documents do not belong to any\nparticular processor, unlike the training documents. Each processor learns a document mixture\nusing the fold-in part for each test document. For each processor, the likelihood is calculated over\nthe words in the fold-in part in a manner analogous to (9), and these likelihoods are normalized to\nform the responsibilities,. To compute perplexity, we compute the likelihood over the test words,\nusing a responsibility-weighted average of probabilities over all processors:\ntesttest\n(10)\nwhere\nComputing perplexity in this manner prevents the possibility of seeing or using test words during\nthe training and fold-in phases.\n4.1 Perplexity\nThe perplexity results for KOS and NIPS in Figure 5 clearly show that the model perplexity is\nessentially the same for the distributed models AD-LDA and AD-HDP at\ntheir single-processor versions at. The figures show the test set perplexity, versus number of\nprocessors,, for different numbers of topics\nmodels which learn the number of topics. The\nHDP (triangles), and we use our distributed algorithms \u2013 AD-LDA (crosses), HD-LDA (squares),\nandas\nfor the LDA-type models, and also for the HDP-\nperplexity is computed by LDA (circles) and\n12"},{"page":13,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nand AD-HDP (stars) \u2013 to compute the\nperplexity as a function of the number of topics is much greater than the variability due to the\nnumber of processors. Note that there is essentially no perplexity difference between AD-LDA and\nHD-LDA.\nandperplexities. The variability in\nP=1P=10P=100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\nNumber of Processors\nPerplexity\nKOS Data Set\nK=8\nK=16\nK=32\nK=64\nHDP\nLDA\nAD\u2212LDA\nHD\u2212LDA\nHDP\nAD\u2212HDP\nP=1P=10P=100\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\nNumber of Processors\nPerplexity\nNIPS Data Set\nK=10\nK=20\nK=40\nK=80\nHDP\nLDA\nAD\u2212LDA\nHD\u2212LDA\nHDP\nAD\u2212HDP\nFigure 5: Test perplexity on KOS (left) and NIPS (right) data versus number of processors P.\ncorresponds to LDA and HDP. At\nand AD-HDP.\nand we show AD-LDA, HD-LDA\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n1350\n1400\n1450\n1500\n1550\n1600\n1650\n1700\n1750\nT=8\nT=16\nT=32\nT=64\nNumber of processors P\nPerplexity\nKOS Data Set\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n1400\n1500\n1600\n1700\n1800\n1900\n2000\nT=10\nT=20\nT=40\nT=80\nNumber of processors P\nPerplexity\nNIPS Data Set\nFigure 6: AD-LDA test perplexity versus number of processors up to the limiting case of number\nof processors equal to number of documents in collection. Left plot shows perplexity for\nKOS and right plot shows perplexity for NIPS.\n13"},{"page":14,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nEven in the limit of a large number of processors, the perplexity for the distributed algorithms\nmatches that for the sequential version. In fact, in the limiting case of just one document per\nprocessor,for KOS andfor NIPS, we see that the perplexities of AD-LDA are\ngenerally no different to those of LDA, as shown in the rightmost point in each curve in Figure 6.\nAD-HDP instantiates fewer topics but produces a similar perplexity to HDP. The average num-\nber of topics instantiated by HDP on KOS was 669 while the average number of topics instantiated\nby AD-HDP was 490 () and 471 (\nAD-HDP instantiated 569 () and 569 (\nbecause of the merging across processors of newly-created topics. The similar perplexity results for\nAD-HDP compared to HDP, despite the fewer topics, is partly due to the relatively small probability\nmass in many of the topics.\nDespite no formal convergence guarantees, the approximate distributed algorithms, AD-LDA\nand AD-HDP, converged to good solutions in every single experiment (of the more than one hun-\ndred) we conducted using multiple real-world data sets. We also tested both our distributed LDA\nalgorithms with adversarial\/non-random distributions of topics across processors using synthesized\ndata. One example of an adversarial distribution of documents is where each document only uses a\nsingle topic, and these documents are distributed such that processor\nabout topic . In this case the distributed topic models have to learn the correct set of\nthough each processor only sees local documents that pertain to just one of the topics. We ran multi-\nple experiments, starting with 1000 documents that were hard-assigned to\ndocument is only about one topic), and distributing the 1000 documents over\nwhere each processor contained documents belonging to the same topic (an analogy is one proces-\nsor only having documents about sports, the next processor only having documents about arts, and\nso on). The perplexity performance of AD-LDA and HD-LDA under these adversarial\/non-random\ndistribution of documents was as good as the performance when the documents were distributed\nrandomly, and as good as the performance of single-processor LDA.\nTo demonstrate that the low perplexities obtained from the distributed algorithms with\nprocessors are not just due to averaging effects, we split the NIPS corpus into one hundred\n15-document collections, and ran LDA separately on each of these hundred collections. The test\nperplexity atcomputed by averaging 100-separate LDA models was 2117, significantly\nhigher than thetest perplexity of 1575 for AD-LDA and HD-LDA. This shows that a\nbaseline approach of simple averaging of results from separate processors performs much worse\nthan the distributed coordinated learning algorithms that we propose in this paper.\n). For NIPS, HDP instantiated 687 topics while\n) topics. AD-HDP instantiates fewer topics\nonly has documents that are\ntopics, even\ntopics (i.e. each\nprocessors,\n4.2 Convergence\nOne could imagine that distributed algorithms, where each processor only sees its own local data,\nmay converge more slowly than single-processor algorithms where the data is global. Consequently,\nwe performed experiments to see whether our distributed algorithms were converging at the same\nrate as their sequential counterparts. If the distributed algorithms were converging slower, the com-\nputational gains of parallelization would be reduced. Our experiments consistently showed that the\nconvergence rate for the distributed LDA algorithms was just as fast as those for the single processor\ncase. As an example, Figure 7 shows test perplexity versus iteration of the Gibbs sampler for the\nNIPS data attopics. During burn-in, up to iteration 200, the distributed algorithms are ac-\ntually converging slightly faster than single processor LDA. Note that one iteration of AD-LDA or\n14"},{"page":15,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nHD-LDA on a parallel multi-processor computer only takes a fraction (at best\ntime of one iteration of LDA on a single processor computer.\n) of the wall-clock\n50 100150 200250 300350 400\n1700\n1800\n1900\n2000\n2100\n2200\nIteration\nPerplexity\n \n \nLDA\nAD\u2212LDA P=10\nAD\u2212LDA P=100\nHD\u2212LDA P=10\nHD\u2212LDA P=100\nFigure 7: Convergence of test perplexity versus iteration for the distributed algorithms AD-LDA\nand HD-LDA using the NIPS data set andtopics.\nWe see slightly different convergence behavior in the non-parametric topic models. AD-HDP\nconverges more slowly than HDP, as shown in Figure 8, due to AD-HDP\u2019s heavy averaging of new\ntopics resulting from merging by topic-id (i.e. no matching). This slower convergence may partially\nbe a result of the lower number of topics instantiated. The number of new topics instantiated in one\npass of AD-HDPis limited to the maximum number of new topics instantiated on any one processor.\nFor example, in the right plot, after 500 iterations, HDP has instantiated 360 topics, whereas AD-\nHDP has instantiated 210 () and 250 (\nthe perplexity of HDP is lower than the perplexity of AD-HDP. After three thousand iterations, AD-\nHDP produces the same perplexity as HDP, which is reassuring because it indicates that AD-HDP\nis ultimately producing a model that has the same predictive ability as HDP. We observe a similar\nresult for the NIPS data set.\nOne way to accelerate the rate of convergence for AD-HDP is to match newly generated topics\nby similarity instead of by topic-id. Figure 9 shows that performing the greedy matching scheme for\nnew topics as described in Algorithm 4 significantly improves the rate of convergence for AD-HDP.\nIn this experiment, we used a threshold of 2 for determining topic similarity. The number of topics\nincreases at a faster rate for AD-HDP with matching, since the greedy matching scheme is more\nflexible in that the number of new topics at each iteration is not limited to the maximum number of\nnew topics instantiated on any one processor. The results show that the greedy matching scheme\nenables AD-HDPto converge almost as quickly as HDP.In practice, only a few new topics\nare generated locally on each processor each iteration, and so the computational overhead of this\nheuristic matching scheme is minimal relative to the time for Gibbs sampling.\n) topics. Correspondingly, at 500 iterations,\n15"},{"page":16,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\n0 50010001500\nIteration\n200025003000\n1200\n1300\n1400\n1500\n1600\n1700\n1800\nPerplexity\nHDP\nAD\u2212HDP P=10\nAD\u2212HDP P=100\n050010001500\nIteration\n200025003000\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nNumber of Topics\nHDP\nAD\u2212HDP P=10\nAD\u2212HDP P=100\nFigure 8: Results forHDPversus AD-HDPwithno matching. Leftplot showstest perplexity versus\niteration for HDP and AD-HDP. Right plot shows number of topics versus iteration for\nHDP and AD-HDP. Results are for the KOS data set.\n050010001500\nIteration\n20002500 3000\n1200\n1300\n1400\n1500\n1600\n1700\n1800\nPerplexity\nHDP\nAD\u2212HDP P=100\nAD\u2212HDP P=100 (with matching)\n0 50010001500\nIteration\n200025003000\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nNumber of Topics\nHDP\nAD\u2212HDP P=100\nAD\u2212HDP P=100 (with matching)\nFigure 9: Results for HDP versus AD-HDP with greedy matching. Left plot shows test perplexity\nversus iteration for HDPand AD-HDP.Right plot shows number of topics versus iteration\nfor HDP and AD-HDP. Results are for the KOS data set.\nTo further check that the distributed algorithms were performing comparably to their single\nprocessor counterparts, we ran experiments to investigate whether the results were sensitive to the\nnumber of topics used in the models, in case the distributed algorithms\u2019 performance worsens when\nthe number of topics becomes very large. Figure 10 shows the test perplexity computed on the\nNIPS data set, as a function of the number of topics, for the LDA algorithms and a fixed number of\nprocessors(the results forthe KOSdata setwerequite similar and therefore not shown). The\nperplexities of the different algorithms closely track each other as number of topics,\nIn fact, in some cases HD-LDA produces slightly lower perplexities than those of single processor\n, increases.\n16"},{"page":17,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nLDA. This lower perplexity may be due to the fact that in HD-LDA test perplexity is computed\nusing P sets of topic parameters, thus it has more parameters than AD-LDA to better fit the data.\n0100200300400 500600 700\n1000\n1100\n1200\n1300\n1400\n1500\n1600\n1700\n1800\n1900\n2000\nNumber of Topics\nPerplexity\nLDA\nAD\u2212LDA P=10\nHD\u2212LDA P=10\nFigure 10: Test perplexity versus number of topics using the NIPS data set (S=5).\n4.3 Precision and Recall\nIn addition to our experiments measuring perplexity, wealso performed precision\/recall calculations\nusing the NEWSGROUPS data set, where each document\u2019s corresponding newsgroup is the class\nlabel. In this experiment we use LDA and AD-LDA to learn topic models on the training data. Once\nthe model is learned, each test document can be treated as a \u201dquery\u201d, where the goal is to retrieve\nrelevant documents from the training set. For each test document, the training documents are ranked\naccording to how probable the test document is under each training document\u2019s mixture\nset of topics. From this ranking, one can calculate mean average precision and area under the\nROC curve.\nand the\n0 1020304050\n0.04\n0.06\n0.08\n0.1\n0.12\n0.14\n0.16\nIteration\nMean Average Precision\nLDA\nAD\u2212LDA P=10\nAD\u2212LDA P=100\n0 10203040 50\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nIteration\nMean Area Under ROC Curve\nLDA\nAD\u2212LDA P=10\nAD\u2212LDA P=100\nFigure 11: Precision\/recall results: (left) Mean average precision for LDA\/AD-LDA. (right) Area\nunder the ROC curve for LDA\/AD-LDA.\n17"},{"page":18,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nFigure 11 shows the mean average precision and the area under the ROC curve achieved by\nLDA and AD-LDA, plotted versus iteration. LDA performs slightly better than AD-LDA for the\nfirst 20 iterations, but AD-LDA catches up and converges to the same mean average precision and\narea under the ROC curve as LDA. This again shows that our distributed\/parallel version of LDA\nproduces a very similar result to the single-processor version.\n5. Scalability\nThe primary motivation for developing distributed algorithms for LDA and HDP is to have highly\nscalable algorithms, in terms of memory and computation time. Memory requirements depend on\nboth memory for data and memory for model parameters. The memory for the data scales with\nthe total number of words in the corpus. The memory for the parameters is linear in the number\nof topics , which is either fixed for the LDA models or learned for the HDP models. The per-\nprocessor per-iteration time and space complexity of LDA and AD-LDA are shown in Table 3.\nAD-LDA\u2019s memory requirement scales well as collection sizes grow, because while corpus size (\nand) can get arbitrarily large, which can be offset by increasing the number of processors,\nthe vocabulary sizewill tend to asymptote, or at least grow more slowly. Similarly the time\ncomplexity scales well since the leading order term\nThe communication cost of the reduce operation, denoted by\ntaken to perform the global sum of the count difference\nstages and can be implemented efficiently in standard language\/protocols such as MPI, the\nMessage Passing Interface. Because of the additional\n, with increasing efficiency as this ratio increases. Space and time complexity of HD-LDA are\nsimilar to that of AD-LDA, but HD-LDA has bigger constants. For a given number of topics,\nwe argue that AD-HDP has similar time complexity as AD-LDA.\nWe performed large-scale speedup experiments with just AD-LDA instead of all three of our\ndistributed topic modeling algorithms because AD-LDA produces very similar results to HD-LDA,\nbut with significantly less computation. We expect that relative speedup performance for HD-LDA\nand AD-HDP should follow that for AD-LDA.\n,\n,\nis divided by.\nin the table, represents the time\n. This is executed in\nterm, parallel efficiency will depend on\n,\nLDA AD-LDA\nSpace\nTime\nTable 3: Space and time complexity of LDA and AD-LDA.\nWe used two multi-million document data sets, WIKIPEDIAand PUBMED,for speedup exper-\niments on a large-scale supercomputer. The supercomputer used was DataStar, a 15.6 TFlop teras-\ncale machine at San Diego Supercomputer Center built from 265 IBM P655 8-way compute nodes.\nWe implemented a parallel version of AD-LDA using the Message Passing Interface protocol. We\nran AD-LDA on WIKIPEDIA usingtopics and PUBMED using\ntributed overand 1024 processors. The speedup results, shown in Figure 12,\nshow relatively high parallel efficiency, with approximately 700 times speedup for WIKIPEDIA\nand 800 times speedup for PUBMED when using\nefficiencies of approximately 0.7 and 0.8 respectively. This speedup is computed relative to the\ntopics dis-\nprocessors, corresponding to parallel\n18"},{"page":19,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\n0200 400\nNumber of processors\n600 8001000\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nSpeedup\nPerfect\nAD\u2212LDA (PUBMED)\nAD\u2212LDA (WIKIPEDIA)\nFigure 12: Parallel speedup results for 64 to 1024 processors on multi-million document datasets\nWIKIPEDIA and PUBMED.\ntime per iteration when using\nit is not possible, due to memory limitations, to run these models on a single processor. Multiple\nruns were timed for both WIKIPEDIA and PUBMED, and the resulting variation in timing was less\nthan 1%, so error bars are not shown in the figure. We see slightly higher parallel efficiency for\nPUBMED versus WIKIPEDIAbecause PUBMED has a larger amount of computation per unit data\ncommunicated,.\nprocessors (i.e. atprocessors speedup=64), since\nThis speedup dramatically reduces the learning time for large topic models. If we were to learn\ntopic model for PUBMED using LDA on a single processor, it would require over 300\ndays instead of the 10 hours required to learn the same model using AD-LDAon 1024 processors. In\nour speedup experiments on these large data sets, we did not directly investigate latency or commu-\nnication bandwidth effects. Nevertheless, one could expect that if the communication time becomes\nvery long compared to the computation time, then it may be worth doing multiple Gibbs sampling\nsweeps on a processor\u2019s local data before performing the synchronization and global update step.\nIn Section 6 we further examine this question of frequency of synchronizations. The relative time\nfor communication versus computation also effects the weak scaling of parallelization, where the\nproblem size increases linearly with the number of processors. We expect that parallel efficiency\nwill be relatively constant for weak scaling since\na\nis constant.\nIn addition to the large-scale speedup experiments run on the 1024-processor parallel super-\ncomputer, we also performed small-scale speedup experiments for AD-HDP on an 8-node parallel\ncluster running MPI. Using the NIPS data set we measured parallel efficiencies of 0.75 and 0.5 for\nand. The latter result on 8 processors means that the HDP model for NIPS can be\nlearned four times faster than on a single processor.\n19"},{"page":20,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\n6. Analysis of Approximate Distributed LDA\nFinally, we investigate the dynamics of AD-LDA learning using toy data to get further insight\ninto how AD-LDA is working. While we have shown experimental results showing that AD-LDA\nproduces models with similar perplexity and similar convergence rates to LDA, it is not obvious\nwhy this algorithm works so well in practice. Our toy example has\ntopics. We generated document collections according to the LDA generative process given by (1).\nWe chose a low dimension vocabulary, , so that we could plot the evolution of the Gibbs sampler\non a two-dimensional word-topic simplex. We first generated data, then learned models using LDA\nand AD-LDA.\nThe left plot of Figure 13 shows thedistance between the model\u2019s estimate of a particular\ntopic-word distribution and the true distribution, as a function of Gibbs iteration, for both single-\nprocessor LDA and AD-LDA with . LDA and AD-LDA have qualitatively the same three-\nphase learning dynamics. The first four or so iterations (labeled initialize) correspond to somewhat\nrandom movement close to the randomly initialized starting point. In the next phase (labeled burn-\nin) both algorithms rapidly move in parameter space toward the posterior mode. And finally after\nburn-in (labeled stationary) both are sampling around the mode. In the right plot we show the sim-\nilarity between AD-LDA and LDA samples taken from the equilibrium distribution \u2013 here plotted\non the two-dimensional planar simplex corresponding to the three-word topic distribution.\nwords and\n0204060 80100\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nIteration\nL1 norm\ninitialize\nburn\u2212in\nstationary\nLDA\nAD\u2212LDA proc1\nAD\u2212LDA proc2\n0.8 0.810.820.830.840.85\n0.445\n0.45\n0.455\n0.46\n0.465\n0.47\n0.475\n \n \ntopic mode\nLDA\nAD\u2212LDA\nFigure 13: (Left)distance to the mode for LDA and for\nsamples of(projected onto the topic simplex) taken from the equilibrium distribution,\nshowing the similarity between LDA and\nthis figure.\nAD-LDA. (Right) Closeup of 50\nAD-LDA. Note the zoomed scale in\nThe left plot of Figure 14 depicts the same trajectory shown in Figure 13 left, projected onto the\ntopic simplex. This plot shows the paths in parameter space of each model, and the same three-phase\nlearning dynamics: taking a few small steps near the starting point, moving up to the true solution,\nand then sampling near the posterior mode for the rest of the iterations. For each Gibbs iteration,\nthe parameters corresponding to each of the two individual processors, and those parameters after\nmerging, are shown for AD-LDA. One can see the alternating pattern of two separate (but close)\n20"},{"page":21,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nparameter estimates on each processor, followed by a merged estimate. We observed that after the\ninitial few iterations, the individual processor steps and the merge step each resulted ina movecloser\nto the mode. One might worry that the AD-LDA algorithm would get trapped close to the initial\nstarting point, e.g., due to repeated label switching or oscillatory behavior of topic labeling across\nprocessors. In practice we have consistently observed that the algorithm quickly discards such\nconfigurations due to the stochastic nature of the moves and latches onto a consistent and stable\nlabeling that rapidly moves it toward the posterior mode. The figure clearly illustrates that LDA and\nAD-LDA have qualitatively similar learning dynamics. The right plot in Figure 14 illustrates the\nsame qualitative behavior as in the left plot, but now for\nInterestingly, across a wide range of experiments, we observed that the variance in the AD-\nLDA word-topic distribution samples is typically only about 70% of the variance in LDA topic\nsamples. Since the samplers are not the same it makes sense that the posterior variance differs\n(i.e. is underestimated) by the parallel sampler. We expect less variance because AD-LDA ignores\nfluctuations in the bulk of . Nonetheless, all of our experiments indicate that the posterior mode\nand means found by the parallel sampler are essentially the same as those found by the sequential\nsampler.\nprocessors.\n0.650.70.750.80.85 0.9\n0.36\n0.38\n0.4\n0.42\n0.44\n0.46\n0.48\n0.5\nstart\ntopic mode\nLDA\nAD\u2212LDA proc1\nAD\u2212LDA proc2\n0.650.7 0.750.80.85 0.9\n0.36\n0.38\n0.4\n0.42\n0.44\n0.46\n0.48\n0.5\nstart\ntopic mode\nLDA\nAD\u2212LDA proc1\nAD\u2212LDA proc2\nAD\u2212LDA proc3\n...etc...\nAD\u2212LDA proc10\nFigure 14: (Left) Projection of topics onto simplex, showing convergence to mode for\n(Right) Same as left plot, but with\n.\n.\nAnother insight can be gained by thinking of LDA as an approximation to stochastic descent in\nthe space of assignment variables . On a single processor, one can view Gibbs sampling during\nburn-in as a stochastic algorithm to move up the likelihood surface. With multiple processors, each\nprocessor computes an upward direction in its own subspace, keeping all other directions fixed.\nThe global update step then recombines these directions by vector-addition, in the same way as one\nwould compute a gradient using finite differences. This is expected to be accurate as long as the\nsurface is locally convex or concave, but will break down at saddle-points. We conjecture AD-LDA\nworks reliably because saddle points are unstable and rare because the posterior is usually highly\npeaked for LDA models and high-dimensional count data sets.\nWhile we see similar perplexities for AD-LDA compared to LDA, we could further ask if the\nAD-LDA algorithm is producing any bias in its estimates of the model parameters. To test this, we\n21"},{"page":22,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n0\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n0.09\nNumber of processors, P\n||E(\u03c6)\u2212\u03c6ref||\nreference = LDA\nreference = true\nFigure 15: Averageerror in word-topic distribution versus P for AD-LDA.\nperformed a series of experiments where we generated synthetic data sets according to the LDA\ngenerative process, with known word-topic distributions\nmodels fromeach ofthesimulated data sets. Wecomputed theexpected value ofthe AD-LDAtopics\nand compared this to two reference values,\nthe other based on multiple LDA samples,\nref\ncloser to the LDA topics\nvariation in learning LDA models from finite data sets is much greater than the variation between\nLDA and AD-LDA on the same data sets.\n. We then learned LDA and AD-LDA\nrefone based on the true distribution,\nLDA. Figure 15 shows that AD-LDA is much\nLDA than either are to the true topics\nref\n,\n, telling us that the sampling\nWhen Does AD-LDA Fail? In all of our experiments thus far, we have seen that our distributed al-\ngorithms learn models with equivalent predictive power as their non-distributed counterparts. How-\never, when global synchronizations are done less frequently (i.e., when the synchronization step is\nperformed after multiple Gibbs sampling sweeps through local data), the distributed algorithms may\nconverge to suboptimal solutions.\nWhen the synchronization interval is increased dramatically, it is possible for AD-LDA to con-\nverge to a suboptimal solution. This can happen because the topics (with the same integer label) on\neach processor can drift far apart, so that topicon one processor diverges from topic\nprocessor. In Figure 16, we show the results of an experiment on KOS where synchronizations only\noccur once every 100 iterations. For processors, AD-LDA performs significantly worse than\nLDA. Theprocessor case is the worst case for AD-LDA, since one half of the total words on\neach processor have the freedom to drift. In contrast, when\nonly locally modify 1\/100of the topic assignments, and so the topics on each processor can not\ndrift far from the global set of topic counts at the previous iteration. Bipartite matching significantly\non another\nprocessors, each processor can\n22"},{"page":23,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nimproves the perplexity in the\nhas indeed caused the topics to drift apart. Fortunately, topic drifting becomes less of a problem as\nmore processors are used, and can be eliminated by frequent synchronization. It is also important\nto note that AD-LDA, where processors synchronize after every iteration, gives essentially\nidentical results as LDA. Our recommendation in practice is to perform the synchronization and\ncount updates after each iteration of the Gibbs sampler. As shown earlier in the paper, this leads\nto performance that is essentially indistinguishable from LDA. Since most multi-processor comput-\ning hardware will tend to have communication bandwidth matched to processor speed (i.e. faster\nand\/or more processors usually come with a faster communication network), synchronizing after\neach iteration of the Gibbs sampler will usually be the optimal strategy.\nprocessor case, suggesting that the lack of communication\n010002000 300040005000\n1700\n1800\n1900\n2000\n2100\n2200\n2300\n2400\n2500\nIteration\nPerplexity\nLDA\nAD\u2212LDA P=2\nAD\u2212LDA P=2, Sync=100\nAD\u2212LDA P=2, Sync=100 (with matching)\nAD\u2212LDA P=100, Sync=100\nFigure 16: Test perplexity versus iteration where synchronizations between processors only occur\nevery 100 iterations, KOS,.\n7. Related Work\nApproximate inference for topic models such as LDA and HDP can be carried out using a variety\nof methods, the most common being variational methods and Markov chain Monte Carlo methods.\nPrevious efforts to parallelize these algorithms have focused on variational methods, which are often\nstraightforward to cast in a distributed framework. For example, Blei et al. (2002) and Nallapati\net al. (2007) describe distributed variational EM methods for LDA. In their distributed variational\napproach, the computationally expensive E-step is easily parallelized because the document-specific\nvariational parameters are independent. Wolfe et al. (2008) investigate the parallelization of both\nthe E and M-steps of variational EM for LDA, under a variety of computer network topologies. In\nthese cases the distributed version of LDA produces identical results to the sequential version of\nthe algorithm. However, memory for variational inference in LDA scales as\nnumber of distinct document-word pairs in the corpus. For typical English-language corpora, the\ntotal number of words in the corpus is less than twice the number of distinct document-word pairs\n, whereis the\n23"},{"page":24,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\n(\nnumber of documents,\nmemory requirement of\n), socan be considered on the order of\n, this memory requirement of\nfor MCMC methods.\n. Sinceis usually much larger than the\nis not nearly as scalable as that the\nParallelized versions of various machine learning algorithms have also been developed. Forman\nand Zhang (2000) describe a parallel k-means algorithm, and W. Kowalczyk and N. Vlassis (2005)\ndescribe an asynchronous parallel EM algorithm for Gaussian mixture learning. A parallel EM\nalgorithm for Probabilistic Latent Semantic Analysis, implemented using Google\u2019s MapReduce\nframework, was described in Das et al. (2007). A review of how to parallelize an array of standard\nmachine learning algorithms using MapReduce was presented by Chu et al. (2007). Rossini et al.\n(2007) presents a framework for statisticians that allows for the parallel computing of independent\ntasks within the R language.\nWhile many of these EM algorithms are readily parallelizable, Gibbs sampling of dependent\nvariables (such as topic assignments) is fundamentally sequential and therefore difficult to paral-\nlelize. One way to parallelize Gibbs sampling is to run multiple independent chains in parallel to\nobtain multiple samples; however, this multiple-chain approach does not address the fact that the\nburn-in within each chain may take a long time. Furthermore, for some applications, one is not in-\nterested in multiple samples from independent chains. For example, if we wish to learn topics for a\nvery large document collection, one is usually satisfied with mean values of word-topic distributions\ntaken from a single chain.\nOne can parallelize a single MCMC chain by decomposing the variables into independent non-\ninteracting blocks that can be sampled concurrently (Kontoghiorghes, 2005). However, when the\nvariables are not independent, sampling variables in parallel is not possible. Brockwell (2006)\npresents ageneral parallel MCMCalgorithm basedon pre-fetching, but itisnotpractical forlearning\ntopic models because it discards most of its computations which makes it relatively inefficient. It\nis possible to construct partially parallel Gibbs samplers, in which the samples are independently\naccepted with some probability. In the limit as this probability goes to zero, this sampler will\napproach the sequential Gibbs sampler, as explained in P.Ferrari et al.(1993). However, this method\nis also not practical when learning topic models because it is computationally inefficient. Younes\n(1998) shows the existence of exact parallel samplers that make use of periodic synchronous random\nfields. However there is no known method for constructing such a sampler.\nOur HD-LDA model is similar to the DCM-LDA model presented by Mimno and McCallum\n(2007). There the authors perform topic modeling on a collection of books by learning a different\ntopic model for each book and then clustering these learned topics together to find global topics. In\nthis model, the concept of a book is directly analogous to our concept of a processor. DCM-LDA\nuses Stochastic EM along with agglomerative clustering to learn topics, while our HD-LDA follows\na fully Bayesian approach for inference. HD-LDA also differs from other topic hierarchies found\nin the literature. The Hierarchical Dirichlet Process model of Teh et al. (2006) places a deeper\nhierarchical prior on the topic mixture, instead of on the word-topic distributions. The Pachinko\nAllocation Model presented by Li and McCallum (2006) deals with a document-specific hierarchy\nof topic-assignments. These types of hierarchies do not directly facilitate proper parallel Gibbs\nsampling as is done in HD-LDA.\n24"},{"page":25,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\n8. Conclusions\nWe have proposed three different algorithms for distributing across multiple processors Gibbs sam-\npling for LDA and HDP. With our approximate distributed algorithm, AD-LDA, we sample from an\napproximation to the posterior distribution by allowing different processors to concurrently sample\ntopic assignments on their local subsets of the data. Despite having no formal convergence guar-\nantees, AD-LDA works very well empirically and is easy to implement. With our hierarchical dis-\ntributed model, HD-LDA, we adapt the underlying LDA model to map to the distributed processor\narchitecture. This model is more complicated than AD-LDA, but it inherits the usual convergence\nproperties of Markov chain Monte Carlo. We discovered that careful selection of hyperparameters\nwas critical to making HD-LDA work well, but this selection was clearly informed by AD-LDA.\nOur distributed algorithm AD-HDP followed the same approach as AD-LDA, but with an additional\nstep to merge newly instantiated topics.\nOur proposed distributed algorithms learn LDA models with predictive performance that is no\ndifferent than single-processor LDA. On each processor they burn-in and converge at the same rate\nas LDA, yielding significant speedups in practice. For HDP, our distributed algorithm eventually\nproduced the same perplexity as the single-processor version of HDP. Prior to reaching the con-\nverged perplexity result, AD-HDP had higher perplexity than HDP since the merging of new topics\nby label slows the rate of topic growth. We also discovered that matching new topics by similarity\nsignificantly improves AD-HDP\u2019s rate of convergence.\nThe space and time complexity of these distributed algorithms make them scalable to run very\nlarge data sets, for example, collections with billions to trillions of words. Using two multi-million\ndocument datasets, and running computations on a 1024-processor parallel supercomputer, we\nshowed how one can achieve a 700-800 times reduction in wall-clock time by using our distributed\napproach.\nThere are several potentially interesting research directions that can be pursued using the algo-\nrithms proposed here as a starting point. One research direction is to use more complex schemes that\nallow data to adaptively move from one processor to another. The distributed schemes presented in\nthis paper can also be used to parallelize topic models that are based on or derived from LDA and\nHDP, and beyond that a potentially larger class of graphical models.\nAcknowledgments\nThis material is based upon work supported by the National Science Foundation. DN and PS were\nsupported byNSFgrants SCI-0225642, CNS-0551510, and IIS-0083489. DNwasalso supported by\na Google Research Award. PS was also supported by ONR grant N00014-18-1-101 and a different\nGoogle Research Award. AA was supported by an NSF graduate fellowship. MW was supported\nby NSF grants IIS-0535278 and IIS-0447903, and ONR grant 00014-06-1-073. Computations were\nperformed at SDSC under an MRAC Allocation.\n25"},{"page":26,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nAppendix A.\nThe auxiliary variable method explained in Escobar and West (1995) and Teh et al. (2006) is used\nto sample, , and. To derive Gibbs sampling equations, we use the following expansions:\n(11)\n(S is Stirling number of first kind) (12)\nThe first expansion follows from the definition of the Beta function, and the second expansion\nmakes use of the Stirling number of the first kind to rewrite the factorial (see (Abramowitz and\nStegun, 1964)).\nNow we derive the sampling equation for. Combining the collapsed distribution (7) with the\nprior on(6) gives the posterior distribution for\n1:\n(13)\nUsing the expansions (11,12) we introduce the auxiliary variables\nand :\n(14)\nThe joint distribution above allows us to create sampling equations for, , and :\nGamma(15)\nBeta (16)\nAntoniak(17)\n1. To avoid notational clutter, we denote conditioned-upon variables and parameters by a dash. These variables can be\ninferred from context.\n26"},{"page":27,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nThe Antoniak distribution is the distribution of the number of occupied tables if\nare sent into a restaurant that follows the Chinese restaurant process with strength parameter\nSampling from the Antoniak distribution is done by sampling\ncustomers\n.\nBernoulli variables:\nBernoulli(18)\n(19)\nUsing the same auxiliary variable techniques, we derive sampling equations for\nvariables are sampled jointly because they are dependent. The posterior distribution for\nand the joint distribution with the auxiliary variables\nand. These\nand\nandare given by:\n(20)\n(21)\nNote that the set of variables ( and ) is unrelated to the set of auxiliary variables introduced\n. The sampling equations for ,, , andare:for\nGamma (22)\nDirichlet(23)\nBeta(24)\nAntoniak (25)\n27"},{"page":28,"text":"NEWMAN, ASUNCION, SMYTH, WELLING\nReferences\nM. Abramowitz and I. Stegun. Handbook of Mathematical Functions with Formulas, Graphs, and\nMathematical Tables. Dover, New York, 1964.\nA. Asuncion and D. Newman.\nhttp:\/\/www.ics.uci.edu\/ mlearn\/MLRepository.html.\nUCI machine learning repository,2007.URL\nD. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. In Advances in Neural Information\nProcessing Systems, volume 14, pages 601\u2013608, Cambridge, MA, 2002. MIT Press.\nD. Blei, A. Ng, and M. Jordan. Latent Dirichlet allocation. Journal of Machine Learning Research,\n3:993\u20131022, 2003.\nA. Brockwell. Parallel Markov chain Monte Carlo simulation by pre-fetching. Journal of Compu-\ntational & Graphical Statistics, 15, No. 1:246\u2013261, 2006.\nR. Burkard and E. C \u00b8ela. Linear assignment problems and extensions. In P. Pardalos and D. Du, ed-\nitors, Handbook of Combinatorial Optimization, Supplement Volume A. Kluwer Academic Pub-\nlishers, 1999.\nG. Casella and C. Robert. Rao-Blackwellisation of sampling schemes. Biometrika, 83(1):81\u201394,\n1996.\nC. Chemudugunta, P. Smyth, and M. Steyvers. Modeling general and specific aspects of documents\nwith a probabilistic topic model. In Advances in Neural Information Processing Systems 19,\npages 241\u2013248. MIT Press, Cambridge, MA, 2007.\nC. Chu, S. Kim, Y. Lin, Y. Yu, G. Bradski, A. Ng, and K. Olukotun. Map-Reduce for machine\nlearning on multicore. In Advances in Neural Information Processing Systems 19, pages 281\u2013\n288. MIT Press, Cambridge, MA, 2007.\nA. Das, M. Datar, A. Garg, and S. Rajaram. Google news personalization: Scalable online collabo-\nrative filtering. In WWW \u201907: Proceedings of the 16th International Conference on World Wide\nWeb, pages 271\u2013280, New York, NY, 2007. ACM.\nM. Escobar and M. West. Bayesian density estimation and inference using mixtures. Journal of the\nAmerican Statistical Association, 90(430):577\u2013588, 1995.\nG. Forman and B. Zhang. Distributed data clustering can be efficient and exact. In ACM KDD\nExplorations, volume 2, pages 34\u201338, New York, NY, 2000. ACM.\nT. Griffiths and M. Steyvers. Finding scientific topics. In Proceedings of the National Academy of\nSciences, volume 101, pages 5228\u20135235, 2004.\nE. Kontoghiorghes.\nMonographs). Chapman & Hall \/ CRC, 2005.\nHandbook of Parallel Computing and Statistics (Statistics, Textbooks and\nW.Liand A.McCallum. Pachinko allocation: DAG-structured mixture models of topic correlations.\nInProceedings oftheInternational Conference onMachine Learning, volume 23, pages 577\u2013584,\nNew York, NY, 2006. ACM.\n28"},{"page":29,"text":"DISTRIBUTED ALGORITHMS FOR TOPIC MODELS\nJ. Liu, W. Wong, and A. Kong. Covariance structure of the Gibbs sampler with applications to the\ncomparisons of estimators and augmentation schemes. Biometrika, 81(1):27\u201340, 1994.\nD. Mimno and A. McCallum. Organizing the OCA: Learning faceted subjects from a library of\ndigital books. In JCDL \u201907: Proceedings of the 2007 conference on digital libraries, pages\n376\u2013385, New York, NY, 2007. ACM.\nR. Nallapati, W. Cohen, and J. Lafferty. Parallelized variational EM for latent Dirichlet allocation:\nAn experimental evaluation of speed and scalability. In ICDMW \u201907: Proceedings of the Seventh\nIEEE International Conference on Data Mining Workshops, pages 349\u2013354, Washington, DC,\n2007. IEEE Computer Society.\nP. Ferrari, A. Frigessi, and R. Schonmann. Convergence of some partially parallel Gibbs samplers\nwith annealing. In Annals of Applied Probability, volume 3, pages 137\u2013153. 1993.\nM. Rosen-Zvi, T. Griffiths, M. Steyvers, and P. Smyth. The author-topic model for authors and\ndocuments. InProceedings oftheConference onUncertainty inArtificial Intelligence, volume 20,\npages 487\u2013494, Arlington, VA, 2004. AUAI Press.\nA. Rossini, L. Tierney, and N. Li. Simple parallel statistical computing in R. Journal of Computa-\ntional & Graphical Statistics, 16(2):399, 2007.\nY. W. Teh, M. Jordan, M. Beal, and D. Blei. Hierarchical Dirichlet processes. Journal of the\nAmerican Statistical Association, 101(476):1566\u20131581, 2006.\nW. Kowalczyk and N. Vlassis. Newscast EM. In Advances in Neural Information Processing\nSystems 17, pages 713\u2013720. MIT Press, Cambridge, MA, 2005.\nJ. Wolfe, A. Haghighi, and D. Klein. Fully distributed EM for very large datasets. In Proceedings\nof the International Conference on Machine Learning, pages 1184\u20131191. ACM, New York, NY,\n2008.\nL. Younes. Synchronous random fields and image restoration. IEEE Transactions on Pattern Anal-\nysis and Machine Intelligence, 20(4):380\u2013390, 1998.\n29"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/David_Newman6\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000.pdf","widgetId":"rgw31_56ab1b42d0e37"},"id":"rgw31_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=220320734&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw32_56ab1b42d0e37"},"id":"rgw32_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=220320734&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":220320734,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":220320734,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2077564610,"url":"researcher\/2077564610_Dat_Quoc_Nguyen","fullname":"Dat Quoc Nguyen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82182307,"url":"researcher\/82182307_Kairit_Sirts","fullname":"Kairit Sirts","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2077644564,"url":"researcher\/2077644564_Mark_Johnson","fullname":"Mark Johnson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Conference Paper","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling","usePlainButton":true,"publicationUid":283478023,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling","title":"Improving Topic Coherence with Latent Feature Word Representations in MAP Estimation for Topic Modeling","displayTitleAsLink":true,"authors":[{"id":2077564610,"url":"researcher\/2077564610_Dat_Quoc_Nguyen","fullname":"Dat Quoc Nguyen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82182307,"url":"researcher\/82182307_Kairit_Sirts","fullname":"Kairit Sirts","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2077644564,"url":"researcher\/2077644564_Mark_Johnson","fullname":"Mark Johnson","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proceedings of the 13th Annual Workshop of the Australasian Language Technology Association; 12\/2015"],"abstract":"Probabilistic topic models are widely used to discover latent topics in document collections, while latent feature word vectors have been used to obtain high performance in many natural language processing (NLP) tasks. In this paper, we present a new approach by incorporating word vectors to directly optimize the maximum a posteriori (MAP) estimation in a topic model. Preliminary results show that the word vectors induced from the experimental corpus can be used to improve the assignments of topics to words.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Dat_Quoc_Nguyen\/publication\/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling\/links\/563a161f08ae337ef298384e.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Dat_Quoc_Nguyen","sourceName":"Dat Quoc Nguyen","hasSourceUrl":true},"publicationUid":283478023,"publicationUrl":"publication\/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling\/links\/563a161f08ae337ef298384e\/smallpreview.png","linkId":"563a161f08ae337ef298384e","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283478023&reference=563a161f08ae337ef298384e&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1b42d0e37"},"id":"rgw36_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283478023&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"563a161f08ae337ef298384e","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220320734,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283478023_Improving_Topic_Coherence_with_Latent_Feature_Word_Representations_in_MAP_Estimation_for_Topic_Modeling\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["For variational inference LDA, we use Blei's implementation . 5 For Gibbs sampling LDA, we use the jLDADMM package 6 (Nguyen, 2015) with common hyper-parameters \u03b2 = 0.01 and \u03b1 = 0.1 (Newman et al., 2009; Hu et al., 2011; Xie and Xing, 2013). We ran Gibbs sampling LDA for 2000 iterations and evaluated the topics assigned to words in the last sample. "],"widgetId":"rgw37_56ab1b42d0e37"},"id":"rgw37_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab1b42d0e37"},"id":"rgw35_56ab1b42d0e37","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283478023&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2083969722,"url":"researcher\/2083969722_Bo_Zhao","fullname":"Bo Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083940763,"url":"researcher\/2083940763_Hucheng_Zhou","fullname":"Hucheng Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083964488,"url":"researcher\/2083964488_Guoqiang_Li","fullname":"Guoqiang Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083966503,"url":"researcher\/2083966503_Yihua_Huang","fullname":"Yihua Huang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform","usePlainButton":true,"publicationUid":283471250,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform","title":"ZenLDA: An Efficient and Scalable Topic Model Training System on Distributed Data-Parallel Platform","displayTitleAsLink":true,"authors":[{"id":2083969722,"url":"researcher\/2083969722_Bo_Zhao","fullname":"Bo Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083940763,"url":"researcher\/2083940763_Hucheng_Zhou","fullname":"Hucheng Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083964488,"url":"researcher\/2083964488_Guoqiang_Li","fullname":"Guoqiang Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083966503,"url":"researcher\/2083966503_Yihua_Huang","fullname":"Yihua Huang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"This paper presents our recent efforts, zenLDA, an efficient and scalable\nCollapsed Gibbs Sampling system for Latent Dirichlet Allocation training, which\nis thought to be challenging that both data parallelism and model parallelism\nare required because of the Big sampling data with up to billions of documents\nand Big model size with up to trillions of parameters. zenLDA combines both\nalgorithm level improvements and system level optimizations. It first presents\na novel CGS algorithm that balances the time complexity, model accuracy and\nparallelization flexibility. The input corpus in zenLDA is represented as a\ndirected graph and model parameters are annotated as the corresponding vertex\nattributes. The distributed training is parallelized by partitioning the graph\nthat in each iteration it first applies CGS step for all partitions in\nparallel, followed by synchronizing the computed model each other. In this way,\nboth data parallelism and model parallelism are achieved by converting them to\ngraph parallelism. We revisited the tradeoff between system efficiency and\nmodel accuracy and presented approximations such as unsynchronized model,\nsparse model initialization and \"converged\" token exclusion. zenLDA is built on\nGraphX in Spark that provides distributed data abstraction (RDD) and expressive\nAPIs to simplify the programming efforts and simultaneously hides the system\ncomplexities. This enables us to implement other CGS algorithm with a few lines\nof code change. To better fit in distributed data-parallel framework and\nachieve comparable performance with contemporary systems, we also presented\nseveral system level optimizations to push the performance limit. zenLDA was\nevaluated it against web-scale corpus, and the result indicates that zenLDA can\nachieve about much better performance than other CGS algorithm we implemented,\nand simultaneously achieve better model accuracy.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1511.00440","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":283471250,"publicationUrl":"publication\/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform\/links\/5656509308ae4988a7b38a22\/smallpreview.png","linkId":"5656509308ae4988a7b38a22","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283471250&reference=5656509308ae4988a7b38a22&eventCode=&origin=publication_list","widgetId":"rgw39_56ab1b42d0e37"},"id":"rgw39_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283471250&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220320734,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283471250_ZenLDA_An_Efficient_and_Scalable_Topic_Model_Training_System_on_Distributed_Data-Parallel_Platform\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["However, an efficient and scalable solution should combine the innovations from both algorithm side and system side. Contemporary topic modelling systems either focus on algorithm side [31] [32] [15] [20] that has different sampling methods, or focus on system side [27] [19] [21]. These separation efforts makes it hard to port new algorithms on old systems. "],"widgetId":"rgw40_56ab1b42d0e37"},"id":"rgw40_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw38_56ab1b42d0e37"},"id":"rgw38_56ab1b42d0e37","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283471250&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2079808098,"url":"researcher\/2079808098_Jingwei_Zhang","fullname":"Jingwei Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079817210,"url":"researcher\/2079817210_Aaron_Gerow","fullname":"Aaron Gerow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2037263079,"url":"researcher\/2037263079_Jaan_Altosaar","fullname":"Jaan Altosaar","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309343709270018%401450764695673_m"}],"authorsPartTwo":{"id":2079820121,"url":"researcher\/2079820121_James_Evans","fullname":"James Evans","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections","usePlainButton":true,"publicationUid":281144924,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections","title":"Fast, Flexible Models for Discovering Topic Correlation across Weakly-Related Collections","displayTitleAsLink":true,"authors":[{"id":2079808098,"url":"researcher\/2079808098_Jingwei_Zhang","fullname":"Jingwei Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079817210,"url":"researcher\/2079817210_Aaron_Gerow","fullname":"Aaron Gerow","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2037263079,"url":"researcher\/2037263079_Jaan_Altosaar","fullname":"Jaan Altosaar","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A309343709270018%401450764695673_m"},{"id":2079820121,"url":"researcher\/2079820121_James_Evans","fullname":"James Evans","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2079797438,"url":"researcher\/2079797438_Richard_Jean_So","fullname":"Richard Jean So","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Weak topic correlation across document collections with different numbers of\ntopics in individual collections presents challenges for existing\ncross-collection topic models. This paper introduces two probabilistic topic\nmodels, Correlated LDA (C-LDA) and Correlated HDP (C-HDP). These address\nproblems that can arise when analyzing large, asymmetric, and potentially\nweakly-related collections. Topic correlations in weakly-related collections\ntypically lie in the tail of the topic distribution, where they would be\noverlooked by models unable to fit large numbers of topics. To efficiently\nmodel this long tail for large-scale analysis, our models implement a parallel\nsampling algorithm based on the Metropolis-Hastings and alias methods (Yuan et\nal., 2015). The models are first evaluated on synthetic data, generated to\nsimulate various collection-level asymmetries. We then present a case study of\nmodeling over 300k documents in collections of sciences and humanities research\nfrom JSTOR.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Jaan_Altosaar\/publication\/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections\/links\/5678e9a708aebcdda0ec48ec.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Jaan_Altosaar","sourceName":"Jaan Altosaar","hasSourceUrl":true},"publicationUid":281144924,"publicationUrl":"publication\/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections\/links\/5678e9a708aebcdda0ec48ec\/smallpreview.png","linkId":"5678e9a708aebcdda0ec48ec","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281144924&reference=5678e9a708aebcdda0ec48ec&eventCode=&origin=publication_list","widgetId":"rgw42_56ab1b42d0e37"},"id":"rgw42_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281144924&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5678e9a708aebcdda0ec48ec","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":220320734,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281144924_Fast_Flexible_Models_for_Discovering_Topic_Correlation_across_Weakly-Related_Collections\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["than O(1), and 2) while the document alias table samples z and u simultaneously, after sampling z from the word alias table u must be sampled using t lc \/n lz (Chen et al., 2011). Parallelizing C-HDP requires an additional empirical method of merging new topics between threads (Newman et al., 2009), which is outside of the scope of this work. Our implementation of both models, C-LDA and C-HDP, are open-sourced online 1 . "],"widgetId":"rgw43_56ab1b42d0e37"},"id":"rgw43_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw41_56ab1b42d0e37"},"id":"rgw41_56ab1b42d0e37","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281144924&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":220320734,"publicationLink":"publication\/220320734_Distributed_Algorithms_for_Topic_Models","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw34_56ab1b42d0e37"},"id":"rgw34_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=220320734&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=91","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":91,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw33_56ab1b42d0e37"},"id":"rgw33_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=220320734&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0912f5107f7dc2714e000000","name":"David Newman","date":null,"nameLink":"profile\/David_Newman6","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Newman6\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Newman6\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"108299da562b7ad069f4a1556a2430b9","showFileSizeNote":false,"fileSize":"1.84 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0912f5107f7dc2714e000000","name":"David Newman","date":null,"nameLink":"profile\/David_Newman6","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/David_Newman6\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/David_Newman6\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"108299da562b7ad069f4a1556a2430b9","showFileSizeNote":false,"fileSize":"1.84 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=6svcl5av174k6NliQzIrn22ur-wwYFuOzK09-hCQfWQcfNe7iGXdy7WrhFDCo5ByyrADODyjGfDtW7hTKrGrMA.vupFHz2wxk586QzyLJDt-SKyjAiz4vsxEsH56-o9mcCITNSOhEQ_wpSFf8guM4t8xxr0QchheQ6S0U8OtKFcfA","clickOnPill":"publication.PublicationFigures.html?_sg=NC2-1ovt5MCkpjejhJdIvjAdu9z9Al3ZXWJDES-WuX2gPhpgibRUd665rDKrXdWF4ekBsFZtyFqJtcO_KUWQ1w.rZNX7TPbQYi8ewUypCIdU9h1zgq0gy_qZFAEeZcbMrII1Uq7g2HsX4ipUWdU9ZJrigOO3uyvI1kjGhp9I7V54A"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Newman6%2Fpublication%2F220320734_Distributed_Algorithms_for_Topic_Models%2Flinks%2F0912f5107f7dc2714e000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=gEcahVxOqMS3-Oz40wMecDfsiW0BR-Rc4t38dFqZhmNG5oDyXTV8ziq1M11jmhZHnpVp7wrn2Mdbi5EGMxHF9w","urlHash":"1ec754371926b894d6c5f8db5db26612","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=4D5eb35yKl80eZjl-63YCfuOjMAZcYf5ApkVs7IJRSkxEw06hPzwSHh5MhSL4eY0bd3r44JuPrzhBs8mJeOm3x7gkeFfyd_YLhTOUn6iwA0.oV6YmN1QfVfzLq4vdbVCfJbJPYB_XgMSGOyP9_zKI1bTTs6YDqzYTRvkQ5plY2hNoThdmufrum2OoYNi_ZrRxg.25PCXrKEAEIFjRilWJ4McRYOFph7TOfYAedzdSdoGbXWakHUyPNZ6TMMkB2nGppBaew-NQ92xNRv-EHM540RtA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0912f5107f7dc2714e000000","trackedDownloads":{"0912f5107f7dc2714e000000":{"v":false,"d":false}},"assetId":"AS:97271358164994@1400202707199","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":220320734,"commentCursorPromo":null,"widgetId":"rgw45_56ab1b42d0e37"},"id":"rgw45_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FDavid_Newman6%2Fpublication%2F220320734_Distributed_Algorithms_for_Topic_Models%2Flinks%2F0912f5107f7dc2714e000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A97271358164994%401400202707199&publicationUid=220320734&linkId=0912f5107f7dc2714e000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Distributed Algorithms for Topic Models","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=XOUmk--gdYP0JLyPTsJq1RwwWgeIS3PDGVRa6zJ_T39KgD4C-JOncUJsxl2eEAOTn7_c3IC90TdoLhFyIwsc10yU9tLxgFFlMB9t5s63NFo.oNDxykk9iQEhnlrFNswRtXNpwMh4I8XW1rCcxfZAYgZ_r1O5wjTq1GPuMwB3s1iEkADYYXThNEs-fZiYKwIrbg.YHusqh7Fg4rV4hRR7O-TXahP6DSHR_6HsdwkupSGx0S1-WgY1CB2kfgygsjLop7n_wyKqCQzzt-db-mAFWCkxw","publicationUid":220320734,"trackedDownloads":{"0912f5107f7dc2714e000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab1b42d0e37"},"id":"rgw47_56ab1b42d0e37","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1b42d0e37"},"id":"rgw48_56ab1b42d0e37","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw49_56ab1b42d0e37"},"id":"rgw49_56ab1b42d0e37","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw50_56ab1b42d0e37"},"id":"rgw50_56ab1b42d0e37","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw51_56ab1b42d0e37"},"id":"rgw51_56ab1b42d0e37","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw46_56ab1b42d0e37"},"id":"rgw46_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw44_56ab1b42d0e37"},"id":"rgw44_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/220320734_Distributed_Algorithms_for_Topic_Models","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1b42d0e37"},"id":"rgw2_56ab1b42d0e37","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":220320734},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=220320734&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1b42d0e37"},"id":"rgw1_56ab1b42d0e37","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"k6d8jsrnyms1RmvxQNzZmoQ6jRKYL3n+UCiB+DRSSQVIy3JrQNWzoWnS1ZiTEkVms30g2RWEVHIxYzV9AVmKKyadXnqvuh7TIYLZi5kNpiJnzYVMhCg+4Oj2rBuTOgMuoymrEHPEwU2gczstS91+TjCpDWfZ4JU9gP5V9m\/kOKI9Lpj3yedEr3yub8JotRyNKGme0P4NHisvKBlGCjSZM0+yQDHtPU9YDJi0AQBVRPr8vv\/s\/lzAwcqYVI6gWMpUroP\/ksWd+o9qlu2xnLDphYqcH4dizXg7s4yaDUNnF1w=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Distributed Algorithms for Topic Models\" \/>\n<meta property=\"og:description\" content=\"We describedistributedalgorithmsfortwowidely-usedtopicmodels,namelytheLatentDirich- let Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In ourdistributed algorithmsthe...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\" \/>\n<meta property=\"rg:id\" content=\"PB:220320734\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1145\/1577069.1755845\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Distributed Algorithms for Topic Models\" \/>\n<meta name=\"citation_author\" content=\"David Newman\" \/>\n<meta name=\"citation_author\" content=\"Arthur U. Asuncion\" \/>\n<meta name=\"citation_author\" content=\"Padhraic Smyth\" \/>\n<meta name=\"citation_author\" content=\"Max Welling\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/08\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Journal of Machine Learning Research\" \/>\n<meta name=\"citation_issn\" content=\"1532-4435\" \/>\n<meta name=\"citation_volume\" content=\"10\" \/>\n<meta name=\"citation_firstpage\" content=\"1801\" \/>\n<meta name=\"citation_lastpage\" content=\"1828\" \/>\n<meta name=\"citation_doi\" content=\"10.1145\/1577069.1755845\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/David_Newman6\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\/links\/0912f5107f7dc2714e000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/220320734_Distributed_Algorithms_for_Topic_Models\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-44206215-fa9a-4e6d-9ca3-468f07ca00d9","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":717,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw52_56ab1b42d0e37"},"id":"rgw52_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-44206215-fa9a-4e6d-9ca3-468f07ca00d9", "89b59aab6ccba995510320413e9a0f658a25f1e2");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-44206215-fa9a-4e6d-9ca3-468f07ca00d9", "89b59aab6ccba995510320413e9a0f658a25f1e2");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw53_56ab1b42d0e37"},"id":"rgw53_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/220320734_Distributed_Algorithms_for_Topic_Models","requestToken":"J37fLLdLgzu8uFLwTB2q4CtpA1DNfhZAa3wR8Hy\/yPqPgSVEYiMCShwvlWE\/QdfhB\/FPFPztqvuv5u6FzyzrA2ivjA\/GRnl5NVLqHyvugdL5HIdkURDpdkokV1txd4AVSyOgZT3behqsDEzYoW+Oiwv7hB8PGcHUbq1D3y+C2ignlXpKVil92FvKnQ9Hd19neWUL8ZOVGXTMMB6iz6S79XS6ugReIMx3qTHpLxPCjXVtT\/A7JduhNZbveGnAS50ThOc\/f5FIXCeAAaHBO7ge9TPwoqtSKKbwsMmB93epaf0=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=abTIiIlehDiPBiJPmqQ-qAMqAl-TtW9kStZKMclcBP7L7wYWOU_NOl_poYCa9pvF","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIwMzIwNzM0X0Rpc3RyaWJ1dGVkX0FsZ29yaXRobXNfZm9yX1RvcGljX01vZGVscw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw55_56ab1b42d0e37"},"id":"rgw55_56ab1b42d0e37","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw54_56ab1b42d0e37"},"id":"rgw54_56ab1b42d0e37","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw56_56ab1b42d0e37"},"id":"rgw56_56ab1b42d0e37","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
