<!DOCTYPE html> <html lang="en" class="" id="rgw51_56ab9d884fd5f"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="ewG4IWI2TAQHXH6KP24FA06ABSBreOU8ze5BpMmpW1mD0X7QSipcStRkfTSWEU8667cuHJTNwRzZwP02M75g3J3Jcw3v3k1RLaLtkXr4h6IacS8EiP6X7YXvifVMxEmpEKGOBEooIDQVOgnsINJLBkpLUBknkvzmCvw3vFUbGuroc4C4S6bwPzCERdRbBfZRk8D4G0URO+kxZIQje4/MhbeIH8lFja9UEGFxcnOSYWIagEvnzfR8VxFscE4hHoCYVX4ivrPOtydtytWRwHMYEm7rpcMe86FXkMeK/SKZj9E="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-788a2b08-9e7f-408c-8136-01b12f7e9921",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Social Signal Processing: Survey of an Emerging Domain" />
<meta property="og:description" content="The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain/links/541a1ae10cf2218008bfa710/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain" />
<meta property="rg:id" content="PB:222430190" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1016/j.imavis.2008.11.007" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Social Signal Processing: Survey of an Emerging Domain" />
<meta name="citation_author" content="Alessandro Vinciarelli" />
<meta name="citation_author" content="Maja Pantic" />
<meta name="citation_author" content="HervÃ© Bourlard" />
<meta name="citation_publication_date" content="2009/11/01" />
<meta name="citation_journal_title" content="Image and Vision Computing" />
<meta name="citation_issn" content="0262-8856" />
<meta name="citation_volume" content="27" />
<meta name="citation_issue" content="12" />
<meta name="citation_firstpage" content="1743" />
<meta name="citation_lastpage" content="1759" />
<meta name="citation_doi" content="10.1016/j.imavis.2008.11.007" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Alessandro_Vinciarelli2/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain/links/541a1ae10cf2218008bfa710.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Social Signal Processing: Survey of an Emerging Domain (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Social Signal Processing: Survey of an Emerging Domain on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9d884fd5f" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9d884fd5f" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9d884fd5f">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1016%2Fj.imavis.2008.11.007&rft.atitle=Social%20Signal%20Processing%3A%20Survey%20of%20an%20Emerging%20Domain&rft.title=Image%20and%20Vision%20Computing&rft.jtitle=Image%20and%20Vision%20Computing&rft.volume=27&rft.issue=12&rft.date=2009&rft.pages=1743-1759&rft.issn=0262-8856&rft.au=Alessandro%20Vinciarelli%2CMaja%20Pantic%2CHerv%C3%A9%20Bourlard&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Social Signal Processing: Survey of an Emerging Domain</h1> <meta itemprop="headline" content="Social Signal Processing: Survey of an Emerging Domain">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain/links/541a1ae10cf2218008bfa710/smallpreview.png">  <div id="rgw8_56ab9d884fd5f" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab9d884fd5f" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Alessandro_Vinciarelli2" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272449690402823%401441968476852_m/Alessandro_Vinciarelli2.png" title="Alessandro Vinciarelli" alt="Alessandro Vinciarelli" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Alessandro Vinciarelli</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab9d884fd5f" data-account-key="Alessandro_Vinciarelli2">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Alessandro_Vinciarelli2"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272449690402823%401441968476852_l/Alessandro_Vinciarelli2.png" title="Alessandro Vinciarelli" alt="Alessandro Vinciarelli" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Alessandro_Vinciarelli2" class="display-name">Alessandro Vinciarelli</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Glasgow" title="University of Glasgow">University of Glasgow</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab9d884fd5f"> <a href="researcher/47824715_Maja_Pantic" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Maja Pantic" alt="Maja Pantic" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Maja Pantic</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab9d884fd5f">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/47824715_Maja_Pantic"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Maja Pantic" alt="Maja Pantic" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/47824715_Maja_Pantic" class="display-name">Maja Pantic</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab9d884fd5f" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Herve_Bourlard" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272700920823814%401442028374120_m" title="Herve Bourlard" alt="Herve Bourlard" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Herve Bourlard</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw14_56ab9d884fd5f" data-account-key="Herve_Bourlard">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Herve_Bourlard"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272700920823814%401442028374120_l" title="Herve Bourlard" alt="Herve Bourlard" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Herve_Bourlard" class="display-name">Herve Bourlard</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Idiap_Research_Institute" title="Idiap Research Institute">Idiap Research Institute</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> University of Twente, Drienerlolaan 5, 7522 NB Enschede, The Netherlands </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0262-8856_Image_and_Vision_Computing"><span itemprop="name">Image and Vision Computing</span></a> </span>    (Impact Factor: 1.59).     <meta itemprop="datePublished" content="2009-11">  11/2009;  27(12):1743-1759.    DOI:&nbsp;10.1016/j.imavis.2008.11.007           <div class="pub-source"> Source: <a href="http://eprints.eemcs.utwente.nl/17123/" rel="nofollow">OAI</a> </div>  </div> <div id="rgw15_56ab9d884fd5f" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence &ndash; the ability to recognize human social signals and social behaviours like turn taking, politeness, and disagreement &ndash; in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for social signal processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially aware computing.</div> </p>  </div>  </div>   </div>     <div id="rgw16_56ab9d884fd5f" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/222430190_fig1_Fig-8-People-detection-Examples-of-people-detection-in-public-spaces-pictures-from" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Fig. 8. People detection. Examples of people detection in public spaces..." data-key="222430190_fig1_Fig-8-People-detection-Examples-of-people-detection-in-public-spaces-pictures-from"> <img class="fig" src="https://www.researchgate.net/profile/Alessandro_Vinciarelli2/publication/222430190/figure/fig1/Fig-8-People-detection-Examples-of-people-detection-in-public-spaces-pictures-from_small.png" alt="Fig. 8. People detection. Examples of people detection in public spaces..." title="Fig. 8. People detection. Examples of people detection in public spaces..."/> </a> </li>  <li> <a href="/figure/222430190_fig2_Fig-9-AU-detection-Outline-of-a-geometric-feature-based-system-for-detection-of-facial" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Fig. 9. AU detection. Outline of a geometric-feature-based system for..." data-key="222430190_fig2_Fig-9-AU-detection-Outline-of-a-geometric-feature-based-system-for-detection-of-facial"> <img class="fig" src="https://www.researchgate.net/profile/Alessandro_Vinciarelli2/publication/222430190/figure/fig2/Fig-9-AU-detection-Outline-of-a-geometric-feature-based-system-for-detection-of-facial_small.png" alt="Fig. 9. AU detection. Outline of a geometric-feature-based system for..." title="Fig. 9. AU detection. Outline of a geometric-feature-based system for..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw17_56ab9d884fd5f" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw31_56ab9d884fd5f">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw43_56ab9d884fd5f">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Alessandro_Vinciarelli2/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain/links/541a1ae10cf2218008bfa710.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Alessandro_Vinciarelli2">Alessandro Vinciarelli</a>, <span class="js-publication-date"> Sep 17, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw45_56ab9d884fd5f" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw46_56ab9d884fd5f" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw47_56ab9d884fd5f" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw48_56ab9d884fd5f" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw49_56ab9d884fd5f" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw50_56ab9d884fd5f" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw44_56ab9d884fd5f" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAlessandro_Vinciarelli2%2Fpublication%2F222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain%2Flinks%2F541a1ae10cf2218008bfa710.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw30_56ab9d884fd5f"  itemprop="articleBody">  <p>Page 1</p> <p>Social signal processing: Survey of an emerging domain<br />Alessandro Vinciarellia,b,*, Maja Panticc,d, HervÃ© Bourlarda,b<br />aIDIAP Research Institute, Computer Vision, CP592, 1920 Martigny, Switzerland<br />bEcole Polytechnique FÃ©dÃ©rale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland<br />cImperial College, 180 Queens Gate, London SW7 2AZ, UK<br />dUniversity of Twente, Drienerlolaan 5, 7522 NB Enschede, The Netherlands<br />a r t i c l e i n f o<br />Article history:<br />Received 16 May 2008<br />Accepted 26 September 2008<br />Keywords:<br />Social signals<br />Computer vision<br />Speech processing<br />Human behaviour analysis<br />Social interactions<br />a b s t r a c t<br />The ability to understand and manage social signals of a person we are communicating with is the core of<br />social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indis-<br />pensable and perhaps the most important for success in life. This paper argues that next-generation com-<br />puting needs to include the essence of social intelligence â the ability to recognize human social signals<br />and social behaviours like turn taking, politeness, and disagreement â in order to become more effective<br />and more efficient. Although each one of us understands the importance of social signals in everyday life<br />situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks,<br />smiles, crossed arms, laughter, and similar, design and development of automated systems for social sig-<br />nal processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a<br />computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommen-<br />dations for enabling the development of the next generation of socially aware computing.<br />? 2008 Elsevier B.V. All rights reserved.<br />1. Introduction<br />The exploration of how human beings react to the world and<br />interact with it and each other remains one of the greatest scien-<br />tific challenges. Perceiving, learning, and adapting to the world<br />are commonly labeled as intelligent behaviour. But what does it<br />mean being intelligent? Is IQ a good measure of human intelli-<br />gence and the best predictor of somebodyâs success in life? There<br />is now a growing research in cognitive sciences, which argues that<br />our common view of intelligence is too narrow, ignoring a crucial<br />range of abilities that matter immensely for how people do in life.<br />This range of abilities is called social intelligence[6,8,19,182] and in-<br />cludes the ability to express and recognize social signals and social<br />behaviours like turn taking, agreement, politeness, and empathy,<br />coupled with the ability to manage them in order to get along well<br />with others while winning their cooperation. Social signals and so-<br />cial behaviours are the expression of ones attitude towards social<br />situation and interplay, and they are manifested through a multi-<br />plicity of non-verbal behavioural cues including facial expressions,<br />body postures and gestures, and vocal outbursts like laughter (see<br />Fig. 1). Social signals typically last for a short time (milliseconds,<br />like turn taking, to minutes, like mirroring), compared to social<br />behaviours that last longer (seconds, like agreement, to minutes,<br />like politeness, to hours or days, like empathy) and are expressed<br />as temporal patterns of non-verbal behavioural cues. The skills of<br />social intelligence have been argued to be indispensable and per-<br />haps the most important for success in life [66].<br />When it comes to computers, however, they are socially igno-<br />rant [143]. Current computing devices do not account for the fact<br />that humanâhuman communication is always socially situated<br />and that discussions are not just facts but part of a larger social<br />interplay. However, not all computers will need social intelligence<br />and none will need all of the related skills humans have. The cur-<br />rent-state-of-the-art categorical computing works well and will al-<br />ways work well for context-independent tasks like making plane<br />reservations and buying and selling stocks. However, this kind of<br />computing is utterly inappropriate for virtual reality applications<br />as well as for interacting with each of the (possibly hundreds) com-<br />puter systems diffused throughout future smart environments<br />(predicted as the future of computing by several visionaries such<br />as Mark Weiser) and aimed at improving the quality of life by<br />anticipating the users needs. Computer systems and devices capa-<br />ble of sensing agreement, inattention, or dispute, and capable of<br />adapting and responding to these social signals in a polite, unintru-<br />sive, or persuasive manner, are likely to be perceived as more nat-<br />ural, efficacious, and trustworthy. For example, in education,<br />pupilsâ social signals inform the teacher of the need to adjust the<br />instructional message. Successful human teachers acknowledge<br />this and work with it; digital conversational embodied agents must<br />0262-8856/$ - see front matter ? 2008 Elsevier B.V. All rights reserved.<br />doi:10.1016/j.imavis.2008.11.007<br />* Corresponding author. Address: IDIAP Research Institute, Computer Vision,<br />CP592, 1920 Martigny, Switzerland. Tel.: +41 27 7217724.<br />E-mail addresses: vincia@idiap.ch (A. Vinciarelli), m.pantic@imperial.ac.uk (M.<br />Pantic), bourlard@idiap.ch (H. Bourlard).<br />Image and Vision Computing 27 (2009) 1743â1759<br />Contents lists available at ScienceDirect<br />Image and Vision Computing<br />journal homepage: www.elsevier.com/locate/imavis</p>  <p>Page 2</p> <p>begin to do the same by employing tools that can accurately sense<br />and interpret social signals and social context of the pupil, learn<br />successful context-dependent social behaviour, and use a proper<br />socially adept presentation language (see, e.g., [141]) to drive the<br />animation of the agent. The research area of machine analysis<br />and employment of human social signals to build more natural,<br />flexible computing technology goes by the general name of socially<br />aware computing as introduced by Pentland [142,143].<br />Although the importance of social signals in everyday life sit-<br />uations is evident, and in spite of recent advances in machine<br />analysis and synthesis of relevant behavioural cues like gaze ex-<br />change, blinks, smiles, head nods, crossed arms, laughter, and<br />similar [137,138], the research efforts in machine analysis and<br />synthesis of human social signals like attention, empathy, polite-<br />ness, flirting, (dis)agreement, etc., are still tentative and pioneer-<br />ing efforts. The importance of studying social interactions and<br />developing automated assessing of human social behaviour from<br />audiovisual recordings is undisputable. It will result in valuable<br />multimodal tools that could revolutionise basic research in cog-<br />nitive and social sciences by raising the quality and shortening<br />the time to conduct research that is now lengthy, laborious,<br />and often imprecise. At the same time, and as outlined above,<br />such tools form a large step ahead in realizing naturalistic, so-<br />cially aware computing and interfaces, built for humans, based<br />on models of human behaviour.<br />Social signal processing (SSP) [143,145,202,203] is the new re-<br />search and technological domain that aims at providing computers<br />with the ability to sense and understand human social signals. De-<br />spitebeinginitsinitialphase,SSPhasalreadyattractedtheattention<br />of the technological community: the MITTechnologyReviewmaga-<br />zine identifies reality mining (one of the main applications of SSP so<br />far, see Section 4, for more details), as 1 of the 10 technologies likely<br />to change the world [69], while management experts expect SSP to<br />change organization studies like the microscope has changed medi-<br />cine few centuries ago [19].<br />Tothebestofourknowledge,thisisthefirstattempttosurveythe<br />pastworkdoneonSSP.Theinnovativeandmultidisciplinarycharac-<br />ter of the research on SSP is the main reason for this state of affairs.<br />For example, in contrast to the research on human affective behav-<br />iouranalysisthatwitnessedtremendousprogressinthepastdecade<br />(for exhaustive surveys in the field see, e.g., [76,140,221]), the re-<br />search on machine analysis of human social behaviour just started<br />to attract the interest of the research community in computer sci-<br />ence. This and the fragmentation of the research over several scien-<br />tific communities including those in psychology, computer vision,<br />speech and signal processing, make the exercise of surveying the<br />current efforts in machine analysis of human social behaviour<br />difficult.<br />The paper begins by examining the context in which the re-<br />search on SSP has arisen and by providing a taxonomy of the target<br />problem domain (Section 2). The paper surveys then the past work<br />done in tackling the problems of machine detection and interpre-<br />tation of social signals and social behaviours in real-world scenar-<br />ios (Section 3). Existing research efforts to apply social signal<br />processing to automatic recognition of socially relevant informa-<br />tion such as someoneâs role, dominance, influence, etc., are sur-<br />veyed next (Section 4). Finally, the paper discusses a number of<br />challenges facing researchers in the field (Section 5). In the authorsâ<br />opinion, these need to be addressed before the research in the field<br />can enter its next phase â deployment of research findings in real-<br />world applications.<br />2. Behavioural cues and social signals: a taxonomy<br />Thereismorethanwordsinsocialinteractions[9],whetherthese<br />take place between humans or between humans and computers<br />[30]. This is well known to social psychologists that have studied<br />non-verbal communication for several decades [96,158]. It is what<br />people experience when they watch a television program in a lan-<br />guage they do not understand and still capture a number of impor-<br />tant social cues such as differences in status between individuals,<br />overall atmosphere of interactions (e.g., tense vs. relaxed), rapport<br />between people (mutual trust vs. mutual distrust), etc.<br />Non-verbal behaviour is a continuous source of signals which<br />convey information about feelings, mental state, personality, and<br />other traits of people [158]. During social interactions, non-verbal<br />behaviourconveysthisinformationnotonlyforeachoftheinvolved<br />individuals, but it also determines the nature and quality of the so-<br />cial relationships they have with others. This happens through a<br />wide spectrum of non-verbal behavioural cues [7,8] that are per-<br />ceived and displayed mostly unconsciously while producing social<br />awareness,i.e.,aspontaneousunderstandingofsocialsituationsthat<br />does not require attention or reasoning [98].<br />The term behavioural cue is typically used to describe a set of<br />temporal changes in neuromuscular and physiological activity that<br />last for short intervals of time (milliseconds to minutes) in contrast<br />to behaviours (e.g., social behaviours like politeness or empathy)<br />that last on average longer (minutes to hours). As summarized in<br />[47] among the types of messages (communicative intentions)<br />conveyed by behavioural cues are the following:<br />? affective/attitudinal/cognitive states (e.g., fear, joy, stress, dis-<br />agreement, ambivalence, and inattention),<br />? emblems (culture-specific interactive signals like wink or<br />thumbs up),<br />Fig. 1. Behavioural cues and social signals. Multiple behavioural cues (vocal behaviour, posture, mutual gaze, interpersonal distance, etc.) combine to produce a social signal<br />(in this case aggressivity or disagreement) that is evident even if the picture shows only the silhouettes of the individuals involved in the interaction.<br />1744<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 3</p> <p>? manipulators (actions used to act on objects in the environment<br />or self-manipulative actions such as lip biting and scratching),<br />? illustrators (actions accompanying speech such as finger point-<br />ing and raised eyebrows), and<br />? regulators (conversational mediators such as the exchange of a<br />look, palm pointing, head nods, and smiles).<br />In most cases, behavioural cues accompany verbal communica-<br />tion and, even if they are invisible, i.e., they are sensed and inter-<br />preted outside conscious awareness, they have a major impact<br />on the perception of verbal messages and social situations [96].<br />Early investigations of verbal and non-verbal components in inter-<br />action (in particular [113] as cited in [96]) have suggested that the<br />verbal messages account for just 7% of the overall social percep-<br />tion. This conclusion has been later argued because the actual<br />weight of the different messages (i.e., verbal vs. non-verbal) de-<br />pends on the context and on the specific kind of interaction [45].<br />However, more recent studies still confirm that the non-verbal<br />behaviour plays a major role in shaping the perception of social sit-<br />uations: e.g., judges assessing the rapport between two people are<br />more accurate when they use only the facial expressions than<br />when they use only the verbal messages exchanged [8]. Overall,<br />the non-verbal social signals seem to be the predominant source<br />of information used in understanding social interactions [9].<br />The rest of this section provides a taxonomy of the SSP problem<br />domain by listing and explaining the most important behavioural<br />cues and their functions in social behaviour. Behavioural cues that<br />we included in this list are those that the research in psychology<br />has recognized as being the most important in human judgments<br />of social behaviour. Table 1 provides a synopsis of those behav-<br />ioural cues, the social signals they are related to, and the technol-<br />ogies that can be used to sense and analyze them. For more<br />exhaustive explanations of non-verbal behaviours and the related<br />behavioural cues, readers are referred to [7,47,96,158].<br />2.1. Physical appearance<br />The physical appearance includes natural characteristics such<br />as height, body shape, physiognomy, skin and hair color, as well<br />as artificial characteristics such as clothes, ornaments, make up,<br />and other manufacts used to modify/accentuate the facial/body<br />aspects.<br />The main social signal associated to physical appearance is the<br />attractiveness. Attractiveness produces a positive halo effect (a phe-<br />nomenon also known as ââwhat is beautiful is goodâ [41]). Attractive<br />people are often judged as having high status and good personality<br />even if no objective basis for such judgments exists [70,208].<br />Attractive people also have higher probability of starting new so-<br />cial relationships with people they do not know [158]. Other phys-<br />ical characteristics are not necessarily related to the attractiveness,<br />but still have a major influence on social perceptions. The most<br />important are height and somatotype (see below). Tall individuals<br />tend to be attributed higher social status and, in some cases, they<br />actually hold a higher status. For example, a survey has shown that<br />the average height of the American CEOs of the Fortune 500 com-<br />panies is around 7.5 cm higher than the average height of the<br />American population. Moreover, 30% of the same CEOs are taller<br />than 190 cm, while only 4% of the rest of the American population<br />lies in the same range of height [63].<br />Different somatotypes (see Fig. 2), tend to elicit the attribution of<br />certain personality traits [25]. For example, endomorphic individu-<br />als (round, fat, and soft) tend to be perceived as more talkative and<br />sympathetic, but also more dependent on others. Mesomorphic<br />individuals (bony, muscular, and athletic) tend to be perceived as<br />more self-reliant, more mature in behaviour and stronger, while<br />ectomorphic individuals (tall, thin, and fragile) tend to be perceived<br />as more tense, more nervous, more pessimistic and inclined to be<br />difficult. These judgments are typically influenced by stereotypes<br />that do not necessarily correspond to the reality, but still influence<br />significantly the social perceptions [96].<br />2.2. Gestures and posture<br />Following the work of Darwin [37], which was the first to de-<br />scribe body expressions associated with emotions in animals and<br />humans, there have been a number of studies on human body pos-<br />tures and gestures communicating emotions. For example, the<br />works in [27,198] investigated perception and display of body pos-<br />tures relevant to basic emotions including happiness, sadness, sur-<br />prise, fear, disgust, and anger, while the studies in<br />investigated bodily expressions of felt and recognized basic emo-<br />tions as visible in specific changes in arm movement, gait parame-<br />ters, and kinematics. Overall, these studies have shown that both<br />posture and body/limb motions change with emotion expressed.<br />[72,152]<br />Table 1<br />The table shows the behavioural cues associated to some of the most important social behaviours as well as the technologies involved in their automatic detection.<br />Social cues Example social behavioursTech.<br />Emotion PersonalityStatusDominance Persuasion RegulationRapport Speech analysisComputer vision Biometry<br />Physical appearance<br />Height<br />Attractiveness<br />Body shape<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />pp<br />Gesture and posture<br />Hand gestures<br />Posture<br />Walking<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />Face and eyes behaviour<br />Facial expressions<br />Gaze behaviour<br />Focus of attention<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />Vocal behaviour<br />Prosody<br />Turn taking<br />Vocal outbursts<br />Silence<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />pp<br />p<br />p<br />p<br />p<br />p<br />p<br />p<br />pp<br />pp<br />p<br />Space and environment<br />Distance<br />Seating arrangement<br />pppp<br />p<br />p<br />p<br />p<br />pp<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1745</p>  <p>Page 4</p> <p>Basic research also provides evidence that gestures like head incli-<br />nation, face touching, and shifting posture often accompany social<br />affective states like shame and embarrassment [26,50]. However,<br />as indicated by researchers in the field (e.g., in [112]), as much as<br />90% of body gestures are associated with speech, representing typ-<br />ical social signals such as illustrators, emblems, and regulators.<br />In other words, gestures are used in most cases to regulate<br />interactions (e.g., to yield the turn in a conversation), to communi-<br />cate a specific meaning (e.g., the thumbs up gesture to show appre-<br />ciation), to punctuate a discourse (e.g., to underline an utterance by<br />rising the index finger), to greet (e.g., by waving hands to say good-<br />bye), etc. [123]. However, in some cases gestures are performed<br />unconsciously and they are interesting from an SSP point of view<br />because they account for honest information [146], i.e., they leak<br />cues related to the actual attitude of a person with respect to a so-<br />cial context. In particular, adaptors express boredom, stress and<br />negative feelings towards others. Adaptors are usually displayed<br />unconsciously and include self-manipulations (e.g., scratching,<br />nose and ear touching, hair twisting), manipulation of small ob-<br />jects (e.g., playing with pens and papers), and self-protection ges-<br />tures (e.g., folding arms or rhythmicly moving legs) [96].<br />Postures are also typically assumed unconsciously and, argu-<br />ably, they are the most reliable cues about the actual attitude of<br />people towards social situations [158]. One of the main classifica-<br />tions of postural behaviours proposes three main criteria to assess<br />the social meaning of postures [166]. The first criterion distin-<br />guishes between inclusive and non-inclusive postures and accounts<br />for how much a given posture takes into account the presence of<br />others. For example, facing in the opposite direction with respect<br />to others is a clear sign of non-inclusion. The second criterion is<br />face-to-face vs. parallel body orientation and concerns mainly people<br />involved in conversations. Face-to-face interactions are in general<br />more active and engaging (the frontal position addresses the need<br />of continuous mutual monitoring), while people sitting parallel to<br />each other tend to be either buddies or less mutually interested.<br />The third criterion is congruence vs. incongruence: symmetric pos-<br />tures tend to account for a deep psychological involvement (see<br />left picture in Fig. 3), while non-symmetric ones correspond to<br />the opposite situation. The postural congruence is an example of<br />a general phenomenon called chameleon effect or mirroring [22],<br />that consists of the mutual imitation of people as a mean to display<br />affiliation and liking. Postural behaviour includes also walking and<br />movements that convey social information such as status, domi-<br />nance, and affective state [109].<br />2.3. Face and eye behaviour<br />The human face is involved in an impressive variety of different<br />activities. It houses the majority of our sensory apparatus: eyes,<br />ears, mouth, and nose, allowing the bearer to see, hear, taste, and<br />smell. Apart from these biological functions, the human face pro-<br />vides a number of signals essential for interpersonal communica-<br />tion in our social life. The face houses the speech production<br />apparatus and is used to identify other members of the species,<br />to regulate the conversation by gazing or nodding, and to interpret<br />what has been said by lip reading. It is our direct and naturally pre-<br />eminent means of communicating and understanding somebodyâs<br />affective state and intentions on the basis of the shown facial<br />expression [89]. Personality, attractiveness, age and gender can<br />be also seen from someoneâs face [8]. Thus the face is a multisignal<br />sender/receiver capable of tremendous flexibility and specificity. It<br />is therefore not surprising that the experiments (see beginning of<br />Section 2) about the relative weight of the different non-verbal<br />components in shaping social perceptions always show that facial<br />behaviour plays a major role [8,68,113].<br />Two major approaches to facial behaviour measurement in psy-<br />chological research are message and sign judgment [23]. The aim<br />of message judgment is to infer what underlies a displayed facial<br />expression, such as affect or personality, while the aim of sign<br />judgment is to describe the surface of the shown behaviour, such<br />as facial movement or facial component shape. Thus, a brow fur-<br />row can be judged as anger in a message judgment and as a facial<br />movement that lowers and pulls the eyebrows closer together in a<br />sign-judgment approach. While message judgment is all about<br />interpretation, sign-judgment attempts to be objective, leaving<br />inference about the conveyed message to higher order decision<br />making.<br />As indicated in [23], most commonly used facial-expression<br />descriptors in message-judgment approaches are the six basic<br />emotions (fear, sadness, happiness, anger, disgust, and surprise;<br />see Fig. 4), proposed by Ekman and discrete emotion theorists,<br />who suggest that these emotions are universally displayed and rec-<br />ognized from facial expressions [89]. In sign-judgment approaches<br />[24], a widely used method for manual labeling of facial actions is<br />the facial action coding system (FACS) [48].<br />FACS associates facial-expression changes with actions of the<br />muscles that produce them. It defines nine different action units<br />(AUs) in the upper face, 18 in the lower face, 11 for head position,<br />9 for eye position, and 14 additional descriptors for miscellaneous<br />actions. AUs are considered to be the smallest visually discernable<br />facial movements. Using FACS, human coders can manually code<br />nearly any anatomically possible facial expression, decomposing<br />it into the specific AUs that produced the expression. As AUs are<br />independent of interpretation, they can be used for any higher or-<br />der decision making process including recognition of basic emo-<br />tions (EMFACS; see [48]), cognitive states like interest and<br />puzzlement [32], psychological states like suicidal depression<br />[50] or pain [212], social behaviours like accord and rapport<br />[8,32], personality traits like extraversion and temperament [50],<br />and social signals like status, trustworthiness, emblems (i.e., cul-<br />ture-specific interactive signals like wink), regulators (i.e., conver-<br />sational mediators like nod and gaze exchange), and illustrators<br />(i.e., cues accompanying speech like raised eyebrows) [8,46,47].<br />FACS provides an objective and comprehensive language for<br />describing facial expressions and relating them back to what is<br />known about their meaning from the behavioural science litera-<br />ture. Because it is comprehensive, FACS also allows for the discov-<br />ery of new patterns related to emotional or situational states. For<br />example, what are the facial behaviours associated with social sig-<br />nals such as empathy, persuasion, and politeness? An example<br />where subjective judgments of expression failed to find relation-<br />ships which were later found with FACS is the failure of naive sub-<br />jects to differentiate deception and intoxication from facial display,<br />whereas reliable differences were shown with FACS [165].<br />Research based upon FACS has also shown that facial actions can<br />Fig. 2. Somatotypes. The figure shows the three body shapes that tend to elicit the<br />perception of specific personality traits.<br />1746<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 5</p> <p>show differences between those telling the truth and lying at a<br />much higher accuracy level than naive subjects making subjective<br />judgments of the same faces [56]. Exhaustive overview of studies<br />on facial and gaze behaviour using FACS can be found in [50].<br />2.4. Vocal behaviour<br />The vocal non-verbal behaviour includes all spoken cues that<br />surround the verbal message and influence its actual meaning.<br />The effect of vocal non-verbal behaviour is particularly evident<br />when the tone of a message is ironic. In this case, the face value<br />of the words is changed into its opposite by just using the appro-<br />priate vocal intonation. The vocal non-verbal behaviour includes<br />five major components: voice quality, linguistic and non-linguistic<br />vocalizations, silences, andturn taking patterns. Each one of them re-<br />lates to social signals that contribute to different aspects of the so-<br />cial perception of a message.<br />The voice quality corresponds to the prosodic features, i.e., pitch,<br />tempo, and energy (see Section 3.3.4, for more details) and, in per-<br />ceptual terms, accounts for how something is said [31]. The pros-<br />ody conveys a wide spectrum of socially relevant cues: emotions<br />like anger or fear are often accompanied by energy bursts in voice<br />(shouts) [168], the pitch influences the perception of dominance<br />and extroversion (in general it is a personality marker [167]), the<br />speaking fluency (typically corresponding to high rhythm and lack<br />of hesitations) increases the perception of competence and results<br />into higher persuasiveness [167]. The vocalizations include also ef-<br />fects that aim at giving particular value to certain utterances or<br />parts of the discourse, e.g., the pitch accents (sudden increases of<br />the pitch to underline a word) [79], or changes in rhythm and en-<br />ergy aiming at structuring the discourse [80].<br />The linguistic vocalizations (also known as segregates) include all<br />the non-words that are typically used as if they were actual words,<br />e.g., ââehmâ,ââah-ahâ, ââuhmâ, etc. Segregates have two main func-<br />tions, the first is to replace words that for some reason cannot be<br />found, e.g., when people do not know how to answer a question<br />and simply utter a prolonged ââehmâ. They are often referred to as<br />disfluencies and often account for a situation of embarrassment or<br />difficulty with respect to a social interaction [64]. The second<br />important function is the so-called back-channeling, i.e., the use<br />of segregates to accompany someone else speaking. In this sense<br />they can express attention, agreement, wonder, as well as the at-<br />tempt of grabbing the floor or contradicting [176].<br />The non-linguistic vocalizations, also known as vocal outbursts,<br />include non-verbal sounds like laughing, sobbing, crying, whisper-<br />ing, groaning, and similar, that may or may not accompany words,<br />and provide some information about the attitude towards social<br />situations. For instance, laughter tends to reward desirable social<br />behaviour [90] and shows affiliation efforts, while crying is often<br />involved in mirroring (also known as chameleon effect [22]), that<br />is in the mutual imitation of people connected by strong social<br />bonds [91]. Also, research in psychology has shown that listeners<br />tend to be accurate in decoding some basic emotions as well as<br />some non-basic affective and social signals such as distress, anxi-<br />ety, boredom, and sexual interest from vocal outbursts like laughs,<br />yawns, coughs, and sighs [163].<br />The silence is often interpreted as simple non-speech, but actu-<br />ally plays a major role in the vocal behaviour [219]. There are three<br />kinds of silence in speech: hesitation silence, psycholinguistic silence,<br />and interactive silence [158]. The first takes place when a speaker<br />has difficulty in talking, e.g., because she is expressing a difficult<br />concept or must face a hostile attitude in listeners. Sometimes,<br />hesitation silences give rise to segregates that are used to fill the si-<br />lence space (hence segregates are called sometimes fillers). The<br />psycholinguistic silences take place when the speaker needs time<br />to encode or decode the speech. This kind of silences happen often<br />at the beginning of an intervention because the speaker needs to<br />think about the next words. In this sense, this is often a sign of dif-<br />ficulty and problems in dealing with a conversation. The interac-<br />tive silences aim at conveying messages about the interactions<br />taking place: silence can be a sign of respect for people we want<br />to listen to, a way of ignoring persons we do not want to answer<br />to, as well as a way to attract the attention to other forms of com-<br />munication like mutual gaze or facial expressions.<br />Another important aspect of vocal non-verbal behaviour is turn<br />taking [154]. This includes two main components: the regulation<br />Fig. 3. Postural congruence. The figure on the left shows how people deeply involved in an interaction tend to assume the same posture. In the other picture, the forward<br />inclination of the person on the right is not reciprocated by the person on the left.<br />Fig. 4. Basic emotions. Prototypic facial expressions of six basic emotions (disgust, happiness, sadness, anger, fear, and surprise).<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1747</p>  <p>Page 6</p> <p>of the conversations, and the coordination (or the lack of it) during<br />the speaker transitions. The regulation in conversations includes<br />behaviours aimed at maintaining, yielding, denying, or requesting<br />the turn. Both gaze and voice quality (e.g., coughing) are used to<br />signal transition relevant points [217]. When it comes to vocal<br />non-verbal cues as conversation regulators, specific pitch and en-<br />ergy patterns show the intention of yielding the turn rather than<br />maintaining the floor. Also, linguistic vocalizations (see above)<br />are often used as a form of back-channeling to request the turn.<br />The second important aspect in turn taking is the coordination at<br />the speaker transitions [20]. Conversations where the latency<br />times between turns are too long sound typically awkward. The<br />reason is that in fluent conversations, the mutual attention reduces<br />the above phenomenon and results into synchronized speaker<br />changes, where the interactants effectively interpret the signals<br />aimed at maintaining or yielding their turns. Overlapping speech<br />is another important phenomenon that accounts for disputes as<br />well as status and dominance displays [180]. Note, however, that<br />the amount of overlapping speech accounts for up to 10% of the to-<br />tal time even in normal conversations [175].<br />2.5. Space and environment<br />The kind and quality of the relationships between individuals<br />influences their interpersonal distance (the physical space between<br />them). One of the most common classifications of mutual distances<br />between individuals suggests the existence of four concentric<br />zones around a person accounting for different kinds of relation-<br />ships with the others [77]: the intimate zone, the casual-personal<br />zone, the socio-consultive zone and the public zone (see Fig. 5a).<br />The intimate zone is the innermost region and it is open only to<br />the closest family members and friends. Its dimension, like in the<br />case of the other zones, depends on the culture and, in the case<br />of western Europe and United States, the intimate zone corre-<br />sponds to a distance of 0.4â0.5 m. In some cases, e.g., crowded<br />buses or elevators, the intimate zone must be necessarily opened<br />to strangers. However, whenever there is enough space, people<br />tend to avoid entering the intimate zone of others. The casual-per-<br />sonal zone ranges (at least in USA and Western Europe) between<br />0.5 and 1.2 m and it typically includes people we are most familiar<br />with (colleagues, friends, etc.). To open such an area to another<br />person in absence of constraints is a major signal of friendship.<br />The socio-consultive distance is roughly between 1 and 2 m (again<br />in USA and Western Europe) and it is the area of formal relation-<br />ships. Not surprisingly, professionals (lawyers, doctors, etc.) typi-<br />cally receive their clients sitting behind desks that have a<br />profundity of around 1 m, so that the distance with respect to their<br />clients is in the range corresponding to the socio-consultive zone.<br />The public zone is beyond 2 m distance and it is, in general, outside<br />the reach of interaction potential. In fact, any exchange taking<br />place at such a distance is typically due to the presence of some<br />obstacle, e.g., a large meeting table that requires people to talk at<br />distance.<br />Social interactions take place in environments that influence<br />behaviours and perceptions of people with their characteristics.<br />One of the most studied environmental variables is the seating<br />arrangement, i.e., the way people take place around a table for<br />different purposes [96,158]. Fig. 5b shows the seating positions<br />that people tend to use to perform different kinds of tasks (the<br />circles are the empty seats) [164]. The seating position depends<br />also on the personality of people: dominant and higher status<br />individuals tend to seat at the shorter side of rectangular tables,<br />or in the middle of the longer sides (both positions ensure high<br />visibility and make easier the control of the conversation flow)<br />[106]. Moreover, extrovert people tend to privilege seating<br />arrangements that minimize interpersonal distances, while intro-<br />vert ones do the opposite [164].<br />3. The state of the art<br />The problem of machine analysis of human social signals in-<br />cludes four subproblem areas (see Fig. 6):<br />(1) recording the scene,<br />(2) detecting people in it,<br />(3) extracting audio and/or visual behavioural cues displayed by<br />people detected in the scene and interpreting this informa-<br />tion in terms of social signals conveyed by the observed<br />behavioural cues,<br />(4) sensing the context in which the scene is recorded and clas-<br />sifying detected social signals into the target social-behav-<br />iour-interpretative categories in a context-sensitive manner.<br />The survey of the past work is divided further into four parts,<br />each of which is dedicated to the efforts in one of the above-listed<br />subproblem areas.<br />3.1. Data capture<br />Data capture refers to using sensors of different kinds to capture<br />and record social interactions taking place in real-world scenarios.<br />The choice of the sensors and their arrangement in a specific<br />recording setup determine the rest of the SSP process and limit<br />the spectrum of behavioural cues that can be extracted. For exam-<br />ple, no gaze behaviour analysis can be performed, if appropriate<br />detectors are not included in the capture system.<br />The most common sensors are microphones and cameras and<br />they can be arranged in structures of increasing complexity: from<br />a single camera and/or microphone to capture simple events like<br />oral presentations [201], to fully equipped smart meeting rooms<br />where several tens of audio and video channels (including micro-<br />Fig. 5. Space and seating. The upper part of the figure shows the concentric zones<br />around each individual associated to different kinds of rapport (d stands for<br />distance). The lower part of the figure shows the preferred seating arrangements for<br />different kinds of social interactions.<br />1748<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 7</p> <p>phone arrays, fisheye cameras, lapel microphones, etc.) are setup<br />and synchronized to capture complex interactions taking place in<br />a group meeting [110,205]. The literature shows also examples of<br />less common sensors such as cellular phones or smart badges<br />equipped with proximity detectors and vocal activity measure-<br />ment devices [43,144], and systems for the measurement of phys-<br />iological activity indicators such as blood pressure and skin<br />conductivity [76]. Recent efforts have tried to investigate the neu-<br />rological basis of social interactions [2] through devices like<br />functional magnetic resonance imaging (fMRI) [119], and Electroen-<br />cephalography (EEG) signals [193].<br />The main challenges in human sensing research domain are pri-<br />vacy and passiveness. The former involves ethical issues to be ad-<br />dressed when people are recorded during spontaneous social<br />interactions. This subject is outside the scope of this paper, but<br />the informed consent principle [51] should be always respected<br />meaning that human subjects should always be aware of being re-<br />corded (e.g., like in broadcast material). Also, the subjects need to<br />authorize explicitly the use and the diffusion of the data and they<br />must have the right of deleting, partially or totally, the recordings<br />where they are portrayed.<br />The second challenge relates to creating capture systems that<br />are passive [125], i.e., unintrusive changing the behaviour of the re-<br />corded individuals as little as possible (in principle, the subjects<br />should not even realize that they are recorded). This is a non-trivial<br />problem because passive systems should involve only non-invasive<br />sensors and the output of these is, in general, more difficult to pro-<br />cess effectively. On the other hand, data captured by more invasive<br />sensors are easier to process, but at the same time such recording<br />setups tend to change the behaviour of the recorded individuals.<br />Recording human naturalistic behaviour while eliciting specific<br />behaviours and retaining the naturalism/spontaneity of the behav-<br />iour is a very difficult problem tackled recently by several research<br />groups [29,135].<br />3.2. Person detection<br />The sensors used for data capture output signals that can be<br />analyzed automatically to extract the behavioural cues underlying<br />social signals and behaviours. In some cases, the signals corre-<br />sponding to different individuals are separated at the origin. For<br />example, physiological signals are recorded by invasive devices<br />physically connected (e.g., through electrodes) to each person.<br />Thus, the resulting signals can be attributed without ambiguity<br />to a given individual. However, it happens more frequently that<br />the signals contain spurious information (e.g., background noise),<br />or they involve more than one individual. This is the case of the<br />most commonly used sensors, microphones, and cameras, and it<br />requires the application of algorithms for person detection capable<br />of isolating the signal segments corresponding to a single individ-<br />ual. The rest of this section discusses how this can be done for mul-<br />tiparty audio and video recordings.<br />3.2.1. Person detection in multiparty audio recordings<br />In the case of audio recordings, person detection is called speak-<br />er segmentation or speaker diarization and consists of splitting the<br />speech recordings into intervals corresponding to a single voice,<br />recognizing automatically who talks when (see [189], for an exten-<br />sive survey). The speaker diarization is the most general case and it<br />includes three main stages: the first is the segmentation of the data<br />into speech and non-speech segments, the second is the detection<br />of the speaker transitions, and the third is the so-called clustering,<br />i.e., the grouping of speaker segments corresponding to a single<br />individual (i.e., to a single voice). In some cases (e.g., broadcast<br />data), no silences are expected between one speaker and the fol-<br />lowing, thus the first step is not necessary. Systems that do not in-<br />clude a speech/non-speech segmentation are typically referred to<br />as speaker segmentation systems.<br />Speech and non-speech segmentation is typically performed<br />using machine learning algorithms trained over different audio<br />classes represented in the data (non-speech can include music,<br />background noises, silence, etc.). Typically used techniques include<br />artificial neural networks [5], k nearest neighbours [107], Gaussian<br />mixture models [61], etc. Most commonly used features include<br />the basic information that can be extracted from any signal (e.g.,<br />energy and autocorrelation [156]), as well as the features typically<br />extracted for speech recognition like Mel frequency cepstrum coeffi-<br />cients (MFCC), Linear predictive coding (LPC), etc. (see [83], for an<br />extensive survey).<br />The detection of the speaker transitions is performed by split-<br />ting the speech segments into short intervals (e.g., 2â3 s) and by<br />measuring the difference (see below) between two consecutive<br />intervals: the highest values of the difference correspond to the<br />speaker changes. The approaches is based on the assumption that<br />the data include at least two speakers. If this is not the case, simple<br />differences in the intonation or the background noise might be de-<br />tected as speaker changes. The way the difference is estimated al-<br />lows one to distinguish between the different approaches to the<br />task: in general each interval is modeled using a single Gaussian<br />(preferred to the GMMs because it simplifies the calculations)<br />and the difference is estimated with the symmetric Kullback-Lei-<br />bler divergence [14]. Alternative approaches [157] use a penal-<br />ized-likelihood-ratio test to verify whether a single interval is<br />modeled better by a single Gaussian (no speaker change) or by<br />two Gaussians (speaker change).<br />The last step of both speaker diarization and segmentation is<br />clustering, i.e., grouping of the segments corresponding to a single<br />voice into a unique cluster. This is commonly carried out through<br />iterative approaches[14,117,157] where the clusters are initialized<br />using the intervals between the speaker changes detected at the<br />previous step (each interval is converted into a set of feature vec-<br />tors using common speech processing techniques [83,156]), and<br />then they are iteratively merged based on the similarity of the<br />models used to represent them (single Gaussians or GMMs). The<br />merging process is stopped when a criterion (e.g., the total likeli-<br />hood of the cluster models starts to decrease) is met.<br />Most recent approaches tend to integrate three steps above-<br />mentioned into a single framework by using hidden Markov<br />models or dynamic Bayesian networks that align feature vectors<br />extracted at regular time steps (e.g., 30 ms) and sequences of states<br />corresponding to speakers in an unsupervised way [4,5].<br />3.2.2. Person detection in multiparty video recordings<br />In the case of video data, the person detection consists in locat-<br />ing faces or full human figures (that must be eventually tracked).<br />Face detection is typically the first step towards facial-expression<br />analysis [139] or gaze behaviour analysis [199] (see [81,215] for<br />extensive surveys on face detection techniques). The detection of<br />full human figures is more frequent in surveillance systems where<br />the only important information is the movement of people across<br />wide public spaces (e.g., train stations or streets) [62,115]. In the<br />SSP framework, the detection of full human figures can be applied<br />to study social signals related to space and distances (see Section<br />2.5), but to the best of our knowledge no attempts have been made<br />yet in this direction.<br />Early approaches to face detection (see e.g., [161,181]) were<br />based on the hypothesis that the presence of a face can be inferred<br />from the pixel values. Thus, they apply classifiers like neural net-<br />works or support vector machines directly over small portions of<br />the video frames (e.g., patches of 20 ? 20 pixels) and map them<br />into a face/non-face classes. The main limitation of such tech-<br />niques is that it is difficult to train classifiers for a non-face class<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1749</p>  <p>Page 8</p> <p>that can include any kind of visual information (see Fig. 7). Other<br />approaches (e.g., [82,58]) try to detect human skin areas in images<br />and then use their spatial distribution to identify faces and facial<br />features (eyes, mouth, and nose). The skin areas are detected by<br />clustering the pixels in the color space. Alternative approaches<br />(e.g., [101]) detect separately individual face elements (eyes, nose,<br />and mouth) and detect a face where such elements have the appro-<br />priate relative positions. These approaches are particularly robust<br />to rotations because they depend on the relative position of face<br />elements, rather than on the orientation with respect to a general<br />reference frame in the image.<br />Another method that can handle out-of-plane head motions is<br />the statistical method for 3D object detection proposed in [169].<br />Other such methods, which have been recently proposed, include<br />those in [83,207]. Most of these methods emphasize statistical<br />learning techniques and use appearance features. Arguably the<br />most commonly employed face detector in automatic facial-<br />expression analysis is the real-time face detector proposed in<br />[204]. This detector consists of a cascade of classifiers trained by<br />AdaBoost. Each classifier employs integral image filters, also called<br />ââbox filtersâ, which are reminiscent of Haar basis functions, and<br />can be computed very fast at any location and scale. This is essen-<br />tial to the speed of the detector. For each stage in the cascade, a<br />subset of features is chosen using a feature selection procedure<br />based on AdaBoost. There are several adapted versions of the face<br />detector described in [204] and the one that is often used is that<br />proposed in [52].<br />The main challenge in detecting human figures is that people<br />wear clothes of different color and appearance, so the pixel values<br />are not a reliable feature for human body detection (see Fig. 8). For<br />this reason, some approaches extract features like the histograms<br />of the edge directions (e.g., [34,223]) from local regions of the<br />images (typically arranged in a regular grid), and then make a deci-<br />sion using classifiers like the support vector machines. The same<br />approach can be improved in the case of the videos, by adding mo-<br />tion information extracted using the optical flow [35]. Other ap-<br />proaches (e.g., [114,194]) try to detect individual body parts and<br />then use general rules of human body anatomy to reason about<br />the body pose (individual body parts have always the same shape<br />and they have the same relative position). For exhaustive survey,<br />see [153].<br />3.3. Social signals detection<br />Once people in the observed scene are detected, the next step in<br />the SSP process is to extract behavioural cues displayed by these<br />people. Those cues include one or more synchronized audio and/<br />or video signals that convey the information about the behaviour<br />of the person. They are the actual source from which socially rele-<br />vant behavioural cues are extracted. The next sections discuss the<br />Data<br />Capture<br />Person<br />Detection<br />Multimodal<br />Behavioural<br />Streams<br />Cues<br />Behavioural<br />Extraction<br />Social Signals<br />Understanding<br />Context<br />Understanding<br />Behavioural<br />Cues<br />Social<br />Behaviours<br />Raw Data<br />Preprocessing<br />Multimodal<br />Behavioural<br />Streams<br />Social Interaction Analysis<br />Fig. 6. Machine analysis of social signals and behaviours: a general scheme. The process includes two main stages: the preprocessing, takes as input the recordings of social<br />interaction and gives as output the multimodal behavioural streams associated with each person. The social interaction analysis maps the multimodal behavioural streams<br />into social signals and social behaviours.<br />Fig. 7. Face detection. General scheme of an appearance-based approach for face detection (picture from ââA tutorial on face detection and recognitionâ, by S. Marcel, http://<br />www.idiap.ch/~marcel).<br />1750<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 9</p> <p>main approaches to social signals detection from audio and/or vi-<br />sual signals captured while monitoring a person.<br />3.3.1. Detection of social signals from physical appearance<br />To the best of our knowledge, only few works address the prob-<br />lem of analyzing the physical appearance of people. However,<br />these works do not aim to interpret this information in terms of so-<br />cial signals. Some approaches have tried to measure automatically<br />the beauty of faces [1,44,73,75,211]. The work in [1] detects sepa-<br />rately the face elements (eyes, lips, etc.) and then maps the ratios<br />between their dimensions and distances into beauty judgments<br />through classifiers trained on images assessed by humans. The<br />work in [44] models the symmetry and the proportions of a face<br />through the geometry of several landmarks (e.g., the corners of<br />the eyes and the tip of the nose), and then applies machine learn-<br />ing techniques to match human judgments. Other techniques (e.g.,<br />[131]) use 3D models of human heads and the distance with re-<br />spect to average faces extracted from large data sets to assess per-<br />sonal beauty. Faces closest to the average seem to be judged as<br />more attractive than others.<br />Also few works were proposed where the body shape, the color<br />of skin, hair, and clothes are extracted automatically (through a<br />clustering of the pixels in the color space) for identification and<br />tracking purposes [16,36,214]. However these works do not ad-<br />dress social signal understanding and are therefore out of the scope<br />of this paper.<br />3.3.2. Detection of social signals from gesture and posture<br />Gesture recognition is an active research domain in computer<br />vision and pattern recognition research communities, but no ef-<br />forts have been made, so far, to interpret the social information<br />carried by gestural behaviours. In fact, the efforts are directed<br />mostly towards the use of gestures as an alternative to keyboard<br />and mouse to operate computers (e.g., [132,172,213]), or to the<br />automatic reading of sign languages (e.g., [40,97]). Also few efforts<br />have been reported towards human affect recognition from body<br />gestures (for an overview see [76,221]). There are two main chal-<br />lenges in recognizing gestures: detecting the body parts involved<br />in the gesture (in general the hands), and modeling the temporal<br />dynamic of the gesture.<br />The first problem is addressed by selecting appropriate visual<br />features: these include, e.g., histograms of oriented gradients<br />(e.g., [183,184]), optical flow (e.g., [3,188]), spatio-temporal salient<br />points (e.g., [129]) and space-time volumes (e.g., [67]). The second<br />problem is addressed by using techniques such as dynamic time<br />warping (e.g., [129]), Hidden Markov models (e.g., [3]), and condi-<br />tional random fields (e.g., [179]).<br />Like in the case of gestures, machine recognition of walking<br />style (or gait) has been investigated as well, but only for purposes<br />different from SSP, namely recognition and identification in bio-<br />metric applications[100,102,206]. The common approach is to seg-<br />ment the silhouette of the human body into individual components<br />(legs, arms, trunk, etc.), and then to represent their geometry dur-<br />ing walking through vectors of distances [206], symmetry opera-<br />tors [78], geometric features of body and stride (e.g., distance<br />between head and feets or pelvis) [17], etc.<br />Also automatic posture recognition has been addressed in few<br />works, mostly aiming at surveillance [57] and activity recogni-<br />tion [206] (see [54,116,153] for extensive overviews of the past<br />work in the field). However, there are few works where the pos-<br />ture is recognized as a social signal, namely to estimate the<br />interest level of children learning to use computers [124], to rec-<br />ognize the affective state of people [38,74] (see [76,221], for<br />exhaustive overview of research efforts in the field), and the<br />influence of culture on affective postures [95].<br />3.3.3. Detection of social signals from gaze and face<br />The problem of machine recognition of human gaze and facial<br />behaviour includes three subproblem areas (see Fig. 9): finding<br />faces in the scene, extracting facial features from the detected face<br />region, analyzing the motion of eyes and other facial features and/<br />or the changes in the appearance of facial features, and classifying<br />this information into some facial-behaviour-interpretative catego-<br />ries (e.g., facial muscle actions (AUs), emotions, social behaviours,<br />etc.).<br />Numerous techniques have been developed for face detection,<br />i.e., identification of all regions in the scene that contain a human<br />face (see Section 3.2). Most of the proposed approaches to facial<br />expression recognition are directed toward static, analytic, 2D fa-<br />cial feature extraction [135,185]. The usually extracted facial fea-<br />tures are either geometric features such as the shapes of the<br />facial components (eyes, mouth, etc.) and the locations of facial<br />fiducial points (corners of the eyes, mouth, etc.) or appearance fea-<br />tures representing the texture of the facial skin in specific facial<br />areas including wrinkles, bulges, and furrows. Appearance-based<br />features include learned image filters from independent compo-<br />nent analysis (ICA), principal component analysis (PCA), local fea-<br />ture analysis (LFA), Gabor filters, integral image filters (also<br />known as box filters and Haar-like filters), features based on<br />edge-oriented histograms, and similar [135]. Several efforts have<br />been also reported which use both geometric and appearance fea-<br />tures (e.g., [185]). These approaches to automatic facial-expression<br />analysis are referred to as hybrid methods. Although it has been<br />reported that methods based on geometric features are often out-<br />performed by those based on appearance features using, e.g., Gabor<br />wavelets or eigenfaces, recent studies show that in some cases geo-<br />metric features can outperform appearance-based ones [135,136].<br />Yet, it seems that using both geometric and appearance features<br />might be the best choice in the case of certain facial expressions<br />[136].<br />Contractions of facial muscles (i.e., AUs explained in Section<br />2.3), which produce facial expressions, induce movements of the<br />Fig. 8. People detection. Examples of people detection in public spaces (pictures from [216]).<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1751</p>  <p>Page 10</p> <p>facial skin and changes in the location and/or appearance of facial<br />features. Such changes can be detected by analyzing optical flow,<br />facial-point- or facial-component-contour-tracking results, or by<br />using an ensemble of classifiers trained to make decisions about<br />the presence of certain changes based on the passed appearance<br />features. The optical flow approach to describing face motion has<br />the advantage of not requiring a facial feature extraction stage of<br />processing. Dense flow information is available throughout the en-<br />tire facial area, regardless of the existence of facial components,<br />even in the areas of smooth texture such as the cheeks and the<br />forehead. Because optical flow is the visible result of movement<br />and is expressed in terms of velocity, it can be used to represent di-<br />rectly facial expressions. Many researchers adopted this approach<br />(for overviews, see [135,139,185]). Until recently, standard optical<br />flow techniques were arguably most commonly used for tracking<br />facial characteristic points and contours as well. In order to address<br />the limitations inherent in optical flow techniques such as the<br />accumulation of error and the sensitivity to noise, occlusion, clut-<br />ter, and changes in illumination, recent efforts in automatic facial-<br />expression recognition use sequential state estimation techniques<br />(such as Kalman filter and particle filter) to track facial feature<br />points in image sequences [135,136,222].<br />Eventually, dense flow information, tracked movements of fa-<br />cial characteristic points, tracked changes in contours of facial<br />components, and/or extracted appearance features are translated<br />into a description of the displayed facial behaviour. This descrip-<br />tion (facial-expression interpretation) is usually given either in<br />terms of shown affective states (emotions) or in terms of activated<br />facial muscles (AUs) underlying the displayed facial behaviour.<br />Most facial-expressions analyzers developed so far target human<br />facial affect analysis and attempt to recognize a small set of proto-<br />typic emotional facial expressions like happiness and anger<br />[140,221]. However, several promising prototype systems were re-<br />ported that can recognize deliberately produced AUs in face images<br />(for overviews, see [135,185]) and even few attempts towards rec-<br />ognition of spontaneously displayed AUs (e.g., [103,108]) and to-<br />wards automatic discrimination between spontaneous and posed<br />facial behaviour such as smiles [195], frowns [197], and pain<br />[104], have been recently reported as well. Although still tentative,<br />few studies have also been recently reported on separating emo-<br />tional states from non-emotional states and on recognition of<br />non-basic affective states in visual and audiovisual recordings of<br />spontaneous human behaviour (e.g., for overview see [170,220]).<br />However, although messages conveyed by AUs like winks, blinks,<br />frowns, smiles, gaze exchanges, etc., can be interpreted in terms<br />of social signals like turn taking, mirroring, empathy, engagement,<br />etc., no efforts have been reported so far on automatic recognition<br />of social behaviours in recordings of spontaneous facial behaviour.<br />Hence, while the focus of the research in the field started to shift to<br />automatic (non-basic-) emotion and AU recognition in spontane-<br />ous facial expressions (produced in a reflex-like manner), efforts<br />towards automatic analysis of human social behaviour from visual<br />and audiovisual recordings of human spontaneous behaviour are<br />still to be made.<br />While the older methods for facial-behaviour analysis employ<br />simple approaches including expert rules and machine learning<br />methods such as neural networks to classify the relevant informa-<br />tion from the input data into some facial-expression-interpretative<br />categories (e.g., basic emotion categories), the more recent (and of-<br />ten more advanced) methods employ probabilistic, statistical, and<br />Fig. 9. AU detection. Outline of a geometric-feature-based system for detection of facial AUs and their temporal phases (onset, apex, offset, and neutral) proposed in [196].<br />1752<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 11</p> <p>ensemble learning techniques, which seem to be particularly suit-<br />able for automatic facial-expression recognition from face image<br />sequences (for comprehensive overviews of the efforts in the field,<br />see[135,221]). Note, however, the present systems for facial-expression<br />analysis typically depend on accurate head, face, and facial feature<br />tracking as input and are still very limited in performance and<br />robustness.<br />3.3.4. Detection of social signals from vocal behaviour<br />The behavioural cues in speech include voice quality, vocal-<br />izations (linguistic and non-linguistic), and silences (see Section<br />2.4, for details). All of them have been the subject of extensive<br />research in speech, but they have rarely been interpreted in<br />terms of social information, even if they account for roughly<br />50% of the total time in spontaneous conversations [21]. With<br />few exceptions, the detection of vocal behaviour has aimed at<br />the improvement of automatic speech recognition (ASR) systems,<br />where the vocal non-verbal behaviour represents a form of noise<br />rather than an information.<br />The voice quality corresponds to the prosody and includes three<br />major aspects, often called the Big Three: pitch, tempo, and energy<br />[31]. The pitch is the frequency of oscillation of the vocal folds dur-<br />ing the emission of voice and it is the characteristic that alone con-<br />tributes more than anything else to the sound of a voice [120,150].<br />The measurement of the pitch, often called fundamental frequency<br />(or F0) because most of the speech energy is concentrated over<br />components corresponding to its integer multiples, can be per-<br />formed with several standard methods proposed in the literature<br />[83,156]. The pitch is typically extracted as the frequency corre-<br />sponding to the first peak of the Fourier transform of short analysis<br />windows (in general 30 ms). Several tools publicly available on the<br />web, e.g., Wavesurfer1[177] and Praat2[18], implement algorithms<br />extracting the pitch from speech recordings. The tempo is typically<br />estimated through the speaking rate, i.e., the number of phonetically<br />relevant units, e.g., vowels [149], per second. Other methods are<br />based on measures extracted from the speech signal like the first<br />spectral moment of the energy [121,122] and typically aim at<br />improving speech recognition systems through speaking rate adap-<br />tation. The energy is a property of any digital signal and simply cor-<br />responds to the sum of the square values of the samples [156].<br />No major efforts have been made so far, to the best of our<br />knowledge, to detect the non-linguistic vocalizations (see Section<br />2.4). The only exceptions are laughter[92,191,192] due to its ubiq-<br />uitous presence in social interactions, and crying[118,134]. Laugh-<br />ter is detected by applying binary classifiers such as Support Vector<br />Machines to features commonly applied in speech recognition like<br />the Mel frequency cepstral coefficients [92], or by modeling percep-<br />tual linear prediction features with Gaussian mixture models and<br />neural networks [191,192]. These efforts are based only on audio<br />signals, but few pioneering efforts towards audiovisual recognition<br />of non-linguistic vocal outbursts have been recently reported. A<br />laughter detector which combines the outputs of an audio-based<br />detector that uses MFCC audio features and a visual detector that<br />uses spatial locations of facial feature points is proposed in [86].<br />They attained 80% average recall rate using three sequences of<br />three subjects in a person-dependent way. In [147], decision-level<br />and feature-level fusion with audio- and video-only laughter<br />detection are compared. The work uses PLP features and displace-<br />ments of the tracked facial points as the audio and visual features,<br />respectively. Both fusion approaches outperformed single-modal<br />detectors, achieving on average 84% recall in a person-independent<br />test. Extension of this work based on utilization of temporal fea-<br />tures has been reported in [148].<br />Linguistic vocalizations have been investigated to detect hesita-<br />tions in spontaneous speech [105,173,174] with the main purpose<br />of improving speech recognition systems. The disfluencies are typ-<br />ically detected by mapping acoustic observations (e.g., pitch and<br />energy) into classes of interest with classifiers like neural networks<br />or support vector machines. The detection of silence is one of the<br />earliest tasks studied in speech analysis and robust algorithms,<br />based on the distribution of the energy, have been developed since<br />the earliest times of digital signal processing [155,156]. Another<br />important aspect of vocal behaviour, i.e., the turn taking, is typi-<br />cally a side-product of the speaker diarization or segmentation<br />step (see Section 3.2).<br />3.3.5. Detection of social signals in space and environment<br />Physical proximity information has been used in reality mining<br />applications (see Section 4) as a social cue accounting for the sim-<br />ple presence or absence of interaction between people [43,144].<br />These works use special cellular phones equipped to sense the<br />presence of similar devices in the vicinity. Automatic detection of<br />seating arrangements has been proposed as a cue for retrieving<br />meeting recordings in [88]. Also, several video-surveillance<br />approaches developed to track people across public spaces can<br />potentially be used for detection of social signals related to the<br />use of the available space (see Section 3.2, for more details).<br />3.4. Context sensing and social behaviour understanding<br />Context plays a crucial role in understanding of human behav-<br />ioural signals, since they are easily misinterpreted if the informa-<br />tion about the situation in which the shown behavioural cues<br />have been displayed is not taken into account. For example, a smile<br />can be a display of politeness (social signal), contentedness (affec-<br />tive cue), joy of seeing a friend (affective cue/social signal), irony/<br />irritation (affective cue/social signal), empathy (emotional re-<br />sponse/social signal), greeting (social signal), to mention just a<br />few possibilities. It is obvious from these examples that in order<br />to determine the communicative intention conveyed by an<br />observed behavioural cue, one must know the context in which<br />the observed signal has been displayed: where the expresser is<br />(outside, inside, in the car, in the kitchen, etc.), what his or her cur-<br />rent task is, are other people involved, when the signal has been<br />displayed (i.e., what is the timing of displayed behavioural signals<br />with respect to changes in the environment), and who the expres-<br />ser is (i.e., it is not probable that each of us will express a particular<br />affective state by modulating the same communicative signals in<br />the same way).<br />Note, however, that while W4 (where, what, when, who) is deal-<br />ing only with the apparent perceptual aspect of the context in<br />which the observed human behaviour is shown, human behaviour<br />understanding is about W5+ (where, what, when, who, why, how),<br />where the why and how are directly related to recognizing commu-<br />nicative intention including social behaviours, affective and cogni-<br />tive states of the observed person. Hence, SSP is about W5+.<br />However, since the problem of context sensing is extremely diffi-<br />cult to solve, especially for a general case (i.e., general-purpose<br />W4 technology does not exist yet [138,137]), answering the why<br />and how questions in a W4-context sensitive manner when analys-<br />ing human behaviour is virtually unexplored area of research. Hav-<br />ing said that, it is not a surprise that most of the present<br />approaches to machine analysis of human behaviour are neither<br />context sensitive nor suitable for handling longer time scales.<br />Hence, the focus of future research efforts in the field should be<br />primarily on tackling the problem of context-constrained analysis<br />of multimodal social signals shown over longer temporal intervals.<br />1Publicly available at http://www.speech.kth.se/wavesurfer/.<br />2Publicly available at http://www.praat.org.<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1753</p>  <p>Page 12</p> <p>Here, we would like to stress the importance of two issues:<br />realizing temporal analysis of social signals and achieving tempo-<br />ral multimodal data fusion.<br />Temporal dynamics of social behavioural cues (i.e., their timing,<br />co-occurrence, speed, etc.) are crucial for the interpretation of the<br />observed social behaviour [8,50]. However, present methods for<br />human behaviour analysis do not address the when context ques-<br />tion â dynamics of displayed behavioural signals is usually not ta-<br />ken into account when analyzing the observed behaviour, let alone<br />analysing the timing of displayed behavioural signals with respect<br />to changes in the environment. Exceptions of this rule include few<br />recent studies on modeling semantic and temporal relationships<br />between facial gestures (i.e., AUs, see Section 2.3) forming a facial<br />expression (e.g., [187]), few studies on discrimination between<br />spontaneous and posed facial gestures like brow actions and smiles<br />based on temporal dynamics of target facial gestures, head and<br />shoulder gestures [195,197], and few studies on multimodal anal-<br />ysis of audio and visual dynamic behaviours for emotion recogni-<br />tion [221]. In general, as already mentioned above, present<br />methods cannot handle longer time scales, model grammars of ob-<br />served persons behaviours, and take temporal and context-depen-<br />dent evolvement of observations into account for more robust<br />performance. These remain major challenges facing the research-<br />ers in the field.<br />Social signals are spoken and wordless messages like head nods,<br />winks,uh,andyeahutterances,whicharesentbymeansofbodyges-<br />tures and postures, facial expressions and gaze, vocal expressions<br />and speech. Hence, automated analyzers of human social signals<br />and social behaviours should be multimodal, fusing and analyzing<br />verbal and non-verbal interactive signals coming from different<br />modalities (speech, body gestures, facial, and vocal expressions).<br />Mostofthepresentaudiovisualandmultimodalsystemsinthefield<br />perform decision-level data fusion (i.e., classifier fusion) in which<br />the input coming from each modality is modeled independently<br />and these single-modal recognition results are combined at the<br />end.Sincehumansdisplayaudioandvisualexpressionsinacomple-<br />mentary and redundant manner, the assumption of conditional<br />independencebetweenaudioandvisualdatastreamsindecision-le-<br />velfusionisincorrectandresultsinthelossofinformationofmutual<br />correlation between the two modalities. To address this problem, a<br />number of model-level fusion methods have been proposed that<br />aim at making use of the correlation between audio and visual data<br />streams, and relax the requirement of synchronization of these<br />streams (e.g., [55,220]). However, how to model multimodal fusion<br />on multiple time scales and how to model temporal correlations<br />within and between different modalities is largely unexplored. A<br />much broader focus on the issues relevant to multimodal temporal<br />fusion is needed including the optimal level of integrating these dif-<br />ferent streams, the optimal function for the integration, and how<br />estimationsofreliabilityofeachstreamcanbeincludedintheinfer-<br />ence process. In addition, how to build context-dependent multi-<br />modal fusion is another open and highly relevant issue.<br />4. Main applications of social signal processing<br />The expression social signal processing has been used for the<br />first time in [145] to group under a collective definition several<br />pioneering works of Alex Pentland and his group at MIT. Some of<br />their works [142,143] extracted automatically the social signals<br />detected in dyadic interactions to predict with an accuracy of<br />more than 70% the outcome of salary negotiations, hiring inter-<br />views, and speed-dating conversations [33]. These works are<br />based on vocal social signals including overall activity (the total<br />amount of energy in the speech signals), influence (the statistical<br />influence of one person on the speaking patterns of the others),<br />consistency (stability of the speaking patterns of each person),<br />and mimicry (the imitation between people involved in the inter-<br />actions). Other works used cellular phones equipped with prox-<br />imity sensors and vocal activity detectors to perform what came<br />to be called reality mining, or social sensing, i.e., automatic anal-<br />ysis of everyday social interactions in groups of several tens of<br />individuals [43,144]. Individuals are represented through vectors<br />accounting for their proximity with others and for the places<br />they are (home, work, etc.). The application of the principal<br />component analysis to such vectors leads to the so called eigen-<br />behaviours [43].<br />In approximately the same period, few other groups worked on<br />the analysis of social interactions in multimedia recordings target-<br />ing three main areas: analysis of interactions in small groups, rec-<br />ognition of roles, and sensing of users interest in computer<br />characters. Results for problems that have been addressed by more<br />than one group are reported in Table 2.<br />The research on interactions in small groups has focused on the<br />detection of dominant persons and on the recognition of collective<br />actions. The problem of dominance is addressed in [85,160], where<br />multimodal approaches combine several non-verbal features,<br />mainly speaking energy, and body movement, to identify at each<br />moment who is the dominant individual. The same kind of features<br />has been applied in [39,111] to recognize the actions performed in<br />meetings like discussions, presentations, etc. In both above appli-<br />cations, the combination of the information extracted from differ-<br />ent modalities is performed with algorithms dynamic Bayesian<br />networks [126] and layered Hidden Markov models [130].<br />The recognition of roles has been addressed in two main con-<br />texts: broadcast material[15,53,200,210] and small scale meetings<br />[13,42,59,218]. The works in [53,200,210] apply social network<br />analysis [209] to detect the role of people in broadcast news and<br />movies, respectively. The social networks are extracted automati-<br />cally using speaker adjacences in [53,200] (people are linked when<br />they are adjacent in the sequence of the speakers), and face recog-<br />nition [210] (people are linked when their faces appear together in<br />a scene). The approach in [15] recognizes the roles of speakers in<br />broadcast news using vocal behaviour (turn taking patterns and<br />intervention duration) and lexical features. The recognition is per-<br />formed using boosting techniques. The roles in meetings are recog-<br />nized with a classifier tree applied to non-verbal behaviour<br />features (overlapping speech, number of interventions, back-chan-<br />neling, etc.) in the case of [13], while speech and fidgeting activity<br />are fed to a multi-SVM classifier in [42,218]. A technique based on<br />the combination of social network analysis and lexical modeling<br />(Boostexter) is presented in [59].<br />The reaction of users to social signals exhibited by computer<br />characters has been investigated in several works showing that<br />people tend to behave with embodied conversational agents<br />(ECA) as they behave with other humans. The effectiveness of com-<br />puters as social actors, i.e., entities involved in the same kind of<br />interactions as humans, has been explored in [127,128], where<br />computers have been shown to be attributed a personality and to<br />elicit the same reactions as those elicited by persons. Similar ef-<br />fects have been shown in[28,133], where children interacting with<br />computers have modified their voice to match the speaking charac-<br />teristics of the animated ECA, showing adaptation patterns typical<br />of humanâhuman interactions [20]. Further evidence of the same<br />phenomenon is available in[10,11], where the interaction between<br />humans and ECA is shown to include the Chameleon effect [22], i.e.,<br />the mutual imitation of individuals due to reciprocal appreciation<br />or to the influence of one individual on the other.<br />Psychologists have compared the performance of humans and<br />machines in detecting socially relevant information like gender<br />and movements associated to emotional states [65,151,152]. The<br />results show that machines tend to have a constant performance<br />1754<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 13</p> <p>across a wide range of conditions (different behavioural cues at<br />disposition), while humans have dramatic changes in performance<br />(sometimes dropping at chance level) when certain behavioural<br />cues are no longer at disposition. This seems to suggest that hu-<br />mans do not use the behavioural cues actually at their disposition,<br />but rather rely on task-specific behavioural cues without which the<br />tasks cannot be performed effectively [65,151,152]. In contrast,<br />automatic approaches (in particular those based on machine learn-<br />ing) are built to rely on any available behavioural cue and their<br />performance simply depends on how much the available cues are<br />actually correlated with the targeted social information.<br />5. Conclusions and future challenges<br />Social signal processing has the ambitious goal of bringing social<br />intelligence[6,66]incomputers.Thefirstresultsinthisresearchdo-<br />main have been sufficiently impressive to attract the praise of the<br />technology [69] and business [19] communities. What is more<br />important is that they have established a viable interface between<br />human sciences and engineering â social interactions and behav-<br />iours,althoughcomplexandrootedinthedeepestaspectsofhuman<br />psychology,canbeanalyzedautomaticallywiththehelpofcomput-<br />ers.Thisinterdisciplinarityis,inouropinion,themostimportantre-<br />sult of research in SSP so far. In fact, the pioneering contributions in<br />SSP[142,143]haveshownthatthesocialsignals,typicallydescribed<br />assoelusiveandsubtlethatonlytrainedpsychologistscanrecognize<br />them[63],areactuallyevidentanddetectableenoughtobecaptured<br />through sensors like microphones and cameras, and interpreted<br />through analysis techniques like machine learning and statistics.<br />However, although fundamental, these are only the first steps<br />andthejourneytowardsartificialsocialintelligenceandsociallyaware<br />computingisstilllong.Intherestofthissection,wediscussfourchal-<br />lengesfacingtheresearchersinthefield,forwhichwebelievearethe<br />crucialturnoverissuesthatneedtobeaddressedbeforetheresearch<br />in the field can enter its next phase â the deployment phase.<br />Thefirstissuerelatestotighteningofthecollaborationbetweensocial<br />scientists and engineers. The analysis of human behaviour in general,<br />and social behaviour in particular, is an inherently multidisciplinary<br />problem [138,221]. More specifically no automatic analysis of social<br />interactions is possible without taking into account the basic mecha-<br />nismsgoverningsocialbehavioursthatthepsychologistshaveinvesti-<br />gated for decades, such as the chameleon effect (mutual imitation of<br />peopleaimedatshowinglikingoraffiliation)[22,99],theinterpersonal<br />adaptation (mutual accommodation of behavioural patterns between<br />interactingindividuals)[20,71],theinteractionalsynchrony(degreeof<br />coordinationduringinteractions)[93],thepresenceorrolesingroups<br />[12,186], the dynamics of conversations[154,217], etc. The collabora-<br />tion between technology and social sciences demands a mutual effort<br />of the two disciplines. On one hand, engineers need to include the so-<br />cial sciences in their reflection, while on the other hand, social scien-<br />tists need to formulate their findings in a form useful for engineers<br />and their work on SSP.<br />The second issue relates to the need of implementing multi-cue,<br />multimodalapproachestoSSP.Non-verbalbehaviourscannotberead<br />like wordsin a book[96,158]; they are notunequivocallyassociated<br />to a specific meaning and their appearance can depend on factors<br />that have nothing to do with social behaviour. Postures correspond<br />in general to social attitudes, but sometimes they are simply com-<br />fortable [166], physical distances typically account for social dis-<br />tances, but sometimes they are simply the effect of physical<br />constraints[77].Moreover,thesamesignalcancorrespondtodiffer-<br />ent social behaviour interpretations depending on context and cul-<br />ture [190] (although many advocate that social signals are natural<br />rather than cultural [171]). In other words, social signals are intrin-<br />sically ambiguous and the best way to deal with such problem is to<br />use multiple behavioural cues extracted from multiple modalities.<br />Numerous studies have theoretically and empiricallydemonstrated<br />the advantage of integration of multiple modalities (at least audio<br />and visual) in human behaviour analysis over single modalities<br />(e.g., [162]). This corresponds, from a technological point of view,<br />to the combination of different classifiers that has extensively been<br />showntobemoreeffectivethansingleclassifiers,aslongastheyare<br />sufficiently diverse, i.e., account for different aspects of the same<br />problem[94].Itisthereforenotsurprisingthatsomeofthemostsuc-<br />cessful works in SSP so far use features extracted from multiple<br />modalities like in [39,85,111]. Note, however, that the relative con-<br />tributions of different modalities and the related behavioural cues<br />to affect judgment of displayed behaviour depend on the targeted<br />behavioural categoryandthe contextinwhichthebehaviouroccurs<br />[49,162].<br />Table 2<br />Results obtained by social signal processing works. For each work, information about the data (kind of interaction, availability, size, the total duration of the recordings), whether<br />it is real-world or acted data, and the reported performance are summarized.<br />Ref.Data Time SourcePerformance<br />Role recognition<br />[13]<br />[15]<br />Meetings (2 recordings, 3 roles)<br />NIST TREC SDR Corpus (35 recordings, publicly available<br />3 roles)<br />The Survival Corpus (11 recordings, publicly available, 5<br />roles)<br />AMI Meeting Corpus (138 recordings, publicly available,<br />4 roles)<br />Radio news bulletins (96 recordings, 6 roles)<br />Movies (3 recordings, 4 roles)<br />The Survival Corpus (11 recordings, publicly available, 5<br />roles)<br />0 h.45 m<br />17 h.00 m<br />Acted<br />Spontaneous<br />50.0% of segments (up to 60 s long) correctly classified<br />80.0% of the news stories correctly labeled in terms of role<br />[42]4 h.30 m Acted 90% of precision in role assignment<br />[59]45 h.00 m Acted 67.9% of the data time correctly labeled in terms of role<br />[200]<br />[210]<br />[218]<br />25 h.00 m<br />5 h.46 m<br />4 h.30 m<br />Spontaneous<br />Spontaneous<br />Spontaneous<br />80% of the data time correctly labeled in terms of role<br />95% of roles correctly assigned<br />Up to 65% of analysis windows (around 10 s long) correctly classified<br />in terms of role<br />Collective action recognition<br />[39] Meetings (30 recordings, publicly available)<br />[111]Meetings (60 recordings, publicly available)<br />2 h.30 m<br />5 h.00 m<br />Acted<br />Acted<br />Action error rate of 12.5%<br />Action error rate of 8.9%<br />Interest level detection<br />[60] Meetings (50 recordings, 3 interest levels)<br />[124] Children playing with video games (10 recordings, 3<br />interest levels)<br />Unknown<br />3 h.20 m<br />Acted<br />Spontaneous<br />75% Precision<br />82% recognition rate<br />Dominance detection<br />[85]<br />[159]<br />[160]<br />Meetings from AMI Corpus (34 segments)<br />Meetings (8 meetings)<br />Meetings (40 recordings)<br />3 h.00 m<br />1 h.35 m<br />20 h.00 m<br />Acted<br />Acted<br />Acted<br />Most dominant person correctly detected in 85% of segments<br />Most dominant person correctly detected in 75% of meetings<br />Most dominant person correctly detected in 60% of meetings<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1755</p>  <p>Page 14</p> <p>The third issue relates to the use of real-world data. Both psy-<br />chologists and engineers tend to produce their data in laboratories<br />and artificial settings (see e.g., [33,68,111]), in order to limit para-<br />sitic effects and elicit the specific phenomena they want to ob-<br />serve. However, this is likely to simplify excessively the situation<br />and to improve artificially the performance of the automatic ap-<br />proaches. Social interaction is one of the most ubiquitous phenom-<br />ena in the world â the media (radio and television) show almost<br />exclusively social interactions (debates, movies, talk-shows)<br />[123]. Also other, less common kinds of data are centered on social<br />interactions, e.g., meeting recordings [110], surveillance material<br />[87], and similar. The use of real-world data will allow analysis<br />of interactions that have an actual impact on the life of the partic-<br />ipants, thus will show the actual effects of goals and motivations<br />that typically drive human behaviour. This includes also the anal-<br />ysis of group interactions, a task difficult from both technological<br />and social point of view because it involves the need of observing<br />multiple people involved in a large number of one-to-one<br />interactions.<br />The last, but not least, challenging issue relates to the the iden-<br />tification of applications likely to benefit from SSP. Applications have<br />the important advantage of linking the effectiveness of detecting<br />social signals to the reality. For example, one of the earliest appli-<br />cations is the prediction of the outcome in transactions recorded at<br />a call center and the results show that the number of successful<br />calls can be increased by around 20% by stopping early the calls<br />that are not promising [19]. This can have not only a positive im-<br />pact on the marketplace, but also provide benchmarking procedures<br />for the SSP research, one of the best means to improve the overall<br />quality of a research domain as extensively shown in fields where<br />international evaluations take place every year (e.g., video analysis<br />in TrecVid [178]).<br />Acknowledgements<br />The work of Dr. Vinciarelli is supported by the Swiss National<br />Science Foundation through the National Center of Competence<br />in Research on Interactive Multimodal Information Management<br />(IM2). The work of Dr. Pantic is supported in part by the ECâs 7th<br />Framework Programme (FP7/2007-2013) under Grant Agreement<br />No. 211486 (SEMAINE), and the European Research Council under<br />the ERC Starting Grant Agreement No. ERC-2007-StG-203143<br />(MAHNOB). The research that has led to this work has been sup-<br />ported in part by the European Communityâs Seventh Framework<br />Programme (FP7/2007-2013),<br />231287 (SSPNet).<br />underGrant Agreement No.<br />References<br />[1] P. Aarabi, D. Hughes, K. Mohajer, M. Emami, The automatic measurement of<br />facial beauty, in: Proceedings of IEEE International Conference on Systems,<br />Man, and Cybernetics, 2001, pp. 2644â2647.<br />[2] R.AdolphsCognitive neuroscience of human social behaviourNature Reviews<br />Neuroscience 4 (3)2003 165â178.<br />[3] M.Ahmad, S.-W.LeeHuman action recognition using shape and CLG-motion<br />flow from multi-view image sequencesPattern Recognition 41 (7)2008 2237â<br />2252.<br />[4] J. Ajmera, Robust Audio Segmentation, Ph.D. thesis, Ãcole Polytechnique<br />FÃ©dÃ©rale de Lausanne (EPFL), 2004.<br />[5] J.Ajmera, I.McCowan, H.BourlardSpeech/music segmentation using entropy<br />and dynamism featuresina HMM<br />Communication 40 (3)2003 351â363.<br />[6] K.AlbrechtSocial Intelligence: The New Science of Success2005, John Wiley &amp;<br />Sons Ltd., 2005.<br />[7] N. Ambady, F. Bernieri, J. Richeson, Towards a histology of social behavior:<br />judgmental accuracy from thin slices of behavior, in: M.P. Zanna (Ed.),<br />Advances in Experimental Social Psychology, 2000, pp. 201â272.<br />[8] N.Ambady, R.RosenthalThin slices of expressive behavior as predictors of<br />interpersonal consequences: a meta-analysisPsychological Bulletin 111<br />(2)1992 256â274.<br />classificationframeworkSpeech<br />[9] M.ArgyleThe<br />Harmondsworth, 1967.<br />[10] J.N.Bailenson,<br />chameleonsJournal of Nonverbal Behavior 31 (4)2007 225â242.<br />[11] J.N.Bailenson,N.Yee,<br />chameleonsComputers in Human Behavior 24 (1)2008 66â87.<br />[12] R.F.BalesInteraction Process Analysis: A Method for the Study of Small<br />Groups1950, Addison-Wesley, Reading, MA, 1950.<br />[13] S. Banerjee, A.I. Rudnicky, Using simple speech based features to detect the<br />state of a meeting and the roles of the meeting participants, in: Proceedings of<br />International Conference on Spoken Language Processing, 2004, pp. 2189â<br />2192.<br />[14] C. Barras, X. Zhu, S. Meignier, J.L. Gauvain, Improving speaker diarization, in:<br />Proceedings of the Rich Transcription Workshop, 2004.<br />[15] R. Barzilay, M. Collins, J. Hirschberg, S. Whittaker, The rules behind the roles:<br />identifying speaker roles in radio broadcasts, in: Proceedings of American<br />Association of Artificial Intelligence Symposium, 2000, pp. 679â684.<br />[16] C.Ben Abdelkader, Y.YacoobStatistical estimation of human anthropometry<br />from a single uncalibrated image, in: K.Franke, S.Petrovic, A.Abraham<br />(Eds.)Computational Forensics2009, Springer-Verlag, Berlin, 2009.<br />[17] A.F. Bobick, A. Johnson, Gait recognition using static activity-specific<br />parameters, in: Proceedings of Computer Vision and Pattern Recognition,<br />2001, pp. 423â430.<br />[18] P.Boersma, D.WeeninkPraat, a system for doing phonetics by computerGlot<br />International 5 (9/10)2001 341â345.<br />[19] M.BuchananThe science of subtle signalsStrategy+Business 482007 68â77.<br />[20] J.K.Burgoon, L.A.Stern, L.DillmanInterpersonal Adaptation: Dyadic Interaction<br />Patterns1995, Cambridge University Press, Cambridge, 1995.<br />[21] N.CampbellConversationalspeech<br />laughterIEEE Transactions on Speech and Language Processing 14 (4)2006<br />1171â1178.<br />[22] T.L.Chartrand, J.A.BarghThe chameleon effect: the perception-behavior link<br />and social interactionJournal of Personality and Social Psychology 76 (6)1999<br />893â910.<br />[23] J.F. Cohn, Foundations of human computing: facial expression and emotion,<br />in: Proceedings of the ACM International Conference on Multimodal<br />Interfaces, 2006, pp. 233â238.<br />[24] J.F. Cohn, P. Ekman, Measuring facial action by manual coding, facial EMG,<br />and automatic facial image analysis, in: J.A. Harrigan, R. Rosenthal, K.R.<br />Scherer (Eds.), Handbook of Nonverbal Behavior Research Methods in the<br />Affective Sciences, 2005, pp. 9â64.<br />[25] J.B.Cortes, F.M.GattiPhysique and self-description of temperamentJournal of<br />Consulting Psychology 29 (5)1965 432â439.<br />[26] M.Costa, W.Dinsbach,A.S.R.Manstead,<br />embarrassment, and nonverbal behaviorJournal of Nonverbal Behavior 25<br />(4)2001 225â240.<br />[27] M.CoulsonAttributing emotion to static body postures: recognition accuracy,<br />confusions, and viewpoint dependenceJournal of Nonverbal Behavior 28<br />(2)2004 117â139.<br />[28] R. Coulston, S. Oviatt, C. Darves, Amplitude convergence in childrenâs<br />conversational speech with animated personas, in: International Conference<br />on Spoken Language Processing, 2002, pp. 2689â2692.<br />[29] R. Cowie, Building the databases needed to understand rich, spontaneous<br />human behaviour, in: Proceedings of the IEEE International Conference on<br />Automatic Face and Gesture Recognition, 2008.<br />[30] R.Cowie, E.Douglas-Cowie, N.Tsapatsoulis, G.Votsis, S.Kollias, W.Fellenz,<br />J.G.TaylorEmotion recognition in humanâcomputer interactionIEEE Signal<br />Processing Magazine 18 (1)2001 32â80.<br />[31] D.CrystalProsodic Systems and Intonation in English1969, Cambridge<br />University Press, Cambridge, 1969.<br />[32] D.W. Cunningham, M. Kleiner, H.H. BÃ¼lthoff, C. Wallraven, The components of<br />conversational facial expressions, in: Proceedings of the Symposium on<br />Applied Perception in Graphics and Visualization, 2004, pp. 143â150.<br />[33] J.R.Curhan, A.PentlandThin slices of negotiation: predicting outcomes from<br />conversational dynamics within the first 5 minutesJournal of Applied<br />Psychology 92 (3)2007 802â811.<br />[34] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in:<br />Proceedings of Conference on Computer Vision and Pattern Recognition, vol.<br />1, 2005, pp. 886â893.<br />[35] N. Dalal, B. Triggs, C. Schmid, Human detection using oriented histograms of<br />flow and appearance, in: Proceedings of the European Conference on<br />Computer Vision, 2006, pp. 428â441.<br />[36] T.Darrell, G.Gordon, M.Harville, J.WoodfillIntegrated person tracking using<br />stereo, color, and pattern detectionInternational Journal of Computer Vision<br />37 (2)2000 175â185.<br />[37] C. Darwin, The Expression of the Emotions in Man and Animals, J. Murray,<br />1872.<br />[38] R.De Silva, N.Bianchi-BerthouzeModeling human affective postures: an<br />informationtheoretic characterization<br />Computational Animation and Virtual World 15 (3â4)2004 269â276.<br />[39] A.Dielmann,S.RenalsAutomatic meeting<br />bayesian networksIEEE Transactions on Multimedia 9 (1)2007 25.<br />[40] L. Ding, A.M. Martinez, Recovering the linguistic components of the manual<br />signs in american sign language, in: Proceedings of IEEE International<br />Conference on Advanced Video and Signal-based Surveillance, 2007, pp.<br />447â452.<br />Psychology of InterpersonalBehaviour1967,Penguin,<br />N.YeeVirtual interpersonaltouchand digital<br />K.Patel,A.C.BeallDetecting digital<br />synthesisand the needfor some<br />P.E.R.BittiSocial presence,<br />of posturefeaturesJournalof<br />segmentationusing dynamic<br />1756<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 15</p> <p>[41] K.Dion, E.Berscheid, E.WalsterWhat is beautiful is goodJournal of Personality<br />and Social Psychology 24 (3)1972 285â290.<br />[42] W. Dong, B. Lepri, A. Cappelletti, A.S. Pentland, F. Pianesi, M. Zancanaro, Using<br />the influence model to recognize functional roles in meetings, in: Proceedings<br />of the International Conference on Multimodal Interfaces, 2007, pp. 271â278.<br />[43] N.Eagle, A.PentlandReality mining: sensing complex social signalsJournal of<br />Personal and Ubiquitous Computing 10 (4)2006 255â268.<br />[44] Y.Eisenthal, G.Dror,E.RuppinFacial<br />machineNeural Computation 18 (1)2005 119â142.<br />[45] P.Ekman (Ed.)Emotion in the Human Face1982, Cambridge University Press,<br />Cambridge, 1982.<br />[46] P.EkmanDarwin, deception, and facial expressionAnnals of the New York<br />Academy of Sciences 1000 (1)2003 205â221.<br />[47] P.Ekman, W.V.FriesenThe repertoire of nonverbal behaviorSemiotica 11969<br />49â98.<br />[48] P. Ekman, W.V. Friesen, J.C. Hager, Facial Action Coding System (FACS):<br />Manual, A Human Face, Salt Lake City, USA, 2002.<br />[49] P. Ekman, T.S. Huang, T.J. Sejnowski, J.C. Hager (Eds.), Final Report to NSF of<br />the Planning Workshop on Facial Expression Understanding, Human<br />Interaction Laboratory, University of California, San Francisco, 1993.<br />[50] P.Ekman, E.L.RosenbergWhat the Face Reveals: Basic and Applied Studies of<br />Spontaneous Expression Using the Facial Action Coding System (FACS)2005,<br />Oxford University Press, Oxford, 2005.<br />[51] R.R.Faden, T.L.Beauchamp, N.M.P.KingA History and Theory of Informed<br />Consent1986, Oxford University Press, Oxford, 1986.<br />[52] I.R.Fasel, B.Fortenberry, J.R.MovellanA generative framework for real time<br />object detection and classificationComputer Vision and Image Understanding<br />98 (1)2005 181â210.<br />[53] S. Favre, H. Salamin, J. Dines, A. Vinciarelli, Role recognition in multiparty<br />recordings using Social Affiliation Networks and discrete distributions, in:<br />Proceedings of the ACM International Conference on Multimodal Interfaces,<br />2008, pp. 29â36.<br />[54] D.A.Forsyth, O.Arikan, L.Ikemoto, J.OâBrien, D.RamananComputational studies<br />of human motion part 1: tracking and motion synthesisFoundations and<br />Trends in Computer Graphics and Vision 1 (2)2006 77â254.<br />[55] N.Fragopanagos,J.G.TaylorEmotion<br />interactionNeural Networks 18 (4)2005 389â405.<br />[56] M.G.Frank, P.EkmanAppearing truthful generalizes across different deception<br />situationsJournal of Personality and Social Psychology 86 (3)2004 486â495.<br />[57] T.Gandhi, M.M.TrivediPedestrian protection systems: issues, survey, and<br />challengesIEEE Transactions on Intelligent Transportation Systems 8 (3)2007<br />413â430.<br />[58] C.Garcia, G.TziritasFace detection using quantized skin color regions merging<br />and wavelet packet analysisIEEE Transactions on Multimedia 1 (3)1999 264â<br />277.<br />[59] N. Garg, S. Favre, H. Salamin, D. Hakkani-TÃ¼r, A. Vinciarelli, Role recognition<br />for meeting participants: an approach based on lexical information and social<br />network analysis, in: Proceedings of the ACM International Conference on<br />Multimedia, 2008, pp. 693â696.<br />[60] D. Gatica-Perez, I. McCowan, D. Zhang, S. Bengio, Detecting group interest-<br />level in meetings, in: Proceedings of IEEE International Conference on<br />Acoustics, Speech and Signal Processing, 2005, pp. 489â492.<br />[61] J.L. Gauvain, L.F. Lamel, G. Adda, Partitioning and transcription of broadcast<br />news data, in: Proceedings of International Conference on Spoken Language<br />Processing, 1998, pp. 1335â1338.<br />[62] D.M.GavrilaVisual analysis of human movement: a surveyComputer Vision<br />and Image Understanding 73 (1)1999 82â98.<br />[63] M.GladwellBlink: The Power of Thinking without Thinking2005, Little Brown<br />&amp; Company, Boston, NY, 2005.<br />[64] C.R.Glass, T.V.Merluzzi, J.L.Biever, K.H.LarsenCognitive assessment of social<br />anxiety: development and<br />questionnaireCognitive Therapy and Research 6 (1)1982 37â55.<br />[65] J.M.Gold, D.Tadin, S.C.Cook, R.B.BlakeThe efficiency of biological motion<br />perceptionPerception and Psychophysics 70 (1)2008 88â95.<br />[66] D. Goleman, Social Intelligence, Hutchinson, 2006.<br />[67] L.Gorelick, M.Blank, E.Shechtman, M.Irani, R.BasriActions as space-time<br />shapesIEEE Transactions on Pattern Analysis and Machine Intelligence 29<br />(12)2007 2247â2253.<br />[68] J.E.Grahe, F.J.BernieriTheimportance<br />rapportJournal of Nonverbal Behavior 23 (4)1999 253â269.<br />[69] K. Greene, 10 Emerging Technologies 2008, MIT Technology Review, February<br />2008.<br />[70] M.R.Greenwald, A.G.BanajiImplicit social cognition: attitudes, self-esteem,<br />and stereotypesPsychological Review 102 (1)1995 4â27.<br />[71] S.W.Gregory,K.Dagan, S.WebsterEvaluating<br />accommodation in conversation partners fundamental frequencies to<br />perceptions of communication qualityJournal of Nonverbal Behavior 21<br />(1)1997 23â43.<br />[72] M.M. Gross, E.A. Crane, B.L. Fredrickson, Effect of felt and recognized<br />emotions on body movements during walking, in: Proceedings of the<br />International Conference on the Expression of Emotions in Health and<br />Disease, 2007.<br />[73] H.Gunes, M.PiccardiAssessing facial beauty through proportion analysis by<br />image processing and supervised learningInternational Journal of Humanâ<br />Computer Studies 64 (12)2006 1184â1199.<br />attractiveness:beauty and the<br />recognition inhumanâcomputer<br />validationofa self-statement<br />ofnonverbal cuesinjudging<br />the relationof vocal<br />[74] H.Gunes, M.PiccardiBi-modal emotion recognition from expressive face and<br />body gesturesJournal of Network and Computer Applications 30 (4)2007<br />1334â1345.<br />[75] H. Gunes, M. Piccardi, T. Jan, Comparative beauty classification for pre-<br />surgery planning, in: Proceedings of the IEEE International Conference on<br />Systems, Man, and Cybernetics, 2004, pp. 2168â2174.<br />[76] H. Gunes, M. Piccardi, M. Pantic, From the lab to the real world: affect<br />recognition using multiple cues and modalities, in: J. Or (Ed.), Affective<br />Computing: Focus on Emotion Expression, Synthesis, and Recognition, 2008,<br />pp. 185â218.<br />[77] E.T. Hall, The Silent Language, Doubleday, 1959.<br />[78] J.B.Hayfron-Acquah, M.S.Nixon, J.N.CarterAutomatic gait recognition by<br />symmetry analysisPattern Recognition Letters 24 (13)2003 2175â2183.<br />[79] J.HirschbergPitch accent in context: predicting intonational prominence from<br />textArtificial Intelligence 63 (1â2)1993 305â340.<br />[80] J. Hirschberg, B. Grosz, Intonational features of local and global discourse<br />structure, in: Proceedings of the Speech and Natural Language Workshop,<br />1992, pp. 441â446.<br />[81] E.Hjelmas, B.K.LowFace detection: a surveyComputer Vision and Image<br />Understanding 83 (3)2001 236â274.<br />[82] C.R.L.Hsu, M.Abdel-Mottaleb, A.K.JainFace detection in colour imagesIEEE<br />Transactions on Pattern Analysis and Machine Intelligence 24 (5)2002 696â<br />706.<br />[83] K.S. Huang, M.M. Trivedi, Robust real-time detection, tracking, and pose<br />estimation of faces in video streams, in: Proceedings of Conference on<br />Computer Vision and Pattern Recognition, 2004, pp. 965â968.<br />[83] X.Huang, A.Acero, H.W.HonSpoken Language Processing2001, Prentice-Hall,<br />Englewood Cliffs, NJ, 2001.<br />[85] H. Hung, D. Jayagopi, C. Yeo, G. Friedland, S. Ba, J.M. Odobez, K. Ramchandran,<br />N. Mirghafori, D. Gatica-Perez, Using audio and video features to classify the<br />most dominant person in a group meeting, in: Proceedings of the ACM<br />International Conference on Multimedia, 2007, pp. 835â838.<br />[86] A. Ito, X. Wang, M. Suzuki, S. Makino, Smile and laughter recognition using<br />speech processing and face recognition from conversation video, in:<br />Proceedings of the International Conference on Cyberworlds, 2005, pp.<br />437â444.<br />[87] Y. Ivanov, C. Stauffer, A. Bobick, W.E.L. Grimson, Video surveillance of<br />interactions, in: Proceedings of the Workshop on Visual Surveillance at<br />Computer Vision and Pattern Recognition, 1999.<br />[88] A. Jaimes, K. Omura, T. Nagamine, K. Hirata, Memory cues for meeting video<br />retrieval, in: Proceedings of Workshop on Continuous Archival and Retrieval<br />of Personal Experiences, 2004, pp. 74â85.<br />[89] D. Keltner, P. Ekman, Facial expression of emotion, in: M. Lewis, J.M.<br />Haviland-Jones (Eds.), Handbook of Emotions, 2000, pp. 236â249.<br />[90] D.Keltner, J.HaidtSocialfunctions<br />analysisCognition and Emotion 13 (5)1999 505â521.<br />[91] D.Keltner, A.M.KringEmotion, social function, and psychopathologyReview of<br />General Psychology 2 (3)1998 320â342.<br />[92] L. Kennedy, D. Ellis, Laughter detection in meetings, in: Proceedings of the<br />NIST Meeting Recognition Workshop, 2004.<br />[93] M.Kimura, I.DaiboInteractional synchrony in conversations about emotional<br />episodes: a measurement by the between-participants pseudosynchrony<br />experimental paradigmJournal of Nonverbal Behavior 30 (3)2006 115â126.<br />[94] J.Kittler, M.Hatef, R.P.W.Duin,<br />Transactions on Pattern Analysis and Machine Intelligence 20 (3)1998 226â<br />239.<br />[95] A.Kleinsmith, R.De Silva, N.Bianchi-BerthouzeCross-cultural differences in<br />recognizing affect from body postureInteracting with Computers 18 (6)2006<br />1371â1389.<br />[96] M.L.Knapp, J.A.HallNonverbal Communication in Human Interaction1972,<br />Harcourt Brace College Publishers, New York, 1972.<br />[97] W.W. Kong, S. Ranganath, Automatic hand trajectory segmentation and<br />phoneme transcriptionfor sign<br />International Conference on Automatic Face and Gesture Recognition, 2008.<br />[98] Z.KundaSocial Cognition1999, MIT Press, Cambridge, MA, 1999.<br />[99] J.L.Lakin, V.E.Jefferis, C.M.Cheng, T.L.ChartrandThe Chameleon effect as social<br />glue:Evidence for theevolutionary<br />mimicryJournal of Nonverbal Behavior 27 (3)2003 145â162.<br />[100] L. Lee, W.E.L. Grimson, Gait analysis for recognition and classification, in:<br />Proceedings of the IEEE International Conference on Automatic Face and<br />Gesture Recognition, 2002, pp. 148â155.<br />[101] T.K. Leung, M.C. Burl, P. Perona, Finding faces in cluttered scenes using<br />random labeled graphmatching, in: Proceedings of the International<br />Conference on Computer Vision, 1995, pp. 637â644.<br />[102] X.Li, S.J.Maybank, S.Yan, D.Tao, D.XuGait components and their application to<br />gender recognitionIEEE Transactions on Systems, Man, and Cybernetics, Part<br />C: Applications and Reviews 38 (2)2008 145â155.<br />[103] G.Littlewort, M.S.Bartlett, I.Fasel, J.Susskind, J.MovellanDynamics of facial<br />expression extracted automatically from videoImage and Vision Computing<br />24 (6)2006 615â625.<br />[104] G.C. Littlewort, M.S. Bartlett, K. Lee, Faces of pain: automated measurement<br />of spontaneous facial expressions of genuine and posed pain, in: Proceedings<br />of the International Conference on Multimodal Interfaces, 2007, pp. 15â21.<br />[105] Y. Liu, E. Shriberg, A. Stolcke, M. Harper, Comparing HMM, maximum<br />entropy, and conditional random fields for disfluency detection, in:<br />ofemotions atfourlevels of<br />J.MatasOncombining classifiersIEEE<br />language,in: Proceedingsof IEEE<br />significance of nonconscious<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1757</p>  <p>Page 16</p> <p>Proceedings of the European Conference on Speech Communication and<br />Technology, 2005.<br />[106] D.F.Lott, R.SommerSeating arrangements and statusJournal of Personality and<br />Social Psychology 7 (1)1967 90â95.<br />[107] L.Lu,H.J.Zhang, H.JiangContent<br />segmentationIEEE Transactions on Speech and Audio Processing 10 (7)2002<br />504â516.<br />[108] S.Lucey, A.B.Ashraf, J.CohnInvestigating spontaneous facial action recognition<br />through AAM representations<br />(Eds.)Handbook of Face Recognition2007, I-Tech Education and Publishing,<br />2007, pp. 275â286.<br />[109] L.Z.McArthur,R.M.BaronToward<br />perceptionPsychological Review 90 (3)1983 215â238.<br />[110] I. McCowan, S. Bengio, D. Gatica-Perez, G. Lathoud, F. Monay, D. Moore, P.<br />Wellner, H. Bourlard, Modeling human interaction in meetings, in:<br />Proceedings of IEEE International Conference on Acoustics, Speech and<br />Signal Processing, 2003, pp. 748â751.<br />[111] I.McCowan, D.Gatica-Perez,<br />D.ZhangAutomatic analysis of multimodal group actions in meetingsIEEE<br />Transactions on Pattern Analysis and Machine Intelligence 27 (3)2005 305â<br />317.<br />[112] D.McNeillHand and Mind: What Gestures Reveal about Thought1996,<br />University Of Chicago Press, IL, 1996.<br />[113] A.Mehrabian, S.R.FerrisInference of attitude from nonverbal communication<br />in two channelsJournal of Counseling Psychology 31 (3)1967 248â252.<br />[114] K. Mikolajczyk, C. Schmid, A. Zisserman, Human detection based on a<br />probabilistic assembly of robust part detectors, in: Proceedings of the<br />European Conference on Computer Vision, 2004, pp. 69â81.<br />[115] T.B.Moeslund, E.GranumA survey of computer vision-based human motion<br />captureComputer Vision and Image Understanding 81 (3)2001 231â268.<br />[116] T.B.Moeslund, A.Hilton, V.KrugerA survey of advances in vision-based human<br />motion capture and analysisComputer Vision and Image Understanding 104<br />(2â3)2006 90â126.<br />[117] Y. Moh, P. Nguyen, J.C. Junqua, Towards domain independent speaker<br />clustering, in: Proceedings of the IEEE International Conference on<br />Acoustics, Speech, and Signal Processing, 2003, pp. 85â88.<br />[118] S.MÃ¶ller, R.SchÃ¶nweilerAnalysis of infant cries for the early detection of<br />hearing impairmentSpeech Communication 28 (3)1999 175â193.<br />[119] P.R.Montague, G.S.Berns, J.D.Cohen, S.M.McClure, G.Pagnoni, M.Dhamala,<br />M.C.Wiest,I.Karpov,R.D.King,<br />simultaneous fMRI during linked social interactionsNeuroimage 16 (4)2002<br />1159â1164.<br />[120] B.C.J.MooreAn Introduction to the Psychology of Hearing1982, Academic<br />Press, New York, 1982.<br />[121] N. Morgan, E. Fosler, N. Mirghafori. Speech recognition using on-line<br />estimation of speaking rate, in: Proceedings of Eurospeech, 1997, pp. 2079â<br />2082.<br />[122] N. Morgan, E. Fosler-Lussier, Combining multiple estimators of speaking rate,<br />in: Proceedings of IEEE International Conference on Acoustics, Speech, and<br />Signal Processing, 1998, pp. 729â732.<br />[123] D.MorrisPeoplewatching2007, Vintage, NY, 2007.<br />[124] S. Mota, R.W. Picard, Automated posture analysis for detecting learners<br />interest level, in: Proceedings of Conference on Computer Vision and Pattern<br />Recognition, 2003, pp. 49â56.<br />[125] S. Mukhopadhyay, B. Smith, Passive capture and structuring of lectures, in:<br />Proceedings of the ACM International Conference on Multimedia, 1999, pp.<br />477â487.<br />[126] K.P. Murphy, Dynamic Bayesian Networks: Representation, Inference and<br />Learning, Ph.D. thesis, University of California Berkeley, 2002.<br />[127] C.Nass, K.M.LeeDoes computer-synthesized speech manifest personality?<br />Experimental tests of recognition, similarity-attraction, and consistency-<br />attractionJournal of Experimental Psychology: Applied 7 (3)2001 171â181.<br />[128] C.Nass, J.SteuerComputers and social actorsHuman Communication Research<br />19 (4)1993 504â527.<br />[129] I.Oikonomopoulos, I.Patras, M.PanticSpatiotemporal salient points for visual<br />recognition of human actionsIEEE Transactions on Systems, Man, and<br />Cybernetics â Part B 36 (3)2006 710â719.<br />[130] N.Oliver, A.Garg, E.HorvitzLayered representations for learning and inferring<br />office activity from multiple sensory channelsComputer Vision and Image<br />Understanding 96 (2)2004 163â180.<br />[131] A.J.OâToole, T.Price, T.Vetter, J.C.Bartlett, V.Blanz3D shape and 2D surface<br />textures of human faces: the role of averages in attractiveness and ageImage<br />and Vision Computing 18 (1)1999 9â19.<br />[132] S.OviattUser-centeredmodeling<br />interfacesProceedings of the IEEE 912003 1457â1468.<br />[133] S.Oviatt, C.Darves, R.CoulstonToward adaptive conversational interfaces:<br />modeling speech convergence with animated personasACM Transactions on<br />ComputerâHuman Interaction 11 (3)2004 300â328.<br />[134] P. Pal, A.N. Iyer, R.E. Yantorno, in: Proceedings of the IEEE International<br />Conference on Acoustics, Speech and Signal Processing, vol. 2, 2006, pp. 721â<br />724.<br />[135] M.Pantic, M.S.BartlettMachine analysis of facial expressions, in: K.Delac,<br />M.Grgic (Eds.)Handbook of Face Recognition2007, I-Tech Education and<br />Publishing, 2007, pp. 377â416.<br />[136] M.Pantic, I.PatrasDynamics of facial expression: recognition of facial actions<br />and their temporal segments from face profile image sequencesIEEE<br />analysisfor audioclassificationand<br />of the face, in:K.Delac,M.Grgic<br />an ecological theoryof social<br />S.Bengio,G.Lathoud,M.Barnard,<br />N.Apple, R.E.FisherHyperscanning:<br />and evaluationof multimodal<br />Transactions on Systems, Man, and Cybernetics â Part B: Cybernetics 36<br />(2)2006 433â449.<br />[137] M.Pantic, A.Pentland, A.Nijholt, T.HuangHuman computing and machine<br />understanding of human behavior: a surveyLecture Notes in Artificial<br />Intelligence, vol. 44512007, Springer-Verlag, Berlin, 2007, pp. 47â71.<br />[138] M.Pantic, A.Pentland, A.Nijholt, T.HuangHuman-centred intelligent humanâ<br />computer interaction (HCI2): how far are we from attaining it?International<br />Journal of Autonomous and Adaptive Communications Systems 1 (2)2008<br />168â187<br />[139] M.Pantic, L.J.M.RothkrantzAutomatic analysis of facial expressions: the state<br />of the artIEEE Transactions on Pattern Analysis and Machine Intelligence 22<br />(12)2000 1424â1445.<br />[140] M.Pantic, L.J.M.RothkrantzToward an affect-sensitive multimodal humanâ<br />computer interactionProceedings of the IEEE 91 (9)2003 1370â1390.<br />[141] C. Pelachaud, V. Carofiglio, B. De Carolis, F. de Rosis, I. Poggi, Embodied<br />contextual agent in information delivering application, in: Proceedings of the<br />International Joint Conference on Autonomous Agents and Multiagent<br />Systems, 2002, pp. 758â765.<br />[142] A. Pentland, Social dynamics: signals and behavior, in: International<br />Conference on Developmental Learning, 2004.<br />[143] A.PentlandSocially aware computation and communicationIEEE Computer 38<br />(3)2005 33â40.<br />[144] A.PentlandAutomatic mapping and modeling of human networksPhysica A<br />3782007 59â67.<br />[145] A.PentlandSocial signal processingIEEE Signal Processing Magazine 24<br />(4)2007 108â111.<br />[146] A.PentlandHonest Signals: How They Shape Our World2008, MIT Press,<br />Cambridge, MA, 2008.<br />[147] S. Petridis, M. Pantic, Audiovisual discrimination between laughter and<br />speech, in: Proceedings of IEEE International Conference on Acoustics,<br />Speech, and Signal Processing, 2008, pp. 5117â5121.<br />[148] S. Petridis, M. Pantic, Audiovisual laughter detection based on temporal<br />features, in: Proceedings of IEEE International Conference on Multimodal<br />Interfaces, 2008, pp. 37â44.<br />[149] T. Pfau, G. Ruske, Estimating the speaking rate by vowel detection, in:<br />Proceedings of the IEEE International Conference on Acoustics, Speech, and<br />Signal Processing, 1998, pp. 945â948.<br />[150] J.O.PicklesAn Introduction to the Physiology of Hearing1982, Academic Press,<br />New York, 1982.<br />[151] F.E.Pollick, V.Lestou, J.Ryu, S.B.ChoEstimating the efficiency of recognizing<br />gender and affect from biological motionVision Research 422002 2345â2355.<br />[152] F.E.Pollick, H.M.Paterson, A.Bruderlin, A.J.SanfordPerceiving affect from arm<br />movementCognition 82 (2)2001 51â61.<br />[153] R.PoppeVision-based human motion analysis: an overviewComputer Vision<br />and Image Understanding 1082007 4â18.<br />[154] G.PsathasConversation Analysis â The Study of Talk-in-Interaction1995, Sage<br />Publications, Beverley Hills, 1995.<br />[155] L. Rabiner, M. Sambur, Voiced-unvoiced-silence detection using the Itakura<br />LPC distance measure, in: Proceedings of the IEEE International Conference<br />on Acoustics, Speech, and Signal Processing, 1977, pp. 323â326.<br />[156] L.R.Rabiner, R.W.SchaferDigital Processing of Speech Signals1978, Prentice-<br />Hall, Englewood Cliffs, NJ, 1978.<br />[157] D.A. Reynolds, W. Campbell, T.T. Gleason, C. Quillen, D. Sturim, P. Torres-<br />Carrasquillo, A. Adami, The 2004 MIT Lincoln laboratory speaker recognition<br />system, in: Proceedings of IEEE International Conference on Acoustics,<br />Speech, and Signal Processing, 2005, pp. 177â180.<br />[158] V.P.Richmond, J.C.McCroskeyNonverbal<br />relations1995, Allyn and Bacon, Bacon, NY, 1995.<br />[159] R.Rienks, D.HeylenDominance detection in meetings using easily obtainable<br />featuresLecture Notes in Computer Science, vol. 38692006, Springer, Berlin,<br />2006, pp. 76â86.<br />[160] R. Rienks, D. Zhang, D. Gatica-Perez, Detection and application of influence<br />rankings in small group meetings, in: Proceedings of the International<br />Conference on Multimodal Interfaces, 2006, pp. 257â264.<br />[161] H.A.Rowley, S.Baluja, T.KanadeNeural network-based face detectionIEEE<br />Transactions on Pattern Analysis and Machine Intelligence 20 (1)1998 23â38.<br />[162] J.A.Russell, J.A.Bachorowski, J.M.Fernandez-DolsFacial and vocal expressions<br />of emotionAnnual Reviews in Psychology 54 (1)2003 329â349.<br />[163] J.A.Russell,J.M.Fernandez-Dols<br />Expression1997, Cambridge University Press, Cambridge, 1997.<br />[164] N.RussoConnotation of seating arrangementsCornell Journal of Social<br />Relations 2 (1)1967 37â44.<br />[165] M.A.Sayette, D.W.Smith, M.J.Breiner, G.T.WilsonThe effect of alcohol on<br />emotional response to a social stressorJournal of Studies on Alcohol 53<br />(6)1992 541â545.<br />[166] A.E.ScheflenThe significance of posture in communication systemsPsychiatry<br />271964 316â331.<br />[167] K.R.SchererPersonality markers in speech1979, Cambridge University Press,<br />Cambridge, 1979.<br />[168] K.R.SchererVocalcommunication<br />paradigmsSpeech Communication 40 (1â2)2003 227â256.<br />[169] H. Schneiderman, T. Kanade, A statistical model for 3D object detection<br />applied to faces and cars, in: Proceedings of Conference on Computer Vision<br />and Pattern Recognition, 2000, pp. 746â751.<br />[170] B. Schuller, R. MÃ¼eller, B. HÃ¶ernler, A. HÃ¶ethker, H. Konosu, G. Rigoll,<br />Audiovisual recognition of spontaneous interest within conversations, in:<br />Behaviors in interpersonal<br />(Eds.)The Psychologyof Facial<br />of emotion:a reviewofresearch<br />1758<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759</p>  <p>Page 17</p> <p>Proceedings of the International Conference on Multimodal Interfaces, 2007,<br />pp. 30â37.<br />[171] U.Segerstrale, P.Molnar (Eds.)Nonverbal Communication: Where Nature<br />Meets Culture1997, Lawrence Erlbaum Associates, London, 1997.<br />[172] A.Sepheri, Y.Yacoob, L.DavisEmploying the hand as an interface deviceJournal<br />of Multimedia 1 (7)2006 18â29.<br />[173] E.ShribergPhonetic consequences of speech disfluencyProceedings of the<br />International Congress of Phonetic Sciences 11999 619â622.<br />[174] E. Shriberg, R. Bates, A. Stolcke, A prosody-only decision-tree model for<br />disfluency detection, in: Proceedings of Eurospeech, 1997, pp. 2383â2386.<br />[175] E. Shriberg, A. Stolcke, D. Baron, Observations of overlap: findings and<br />implications for automatic processing of multiparty conversation, in:<br />Proceedings of Eurospeech, 2001, pp. 1359â1362.<br />[176] P.E.Shrout, D.W.FiskeNonverbal behaviors and social evaluationJournal of<br />Personality 49 (2)1981 115â128.<br />[177] K. SjÃ¶lander, J. Beskow, Wavesurfer â an open source speech tool, in:<br />Proceedings of International Conference on Spoken Language Processing,<br />2000, pp. 464â467.<br />[178] A.F. Smeaton, P. Over, W. Kraaij, Evaluation campaigns and TRECVid, in:<br />Proceedings of the ACM International Workshop on Multimedia Information<br />Retrieval, 2006, pp. 321â330.<br />[179] C.Sminchisescu, A.Kanaujia, D.MetaxasConditional models for contextual<br />human motion recognitionComputer Vision and Image Understanding 104<br />(2â3)2006 210â220.<br />[180] L.Smith-Lovin, C.BrodyInterruptions in group discussions: the effects of gender<br />and group compositionAmerican Sociological Review 54 (3)1989 424â435.<br />[181] K.K.Sung, T.PoggioExample-based learning for view-based human face<br />detectionIEEE Transactions on Pattern Analysis and Machine Intelligence 20<br />(1)1998 39â51.<br />[182] E.L.ThorndikeIntelligence and its useHarperâs Magazine 1401920 227â235.<br />[183] C.ThurauBehaviorhistogramsfor<br />detectionLecture Notes in Computer Science, vol. 48142007, Springer-<br />Verlag, Berlin, 2007, pp. 271â284.<br />[184] C. Thurau, V. Hlavac, Pose primitive based human action recognition in videos<br />or still images, in: Proceedings of the IEEE International Conference on<br />Computer Vision and Pattern Recognition, 2008, pp. 1â6.<br />[185] Y. Tian, T. Kanade, J.F. Cohn, Facial expression analysis, in: S.Z. Li, A.K. Jain<br />(Eds.), Handbook of Face Recognition, 2005, pp. 247â276.<br />[186] H.L.TischlerIntroduction to Sociology1990, Harcourt Brace College Publishers,<br />New York, 1990.<br />[187] Y.Tong, W.Liao, Q.JiFacial action unit recognition by exploiting their dynamic<br />and semantic relationshipsIEEE Transactions on Pattern Analysis and<br />Machine Intelligence 29 (10)2007 1683â1699.<br />[188] D. Tran, A. Sorokin, D.A. Forsyth, Human activity recognition with metric<br />learning,in:Proceedings ofthe EuropeanConferenceonComputerVision,2008.<br />[189] S.E.Tranter, D.A.ReynoldsAn overview of automatic speaker diarization<br />systemsIEEE Transactions on Audio, Speech, and Language Processing 14<br />(5)2006 1557â1565.<br />[190] H.C.TriandisCulture and Social Behavior1994, McGraw-Hill, New York, 1994.<br />[191] K.P. Truong, D.A. Leeuwen, Automatic detection of laughter, in: Proceedings<br />of the European Conference on Speech Communication and Technology,<br />2005, pp. 485â488.<br />[192] K.P.Truong, D.A.van LeeuwenAutomatic discrimination between laughter and<br />speechSpeech Communication 49 (2)2007 144â158.<br />[193] L.Q.Uddin, M.Iacoboni, C.Lange, J.P.KeenanThe self and social cognition: the<br />role of cortical midline structures and mirror neuronsTrends in Cognitive<br />Sciences 11 (4)2007 153â157.<br />[194] A. Utsumi, N. Tetsutani, Human detection using geometrical pixel value<br />structures, in: Proceedings of the IEEE International Conference on Automatic<br />Face and Gesture Recognition, 2002, pp. 34â39.<br />[195] M.F. Valstar, H. Gunes, M. Pantic, How to distinguish posed from spontaneous<br />smiles using geometric features, in: Proceedings of the International<br />Conference on Multimodal Interfaces, 2007, pp. 38â45.<br />[196] M.F. Valstar, M. Pantic, Fully automatic facial action unit detection and<br />temporal analysis, in: Proceedings of the International Conference on<br />Computer Vision and Pattern Recognition Workshop, 2006, pp. 149â150.<br />[197] M.F. Valstar, M. Pantic, Z. Ambadar, J.F. Cohn, Spontaneous vs. posed facial<br />behavior: automatic analysis of brow actions, in: Proceedings of the<br />International Conference on Multimodal Interfaces, 2006, pp. 162â170.<br />action recognition andhuman<br />[198] J.Van den Stock, R.Righart, B.de GelderBody expressions influence recognition<br />of emotions in the face and voiceEmotion 7 (3)2007 487â494.<br />[199] R. Vertegaal, R. Slagter, G. van der Veer, A. Nijholt, Eye gaze patterns in<br />conversations: there is more to conversational agents than meets the eyes,<br />in: Proceedings of the SIGCHI Conference on Human Factors in Computing<br />Systems, 2001, pp. 301â308.<br />[200] A.VinciarelliSpeakers role recognition in multiparty audio recordings using<br />social network analysis and duration distribution modelingIEEE Transactions<br />on Multimedia 9 (9)2007 1215â1226.<br />[201] A.Vinciarelli, J.-M.OdobezApplication of information retrieval technologies to<br />presentation slidesIEEE Transactions on Multimedia 8 (5)2006 981â995.<br />[202] A. Vinciarelli, M. Pantic, H. Bourlard, A. Pentland, Social signal processing:<br />state-of-the-art and future perspectives of an emerging domain, in:<br />Proceedings of the ACM International Conference on Multimedia, 2008, pp.<br />1061â1070.<br />[203] A. Vinciarelli, M. Pantic, H. Bourlard, A. Pentland, Social signals, their function,<br />and automatic analysis: a survey, in: Proceedings of the ACM International<br />Conference on Multimodal Interfaces, 2008, pp. 61â68.<br />[204] P.Viola, M.JonesRobust real-time face detectionComputer Vision 57 (2)2004<br />137â154.<br />[205] A. Waibel, T. Schultz, M. Bett, M. Denecke, R. Malkin, I. Rogina, R. Stiefelhagen.<br />SMaRT: the Smart Meeting Room task at ISL, in: Proceedings of IEEE<br />International Conference on Acoustics, Speech, and Signal Processing, 2003,<br />pp. 752â755.<br />[206] L.Wang, W.Hu, T.TanRecent developments in human motion analysisPattern<br />Recognition 36 (3)2003 585â601.<br />[207] P. Wang, Q. Ji, Multi-view face detection under complex scene based on<br />combined SVMs, in: Proceedings of International Conference on Pattern<br />Recognition, 2004, pp. 179â182.<br />[208] R.M.Warner, D.B.SugarmanAttributions of personality based on physical<br />appearance, speech, and handwritingJournal of Personality and Social<br />Psychology 50 (4)1986 792â799.<br />[209] S.Wasserman,K.FaustSocialNetwork<br />Applications1994, Cambridge University Press, Cambridge, 1994.<br />[210] C.Y. Weng, W.T. Chu, J.L. Wu, Movie analysis based on roles social network,<br />in: Proceedings of IEEE International Conference on Multimedia and Expo,<br />2007, pp. 1403â1406.<br />[211] J. Whitehill, J.R. Movellan, Personalized facial attractiveness prediction, in:<br />Proceedings of the IEEE International Conference on Automatic Face and<br />Gesture Recognition, 2008.<br />[212] A.C.C.WilliamsFacial expression of pain: an evolutionary accountBehavioral<br />and Brain Sciences 25 (4)2003 439â455.<br />[213] Y. Wu, T.S. Huang. Vision-based gesture recognition: a review, in:<br />Proceedings of the International Gesture Workshop, 1999, pp. 103â109.<br />[214] Y.Yacoob, L.DavisDetection and analysis of hairIEEE Transactions on Pattern<br />Analysis and Machine Intelligence 28 (7)2006 1164â1169.<br />[215] M.H.Yang, D.Kriegman, N.AhujaDetecting faces in images: a surveyIEEE<br />Transactions on Pattern Analysis and Machine Intelligence 24 (1)2002 34â58.<br />[216] J. Yao, J.-M. Odobez, Fast human detection from videos using covariance<br />features, in: Proceedings of European Conference on Computer Vision Visual<br />Surveillance Workshop, 2008.<br />[217] G.YulePragmatics1996, Oxford University Press, Oxford, 1996.<br />[218] M. Zancanaro, B. Lepri, F. Pianesi, Automatic detection of group functional<br />roles in face to face interactions, in: Proceedings of the International<br />Conference on Multimodal Interfaces, 2006, pp. 28â34.<br />[219] B.ZellnerPausesandthe temporal<br />(Ed.)Fundamentals of Speech Synthesis and Speech Recognition1994, John<br />Wiley &amp; Sons, 1994, pp. 41â62.<br />[220] Z.Zeng, Y.Fu, G.I.Roisman, Z.Wen, Y.Hu, T.S.HuangSpontaneous emotional<br />facial expression detectionJournal of Multimedia 1 (5)2006 1â8.<br />[221] Z.Zeng, M.Pantic, G.I.Roisman, T.H.HuangA survey of affect recognition<br />methods: audio, visual and spontaneous expressionsIEEE Transactions on<br />Pattern Analysis and Machine Intelligence 31 (1)2009 39â58.<br />[222] Y.Zhang, Q.JiActive and dynamic information fusion for facial expression<br />understanding from image sequencesIEEE Transactions on Pattern Analysis<br />and Machine Intelligence 27 (5)2005 699â714.<br />[223] Q. Zhu, S. Avidan, M.C. Yeh, K.T. Cheng, Fast human detection using a<br />cascadeofhistograms oforiented<br />Computer Vision and Pattern Recognition, 2006, pp. 1491â1498.<br />Analysis:Methodsand<br />structureofspeech, in: E.Keller<br />gradients,in: Proceedingsof<br />A. Vinciarelli et al./Image and Vision Computing 27 (2009) 1743â1759<br />1759</p>  <a href="https://www.researchgate.net/profile/Alessandro_Vinciarelli2/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain/links/541a1ae10cf2218008bfa710.pdf">Download full-text</a> </div> <div id="rgw22_56ab9d884fd5f" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab9d884fd5f">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56ab9d884fd5f"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Alessandro_Vinciarelli2/publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain/links/541a1ae10cf2218008bfa710.pdf" class="publication-viewer" title="download.pdf">download.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Alessandro_Vinciarelli2">Alessandro Vinciarelli</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab9d884fd5f"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.1208&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Social Signal Processing: Survey of an Emerging Domain">Social Signal Processing: Survey of an Emerging Do...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.159.1208&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw32_56ab9d884fd5f" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  <small> (296)  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (298) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw33_56ab9d884fd5f" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw34_56ab9d884fd5f" >  <div class="indent-left">  <div id="rgw35_56ab9d884fd5f" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Milos_Cerak" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: MiloÅ¡ CerÅak </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw36_56ab9d884fd5f">  <li class="citation-context-item"> "Human laugh is a crucial social signal due to the range of inner meanings it carries. This social signaling event [1] may denote the topical changes, communication synchrony and positive affect; on the other hand, it may also show disagreement or satirist views. Therefore, automatic human laugh occurrence or laughter detection in speech may have many applications in spoken dialog and discourse analysis. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link ga-publication-item" href="publication/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings"> <span class="publication-title js-publication-title">An Analysis of Rhythmic Staccato-Vocalization Based on Frequency Demodulation for Laughter Detection in Conversational Meetings</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2092800051_Sucheta_Ghosh" class="authors js-author-name ga-publications-authors">Sucheta Ghosh</a> &middot;     <a href="researcher/2092878740_Milos_Cernak" class="authors js-author-name ga-publications-authors">Milos Cernak</a> &middot;     <a href="researcher/2024915222_Sarbani_Palit" class="authors js-author-name ga-publications-authors">Sarbani Palit</a> &middot;     <a href="researcher/2087625501_B_B_Chaudhuri" class="authors js-author-name ga-publications-authors">B. B. Chaudhuri</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Human laugh is able to convey various kinds of meanings in human
communications. There exists various kinds of human laugh signal, for example:
vocalized laugh and non vocalized laugh. Following the theories of psychology,
among all the vocalized laugh type, rhythmic staccato-vocalization
significantly evokes the positive responses in the interactions. In this paper
we attempt to exploit this observation to detect human laugh occurrences, i.e.,
the laughter, in multiparty conversations from the AMI meeting corpus. First,
we separate the high energy frames from speech, leaving out the low energy
frames through power spectral density estimation. We borrow the algorithm of
rhythm detection from the area of music analysis to use that on the high energy
frames. Finally, we detect rhythmic laugh frames, analyzing the candidate
rhythmic frames using statistics. This novel approach for detection of
`positive&#39; rhythmic human laughter performs better than the standard laughter
classification baseline. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2016  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Milos_Cerak/publication/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings/links/56a66fa208ae44a674fe1e1d.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw37_56ab9d884fd5f" >  <div class="indent-left">  <div id="rgw38_56ab9d884fd5f" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS2351978915009798%2Fpdf%3Fmd5%3Df9cd871b7d6041095672d440d95c7122%26pid%3D1-s2.0-S2351978915009798-main.pdf" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: sciencedirect.com </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw39_56ab9d884fd5f">  <li class="citation-context-item"> "2 Based on these measurements, various aggregate statistics were computed on a frame by frame basis resulting in measures of four visual analytics, namely affect, proximity, engagement and body motion. These visual analytics are normalized to the scale of [0] [1]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link ga-publication-item" href="publication/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods"> <span class="publication-title js-publication-title">Cross-cultural Training Analysis Via Social Science and Computer Vision Methods</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70071592_Peter_Tu" class="authors js-author-name ga-publications-authors">Peter Tu</a> &middot;     <a href="researcher/2026807091_Jixu_Chen" class="authors js-author-name ga-publications-authors">Jixu Chen</a> &middot;     <a href="researcher/2083964740_Ming-Ching_Chang" class="authors js-author-name ga-publications-authors">Ming-Ching Chang</a> &middot;     <a href="researcher/2084818394_Ting_Yu" class="authors js-author-name ga-publications-authors">Ting Yu</a> &middot;     <a href="researcher/2084884006_Tai-Peng_Tian" class="authors js-author-name ga-publications-authors">Tai-Peng Tian</a> &middot;     <a href="researcher/2084683768_Gabriela_Rubin" class="authors js-author-name ga-publications-authors">Gabriela Rubin</a> &middot;     <a href="researcher/2084883118_Julia_Hockett" class="authors js-author-name ga-publications-authors">Julia Hockett</a> &middot;     <a href="researcher/2069356946_Aubrey_Logan-Terry" class="authors js-author-name ga-publications-authors">Aubrey Logan-Terry</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Computer Vision technology is an invaluable addition to cross-cultural communication training for military personnel. It allows trainers to assess trainees in real time and provide feedback grounded in social science research. The present study reports on a joint analysis of military cross-cultural training data by Computer Vision specialists from GE Global Research as well as analyses from Georgetown University&#39;s Social Interaction Research Group (SIRG). Data for this study were collected over 10 days at the Army Infantry Basic Officer Leaders Course (IBOLC). 80 lieutenants participated in classroom role-play scenarios designed to assess their ability to communicate cross-culturally. GE and SIRG researchers video-recorded interactions among the role players and Soldiers and correlations were observed between these automatic interpretations and those gleaned by the SIRG analysis team in order to augment understanding of the efficacy of the cross-cultural training. For the Computer Vision methods, Each person was represented as a stream of visual cues which include: position, articulated motion, facial expressions and gaze directions. The social science researchers conducted multimodal (including embodied elements such as eye gaze, hand gestures, and body positioning), mixed method (qualitative and quantitative) discourse analyses of the data. SIRG researchers developed a coding scheme, marking specific human behavioral features within each interaction. From such coding, SIRG identified key skills in cross-cultural interaction, including observation and adaptation to unfamiliar communicative norms, rapport building, and trouble recovery (for details see Logan-Terry &amp; Damari, forthcoming). Various correlations between raw computer vision measurements and the social science coding scheme was observed. Such results represent a significant step towards establishing the efficacy of the joint analysis of automated Computer Vision and established social science methods with regards to complex social interaction analysis. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Dec 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  " href="publication/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw40_56ab9d884fd5f" >  <div class="indent-left">  <div id="rgw41_56ab9d884fd5f" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281809119_Learning_Social_Relation_Traits_from_Face_Images">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Zhanpeng_Zhang2" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Zhanpeng Zhang </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw42_56ab9d884fd5f">  <li class="citation-context-item"> "Social signal processing. Understanding social relation is an important research topic in social signal processing [4] [29] [30] [36] [37], an important multidisciplinary problem that has attracted a surge of interest from computer vision community. Social signal processing mainly involves facial expression recognition [23] and affective behaviour analysis [28]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link ga-publication-item" href="publication/281809119_Learning_Social_Relation_Traits_from_Face_Images"> <span class="publication-title js-publication-title">Learning Social Relation Traits from Face Images</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2047898358_Zhanpeng_Zhang" class="authors js-author-name ga-publications-authors">Zhanpeng Zhang</a> &middot;     <a href="researcher/2046395825_Ping_Luo" class="authors js-author-name ga-publications-authors">Ping Luo</a> &middot;     <a href="researcher/13950471_Chen_Change_Loy" class="authors js-author-name ga-publications-authors">Chen Change Loy</a> &middot;     <a href="researcher/39490087_Xiaoou_Tang" class="authors js-author-name ga-publications-authors">Xiaoou Tang</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Social relation defines the association, e.g, warm, friendliness, and
dominance, between two or more people. Motivated by psychological studies, we
investigate if such fine-grained and high-level relation traits can be
characterised and quantified from face images in the wild. To address this
challenging problem we propose a deep model that learns a rich face
representation to capture gender, expression, head pose, and age-related
attributes, and then performs pairwise-face reasoning for relation prediction.
To learn from heterogeneous attribute sources, we formulate a new network
architecture with a bridging layer to leverage the inherent correspondences
among these datasets. It can also cope with missing target attribute labels.
Extensive experiments show that our approach is effective for fine-grained
social relation learning in images and videos. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Sep 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Zhanpeng_Zhang2/publication/281809119_Learning_Social_Relation_Traits_from_Face_Images/links/5601394d08aeafc8ac8c8547.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw27_56ab9d884fd5f" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw28_56ab9d884fd5f">  </ul> </div> </div>   <div id="rgw18_56ab9d884fd5f" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw19_56ab9d884fd5f"> <div> <h5> <a href="publication/283819492_Towards_social_power_intelligent_agents" class="color-inherit ga-similar-publication-title"><span class="publication-title">Towards social power intelligent agents</span></a>  </h5>  <div class="authors"> <a href="researcher/70310030_G_Pereira" class="authors ga-similar-publication-author">G. Pereira</a>, <a href="researcher/10597052_R_Prada" class="authors ga-similar-publication-author">R. Prada</a>, <a href="researcher/2084588773_PA_Santos" class="authors ga-similar-publication-author">P.A. Santos</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56ab9d884fd5f"> <div> <h5> <a href="publication/276452332_University_Students%27_Strengths_Associated_with_an_Optimal_Academic_and_Professional_Performance" class="color-inherit ga-similar-publication-title"><span class="publication-title">University Studentsâ Strengths Associated with an Optimal Academic and Professional Performance</span></a>  </h5>  <div class="authors"> <a href="researcher/2065421174_Omar_Saldana" class="authors ga-similar-publication-author">Omar SaldaÃ±a</a>, <a href="researcher/21812076_Jordi_Escartin" class="authors ga-similar-publication-author">Jordi EscartÃ­n</a>, <a href="researcher/2073871821_Luis_Torres" class="authors ga-similar-publication-author">Luis Torres</a>, <a href="researcher/2066138904_Ana_Varela-Rey" class="authors ga-similar-publication-author">Ana Varela-Rey</a>, <a href="researcher/21812074_Javier_Martin-Pena" class="authors ga-similar-publication-author">Javier MartÃ­n-PeÃ±a</a>, <a href="researcher/25485316_Alvaro_Rodriguez-Carballeira" class="authors ga-similar-publication-author">Ãlvaro RodrÃ­guez-Carballeira</a>, <a href="researcher/2073897564_Yirsa_Jimenez" class="authors ga-similar-publication-author">Yirsa JimÃ©nez</a>, <a href="researcher/21812070_Tomeu_Vidal" class="authors ga-similar-publication-author">Tomeu Vidal</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab9d884fd5f"> <div> <h5> <a href="publication/269321614_An_interdisciplinary_taxonomy_of_social_cues_and_signals_in_the_service_of_engineering_robotic_social_intelligence" class="color-inherit ga-similar-publication-title"><span class="publication-title">An interdisciplinary taxonomy of social cues and signals in the service of engineering robotic social intelligence</span></a>  </h5>  <div class="authors"> <a href="researcher/2040168577_Travis_J_Wiltshire" class="authors ga-similar-publication-author">Travis J. Wiltshire</a>, <a href="researcher/2065610193_Emilio_J_C_Lobato" class="authors ga-similar-publication-author">Emilio J. C. Lobato</a>, <a href="researcher/2083724387_Jonathan_Velez" class="authors ga-similar-publication-author">Jonathan Velez</a>, <a href="researcher/70109486_Florian_G_Jentsch" class="authors ga-similar-publication-author">Florian G. Jentsch</a>, <a href="researcher/50228720_Stephen_M_Fiore" class="authors ga-similar-publication-author">Stephen M. Fiore</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw52_56ab9d884fd5f" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw53_56ab9d884fd5f">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw54_56ab9d884fd5f" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=8GoM9E4BaDn4LKP2D3pu2bbUqB-_xEAQY5hEeMnELySSX_ldVsFjf7-U1yOBnUGD" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="jbOT4Z4X4OT0fa2oeaZL+hsLixUHPJQBHCmINeu5RqrcH43oIYpMBq4iOfrBtpxfp/P/lLJCub+837ozR3ScuAQG74o1i4szH4i5oS+gR5dA3DsqJNQ+VOOTZ+FtppQuVt2wA/n4Ui1uBBSmdLDQuQE5DR9DZwfV6Fg0Vpfdxx40sP9V2GxKlUteDPAAifEEbAQV8HIWY6B3UpU+prZymL7mOdVn8EGHjGQh//Q2ldx5X7VKvdzGcmdAhEUDmm8YvmNmkk4q954TtXh1Z2KLlUki8sBghryB84FuPoF6EEM="/> <input type="hidden" name="urlAfterLogin" value="publication/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIyNDMwMTkwX1NvY2lhbF9TaWduYWxfUHJvY2Vzc2luZ19TdXJ2ZXlfb2ZfYW5fRW1lcmdpbmdfRG9tYWlu"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIyNDMwMTkwX1NvY2lhbF9TaWduYWxfUHJvY2Vzc2luZ19TdXJ2ZXlfb2ZfYW5fRW1lcmdpbmdfRG9tYWlu"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIyNDMwMTkwX1NvY2lhbF9TaWduYWxfUHJvY2Vzc2luZ19TdXJ2ZXlfb2ZfYW5fRW1lcmdpbmdfRG9tYWlu"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw55_56ab9d884fd5f"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 587;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Alessandro Vinciarelli","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272449690402823%401441968476852_m\/Alessandro_Vinciarelli2.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Alessandro_Vinciarelli2","institution":"University of Glasgow","institutionUrl":false,"widgetId":"rgw4_56ab9d884fd5f"},"id":"rgw4_56ab9d884fd5f","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1842400","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9d884fd5f"},"id":"rgw3_56ab9d884fd5f","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=222430190","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":222430190,"title":"Social Signal Processing: Survey of an Emerging Domain","journalTitle":"Image and Vision Computing","journalDetailsTooltip":{"data":{"journalTitle":"Image and Vision Computing","journalAbbrev":"IMAGE VISION COMPUT","publisher":"Elsevier","issn":"0262-8856","impactFactor":"1.59","fiveYearImpactFactor":"2.38","citedHalfLife":"8.70","immediacyIndex":"0.22","eigenFactor":"0.01","articleInfluence":"0.92","widgetId":"rgw6_56ab9d884fd5f"},"id":"rgw6_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0262-8856","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"University of Twente, Drienerlolaan 5, 7522 NB Enschede, The Netherlands","type":"Article","details":{"doi":"10.1016\/j.imavis.2008.11.007","journalInfos":{"journal":"","publicationDate":"11\/2009;","publicationDateRobot":"2009-11","article":"27(12):1743-1759.","journalTitle":"Image and Vision Computing","journalUrl":"journal\/0262-8856_Image_and_Vision_Computing","impactFactor":1.59}},"source":{"sourceUrl":"http:\/\/eprints.eemcs.utwente.nl\/17123\/","sourceName":"OAI"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1016\/j.imavis.2008.11.007"},{"key":"rft.atitle","value":"Social Signal Processing: Survey of an Emerging Domain"},{"key":"rft.title","value":"Image and Vision Computing"},{"key":"rft.jtitle","value":"Image and Vision Computing"},{"key":"rft.volume","value":"27"},{"key":"rft.issue","value":"12"},{"key":"rft.date","value":"2009"},{"key":"rft.pages","value":"1743-1759"},{"key":"rft.issn","value":"0262-8856"},{"key":"rft.au","value":"Alessandro Vinciarelli,Maja Pantic,Herv\u00e9 Bourlard"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab9d884fd5f"},"id":"rgw7_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=222430190","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":222430190,"peopleItems":[{"data":{"authorNameOnPublication":"Alessandro Vinciarelli","accountUrl":"profile\/Alessandro_Vinciarelli2","accountKey":"Alessandro_Vinciarelli2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272449690402823%401441968476852_m\/Alessandro_Vinciarelli2.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Alessandro Vinciarelli","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Glasgow","professionalInstitutionUrl":"institution\/University_of_Glasgow"}},"professionalInstitutionName":"University of Glasgow","professionalInstitutionUrl":"institution\/University_of_Glasgow","url":"profile\/Alessandro_Vinciarelli2","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272449690402823%401441968476852_l\/Alessandro_Vinciarelli2.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Alessandro_Vinciarelli2","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab9d884fd5f"},"id":"rgw10_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1842400&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Glasgow","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":222430190,"widgetId":"rgw9_56ab9d884fd5f"},"id":"rgw9_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1842400&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=222430190","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/47824715_Maja_Pantic","authorNameOnPublication":"Maja Pantic","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Maja Pantic","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/47824715_Maja_Pantic","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab9d884fd5f"},"id":"rgw12_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=47824715&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab9d884fd5f"},"id":"rgw11_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=47824715&authorNameOnPublication=Maja%20Pantic","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Herve Bourlard","accountUrl":"profile\/Herve_Bourlard","accountKey":"Herve_Bourlard","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272700920823814%401442028374120_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Herve Bourlard","profile":{"professionalInstitution":{"professionalInstitutionName":"Idiap Research Institute","professionalInstitutionUrl":"institution\/Idiap_Research_Institute"}},"professionalInstitutionName":"Idiap Research Institute","professionalInstitutionUrl":"institution\/Idiap_Research_Institute","url":"profile\/Herve_Bourlard","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272700920823814%401442028374120_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Herve_Bourlard","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw14_56ab9d884fd5f"},"id":"rgw14_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2835273&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Idiap Research Institute","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":2,"publicationUid":222430190,"widgetId":"rgw13_56ab9d884fd5f"},"id":"rgw13_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2835273&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=2&publicationUid=222430190","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab9d884fd5f"},"id":"rgw8_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=222430190&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":222430190,"abstract":"<noscript><\/noscript><div>The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been argued to be indispensable and perhaps the most important for success in life. This paper argues that next-generation computing needs to include the essence of social intelligence &ndash; the ability to recognize human social signals and social behaviours like turn taking, politeness, and disagreement &ndash; in order to become more effective and more efficient. Although each one of us understands the importance of social signals in everyday life situations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks, smiles, crossed arms, laughter, and similar, design and development of automated systems for social signal processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a computer, it summarizes the relevant findings in social psychology, and it proposes a set of recommendations for enabling the development of the next generation of socially aware computing.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw15_56ab9d884fd5f"},"id":"rgw15_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=222430190","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190\/figure\/fig1\/Fig-8-People-detection-Examples-of-people-detection-in-public-spaces-pictures-from.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190\/figure\/fig1\/Fig-8-People-detection-Examples-of-people-detection-in-public-spaces-pictures-from_small.png","figureUrl":"\/figure\/222430190_fig1_Fig-8-People-detection-Examples-of-people-detection-in-public-spaces-pictures-from","selected":false,"title":"Fig. 8. People detection. Examples of people detection in public spaces...","key":"222430190_fig1_Fig-8-People-detection-Examples-of-people-detection-in-public-spaces-pictures-from"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190\/figure\/fig2\/Fig-9-AU-detection-Outline-of-a-geometric-feature-based-system-for-detection-of-facial.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190\/figure\/fig2\/Fig-9-AU-detection-Outline-of-a-geometric-feature-based-system-for-detection-of-facial_small.png","figureUrl":"\/figure\/222430190_fig2_Fig-9-AU-detection-Outline-of-a-geometric-feature-based-system-for-detection-of-facial","selected":false,"title":"Fig. 9. AU detection. Outline of a geometric-feature-based system for...","key":"222430190_fig2_Fig-9-AU-detection-Outline-of-a-geometric-feature-based-system-for-detection-of-facial"}],"readerDocId":"6894537","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw16_56ab9d884fd5f"},"id":"rgw16_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=6894537&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw17_56ab9d884fd5f"},"id":"rgw17_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9d884fd5f"},"id":"rgw5_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=222430190&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70310030,"url":"researcher\/70310030_G_Pereira","fullname":"G. Pereira","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10597052,"url":"researcher\/10597052_R_Prada","fullname":"R. Prada","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084588773,"url":"researcher\/2084588773_PA_Santos","fullname":"P.A. Santos","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283819492_Towards_social_power_intelligent_agents","usePlainButton":true,"publicationUid":283819492,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283819492_Towards_social_power_intelligent_agents","title":"Towards social power intelligent agents","displayTitleAsLink":true,"authors":[{"id":70310030,"url":"researcher\/70310030_G_Pereira","fullname":"G. Pereira","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10597052,"url":"researcher\/10597052_R_Prada","fullname":"R. Prada","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084588773,"url":"researcher\/2084588773_PA_Santos","fullname":"P.A. Santos","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283819492_Towards_social_power_intelligent_agents","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283819492_Towards_social_power_intelligent_agents\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab9d884fd5f"},"id":"rgw19_56ab9d884fd5f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283819492","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2065421174,"url":"researcher\/2065421174_Omar_Saldana","fullname":"Omar Salda\u00f1a","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21812076,"url":"researcher\/21812076_Jordi_Escartin","fullname":"Jordi Escart\u00edn","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073871821,"url":"researcher\/2073871821_Luis_Torres","fullname":"Luis Torres","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2066138904,"url":"researcher\/2066138904_Ana_Varela-Rey","fullname":"Ana Varela-Rey","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":4,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 2014","journal":"Procedia - Social and Behavioral Sciences","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/276452332_University_Students'_Strengths_Associated_with_an_Optimal_Academic_and_Professional_Performance","usePlainButton":true,"publicationUid":276452332,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/276452332_University_Students%27_Strengths_Associated_with_an_Optimal_Academic_and_Professional_Performance","title":"University Students\u2019 Strengths Associated with an Optimal Academic and Professional Performance","displayTitleAsLink":true,"authors":[{"id":2065421174,"url":"researcher\/2065421174_Omar_Saldana","fullname":"Omar Salda\u00f1a","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21812076,"url":"researcher\/21812076_Jordi_Escartin","fullname":"Jordi Escart\u00edn","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073871821,"url":"researcher\/2073871821_Luis_Torres","fullname":"Luis Torres","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2066138904,"url":"researcher\/2066138904_Ana_Varela-Rey","fullname":"Ana Varela-Rey","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21812074,"url":"researcher\/21812074_Javier_Martin-Pena","fullname":"Javier Mart\u00edn-Pe\u00f1a","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":25485316,"url":"researcher\/25485316_Alvaro_Rodriguez-Carballeira","fullname":"\u00c1lvaro Rodr\u00edguez-Carballeira","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2073897564,"url":"researcher\/2073897564_Yirsa_Jimenez","fullname":"Yirsa Jim\u00e9nez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":21812070,"url":"researcher\/21812070_Tomeu_Vidal","fullname":"Tomeu Vidal","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Procedia - Social and Behavioral Sciences 08\/2014; 141:30-34. DOI:10.1016\/j.sbspro.2014.05.008"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/276452332_University_Students'_Strengths_Associated_with_an_Optimal_Academic_and_Professional_Performance","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/276452332_University_Students'_Strengths_Associated_with_an_Optimal_Academic_and_Professional_Performance\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab9d884fd5f"},"id":"rgw20_56ab9d884fd5f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=276452332","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2040168577,"url":"researcher\/2040168577_Travis_J_Wiltshire","fullname":"Travis J. Wiltshire","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2065610193,"url":"researcher\/2065610193_Emilio_J_C_Lobato","fullname":"Emilio J. C. Lobato","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083724387,"url":"researcher\/2083724387_Jonathan_Velez","fullname":"Jonathan Velez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":70109486,"url":"researcher\/70109486_Florian_G_Jentsch","fullname":"Florian G. Jentsch","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Jun 2014","journal":"Proceedings of SPIE - The International Society for Optical Engineering","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/269321614_An_interdisciplinary_taxonomy_of_social_cues_and_signals_in_the_service_of_engineering_robotic_social_intelligence","usePlainButton":true,"publicationUid":269321614,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.20","url":"publication\/269321614_An_interdisciplinary_taxonomy_of_social_cues_and_signals_in_the_service_of_engineering_robotic_social_intelligence","title":"An interdisciplinary taxonomy of social cues and signals in the service of engineering robotic social intelligence","displayTitleAsLink":true,"authors":[{"id":2040168577,"url":"researcher\/2040168577_Travis_J_Wiltshire","fullname":"Travis J. Wiltshire","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2065610193,"url":"researcher\/2065610193_Emilio_J_C_Lobato","fullname":"Emilio J. C. Lobato","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083724387,"url":"researcher\/2083724387_Jonathan_Velez","fullname":"Jonathan Velez","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70109486,"url":"researcher\/70109486_Florian_G_Jentsch","fullname":"Florian G. Jentsch","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":50228720,"url":"researcher\/50228720_Stephen_M_Fiore","fullname":"Stephen M. Fiore","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["SPIE Defense + Security; 06\/2014"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/269321614_An_interdisciplinary_taxonomy_of_social_cues_and_signals_in_the_service_of_engineering_robotic_social_intelligence","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/269321614_An_interdisciplinary_taxonomy_of_social_cues_and_signals_in_the_service_of_engineering_robotic_social_intelligence\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab9d884fd5f"},"id":"rgw21_56ab9d884fd5f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=269321614","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw18_56ab9d884fd5f"},"id":"rgw18_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=222430190&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":222430190,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":222430190,"publicationType":"article","linkId":"541a1ae10cf2218008bfa710","fileName":"download.pdf","fileUrl":"profile\/Alessandro_Vinciarelli2\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710.pdf","name":"Alessandro Vinciarelli","nameUrl":"profile\/Alessandro_Vinciarelli2","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"1.37 MB","widgetId":"rgw24_56ab9d884fd5f"},"id":"rgw24_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=222430190&linkId=541a1ae10cf2218008bfa710&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":222430190,"publicationType":"article","linkId":"0e6080d2f0c46d4f0acb459f","fileName":"Social Signal Processing: Survey of an Emerging Domain","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.159.1208&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.159.1208&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw25_56ab9d884fd5f"},"id":"rgw25_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=222430190&linkId=0e6080d2f0c46d4f0acb459f&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw23_56ab9d884fd5f"},"id":"rgw23_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=222430190&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":47,"valueFormatted":"47","widgetId":"rgw26_56ab9d884fd5f"},"id":"rgw26_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=222430190","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab9d884fd5f"},"id":"rgw22_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=222430190&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":222430190,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw28_56ab9d884fd5f"},"id":"rgw28_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=222430190&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":47,"valueFormatted":"47","widgetId":"rgw29_56ab9d884fd5f"},"id":"rgw29_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=222430190","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw27_56ab9d884fd5f"},"id":"rgw27_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=222430190&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Social signal processing: Survey of an emerging domain\nAlessandro Vinciarellia,b,*, Maja Panticc,d, Herv\u00e9 Bourlarda,b\naIDIAP Research Institute, Computer Vision, CP592, 1920 Martigny, Switzerland\nbEcole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL), CH-1015 Lausanne, Switzerland\ncImperial College, 180 Queens Gate, London SW7 2AZ, UK\ndUniversity of Twente, Drienerlolaan 5, 7522 NB Enschede, The Netherlands\na r t i c l e i n f o\nArticle history:\nReceived 16 May 2008\nAccepted 26 September 2008\nKeywords:\nSocial signals\nComputer vision\nSpeech processing\nHuman behaviour analysis\nSocial interactions\na b s t r a c t\nThe ability to understand and manage social signals of a person we are communicating with is the core of\nsocial intelligence. Social intelligence is a facet of human intelligence that has been argued to be indis-\npensable and perhaps the most important for success in life. This paper argues that next-generation com-\nputing needs to include the essence of social intelligence \u2013 the ability to recognize human social signals\nand social behaviours like turn taking, politeness, and disagreement \u2013 in order to become more effective\nand more efficient. Although each one of us understands the importance of social signals in everyday life\nsituations, and in spite of recent advances in machine analysis of relevant behavioural cues like blinks,\nsmiles, crossed arms, laughter, and similar, design and development of automated systems for social sig-\nnal processing (SSP) are rather difficult. This paper surveys the past efforts in solving these problems by a\ncomputer, it summarizes the relevant findings in social psychology, and it proposes a set of recommen-\ndations for enabling the development of the next generation of socially aware computing.\n? 2008 Elsevier B.V. All rights reserved.\n1. Introduction\nThe exploration of how human beings react to the world and\ninteract with it and each other remains one of the greatest scien-\ntific challenges. Perceiving, learning, and adapting to the world\nare commonly labeled as intelligent behaviour. But what does it\nmean being intelligent? Is IQ a good measure of human intelli-\ngence and the best predictor of somebody\u2019s success in life? There\nis now a growing research in cognitive sciences, which argues that\nour common view of intelligence is too narrow, ignoring a crucial\nrange of abilities that matter immensely for how people do in life.\nThis range of abilities is called social intelligence[6,8,19,182] and in-\ncludes the ability to express and recognize social signals and social\nbehaviours like turn taking, agreement, politeness, and empathy,\ncoupled with the ability to manage them in order to get along well\nwith others while winning their cooperation. Social signals and so-\ncial behaviours are the expression of ones attitude towards social\nsituation and interplay, and they are manifested through a multi-\nplicity of non-verbal behavioural cues including facial expressions,\nbody postures and gestures, and vocal outbursts like laughter (see\nFig. 1). Social signals typically last for a short time (milliseconds,\nlike turn taking, to minutes, like mirroring), compared to social\nbehaviours that last longer (seconds, like agreement, to minutes,\nlike politeness, to hours or days, like empathy) and are expressed\nas temporal patterns of non-verbal behavioural cues. The skills of\nsocial intelligence have been argued to be indispensable and per-\nhaps the most important for success in life [66].\nWhen it comes to computers, however, they are socially igno-\nrant [143]. Current computing devices do not account for the fact\nthat human\u2013human communication is always socially situated\nand that discussions are not just facts but part of a larger social\ninterplay. However, not all computers will need social intelligence\nand none will need all of the related skills humans have. The cur-\nrent-state-of-the-art categorical computing works well and will al-\nways work well for context-independent tasks like making plane\nreservations and buying and selling stocks. However, this kind of\ncomputing is utterly inappropriate for virtual reality applications\nas well as for interacting with each of the (possibly hundreds) com-\nputer systems diffused throughout future smart environments\n(predicted as the future of computing by several visionaries such\nas Mark Weiser) and aimed at improving the quality of life by\nanticipating the users needs. Computer systems and devices capa-\nble of sensing agreement, inattention, or dispute, and capable of\nadapting and responding to these social signals in a polite, unintru-\nsive, or persuasive manner, are likely to be perceived as more nat-\nural, efficacious, and trustworthy. For example, in education,\npupils\u2019 social signals inform the teacher of the need to adjust the\ninstructional message. Successful human teachers acknowledge\nthis and work with it; digital conversational embodied agents must\n0262-8856\/$ - see front matter ? 2008 Elsevier B.V. All rights reserved.\ndoi:10.1016\/j.imavis.2008.11.007\n* Corresponding author. Address: IDIAP Research Institute, Computer Vision,\nCP592, 1920 Martigny, Switzerland. Tel.: +41 27 7217724.\nE-mail addresses: vincia@idiap.ch (A. Vinciarelli), m.pantic@imperial.ac.uk (M.\nPantic), bourlard@idiap.ch (H. Bourlard).\nImage and Vision Computing 27 (2009) 1743\u20131759\nContents lists available at ScienceDirect\nImage and Vision Computing\njournal homepage: www.elsevier.com\/locate\/imavis"},{"page":2,"text":"begin to do the same by employing tools that can accurately sense\nand interpret social signals and social context of the pupil, learn\nsuccessful context-dependent social behaviour, and use a proper\nsocially adept presentation language (see, e.g., [141]) to drive the\nanimation of the agent. The research area of machine analysis\nand employment of human social signals to build more natural,\nflexible computing technology goes by the general name of socially\naware computing as introduced by Pentland [142,143].\nAlthough the importance of social signals in everyday life sit-\nuations is evident, and in spite of recent advances in machine\nanalysis and synthesis of relevant behavioural cues like gaze ex-\nchange, blinks, smiles, head nods, crossed arms, laughter, and\nsimilar [137,138], the research efforts in machine analysis and\nsynthesis of human social signals like attention, empathy, polite-\nness, flirting, (dis)agreement, etc., are still tentative and pioneer-\ning efforts. The importance of studying social interactions and\ndeveloping automated assessing of human social behaviour from\naudiovisual recordings is undisputable. It will result in valuable\nmultimodal tools that could revolutionise basic research in cog-\nnitive and social sciences by raising the quality and shortening\nthe time to conduct research that is now lengthy, laborious,\nand often imprecise. At the same time, and as outlined above,\nsuch tools form a large step ahead in realizing naturalistic, so-\ncially aware computing and interfaces, built for humans, based\non models of human behaviour.\nSocial signal processing (SSP) [143,145,202,203] is the new re-\nsearch and technological domain that aims at providing computers\nwith the ability to sense and understand human social signals. De-\nspitebeinginitsinitialphase,SSPhasalreadyattractedtheattention\nof the technological community: the MITTechnologyReviewmaga-\nzine identifies reality mining (one of the main applications of SSP so\nfar, see Section 4, for more details), as 1 of the 10 technologies likely\nto change the world [69], while management experts expect SSP to\nchange organization studies like the microscope has changed medi-\ncine few centuries ago [19].\nTothebestofourknowledge,thisisthefirstattempttosurveythe\npastworkdoneonSSP.Theinnovativeandmultidisciplinarycharac-\nter of the research on SSP is the main reason for this state of affairs.\nFor example, in contrast to the research on human affective behav-\niouranalysisthatwitnessedtremendousprogressinthepastdecade\n(for exhaustive surveys in the field see, e.g., [76,140,221]), the re-\nsearch on machine analysis of human social behaviour just started\nto attract the interest of the research community in computer sci-\nence. This and the fragmentation of the research over several scien-\ntific communities including those in psychology, computer vision,\nspeech and signal processing, make the exercise of surveying the\ncurrent efforts in machine analysis of human social behaviour\ndifficult.\nThe paper begins by examining the context in which the re-\nsearch on SSP has arisen and by providing a taxonomy of the target\nproblem domain (Section 2). The paper surveys then the past work\ndone in tackling the problems of machine detection and interpre-\ntation of social signals and social behaviours in real-world scenar-\nios (Section 3). Existing research efforts to apply social signal\nprocessing to automatic recognition of socially relevant informa-\ntion such as someone\u2019s role, dominance, influence, etc., are sur-\nveyed next (Section 4). Finally, the paper discusses a number of\nchallenges facing researchers in the field (Section 5). In the authors\u2019\nopinion, these need to be addressed before the research in the field\ncan enter its next phase \u2013 deployment of research findings in real-\nworld applications.\n2. Behavioural cues and social signals: a taxonomy\nThereismorethanwordsinsocialinteractions[9],whetherthese\ntake place between humans or between humans and computers\n[30]. This is well known to social psychologists that have studied\nnon-verbal communication for several decades [96,158]. It is what\npeople experience when they watch a television program in a lan-\nguage they do not understand and still capture a number of impor-\ntant social cues such as differences in status between individuals,\noverall atmosphere of interactions (e.g., tense vs. relaxed), rapport\nbetween people (mutual trust vs. mutual distrust), etc.\nNon-verbal behaviour is a continuous source of signals which\nconvey information about feelings, mental state, personality, and\nother traits of people [158]. During social interactions, non-verbal\nbehaviourconveysthisinformationnotonlyforeachoftheinvolved\nindividuals, but it also determines the nature and quality of the so-\ncial relationships they have with others. This happens through a\nwide spectrum of non-verbal behavioural cues [7,8] that are per-\nceived and displayed mostly unconsciously while producing social\nawareness,i.e.,aspontaneousunderstandingofsocialsituationsthat\ndoes not require attention or reasoning [98].\nThe term behavioural cue is typically used to describe a set of\ntemporal changes in neuromuscular and physiological activity that\nlast for short intervals of time (milliseconds to minutes) in contrast\nto behaviours (e.g., social behaviours like politeness or empathy)\nthat last on average longer (minutes to hours). As summarized in\n[47] among the types of messages (communicative intentions)\nconveyed by behavioural cues are the following:\n? affective\/attitudinal\/cognitive states (e.g., fear, joy, stress, dis-\nagreement, ambivalence, and inattention),\n? emblems (culture-specific interactive signals like wink or\nthumbs up),\nFig. 1. Behavioural cues and social signals. Multiple behavioural cues (vocal behaviour, posture, mutual gaze, interpersonal distance, etc.) combine to produce a social signal\n(in this case aggressivity or disagreement) that is evident even if the picture shows only the silhouettes of the individuals involved in the interaction.\n1744\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":3,"text":"? manipulators (actions used to act on objects in the environment\nor self-manipulative actions such as lip biting and scratching),\n? illustrators (actions accompanying speech such as finger point-\ning and raised eyebrows), and\n? regulators (conversational mediators such as the exchange of a\nlook, palm pointing, head nods, and smiles).\nIn most cases, behavioural cues accompany verbal communica-\ntion and, even if they are invisible, i.e., they are sensed and inter-\npreted outside conscious awareness, they have a major impact\non the perception of verbal messages and social situations [96].\nEarly investigations of verbal and non-verbal components in inter-\naction (in particular [113] as cited in [96]) have suggested that the\nverbal messages account for just 7% of the overall social percep-\ntion. This conclusion has been later argued because the actual\nweight of the different messages (i.e., verbal vs. non-verbal) de-\npends on the context and on the specific kind of interaction [45].\nHowever, more recent studies still confirm that the non-verbal\nbehaviour plays a major role in shaping the perception of social sit-\nuations: e.g., judges assessing the rapport between two people are\nmore accurate when they use only the facial expressions than\nwhen they use only the verbal messages exchanged [8]. Overall,\nthe non-verbal social signals seem to be the predominant source\nof information used in understanding social interactions [9].\nThe rest of this section provides a taxonomy of the SSP problem\ndomain by listing and explaining the most important behavioural\ncues and their functions in social behaviour. Behavioural cues that\nwe included in this list are those that the research in psychology\nhas recognized as being the most important in human judgments\nof social behaviour. Table 1 provides a synopsis of those behav-\nioural cues, the social signals they are related to, and the technol-\nogies that can be used to sense and analyze them. For more\nexhaustive explanations of non-verbal behaviours and the related\nbehavioural cues, readers are referred to [7,47,96,158].\n2.1. Physical appearance\nThe physical appearance includes natural characteristics such\nas height, body shape, physiognomy, skin and hair color, as well\nas artificial characteristics such as clothes, ornaments, make up,\nand other manufacts used to modify\/accentuate the facial\/body\naspects.\nThe main social signal associated to physical appearance is the\nattractiveness. Attractiveness produces a positive halo effect (a phe-\nnomenon also known as \u2018\u2018what is beautiful is good\u201d [41]). Attractive\npeople are often judged as having high status and good personality\neven if no objective basis for such judgments exists [70,208].\nAttractive people also have higher probability of starting new so-\ncial relationships with people they do not know [158]. Other phys-\nical characteristics are not necessarily related to the attractiveness,\nbut still have a major influence on social perceptions. The most\nimportant are height and somatotype (see below). Tall individuals\ntend to be attributed higher social status and, in some cases, they\nactually hold a higher status. For example, a survey has shown that\nthe average height of the American CEOs of the Fortune 500 com-\npanies is around 7.5 cm higher than the average height of the\nAmerican population. Moreover, 30% of the same CEOs are taller\nthan 190 cm, while only 4% of the rest of the American population\nlies in the same range of height [63].\nDifferent somatotypes (see Fig. 2), tend to elicit the attribution of\ncertain personality traits [25]. For example, endomorphic individu-\nals (round, fat, and soft) tend to be perceived as more talkative and\nsympathetic, but also more dependent on others. Mesomorphic\nindividuals (bony, muscular, and athletic) tend to be perceived as\nmore self-reliant, more mature in behaviour and stronger, while\nectomorphic individuals (tall, thin, and fragile) tend to be perceived\nas more tense, more nervous, more pessimistic and inclined to be\ndifficult. These judgments are typically influenced by stereotypes\nthat do not necessarily correspond to the reality, but still influence\nsignificantly the social perceptions [96].\n2.2. Gestures and posture\nFollowing the work of Darwin [37], which was the first to de-\nscribe body expressions associated with emotions in animals and\nhumans, there have been a number of studies on human body pos-\ntures and gestures communicating emotions. For example, the\nworks in [27,198] investigated perception and display of body pos-\ntures relevant to basic emotions including happiness, sadness, sur-\nprise, fear, disgust, and anger, while the studies in\ninvestigated bodily expressions of felt and recognized basic emo-\ntions as visible in specific changes in arm movement, gait parame-\nters, and kinematics. Overall, these studies have shown that both\nposture and body\/limb motions change with emotion expressed.\n[72,152]\nTable 1\nThe table shows the behavioural cues associated to some of the most important social behaviours as well as the technologies involved in their automatic detection.\nSocial cues Example social behavioursTech.\nEmotion PersonalityStatusDominance Persuasion RegulationRapport Speech analysisComputer vision Biometry\nPhysical appearance\nHeight\nAttractiveness\nBody shape\np\np\np\np\np\np\np\np\np\np\np\np\np\npp\nGesture and posture\nHand gestures\nPosture\nWalking\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\nFace and eyes behaviour\nFacial expressions\nGaze behaviour\nFocus of attention\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\np\nVocal behaviour\nProsody\nTurn taking\nVocal outbursts\nSilence\np\np\np\np\np\np\np\np\np\np\npp\np\np\np\np\np\np\np\npp\npp\np\nSpace and environment\nDistance\nSeating arrangement\npppp\np\np\np\np\npp\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1745"},{"page":4,"text":"Basic research also provides evidence that gestures like head incli-\nnation, face touching, and shifting posture often accompany social\naffective states like shame and embarrassment [26,50]. However,\nas indicated by researchers in the field (e.g., in [112]), as much as\n90% of body gestures are associated with speech, representing typ-\nical social signals such as illustrators, emblems, and regulators.\nIn other words, gestures are used in most cases to regulate\ninteractions (e.g., to yield the turn in a conversation), to communi-\ncate a specific meaning (e.g., the thumbs up gesture to show appre-\nciation), to punctuate a discourse (e.g., to underline an utterance by\nrising the index finger), to greet (e.g., by waving hands to say good-\nbye), etc. [123]. However, in some cases gestures are performed\nunconsciously and they are interesting from an SSP point of view\nbecause they account for honest information [146], i.e., they leak\ncues related to the actual attitude of a person with respect to a so-\ncial context. In particular, adaptors express boredom, stress and\nnegative feelings towards others. Adaptors are usually displayed\nunconsciously and include self-manipulations (e.g., scratching,\nnose and ear touching, hair twisting), manipulation of small ob-\njects (e.g., playing with pens and papers), and self-protection ges-\ntures (e.g., folding arms or rhythmicly moving legs) [96].\nPostures are also typically assumed unconsciously and, argu-\nably, they are the most reliable cues about the actual attitude of\npeople towards social situations [158]. One of the main classifica-\ntions of postural behaviours proposes three main criteria to assess\nthe social meaning of postures [166]. The first criterion distin-\nguishes between inclusive and non-inclusive postures and accounts\nfor how much a given posture takes into account the presence of\nothers. For example, facing in the opposite direction with respect\nto others is a clear sign of non-inclusion. The second criterion is\nface-to-face vs. parallel body orientation and concerns mainly people\ninvolved in conversations. Face-to-face interactions are in general\nmore active and engaging (the frontal position addresses the need\nof continuous mutual monitoring), while people sitting parallel to\neach other tend to be either buddies or less mutually interested.\nThe third criterion is congruence vs. incongruence: symmetric pos-\ntures tend to account for a deep psychological involvement (see\nleft picture in Fig. 3), while non-symmetric ones correspond to\nthe opposite situation. The postural congruence is an example of\na general phenomenon called chameleon effect or mirroring [22],\nthat consists of the mutual imitation of people as a mean to display\naffiliation and liking. Postural behaviour includes also walking and\nmovements that convey social information such as status, domi-\nnance, and affective state [109].\n2.3. Face and eye behaviour\nThe human face is involved in an impressive variety of different\nactivities. It houses the majority of our sensory apparatus: eyes,\nears, mouth, and nose, allowing the bearer to see, hear, taste, and\nsmell. Apart from these biological functions, the human face pro-\nvides a number of signals essential for interpersonal communica-\ntion in our social life. The face houses the speech production\napparatus and is used to identify other members of the species,\nto regulate the conversation by gazing or nodding, and to interpret\nwhat has been said by lip reading. It is our direct and naturally pre-\neminent means of communicating and understanding somebody\u2019s\naffective state and intentions on the basis of the shown facial\nexpression [89]. Personality, attractiveness, age and gender can\nbe also seen from someone\u2019s face [8]. Thus the face is a multisignal\nsender\/receiver capable of tremendous flexibility and specificity. It\nis therefore not surprising that the experiments (see beginning of\nSection 2) about the relative weight of the different non-verbal\ncomponents in shaping social perceptions always show that facial\nbehaviour plays a major role [8,68,113].\nTwo major approaches to facial behaviour measurement in psy-\nchological research are message and sign judgment [23]. The aim\nof message judgment is to infer what underlies a displayed facial\nexpression, such as affect or personality, while the aim of sign\njudgment is to describe the surface of the shown behaviour, such\nas facial movement or facial component shape. Thus, a brow fur-\nrow can be judged as anger in a message judgment and as a facial\nmovement that lowers and pulls the eyebrows closer together in a\nsign-judgment approach. While message judgment is all about\ninterpretation, sign-judgment attempts to be objective, leaving\ninference about the conveyed message to higher order decision\nmaking.\nAs indicated in [23], most commonly used facial-expression\ndescriptors in message-judgment approaches are the six basic\nemotions (fear, sadness, happiness, anger, disgust, and surprise;\nsee Fig. 4), proposed by Ekman and discrete emotion theorists,\nwho suggest that these emotions are universally displayed and rec-\nognized from facial expressions [89]. In sign-judgment approaches\n[24], a widely used method for manual labeling of facial actions is\nthe facial action coding system (FACS) [48].\nFACS associates facial-expression changes with actions of the\nmuscles that produce them. It defines nine different action units\n(AUs) in the upper face, 18 in the lower face, 11 for head position,\n9 for eye position, and 14 additional descriptors for miscellaneous\nactions. AUs are considered to be the smallest visually discernable\nfacial movements. Using FACS, human coders can manually code\nnearly any anatomically possible facial expression, decomposing\nit into the specific AUs that produced the expression. As AUs are\nindependent of interpretation, they can be used for any higher or-\nder decision making process including recognition of basic emo-\ntions (EMFACS; see [48]), cognitive states like interest and\npuzzlement [32], psychological states like suicidal depression\n[50] or pain [212], social behaviours like accord and rapport\n[8,32], personality traits like extraversion and temperament [50],\nand social signals like status, trustworthiness, emblems (i.e., cul-\nture-specific interactive signals like wink), regulators (i.e., conver-\nsational mediators like nod and gaze exchange), and illustrators\n(i.e., cues accompanying speech like raised eyebrows) [8,46,47].\nFACS provides an objective and comprehensive language for\ndescribing facial expressions and relating them back to what is\nknown about their meaning from the behavioural science litera-\nture. Because it is comprehensive, FACS also allows for the discov-\nery of new patterns related to emotional or situational states. For\nexample, what are the facial behaviours associated with social sig-\nnals such as empathy, persuasion, and politeness? An example\nwhere subjective judgments of expression failed to find relation-\nships which were later found with FACS is the failure of naive sub-\njects to differentiate deception and intoxication from facial display,\nwhereas reliable differences were shown with FACS [165].\nResearch based upon FACS has also shown that facial actions can\nFig. 2. Somatotypes. The figure shows the three body shapes that tend to elicit the\nperception of specific personality traits.\n1746\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":5,"text":"show differences between those telling the truth and lying at a\nmuch higher accuracy level than naive subjects making subjective\njudgments of the same faces [56]. Exhaustive overview of studies\non facial and gaze behaviour using FACS can be found in [50].\n2.4. Vocal behaviour\nThe vocal non-verbal behaviour includes all spoken cues that\nsurround the verbal message and influence its actual meaning.\nThe effect of vocal non-verbal behaviour is particularly evident\nwhen the tone of a message is ironic. In this case, the face value\nof the words is changed into its opposite by just using the appro-\npriate vocal intonation. The vocal non-verbal behaviour includes\nfive major components: voice quality, linguistic and non-linguistic\nvocalizations, silences, andturn taking patterns. Each one of them re-\nlates to social signals that contribute to different aspects of the so-\ncial perception of a message.\nThe voice quality corresponds to the prosodic features, i.e., pitch,\ntempo, and energy (see Section 3.3.4, for more details) and, in per-\nceptual terms, accounts for how something is said [31]. The pros-\nody conveys a wide spectrum of socially relevant cues: emotions\nlike anger or fear are often accompanied by energy bursts in voice\n(shouts) [168], the pitch influences the perception of dominance\nand extroversion (in general it is a personality marker [167]), the\nspeaking fluency (typically corresponding to high rhythm and lack\nof hesitations) increases the perception of competence and results\ninto higher persuasiveness [167]. The vocalizations include also ef-\nfects that aim at giving particular value to certain utterances or\nparts of the discourse, e.g., the pitch accents (sudden increases of\nthe pitch to underline a word) [79], or changes in rhythm and en-\nergy aiming at structuring the discourse [80].\nThe linguistic vocalizations (also known as segregates) include all\nthe non-words that are typically used as if they were actual words,\ne.g., \u2018\u2018ehm\u201d,\u2018\u2018ah-ah\u201d, \u2018\u2018uhm\u201d, etc. Segregates have two main func-\ntions, the first is to replace words that for some reason cannot be\nfound, e.g., when people do not know how to answer a question\nand simply utter a prolonged \u2018\u2018ehm\u201d. They are often referred to as\ndisfluencies and often account for a situation of embarrassment or\ndifficulty with respect to a social interaction [64]. The second\nimportant function is the so-called back-channeling, i.e., the use\nof segregates to accompany someone else speaking. In this sense\nthey can express attention, agreement, wonder, as well as the at-\ntempt of grabbing the floor or contradicting [176].\nThe non-linguistic vocalizations, also known as vocal outbursts,\ninclude non-verbal sounds like laughing, sobbing, crying, whisper-\ning, groaning, and similar, that may or may not accompany words,\nand provide some information about the attitude towards social\nsituations. For instance, laughter tends to reward desirable social\nbehaviour [90] and shows affiliation efforts, while crying is often\ninvolved in mirroring (also known as chameleon effect [22]), that\nis in the mutual imitation of people connected by strong social\nbonds [91]. Also, research in psychology has shown that listeners\ntend to be accurate in decoding some basic emotions as well as\nsome non-basic affective and social signals such as distress, anxi-\nety, boredom, and sexual interest from vocal outbursts like laughs,\nyawns, coughs, and sighs [163].\nThe silence is often interpreted as simple non-speech, but actu-\nally plays a major role in the vocal behaviour [219]. There are three\nkinds of silence in speech: hesitation silence, psycholinguistic silence,\nand interactive silence [158]. The first takes place when a speaker\nhas difficulty in talking, e.g., because she is expressing a difficult\nconcept or must face a hostile attitude in listeners. Sometimes,\nhesitation silences give rise to segregates that are used to fill the si-\nlence space (hence segregates are called sometimes fillers). The\npsycholinguistic silences take place when the speaker needs time\nto encode or decode the speech. This kind of silences happen often\nat the beginning of an intervention because the speaker needs to\nthink about the next words. In this sense, this is often a sign of dif-\nficulty and problems in dealing with a conversation. The interac-\ntive silences aim at conveying messages about the interactions\ntaking place: silence can be a sign of respect for people we want\nto listen to, a way of ignoring persons we do not want to answer\nto, as well as a way to attract the attention to other forms of com-\nmunication like mutual gaze or facial expressions.\nAnother important aspect of vocal non-verbal behaviour is turn\ntaking [154]. This includes two main components: the regulation\nFig. 3. Postural congruence. The figure on the left shows how people deeply involved in an interaction tend to assume the same posture. In the other picture, the forward\ninclination of the person on the right is not reciprocated by the person on the left.\nFig. 4. Basic emotions. Prototypic facial expressions of six basic emotions (disgust, happiness, sadness, anger, fear, and surprise).\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1747"},{"page":6,"text":"of the conversations, and the coordination (or the lack of it) during\nthe speaker transitions. The regulation in conversations includes\nbehaviours aimed at maintaining, yielding, denying, or requesting\nthe turn. Both gaze and voice quality (e.g., coughing) are used to\nsignal transition relevant points [217]. When it comes to vocal\nnon-verbal cues as conversation regulators, specific pitch and en-\nergy patterns show the intention of yielding the turn rather than\nmaintaining the floor. Also, linguistic vocalizations (see above)\nare often used as a form of back-channeling to request the turn.\nThe second important aspect in turn taking is the coordination at\nthe speaker transitions [20]. Conversations where the latency\ntimes between turns are too long sound typically awkward. The\nreason is that in fluent conversations, the mutual attention reduces\nthe above phenomenon and results into synchronized speaker\nchanges, where the interactants effectively interpret the signals\naimed at maintaining or yielding their turns. Overlapping speech\nis another important phenomenon that accounts for disputes as\nwell as status and dominance displays [180]. Note, however, that\nthe amount of overlapping speech accounts for up to 10% of the to-\ntal time even in normal conversations [175].\n2.5. Space and environment\nThe kind and quality of the relationships between individuals\ninfluences their interpersonal distance (the physical space between\nthem). One of the most common classifications of mutual distances\nbetween individuals suggests the existence of four concentric\nzones around a person accounting for different kinds of relation-\nships with the others [77]: the intimate zone, the casual-personal\nzone, the socio-consultive zone and the public zone (see Fig. 5a).\nThe intimate zone is the innermost region and it is open only to\nthe closest family members and friends. Its dimension, like in the\ncase of the other zones, depends on the culture and, in the case\nof western Europe and United States, the intimate zone corre-\nsponds to a distance of 0.4\u20130.5 m. In some cases, e.g., crowded\nbuses or elevators, the intimate zone must be necessarily opened\nto strangers. However, whenever there is enough space, people\ntend to avoid entering the intimate zone of others. The casual-per-\nsonal zone ranges (at least in USA and Western Europe) between\n0.5 and 1.2 m and it typically includes people we are most familiar\nwith (colleagues, friends, etc.). To open such an area to another\nperson in absence of constraints is a major signal of friendship.\nThe socio-consultive distance is roughly between 1 and 2 m (again\nin USA and Western Europe) and it is the area of formal relation-\nships. Not surprisingly, professionals (lawyers, doctors, etc.) typi-\ncally receive their clients sitting behind desks that have a\nprofundity of around 1 m, so that the distance with respect to their\nclients is in the range corresponding to the socio-consultive zone.\nThe public zone is beyond 2 m distance and it is, in general, outside\nthe reach of interaction potential. In fact, any exchange taking\nplace at such a distance is typically due to the presence of some\nobstacle, e.g., a large meeting table that requires people to talk at\ndistance.\nSocial interactions take place in environments that influence\nbehaviours and perceptions of people with their characteristics.\nOne of the most studied environmental variables is the seating\narrangement, i.e., the way people take place around a table for\ndifferent purposes [96,158]. Fig. 5b shows the seating positions\nthat people tend to use to perform different kinds of tasks (the\ncircles are the empty seats) [164]. The seating position depends\nalso on the personality of people: dominant and higher status\nindividuals tend to seat at the shorter side of rectangular tables,\nor in the middle of the longer sides (both positions ensure high\nvisibility and make easier the control of the conversation flow)\n[106]. Moreover, extrovert people tend to privilege seating\narrangements that minimize interpersonal distances, while intro-\nvert ones do the opposite [164].\n3. The state of the art\nThe problem of machine analysis of human social signals in-\ncludes four subproblem areas (see Fig. 6):\n(1) recording the scene,\n(2) detecting people in it,\n(3) extracting audio and\/or visual behavioural cues displayed by\npeople detected in the scene and interpreting this informa-\ntion in terms of social signals conveyed by the observed\nbehavioural cues,\n(4) sensing the context in which the scene is recorded and clas-\nsifying detected social signals into the target social-behav-\niour-interpretative categories in a context-sensitive manner.\nThe survey of the past work is divided further into four parts,\neach of which is dedicated to the efforts in one of the above-listed\nsubproblem areas.\n3.1. Data capture\nData capture refers to using sensors of different kinds to capture\nand record social interactions taking place in real-world scenarios.\nThe choice of the sensors and their arrangement in a specific\nrecording setup determine the rest of the SSP process and limit\nthe spectrum of behavioural cues that can be extracted. For exam-\nple, no gaze behaviour analysis can be performed, if appropriate\ndetectors are not included in the capture system.\nThe most common sensors are microphones and cameras and\nthey can be arranged in structures of increasing complexity: from\na single camera and\/or microphone to capture simple events like\noral presentations [201], to fully equipped smart meeting rooms\nwhere several tens of audio and video channels (including micro-\nFig. 5. Space and seating. The upper part of the figure shows the concentric zones\naround each individual associated to different kinds of rapport (d stands for\ndistance). The lower part of the figure shows the preferred seating arrangements for\ndifferent kinds of social interactions.\n1748\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":7,"text":"phone arrays, fisheye cameras, lapel microphones, etc.) are setup\nand synchronized to capture complex interactions taking place in\na group meeting [110,205]. The literature shows also examples of\nless common sensors such as cellular phones or smart badges\nequipped with proximity detectors and vocal activity measure-\nment devices [43,144], and systems for the measurement of phys-\niological activity indicators such as blood pressure and skin\nconductivity [76]. Recent efforts have tried to investigate the neu-\nrological basis of social interactions [2] through devices like\nfunctional magnetic resonance imaging (fMRI) [119], and Electroen-\ncephalography (EEG) signals [193].\nThe main challenges in human sensing research domain are pri-\nvacy and passiveness. The former involves ethical issues to be ad-\ndressed when people are recorded during spontaneous social\ninteractions. This subject is outside the scope of this paper, but\nthe informed consent principle [51] should be always respected\nmeaning that human subjects should always be aware of being re-\ncorded (e.g., like in broadcast material). Also, the subjects need to\nauthorize explicitly the use and the diffusion of the data and they\nmust have the right of deleting, partially or totally, the recordings\nwhere they are portrayed.\nThe second challenge relates to creating capture systems that\nare passive [125], i.e., unintrusive changing the behaviour of the re-\ncorded individuals as little as possible (in principle, the subjects\nshould not even realize that they are recorded). This is a non-trivial\nproblem because passive systems should involve only non-invasive\nsensors and the output of these is, in general, more difficult to pro-\ncess effectively. On the other hand, data captured by more invasive\nsensors are easier to process, but at the same time such recording\nsetups tend to change the behaviour of the recorded individuals.\nRecording human naturalistic behaviour while eliciting specific\nbehaviours and retaining the naturalism\/spontaneity of the behav-\niour is a very difficult problem tackled recently by several research\ngroups [29,135].\n3.2. Person detection\nThe sensors used for data capture output signals that can be\nanalyzed automatically to extract the behavioural cues underlying\nsocial signals and behaviours. In some cases, the signals corre-\nsponding to different individuals are separated at the origin. For\nexample, physiological signals are recorded by invasive devices\nphysically connected (e.g., through electrodes) to each person.\nThus, the resulting signals can be attributed without ambiguity\nto a given individual. However, it happens more frequently that\nthe signals contain spurious information (e.g., background noise),\nor they involve more than one individual. This is the case of the\nmost commonly used sensors, microphones, and cameras, and it\nrequires the application of algorithms for person detection capable\nof isolating the signal segments corresponding to a single individ-\nual. The rest of this section discusses how this can be done for mul-\ntiparty audio and video recordings.\n3.2.1. Person detection in multiparty audio recordings\nIn the case of audio recordings, person detection is called speak-\ner segmentation or speaker diarization and consists of splitting the\nspeech recordings into intervals corresponding to a single voice,\nrecognizing automatically who talks when (see [189], for an exten-\nsive survey). The speaker diarization is the most general case and it\nincludes three main stages: the first is the segmentation of the data\ninto speech and non-speech segments, the second is the detection\nof the speaker transitions, and the third is the so-called clustering,\ni.e., the grouping of speaker segments corresponding to a single\nindividual (i.e., to a single voice). In some cases (e.g., broadcast\ndata), no silences are expected between one speaker and the fol-\nlowing, thus the first step is not necessary. Systems that do not in-\nclude a speech\/non-speech segmentation are typically referred to\nas speaker segmentation systems.\nSpeech and non-speech segmentation is typically performed\nusing machine learning algorithms trained over different audio\nclasses represented in the data (non-speech can include music,\nbackground noises, silence, etc.). Typically used techniques include\nartificial neural networks [5], k nearest neighbours [107], Gaussian\nmixture models [61], etc. Most commonly used features include\nthe basic information that can be extracted from any signal (e.g.,\nenergy and autocorrelation [156]), as well as the features typically\nextracted for speech recognition like Mel frequency cepstrum coeffi-\ncients (MFCC), Linear predictive coding (LPC), etc. (see [83], for an\nextensive survey).\nThe detection of the speaker transitions is performed by split-\nting the speech segments into short intervals (e.g., 2\u20133 s) and by\nmeasuring the difference (see below) between two consecutive\nintervals: the highest values of the difference correspond to the\nspeaker changes. The approaches is based on the assumption that\nthe data include at least two speakers. If this is not the case, simple\ndifferences in the intonation or the background noise might be de-\ntected as speaker changes. The way the difference is estimated al-\nlows one to distinguish between the different approaches to the\ntask: in general each interval is modeled using a single Gaussian\n(preferred to the GMMs because it simplifies the calculations)\nand the difference is estimated with the symmetric Kullback-Lei-\nbler divergence [14]. Alternative approaches [157] use a penal-\nized-likelihood-ratio test to verify whether a single interval is\nmodeled better by a single Gaussian (no speaker change) or by\ntwo Gaussians (speaker change).\nThe last step of both speaker diarization and segmentation is\nclustering, i.e., grouping of the segments corresponding to a single\nvoice into a unique cluster. This is commonly carried out through\niterative approaches[14,117,157] where the clusters are initialized\nusing the intervals between the speaker changes detected at the\nprevious step (each interval is converted into a set of feature vec-\ntors using common speech processing techniques [83,156]), and\nthen they are iteratively merged based on the similarity of the\nmodels used to represent them (single Gaussians or GMMs). The\nmerging process is stopped when a criterion (e.g., the total likeli-\nhood of the cluster models starts to decrease) is met.\nMost recent approaches tend to integrate three steps above-\nmentioned into a single framework by using hidden Markov\nmodels or dynamic Bayesian networks that align feature vectors\nextracted at regular time steps (e.g., 30 ms) and sequences of states\ncorresponding to speakers in an unsupervised way [4,5].\n3.2.2. Person detection in multiparty video recordings\nIn the case of video data, the person detection consists in locat-\ning faces or full human figures (that must be eventually tracked).\nFace detection is typically the first step towards facial-expression\nanalysis [139] or gaze behaviour analysis [199] (see [81,215] for\nextensive surveys on face detection techniques). The detection of\nfull human figures is more frequent in surveillance systems where\nthe only important information is the movement of people across\nwide public spaces (e.g., train stations or streets) [62,115]. In the\nSSP framework, the detection of full human figures can be applied\nto study social signals related to space and distances (see Section\n2.5), but to the best of our knowledge no attempts have been made\nyet in this direction.\nEarly approaches to face detection (see e.g., [161,181]) were\nbased on the hypothesis that the presence of a face can be inferred\nfrom the pixel values. Thus, they apply classifiers like neural net-\nworks or support vector machines directly over small portions of\nthe video frames (e.g., patches of 20 ? 20 pixels) and map them\ninto a face\/non-face classes. The main limitation of such tech-\nniques is that it is difficult to train classifiers for a non-face class\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1749"},{"page":8,"text":"that can include any kind of visual information (see Fig. 7). Other\napproaches (e.g., [82,58]) try to detect human skin areas in images\nand then use their spatial distribution to identify faces and facial\nfeatures (eyes, mouth, and nose). The skin areas are detected by\nclustering the pixels in the color space. Alternative approaches\n(e.g., [101]) detect separately individual face elements (eyes, nose,\nand mouth) and detect a face where such elements have the appro-\npriate relative positions. These approaches are particularly robust\nto rotations because they depend on the relative position of face\nelements, rather than on the orientation with respect to a general\nreference frame in the image.\nAnother method that can handle out-of-plane head motions is\nthe statistical method for 3D object detection proposed in [169].\nOther such methods, which have been recently proposed, include\nthose in [83,207]. Most of these methods emphasize statistical\nlearning techniques and use appearance features. Arguably the\nmost commonly employed face detector in automatic facial-\nexpression analysis is the real-time face detector proposed in\n[204]. This detector consists of a cascade of classifiers trained by\nAdaBoost. Each classifier employs integral image filters, also called\n\u2018\u2018box filters\u201d, which are reminiscent of Haar basis functions, and\ncan be computed very fast at any location and scale. This is essen-\ntial to the speed of the detector. For each stage in the cascade, a\nsubset of features is chosen using a feature selection procedure\nbased on AdaBoost. There are several adapted versions of the face\ndetector described in [204] and the one that is often used is that\nproposed in [52].\nThe main challenge in detecting human figures is that people\nwear clothes of different color and appearance, so the pixel values\nare not a reliable feature for human body detection (see Fig. 8). For\nthis reason, some approaches extract features like the histograms\nof the edge directions (e.g., [34,223]) from local regions of the\nimages (typically arranged in a regular grid), and then make a deci-\nsion using classifiers like the support vector machines. The same\napproach can be improved in the case of the videos, by adding mo-\ntion information extracted using the optical flow [35]. Other ap-\nproaches (e.g., [114,194]) try to detect individual body parts and\nthen use general rules of human body anatomy to reason about\nthe body pose (individual body parts have always the same shape\nand they have the same relative position). For exhaustive survey,\nsee [153].\n3.3. Social signals detection\nOnce people in the observed scene are detected, the next step in\nthe SSP process is to extract behavioural cues displayed by these\npeople. Those cues include one or more synchronized audio and\/\nor video signals that convey the information about the behaviour\nof the person. They are the actual source from which socially rele-\nvant behavioural cues are extracted. The next sections discuss the\nData\nCapture\nPerson\nDetection\nMultimodal\nBehavioural\nStreams\nCues\nBehavioural\nExtraction\nSocial Signals\nUnderstanding\nContext\nUnderstanding\nBehavioural\nCues\nSocial\nBehaviours\nRaw Data\nPreprocessing\nMultimodal\nBehavioural\nStreams\nSocial Interaction Analysis\nFig. 6. Machine analysis of social signals and behaviours: a general scheme. The process includes two main stages: the preprocessing, takes as input the recordings of social\ninteraction and gives as output the multimodal behavioural streams associated with each person. The social interaction analysis maps the multimodal behavioural streams\ninto social signals and social behaviours.\nFig. 7. Face detection. General scheme of an appearance-based approach for face detection (picture from \u2018\u2018A tutorial on face detection and recognition\u201d, by S. Marcel, http:\/\/\nwww.idiap.ch\/~marcel).\n1750\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":9,"text":"main approaches to social signals detection from audio and\/or vi-\nsual signals captured while monitoring a person.\n3.3.1. Detection of social signals from physical appearance\nTo the best of our knowledge, only few works address the prob-\nlem of analyzing the physical appearance of people. However,\nthese works do not aim to interpret this information in terms of so-\ncial signals. Some approaches have tried to measure automatically\nthe beauty of faces [1,44,73,75,211]. The work in [1] detects sepa-\nrately the face elements (eyes, lips, etc.) and then maps the ratios\nbetween their dimensions and distances into beauty judgments\nthrough classifiers trained on images assessed by humans. The\nwork in [44] models the symmetry and the proportions of a face\nthrough the geometry of several landmarks (e.g., the corners of\nthe eyes and the tip of the nose), and then applies machine learn-\ning techniques to match human judgments. Other techniques (e.g.,\n[131]) use 3D models of human heads and the distance with re-\nspect to average faces extracted from large data sets to assess per-\nsonal beauty. Faces closest to the average seem to be judged as\nmore attractive than others.\nAlso few works were proposed where the body shape, the color\nof skin, hair, and clothes are extracted automatically (through a\nclustering of the pixels in the color space) for identification and\ntracking purposes [16,36,214]. However these works do not ad-\ndress social signal understanding and are therefore out of the scope\nof this paper.\n3.3.2. Detection of social signals from gesture and posture\nGesture recognition is an active research domain in computer\nvision and pattern recognition research communities, but no ef-\nforts have been made, so far, to interpret the social information\ncarried by gestural behaviours. In fact, the efforts are directed\nmostly towards the use of gestures as an alternative to keyboard\nand mouse to operate computers (e.g., [132,172,213]), or to the\nautomatic reading of sign languages (e.g., [40,97]). Also few efforts\nhave been reported towards human affect recognition from body\ngestures (for an overview see [76,221]). There are two main chal-\nlenges in recognizing gestures: detecting the body parts involved\nin the gesture (in general the hands), and modeling the temporal\ndynamic of the gesture.\nThe first problem is addressed by selecting appropriate visual\nfeatures: these include, e.g., histograms of oriented gradients\n(e.g., [183,184]), optical flow (e.g., [3,188]), spatio-temporal salient\npoints (e.g., [129]) and space-time volumes (e.g., [67]). The second\nproblem is addressed by using techniques such as dynamic time\nwarping (e.g., [129]), Hidden Markov models (e.g., [3]), and condi-\ntional random fields (e.g., [179]).\nLike in the case of gestures, machine recognition of walking\nstyle (or gait) has been investigated as well, but only for purposes\ndifferent from SSP, namely recognition and identification in bio-\nmetric applications[100,102,206]. The common approach is to seg-\nment the silhouette of the human body into individual components\n(legs, arms, trunk, etc.), and then to represent their geometry dur-\ning walking through vectors of distances [206], symmetry opera-\ntors [78], geometric features of body and stride (e.g., distance\nbetween head and feets or pelvis) [17], etc.\nAlso automatic posture recognition has been addressed in few\nworks, mostly aiming at surveillance [57] and activity recogni-\ntion [206] (see [54,116,153] for extensive overviews of the past\nwork in the field). However, there are few works where the pos-\nture is recognized as a social signal, namely to estimate the\ninterest level of children learning to use computers [124], to rec-\nognize the affective state of people [38,74] (see [76,221], for\nexhaustive overview of research efforts in the field), and the\ninfluence of culture on affective postures [95].\n3.3.3. Detection of social signals from gaze and face\nThe problem of machine recognition of human gaze and facial\nbehaviour includes three subproblem areas (see Fig. 9): finding\nfaces in the scene, extracting facial features from the detected face\nregion, analyzing the motion of eyes and other facial features and\/\nor the changes in the appearance of facial features, and classifying\nthis information into some facial-behaviour-interpretative catego-\nries (e.g., facial muscle actions (AUs), emotions, social behaviours,\netc.).\nNumerous techniques have been developed for face detection,\ni.e., identification of all regions in the scene that contain a human\nface (see Section 3.2). Most of the proposed approaches to facial\nexpression recognition are directed toward static, analytic, 2D fa-\ncial feature extraction [135,185]. The usually extracted facial fea-\ntures are either geometric features such as the shapes of the\nfacial components (eyes, mouth, etc.) and the locations of facial\nfiducial points (corners of the eyes, mouth, etc.) or appearance fea-\ntures representing the texture of the facial skin in specific facial\nareas including wrinkles, bulges, and furrows. Appearance-based\nfeatures include learned image filters from independent compo-\nnent analysis (ICA), principal component analysis (PCA), local fea-\nture analysis (LFA), Gabor filters, integral image filters (also\nknown as box filters and Haar-like filters), features based on\nedge-oriented histograms, and similar [135]. Several efforts have\nbeen also reported which use both geometric and appearance fea-\ntures (e.g., [185]). These approaches to automatic facial-expression\nanalysis are referred to as hybrid methods. Although it has been\nreported that methods based on geometric features are often out-\nperformed by those based on appearance features using, e.g., Gabor\nwavelets or eigenfaces, recent studies show that in some cases geo-\nmetric features can outperform appearance-based ones [135,136].\nYet, it seems that using both geometric and appearance features\nmight be the best choice in the case of certain facial expressions\n[136].\nContractions of facial muscles (i.e., AUs explained in Section\n2.3), which produce facial expressions, induce movements of the\nFig. 8. People detection. Examples of people detection in public spaces (pictures from [216]).\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1751"},{"page":10,"text":"facial skin and changes in the location and\/or appearance of facial\nfeatures. Such changes can be detected by analyzing optical flow,\nfacial-point- or facial-component-contour-tracking results, or by\nusing an ensemble of classifiers trained to make decisions about\nthe presence of certain changes based on the passed appearance\nfeatures. The optical flow approach to describing face motion has\nthe advantage of not requiring a facial feature extraction stage of\nprocessing. Dense flow information is available throughout the en-\ntire facial area, regardless of the existence of facial components,\neven in the areas of smooth texture such as the cheeks and the\nforehead. Because optical flow is the visible result of movement\nand is expressed in terms of velocity, it can be used to represent di-\nrectly facial expressions. Many researchers adopted this approach\n(for overviews, see [135,139,185]). Until recently, standard optical\nflow techniques were arguably most commonly used for tracking\nfacial characteristic points and contours as well. In order to address\nthe limitations inherent in optical flow techniques such as the\naccumulation of error and the sensitivity to noise, occlusion, clut-\nter, and changes in illumination, recent efforts in automatic facial-\nexpression recognition use sequential state estimation techniques\n(such as Kalman filter and particle filter) to track facial feature\npoints in image sequences [135,136,222].\nEventually, dense flow information, tracked movements of fa-\ncial characteristic points, tracked changes in contours of facial\ncomponents, and\/or extracted appearance features are translated\ninto a description of the displayed facial behaviour. This descrip-\ntion (facial-expression interpretation) is usually given either in\nterms of shown affective states (emotions) or in terms of activated\nfacial muscles (AUs) underlying the displayed facial behaviour.\nMost facial-expressions analyzers developed so far target human\nfacial affect analysis and attempt to recognize a small set of proto-\ntypic emotional facial expressions like happiness and anger\n[140,221]. However, several promising prototype systems were re-\nported that can recognize deliberately produced AUs in face images\n(for overviews, see [135,185]) and even few attempts towards rec-\nognition of spontaneously displayed AUs (e.g., [103,108]) and to-\nwards automatic discrimination between spontaneous and posed\nfacial behaviour such as smiles [195], frowns [197], and pain\n[104], have been recently reported as well. Although still tentative,\nfew studies have also been recently reported on separating emo-\ntional states from non-emotional states and on recognition of\nnon-basic affective states in visual and audiovisual recordings of\nspontaneous human behaviour (e.g., for overview see [170,220]).\nHowever, although messages conveyed by AUs like winks, blinks,\nfrowns, smiles, gaze exchanges, etc., can be interpreted in terms\nof social signals like turn taking, mirroring, empathy, engagement,\netc., no efforts have been reported so far on automatic recognition\nof social behaviours in recordings of spontaneous facial behaviour.\nHence, while the focus of the research in the field started to shift to\nautomatic (non-basic-) emotion and AU recognition in spontane-\nous facial expressions (produced in a reflex-like manner), efforts\ntowards automatic analysis of human social behaviour from visual\nand audiovisual recordings of human spontaneous behaviour are\nstill to be made.\nWhile the older methods for facial-behaviour analysis employ\nsimple approaches including expert rules and machine learning\nmethods such as neural networks to classify the relevant informa-\ntion from the input data into some facial-expression-interpretative\ncategories (e.g., basic emotion categories), the more recent (and of-\nten more advanced) methods employ probabilistic, statistical, and\nFig. 9. AU detection. Outline of a geometric-feature-based system for detection of facial AUs and their temporal phases (onset, apex, offset, and neutral) proposed in [196].\n1752\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":11,"text":"ensemble learning techniques, which seem to be particularly suit-\nable for automatic facial-expression recognition from face image\nsequences (for comprehensive overviews of the efforts in the field,\nsee[135,221]). Note, however, the present systems for facial-expression\nanalysis typically depend on accurate head, face, and facial feature\ntracking as input and are still very limited in performance and\nrobustness.\n3.3.4. Detection of social signals from vocal behaviour\nThe behavioural cues in speech include voice quality, vocal-\nizations (linguistic and non-linguistic), and silences (see Section\n2.4, for details). All of them have been the subject of extensive\nresearch in speech, but they have rarely been interpreted in\nterms of social information, even if they account for roughly\n50% of the total time in spontaneous conversations [21]. With\nfew exceptions, the detection of vocal behaviour has aimed at\nthe improvement of automatic speech recognition (ASR) systems,\nwhere the vocal non-verbal behaviour represents a form of noise\nrather than an information.\nThe voice quality corresponds to the prosody and includes three\nmajor aspects, often called the Big Three: pitch, tempo, and energy\n[31]. The pitch is the frequency of oscillation of the vocal folds dur-\ning the emission of voice and it is the characteristic that alone con-\ntributes more than anything else to the sound of a voice [120,150].\nThe measurement of the pitch, often called fundamental frequency\n(or F0) because most of the speech energy is concentrated over\ncomponents corresponding to its integer multiples, can be per-\nformed with several standard methods proposed in the literature\n[83,156]. The pitch is typically extracted as the frequency corre-\nsponding to the first peak of the Fourier transform of short analysis\nwindows (in general 30 ms). Several tools publicly available on the\nweb, e.g., Wavesurfer1[177] and Praat2[18], implement algorithms\nextracting the pitch from speech recordings. The tempo is typically\nestimated through the speaking rate, i.e., the number of phonetically\nrelevant units, e.g., vowels [149], per second. Other methods are\nbased on measures extracted from the speech signal like the first\nspectral moment of the energy [121,122] and typically aim at\nimproving speech recognition systems through speaking rate adap-\ntation. The energy is a property of any digital signal and simply cor-\nresponds to the sum of the square values of the samples [156].\nNo major efforts have been made so far, to the best of our\nknowledge, to detect the non-linguistic vocalizations (see Section\n2.4). The only exceptions are laughter[92,191,192] due to its ubiq-\nuitous presence in social interactions, and crying[118,134]. Laugh-\nter is detected by applying binary classifiers such as Support Vector\nMachines to features commonly applied in speech recognition like\nthe Mel frequency cepstral coefficients [92], or by modeling percep-\ntual linear prediction features with Gaussian mixture models and\nneural networks [191,192]. These efforts are based only on audio\nsignals, but few pioneering efforts towards audiovisual recognition\nof non-linguistic vocal outbursts have been recently reported. A\nlaughter detector which combines the outputs of an audio-based\ndetector that uses MFCC audio features and a visual detector that\nuses spatial locations of facial feature points is proposed in [86].\nThey attained 80% average recall rate using three sequences of\nthree subjects in a person-dependent way. In [147], decision-level\nand feature-level fusion with audio- and video-only laughter\ndetection are compared. The work uses PLP features and displace-\nments of the tracked facial points as the audio and visual features,\nrespectively. Both fusion approaches outperformed single-modal\ndetectors, achieving on average 84% recall in a person-independent\ntest. Extension of this work based on utilization of temporal fea-\ntures has been reported in [148].\nLinguistic vocalizations have been investigated to detect hesita-\ntions in spontaneous speech [105,173,174] with the main purpose\nof improving speech recognition systems. The disfluencies are typ-\nically detected by mapping acoustic observations (e.g., pitch and\nenergy) into classes of interest with classifiers like neural networks\nor support vector machines. The detection of silence is one of the\nearliest tasks studied in speech analysis and robust algorithms,\nbased on the distribution of the energy, have been developed since\nthe earliest times of digital signal processing [155,156]. Another\nimportant aspect of vocal behaviour, i.e., the turn taking, is typi-\ncally a side-product of the speaker diarization or segmentation\nstep (see Section 3.2).\n3.3.5. Detection of social signals in space and environment\nPhysical proximity information has been used in reality mining\napplications (see Section 4) as a social cue accounting for the sim-\nple presence or absence of interaction between people [43,144].\nThese works use special cellular phones equipped to sense the\npresence of similar devices in the vicinity. Automatic detection of\nseating arrangements has been proposed as a cue for retrieving\nmeeting recordings in [88]. Also, several video-surveillance\napproaches developed to track people across public spaces can\npotentially be used for detection of social signals related to the\nuse of the available space (see Section 3.2, for more details).\n3.4. Context sensing and social behaviour understanding\nContext plays a crucial role in understanding of human behav-\nioural signals, since they are easily misinterpreted if the informa-\ntion about the situation in which the shown behavioural cues\nhave been displayed is not taken into account. For example, a smile\ncan be a display of politeness (social signal), contentedness (affec-\ntive cue), joy of seeing a friend (affective cue\/social signal), irony\/\nirritation (affective cue\/social signal), empathy (emotional re-\nsponse\/social signal), greeting (social signal), to mention just a\nfew possibilities. It is obvious from these examples that in order\nto determine the communicative intention conveyed by an\nobserved behavioural cue, one must know the context in which\nthe observed signal has been displayed: where the expresser is\n(outside, inside, in the car, in the kitchen, etc.), what his or her cur-\nrent task is, are other people involved, when the signal has been\ndisplayed (i.e., what is the timing of displayed behavioural signals\nwith respect to changes in the environment), and who the expres-\nser is (i.e., it is not probable that each of us will express a particular\naffective state by modulating the same communicative signals in\nthe same way).\nNote, however, that while W4 (where, what, when, who) is deal-\ning only with the apparent perceptual aspect of the context in\nwhich the observed human behaviour is shown, human behaviour\nunderstanding is about W5+ (where, what, when, who, why, how),\nwhere the why and how are directly related to recognizing commu-\nnicative intention including social behaviours, affective and cogni-\ntive states of the observed person. Hence, SSP is about W5+.\nHowever, since the problem of context sensing is extremely diffi-\ncult to solve, especially for a general case (i.e., general-purpose\nW4 technology does not exist yet [138,137]), answering the why\nand how questions in a W4-context sensitive manner when analys-\ning human behaviour is virtually unexplored area of research. Hav-\ning said that, it is not a surprise that most of the present\napproaches to machine analysis of human behaviour are neither\ncontext sensitive nor suitable for handling longer time scales.\nHence, the focus of future research efforts in the field should be\nprimarily on tackling the problem of context-constrained analysis\nof multimodal social signals shown over longer temporal intervals.\n1Publicly available at http:\/\/www.speech.kth.se\/wavesurfer\/.\n2Publicly available at http:\/\/www.praat.org.\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1753"},{"page":12,"text":"Here, we would like to stress the importance of two issues:\nrealizing temporal analysis of social signals and achieving tempo-\nral multimodal data fusion.\nTemporal dynamics of social behavioural cues (i.e., their timing,\nco-occurrence, speed, etc.) are crucial for the interpretation of the\nobserved social behaviour [8,50]. However, present methods for\nhuman behaviour analysis do not address the when context ques-\ntion \u2013 dynamics of displayed behavioural signals is usually not ta-\nken into account when analyzing the observed behaviour, let alone\nanalysing the timing of displayed behavioural signals with respect\nto changes in the environment. Exceptions of this rule include few\nrecent studies on modeling semantic and temporal relationships\nbetween facial gestures (i.e., AUs, see Section 2.3) forming a facial\nexpression (e.g., [187]), few studies on discrimination between\nspontaneous and posed facial gestures like brow actions and smiles\nbased on temporal dynamics of target facial gestures, head and\nshoulder gestures [195,197], and few studies on multimodal anal-\nysis of audio and visual dynamic behaviours for emotion recogni-\ntion [221]. In general, as already mentioned above, present\nmethods cannot handle longer time scales, model grammars of ob-\nserved persons behaviours, and take temporal and context-depen-\ndent evolvement of observations into account for more robust\nperformance. These remain major challenges facing the research-\ners in the field.\nSocial signals are spoken and wordless messages like head nods,\nwinks,uh,andyeahutterances,whicharesentbymeansofbodyges-\ntures and postures, facial expressions and gaze, vocal expressions\nand speech. Hence, automated analyzers of human social signals\nand social behaviours should be multimodal, fusing and analyzing\nverbal and non-verbal interactive signals coming from different\nmodalities (speech, body gestures, facial, and vocal expressions).\nMostofthepresentaudiovisualandmultimodalsystemsinthefield\nperform decision-level data fusion (i.e., classifier fusion) in which\nthe input coming from each modality is modeled independently\nand these single-modal recognition results are combined at the\nend.Sincehumansdisplayaudioandvisualexpressionsinacomple-\nmentary and redundant manner, the assumption of conditional\nindependencebetweenaudioandvisualdatastreamsindecision-le-\nvelfusionisincorrectandresultsinthelossofinformationofmutual\ncorrelation between the two modalities. To address this problem, a\nnumber of model-level fusion methods have been proposed that\naim at making use of the correlation between audio and visual data\nstreams, and relax the requirement of synchronization of these\nstreams (e.g., [55,220]). However, how to model multimodal fusion\non multiple time scales and how to model temporal correlations\nwithin and between different modalities is largely unexplored. A\nmuch broader focus on the issues relevant to multimodal temporal\nfusion is needed including the optimal level of integrating these dif-\nferent streams, the optimal function for the integration, and how\nestimationsofreliabilityofeachstreamcanbeincludedintheinfer-\nence process. In addition, how to build context-dependent multi-\nmodal fusion is another open and highly relevant issue.\n4. Main applications of social signal processing\nThe expression social signal processing has been used for the\nfirst time in [145] to group under a collective definition several\npioneering works of Alex Pentland and his group at MIT. Some of\ntheir works [142,143] extracted automatically the social signals\ndetected in dyadic interactions to predict with an accuracy of\nmore than 70% the outcome of salary negotiations, hiring inter-\nviews, and speed-dating conversations [33]. These works are\nbased on vocal social signals including overall activity (the total\namount of energy in the speech signals), influence (the statistical\ninfluence of one person on the speaking patterns of the others),\nconsistency (stability of the speaking patterns of each person),\nand mimicry (the imitation between people involved in the inter-\nactions). Other works used cellular phones equipped with prox-\nimity sensors and vocal activity detectors to perform what came\nto be called reality mining, or social sensing, i.e., automatic anal-\nysis of everyday social interactions in groups of several tens of\nindividuals [43,144]. Individuals are represented through vectors\naccounting for their proximity with others and for the places\nthey are (home, work, etc.). The application of the principal\ncomponent analysis to such vectors leads to the so called eigen-\nbehaviours [43].\nIn approximately the same period, few other groups worked on\nthe analysis of social interactions in multimedia recordings target-\ning three main areas: analysis of interactions in small groups, rec-\nognition of roles, and sensing of users interest in computer\ncharacters. Results for problems that have been addressed by more\nthan one group are reported in Table 2.\nThe research on interactions in small groups has focused on the\ndetection of dominant persons and on the recognition of collective\nactions. The problem of dominance is addressed in [85,160], where\nmultimodal approaches combine several non-verbal features,\nmainly speaking energy, and body movement, to identify at each\nmoment who is the dominant individual. The same kind of features\nhas been applied in [39,111] to recognize the actions performed in\nmeetings like discussions, presentations, etc. In both above appli-\ncations, the combination of the information extracted from differ-\nent modalities is performed with algorithms dynamic Bayesian\nnetworks [126] and layered Hidden Markov models [130].\nThe recognition of roles has been addressed in two main con-\ntexts: broadcast material[15,53,200,210] and small scale meetings\n[13,42,59,218]. The works in [53,200,210] apply social network\nanalysis [209] to detect the role of people in broadcast news and\nmovies, respectively. The social networks are extracted automati-\ncally using speaker adjacences in [53,200] (people are linked when\nthey are adjacent in the sequence of the speakers), and face recog-\nnition [210] (people are linked when their faces appear together in\na scene). The approach in [15] recognizes the roles of speakers in\nbroadcast news using vocal behaviour (turn taking patterns and\nintervention duration) and lexical features. The recognition is per-\nformed using boosting techniques. The roles in meetings are recog-\nnized with a classifier tree applied to non-verbal behaviour\nfeatures (overlapping speech, number of interventions, back-chan-\nneling, etc.) in the case of [13], while speech and fidgeting activity\nare fed to a multi-SVM classifier in [42,218]. A technique based on\nthe combination of social network analysis and lexical modeling\n(Boostexter) is presented in [59].\nThe reaction of users to social signals exhibited by computer\ncharacters has been investigated in several works showing that\npeople tend to behave with embodied conversational agents\n(ECA) as they behave with other humans. The effectiveness of com-\nputers as social actors, i.e., entities involved in the same kind of\ninteractions as humans, has been explored in [127,128], where\ncomputers have been shown to be attributed a personality and to\nelicit the same reactions as those elicited by persons. Similar ef-\nfects have been shown in[28,133], where children interacting with\ncomputers have modified their voice to match the speaking charac-\nteristics of the animated ECA, showing adaptation patterns typical\nof human\u2013human interactions [20]. Further evidence of the same\nphenomenon is available in[10,11], where the interaction between\nhumans and ECA is shown to include the Chameleon effect [22], i.e.,\nthe mutual imitation of individuals due to reciprocal appreciation\nor to the influence of one individual on the other.\nPsychologists have compared the performance of humans and\nmachines in detecting socially relevant information like gender\nand movements associated to emotional states [65,151,152]. The\nresults show that machines tend to have a constant performance\n1754\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":13,"text":"across a wide range of conditions (different behavioural cues at\ndisposition), while humans have dramatic changes in performance\n(sometimes dropping at chance level) when certain behavioural\ncues are no longer at disposition. This seems to suggest that hu-\nmans do not use the behavioural cues actually at their disposition,\nbut rather rely on task-specific behavioural cues without which the\ntasks cannot be performed effectively [65,151,152]. In contrast,\nautomatic approaches (in particular those based on machine learn-\ning) are built to rely on any available behavioural cue and their\nperformance simply depends on how much the available cues are\nactually correlated with the targeted social information.\n5. Conclusions and future challenges\nSocial signal processing has the ambitious goal of bringing social\nintelligence[6,66]incomputers.Thefirstresultsinthisresearchdo-\nmain have been sufficiently impressive to attract the praise of the\ntechnology [69] and business [19] communities. What is more\nimportant is that they have established a viable interface between\nhuman sciences and engineering \u2013 social interactions and behav-\niours,althoughcomplexandrootedinthedeepestaspectsofhuman\npsychology,canbeanalyzedautomaticallywiththehelpofcomput-\ners.Thisinterdisciplinarityis,inouropinion,themostimportantre-\nsult of research in SSP so far. In fact, the pioneering contributions in\nSSP[142,143]haveshownthatthesocialsignals,typicallydescribed\nassoelusiveandsubtlethatonlytrainedpsychologistscanrecognize\nthem[63],areactuallyevidentanddetectableenoughtobecaptured\nthrough sensors like microphones and cameras, and interpreted\nthrough analysis techniques like machine learning and statistics.\nHowever, although fundamental, these are only the first steps\nandthejourneytowardsartificialsocialintelligenceandsociallyaware\ncomputingisstilllong.Intherestofthissection,wediscussfourchal-\nlengesfacingtheresearchersinthefield,forwhichwebelievearethe\ncrucialturnoverissuesthatneedtobeaddressedbeforetheresearch\nin the field can enter its next phase \u2013 the deployment phase.\nThefirstissuerelatestotighteningofthecollaborationbetweensocial\nscientists and engineers. The analysis of human behaviour in general,\nand social behaviour in particular, is an inherently multidisciplinary\nproblem [138,221]. More specifically no automatic analysis of social\ninteractions is possible without taking into account the basic mecha-\nnismsgoverningsocialbehavioursthatthepsychologistshaveinvesti-\ngated for decades, such as the chameleon effect (mutual imitation of\npeopleaimedatshowinglikingoraffiliation)[22,99],theinterpersonal\nadaptation (mutual accommodation of behavioural patterns between\ninteractingindividuals)[20,71],theinteractionalsynchrony(degreeof\ncoordinationduringinteractions)[93],thepresenceorrolesingroups\n[12,186], the dynamics of conversations[154,217], etc. The collabora-\ntion between technology and social sciences demands a mutual effort\nof the two disciplines. On one hand, engineers need to include the so-\ncial sciences in their reflection, while on the other hand, social scien-\ntists need to formulate their findings in a form useful for engineers\nand their work on SSP.\nThe second issue relates to the need of implementing multi-cue,\nmultimodalapproachestoSSP.Non-verbalbehaviourscannotberead\nlike wordsin a book[96,158]; they are notunequivocallyassociated\nto a specific meaning and their appearance can depend on factors\nthat have nothing to do with social behaviour. Postures correspond\nin general to social attitudes, but sometimes they are simply com-\nfortable [166], physical distances typically account for social dis-\ntances, but sometimes they are simply the effect of physical\nconstraints[77].Moreover,thesamesignalcancorrespondtodiffer-\nent social behaviour interpretations depending on context and cul-\nture [190] (although many advocate that social signals are natural\nrather than cultural [171]). In other words, social signals are intrin-\nsically ambiguous and the best way to deal with such problem is to\nuse multiple behavioural cues extracted from multiple modalities.\nNumerous studies have theoretically and empiricallydemonstrated\nthe advantage of integration of multiple modalities (at least audio\nand visual) in human behaviour analysis over single modalities\n(e.g., [162]). This corresponds, from a technological point of view,\nto the combination of different classifiers that has extensively been\nshowntobemoreeffectivethansingleclassifiers,aslongastheyare\nsufficiently diverse, i.e., account for different aspects of the same\nproblem[94].Itisthereforenotsurprisingthatsomeofthemostsuc-\ncessful works in SSP so far use features extracted from multiple\nmodalities like in [39,85,111]. Note, however, that the relative con-\ntributions of different modalities and the related behavioural cues\nto affect judgment of displayed behaviour depend on the targeted\nbehavioural categoryandthe contextinwhichthebehaviouroccurs\n[49,162].\nTable 2\nResults obtained by social signal processing works. For each work, information about the data (kind of interaction, availability, size, the total duration of the recordings), whether\nit is real-world or acted data, and the reported performance are summarized.\nRef.Data Time SourcePerformance\nRole recognition\n[13]\n[15]\nMeetings (2 recordings, 3 roles)\nNIST TREC SDR Corpus (35 recordings, publicly available\n3 roles)\nThe Survival Corpus (11 recordings, publicly available, 5\nroles)\nAMI Meeting Corpus (138 recordings, publicly available,\n4 roles)\nRadio news bulletins (96 recordings, 6 roles)\nMovies (3 recordings, 4 roles)\nThe Survival Corpus (11 recordings, publicly available, 5\nroles)\n0 h.45 m\n17 h.00 m\nActed\nSpontaneous\n50.0% of segments (up to 60 s long) correctly classified\n80.0% of the news stories correctly labeled in terms of role\n[42]4 h.30 m Acted 90% of precision in role assignment\n[59]45 h.00 m Acted 67.9% of the data time correctly labeled in terms of role\n[200]\n[210]\n[218]\n25 h.00 m\n5 h.46 m\n4 h.30 m\nSpontaneous\nSpontaneous\nSpontaneous\n80% of the data time correctly labeled in terms of role\n95% of roles correctly assigned\nUp to 65% of analysis windows (around 10 s long) correctly classified\nin terms of role\nCollective action recognition\n[39] Meetings (30 recordings, publicly available)\n[111]Meetings (60 recordings, publicly available)\n2 h.30 m\n5 h.00 m\nActed\nActed\nAction error rate of 12.5%\nAction error rate of 8.9%\nInterest level detection\n[60] Meetings (50 recordings, 3 interest levels)\n[124] Children playing with video games (10 recordings, 3\ninterest levels)\nUnknown\n3 h.20 m\nActed\nSpontaneous\n75% Precision\n82% recognition rate\nDominance detection\n[85]\n[159]\n[160]\nMeetings from AMI Corpus (34 segments)\nMeetings (8 meetings)\nMeetings (40 recordings)\n3 h.00 m\n1 h.35 m\n20 h.00 m\nActed\nActed\nActed\nMost dominant person correctly detected in 85% of segments\nMost dominant person correctly detected in 75% of meetings\nMost dominant person correctly detected in 60% of meetings\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1755"},{"page":14,"text":"The third issue relates to the use of real-world data. Both psy-\nchologists and engineers tend to produce their data in laboratories\nand artificial settings (see e.g., [33,68,111]), in order to limit para-\nsitic effects and elicit the specific phenomena they want to ob-\nserve. However, this is likely to simplify excessively the situation\nand to improve artificially the performance of the automatic ap-\nproaches. Social interaction is one of the most ubiquitous phenom-\nena in the world \u2013 the media (radio and television) show almost\nexclusively social interactions (debates, movies, talk-shows)\n[123]. Also other, less common kinds of data are centered on social\ninteractions, e.g., meeting recordings [110], surveillance material\n[87], and similar. The use of real-world data will allow analysis\nof interactions that have an actual impact on the life of the partic-\nipants, thus will show the actual effects of goals and motivations\nthat typically drive human behaviour. This includes also the anal-\nysis of group interactions, a task difficult from both technological\nand social point of view because it involves the need of observing\nmultiple people involved in a large number of one-to-one\ninteractions.\nThe last, but not least, challenging issue relates to the the iden-\ntification of applications likely to benefit from SSP. Applications have\nthe important advantage of linking the effectiveness of detecting\nsocial signals to the reality. For example, one of the earliest appli-\ncations is the prediction of the outcome in transactions recorded at\na call center and the results show that the number of successful\ncalls can be increased by around 20% by stopping early the calls\nthat are not promising [19]. This can have not only a positive im-\npact on the marketplace, but also provide benchmarking procedures\nfor the SSP research, one of the best means to improve the overall\nquality of a research domain as extensively shown in fields where\ninternational evaluations take place every year (e.g., video analysis\nin TrecVid [178]).\nAcknowledgements\nThe work of Dr. Vinciarelli is supported by the Swiss National\nScience Foundation through the National Center of Competence\nin Research on Interactive Multimodal Information Management\n(IM2). The work of Dr. Pantic is supported in part by the EC\u2019s 7th\nFramework Programme (FP7\/2007-2013) under Grant Agreement\nNo. 211486 (SEMAINE), and the European Research Council under\nthe ERC Starting Grant Agreement No. ERC-2007-StG-203143\n(MAHNOB). The research that has led to this work has been sup-\nported in part by the European Community\u2019s Seventh Framework\nProgramme (FP7\/2007-2013),\n231287 (SSPNet).\nunderGrant Agreement No.\nReferences\n[1] P. Aarabi, D. Hughes, K. Mohajer, M. Emami, The automatic measurement of\nfacial beauty, in: Proceedings of IEEE International Conference on Systems,\nMan, and Cybernetics, 2001, pp. 2644\u20132647.\n[2] R.AdolphsCognitive neuroscience of human social behaviourNature Reviews\nNeuroscience 4 (3)2003 165\u2013178.\n[3] M.Ahmad, S.-W.LeeHuman action recognition using shape and CLG-motion\nflow from multi-view image sequencesPattern Recognition 41 (7)2008 2237\u2013\n2252.\n[4] J. Ajmera, Robust Audio Segmentation, Ph.D. thesis, \u00c9cole Polytechnique\nF\u00e9d\u00e9rale de Lausanne (EPFL), 2004.\n[5] J.Ajmera, I.McCowan, H.BourlardSpeech\/music segmentation using entropy\nand dynamism featuresina HMM\nCommunication 40 (3)2003 351\u2013363.\n[6] K.AlbrechtSocial Intelligence: The New Science of Success2005, John Wiley &\nSons Ltd., 2005.\n[7] N. Ambady, F. Bernieri, J. Richeson, Towards a histology of social behavior:\njudgmental accuracy from thin slices of behavior, in: M.P. Zanna (Ed.),\nAdvances in Experimental Social Psychology, 2000, pp. 201\u2013272.\n[8] N.Ambady, R.RosenthalThin slices of expressive behavior as predictors of\ninterpersonal consequences: a meta-analysisPsychological Bulletin 111\n(2)1992 256\u2013274.\nclassificationframeworkSpeech\n[9] M.ArgyleThe\nHarmondsworth, 1967.\n[10] J.N.Bailenson,\nchameleonsJournal of Nonverbal Behavior 31 (4)2007 225\u2013242.\n[11] J.N.Bailenson,N.Yee,\nchameleonsComputers in Human Behavior 24 (1)2008 66\u201387.\n[12] R.F.BalesInteraction Process Analysis: A Method for the Study of Small\nGroups1950, Addison-Wesley, Reading, MA, 1950.\n[13] S. Banerjee, A.I. Rudnicky, Using simple speech based features to detect the\nstate of a meeting and the roles of the meeting participants, in: Proceedings of\nInternational Conference on Spoken Language Processing, 2004, pp. 2189\u2013\n2192.\n[14] C. Barras, X. Zhu, S. Meignier, J.L. Gauvain, Improving speaker diarization, in:\nProceedings of the Rich Transcription Workshop, 2004.\n[15] R. Barzilay, M. Collins, J. Hirschberg, S. Whittaker, The rules behind the roles:\nidentifying speaker roles in radio broadcasts, in: Proceedings of American\nAssociation of Artificial Intelligence Symposium, 2000, pp. 679\u2013684.\n[16] C.Ben Abdelkader, Y.YacoobStatistical estimation of human anthropometry\nfrom a single uncalibrated image, in: K.Franke, S.Petrovic, A.Abraham\n(Eds.)Computational Forensics2009, Springer-Verlag, Berlin, 2009.\n[17] A.F. Bobick, A. Johnson, Gait recognition using static activity-specific\nparameters, in: Proceedings of Computer Vision and Pattern Recognition,\n2001, pp. 423\u2013430.\n[18] P.Boersma, D.WeeninkPraat, a system for doing phonetics by computerGlot\nInternational 5 (9\/10)2001 341\u2013345.\n[19] M.BuchananThe science of subtle signalsStrategy+Business 482007 68\u201377.\n[20] J.K.Burgoon, L.A.Stern, L.DillmanInterpersonal Adaptation: Dyadic Interaction\nPatterns1995, Cambridge University Press, Cambridge, 1995.\n[21] N.CampbellConversationalspeech\nlaughterIEEE Transactions on Speech and Language Processing 14 (4)2006\n1171\u20131178.\n[22] T.L.Chartrand, J.A.BarghThe chameleon effect: the perception-behavior link\nand social interactionJournal of Personality and Social Psychology 76 (6)1999\n893\u2013910.\n[23] J.F. Cohn, Foundations of human computing: facial expression and emotion,\nin: Proceedings of the ACM International Conference on Multimodal\nInterfaces, 2006, pp. 233\u2013238.\n[24] J.F. Cohn, P. Ekman, Measuring facial action by manual coding, facial EMG,\nand automatic facial image analysis, in: J.A. Harrigan, R. Rosenthal, K.R.\nScherer (Eds.), Handbook of Nonverbal Behavior Research Methods in the\nAffective Sciences, 2005, pp. 9\u201364.\n[25] J.B.Cortes, F.M.GattiPhysique and self-description of temperamentJournal of\nConsulting Psychology 29 (5)1965 432\u2013439.\n[26] M.Costa, W.Dinsbach,A.S.R.Manstead,\nembarrassment, and nonverbal behaviorJournal of Nonverbal Behavior 25\n(4)2001 225\u2013240.\n[27] M.CoulsonAttributing emotion to static body postures: recognition accuracy,\nconfusions, and viewpoint dependenceJournal of Nonverbal Behavior 28\n(2)2004 117\u2013139.\n[28] R. Coulston, S. Oviatt, C. Darves, Amplitude convergence in children\u2019s\nconversational speech with animated personas, in: International Conference\non Spoken Language Processing, 2002, pp. 2689\u20132692.\n[29] R. Cowie, Building the databases needed to understand rich, spontaneous\nhuman behaviour, in: Proceedings of the IEEE International Conference on\nAutomatic Face and Gesture Recognition, 2008.\n[30] R.Cowie, E.Douglas-Cowie, N.Tsapatsoulis, G.Votsis, S.Kollias, W.Fellenz,\nJ.G.TaylorEmotion recognition in human\u2013computer interactionIEEE Signal\nProcessing Magazine 18 (1)2001 32\u201380.\n[31] D.CrystalProsodic Systems and Intonation in English1969, Cambridge\nUniversity Press, Cambridge, 1969.\n[32] D.W. Cunningham, M. Kleiner, H.H. B\u00fclthoff, C. Wallraven, The components of\nconversational facial expressions, in: Proceedings of the Symposium on\nApplied Perception in Graphics and Visualization, 2004, pp. 143\u2013150.\n[33] J.R.Curhan, A.PentlandThin slices of negotiation: predicting outcomes from\nconversational dynamics within the first 5 minutesJournal of Applied\nPsychology 92 (3)2007 802\u2013811.\n[34] N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in:\nProceedings of Conference on Computer Vision and Pattern Recognition, vol.\n1, 2005, pp. 886\u2013893.\n[35] N. Dalal, B. Triggs, C. Schmid, Human detection using oriented histograms of\nflow and appearance, in: Proceedings of the European Conference on\nComputer Vision, 2006, pp. 428\u2013441.\n[36] T.Darrell, G.Gordon, M.Harville, J.WoodfillIntegrated person tracking using\nstereo, color, and pattern detectionInternational Journal of Computer Vision\n37 (2)2000 175\u2013185.\n[37] C. Darwin, The Expression of the Emotions in Man and Animals, J. Murray,\n1872.\n[38] R.De Silva, N.Bianchi-BerthouzeModeling human affective postures: an\ninformationtheoretic characterization\nComputational Animation and Virtual World 15 (3\u20134)2004 269\u2013276.\n[39] A.Dielmann,S.RenalsAutomatic meeting\nbayesian networksIEEE Transactions on Multimedia 9 (1)2007 25.\n[40] L. Ding, A.M. Martinez, Recovering the linguistic components of the manual\nsigns in american sign language, in: Proceedings of IEEE International\nConference on Advanced Video and Signal-based Surveillance, 2007, pp.\n447\u2013452.\nPsychology of InterpersonalBehaviour1967,Penguin,\nN.YeeVirtual interpersonaltouchand digital\nK.Patel,A.C.BeallDetecting digital\nsynthesisand the needfor some\nP.E.R.BittiSocial presence,\nof posturefeaturesJournalof\nsegmentationusing dynamic\n1756\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":15,"text":"[41] K.Dion, E.Berscheid, E.WalsterWhat is beautiful is goodJournal of Personality\nand Social Psychology 24 (3)1972 285\u2013290.\n[42] W. Dong, B. Lepri, A. Cappelletti, A.S. Pentland, F. Pianesi, M. Zancanaro, Using\nthe influence model to recognize functional roles in meetings, in: Proceedings\nof the International Conference on Multimodal Interfaces, 2007, pp. 271\u2013278.\n[43] N.Eagle, A.PentlandReality mining: sensing complex social signalsJournal of\nPersonal and Ubiquitous Computing 10 (4)2006 255\u2013268.\n[44] Y.Eisenthal, G.Dror,E.RuppinFacial\nmachineNeural Computation 18 (1)2005 119\u2013142.\n[45] P.Ekman (Ed.)Emotion in the Human Face1982, Cambridge University Press,\nCambridge, 1982.\n[46] P.EkmanDarwin, deception, and facial expressionAnnals of the New York\nAcademy of Sciences 1000 (1)2003 205\u2013221.\n[47] P.Ekman, W.V.FriesenThe repertoire of nonverbal behaviorSemiotica 11969\n49\u201398.\n[48] P. Ekman, W.V. Friesen, J.C. Hager, Facial Action Coding System (FACS):\nManual, A Human Face, Salt Lake City, USA, 2002.\n[49] P. Ekman, T.S. Huang, T.J. Sejnowski, J.C. Hager (Eds.), Final Report to NSF of\nthe Planning Workshop on Facial Expression Understanding, Human\nInteraction Laboratory, University of California, San Francisco, 1993.\n[50] P.Ekman, E.L.RosenbergWhat the Face Reveals: Basic and Applied Studies of\nSpontaneous Expression Using the Facial Action Coding System (FACS)2005,\nOxford University Press, Oxford, 2005.\n[51] R.R.Faden, T.L.Beauchamp, N.M.P.KingA History and Theory of Informed\nConsent1986, Oxford University Press, Oxford, 1986.\n[52] I.R.Fasel, B.Fortenberry, J.R.MovellanA generative framework for real time\nobject detection and classificationComputer Vision and Image Understanding\n98 (1)2005 181\u2013210.\n[53] S. Favre, H. Salamin, J. Dines, A. Vinciarelli, Role recognition in multiparty\nrecordings using Social Affiliation Networks and discrete distributions, in:\nProceedings of the ACM International Conference on Multimodal Interfaces,\n2008, pp. 29\u201336.\n[54] D.A.Forsyth, O.Arikan, L.Ikemoto, J.O\u2019Brien, D.RamananComputational studies\nof human motion part 1: tracking and motion synthesisFoundations and\nTrends in Computer Graphics and Vision 1 (2)2006 77\u2013254.\n[55] N.Fragopanagos,J.G.TaylorEmotion\ninteractionNeural Networks 18 (4)2005 389\u2013405.\n[56] M.G.Frank, P.EkmanAppearing truthful generalizes across different deception\nsituationsJournal of Personality and Social Psychology 86 (3)2004 486\u2013495.\n[57] T.Gandhi, M.M.TrivediPedestrian protection systems: issues, survey, and\nchallengesIEEE Transactions on Intelligent Transportation Systems 8 (3)2007\n413\u2013430.\n[58] C.Garcia, G.TziritasFace detection using quantized skin color regions merging\nand wavelet packet analysisIEEE Transactions on Multimedia 1 (3)1999 264\u2013\n277.\n[59] N. Garg, S. Favre, H. Salamin, D. Hakkani-T\u00fcr, A. Vinciarelli, Role recognition\nfor meeting participants: an approach based on lexical information and social\nnetwork analysis, in: Proceedings of the ACM International Conference on\nMultimedia, 2008, pp. 693\u2013696.\n[60] D. Gatica-Perez, I. McCowan, D. Zhang, S. Bengio, Detecting group interest-\nlevel in meetings, in: Proceedings of IEEE International Conference on\nAcoustics, Speech and Signal Processing, 2005, pp. 489\u2013492.\n[61] J.L. Gauvain, L.F. Lamel, G. Adda, Partitioning and transcription of broadcast\nnews data, in: Proceedings of International Conference on Spoken Language\nProcessing, 1998, pp. 1335\u20131338.\n[62] D.M.GavrilaVisual analysis of human movement: a surveyComputer Vision\nand Image Understanding 73 (1)1999 82\u201398.\n[63] M.GladwellBlink: The Power of Thinking without Thinking2005, Little Brown\n& Company, Boston, NY, 2005.\n[64] C.R.Glass, T.V.Merluzzi, J.L.Biever, K.H.LarsenCognitive assessment of social\nanxiety: development and\nquestionnaireCognitive Therapy and Research 6 (1)1982 37\u201355.\n[65] J.M.Gold, D.Tadin, S.C.Cook, R.B.BlakeThe efficiency of biological motion\nperceptionPerception and Psychophysics 70 (1)2008 88\u201395.\n[66] D. Goleman, Social Intelligence, Hutchinson, 2006.\n[67] L.Gorelick, M.Blank, E.Shechtman, M.Irani, R.BasriActions as space-time\nshapesIEEE Transactions on Pattern Analysis and Machine Intelligence 29\n(12)2007 2247\u20132253.\n[68] J.E.Grahe, F.J.BernieriTheimportance\nrapportJournal of Nonverbal Behavior 23 (4)1999 253\u2013269.\n[69] K. Greene, 10 Emerging Technologies 2008, MIT Technology Review, February\n2008.\n[70] M.R.Greenwald, A.G.BanajiImplicit social cognition: attitudes, self-esteem,\nand stereotypesPsychological Review 102 (1)1995 4\u201327.\n[71] S.W.Gregory,K.Dagan, S.WebsterEvaluating\naccommodation in conversation partners fundamental frequencies to\nperceptions of communication qualityJournal of Nonverbal Behavior 21\n(1)1997 23\u201343.\n[72] M.M. Gross, E.A. Crane, B.L. Fredrickson, Effect of felt and recognized\nemotions on body movements during walking, in: Proceedings of the\nInternational Conference on the Expression of Emotions in Health and\nDisease, 2007.\n[73] H.Gunes, M.PiccardiAssessing facial beauty through proportion analysis by\nimage processing and supervised learningInternational Journal of Human\u2013\nComputer Studies 64 (12)2006 1184\u20131199.\nattractiveness:beauty and the\nrecognition inhuman\u2013computer\nvalidationofa self-statement\nofnonverbal cuesinjudging\nthe relationof vocal\n[74] H.Gunes, M.PiccardiBi-modal emotion recognition from expressive face and\nbody gesturesJournal of Network and Computer Applications 30 (4)2007\n1334\u20131345.\n[75] H. Gunes, M. Piccardi, T. Jan, Comparative beauty classification for pre-\nsurgery planning, in: Proceedings of the IEEE International Conference on\nSystems, Man, and Cybernetics, 2004, pp. 2168\u20132174.\n[76] H. Gunes, M. Piccardi, M. Pantic, From the lab to the real world: affect\nrecognition using multiple cues and modalities, in: J. Or (Ed.), Affective\nComputing: Focus on Emotion Expression, Synthesis, and Recognition, 2008,\npp. 185\u2013218.\n[77] E.T. Hall, The Silent Language, Doubleday, 1959.\n[78] J.B.Hayfron-Acquah, M.S.Nixon, J.N.CarterAutomatic gait recognition by\nsymmetry analysisPattern Recognition Letters 24 (13)2003 2175\u20132183.\n[79] J.HirschbergPitch accent in context: predicting intonational prominence from\ntextArtificial Intelligence 63 (1\u20132)1993 305\u2013340.\n[80] J. Hirschberg, B. Grosz, Intonational features of local and global discourse\nstructure, in: Proceedings of the Speech and Natural Language Workshop,\n1992, pp. 441\u2013446.\n[81] E.Hjelmas, B.K.LowFace detection: a surveyComputer Vision and Image\nUnderstanding 83 (3)2001 236\u2013274.\n[82] C.R.L.Hsu, M.Abdel-Mottaleb, A.K.JainFace detection in colour imagesIEEE\nTransactions on Pattern Analysis and Machine Intelligence 24 (5)2002 696\u2013\n706.\n[83] K.S. Huang, M.M. Trivedi, Robust real-time detection, tracking, and pose\nestimation of faces in video streams, in: Proceedings of Conference on\nComputer Vision and Pattern Recognition, 2004, pp. 965\u2013968.\n[83] X.Huang, A.Acero, H.W.HonSpoken Language Processing2001, Prentice-Hall,\nEnglewood Cliffs, NJ, 2001.\n[85] H. Hung, D. Jayagopi, C. Yeo, G. Friedland, S. Ba, J.M. Odobez, K. Ramchandran,\nN. Mirghafori, D. Gatica-Perez, Using audio and video features to classify the\nmost dominant person in a group meeting, in: Proceedings of the ACM\nInternational Conference on Multimedia, 2007, pp. 835\u2013838.\n[86] A. Ito, X. Wang, M. Suzuki, S. Makino, Smile and laughter recognition using\nspeech processing and face recognition from conversation video, in:\nProceedings of the International Conference on Cyberworlds, 2005, pp.\n437\u2013444.\n[87] Y. Ivanov, C. Stauffer, A. Bobick, W.E.L. Grimson, Video surveillance of\ninteractions, in: Proceedings of the Workshop on Visual Surveillance at\nComputer Vision and Pattern Recognition, 1999.\n[88] A. Jaimes, K. Omura, T. Nagamine, K. Hirata, Memory cues for meeting video\nretrieval, in: Proceedings of Workshop on Continuous Archival and Retrieval\nof Personal Experiences, 2004, pp. 74\u201385.\n[89] D. Keltner, P. Ekman, Facial expression of emotion, in: M. Lewis, J.M.\nHaviland-Jones (Eds.), Handbook of Emotions, 2000, pp. 236\u2013249.\n[90] D.Keltner, J.HaidtSocialfunctions\nanalysisCognition and Emotion 13 (5)1999 505\u2013521.\n[91] D.Keltner, A.M.KringEmotion, social function, and psychopathologyReview of\nGeneral Psychology 2 (3)1998 320\u2013342.\n[92] L. Kennedy, D. Ellis, Laughter detection in meetings, in: Proceedings of the\nNIST Meeting Recognition Workshop, 2004.\n[93] M.Kimura, I.DaiboInteractional synchrony in conversations about emotional\nepisodes: a measurement by the between-participants pseudosynchrony\nexperimental paradigmJournal of Nonverbal Behavior 30 (3)2006 115\u2013126.\n[94] J.Kittler, M.Hatef, R.P.W.Duin,\nTransactions on Pattern Analysis and Machine Intelligence 20 (3)1998 226\u2013\n239.\n[95] A.Kleinsmith, R.De Silva, N.Bianchi-BerthouzeCross-cultural differences in\nrecognizing affect from body postureInteracting with Computers 18 (6)2006\n1371\u20131389.\n[96] M.L.Knapp, J.A.HallNonverbal Communication in Human Interaction1972,\nHarcourt Brace College Publishers, New York, 1972.\n[97] W.W. Kong, S. Ranganath, Automatic hand trajectory segmentation and\nphoneme transcriptionfor sign\nInternational Conference on Automatic Face and Gesture Recognition, 2008.\n[98] Z.KundaSocial Cognition1999, MIT Press, Cambridge, MA, 1999.\n[99] J.L.Lakin, V.E.Jefferis, C.M.Cheng, T.L.ChartrandThe Chameleon effect as social\nglue:Evidence for theevolutionary\nmimicryJournal of Nonverbal Behavior 27 (3)2003 145\u2013162.\n[100] L. Lee, W.E.L. Grimson, Gait analysis for recognition and classification, in:\nProceedings of the IEEE International Conference on Automatic Face and\nGesture Recognition, 2002, pp. 148\u2013155.\n[101] T.K. Leung, M.C. Burl, P. Perona, Finding faces in cluttered scenes using\nrandom labeled graphmatching, in: Proceedings of the International\nConference on Computer Vision, 1995, pp. 637\u2013644.\n[102] X.Li, S.J.Maybank, S.Yan, D.Tao, D.XuGait components and their application to\ngender recognitionIEEE Transactions on Systems, Man, and Cybernetics, Part\nC: Applications and Reviews 38 (2)2008 145\u2013155.\n[103] G.Littlewort, M.S.Bartlett, I.Fasel, J.Susskind, J.MovellanDynamics of facial\nexpression extracted automatically from videoImage and Vision Computing\n24 (6)2006 615\u2013625.\n[104] G.C. Littlewort, M.S. Bartlett, K. Lee, Faces of pain: automated measurement\nof spontaneous facial expressions of genuine and posed pain, in: Proceedings\nof the International Conference on Multimodal Interfaces, 2007, pp. 15\u201321.\n[105] Y. Liu, E. Shriberg, A. Stolcke, M. Harper, Comparing HMM, maximum\nentropy, and conditional random fields for disfluency detection, in:\nofemotions atfourlevels of\nJ.MatasOncombining classifiersIEEE\nlanguage,in: Proceedingsof IEEE\nsignificance of nonconscious\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1757"},{"page":16,"text":"Proceedings of the European Conference on Speech Communication and\nTechnology, 2005.\n[106] D.F.Lott, R.SommerSeating arrangements and statusJournal of Personality and\nSocial Psychology 7 (1)1967 90\u201395.\n[107] L.Lu,H.J.Zhang, H.JiangContent\nsegmentationIEEE Transactions on Speech and Audio Processing 10 (7)2002\n504\u2013516.\n[108] S.Lucey, A.B.Ashraf, J.CohnInvestigating spontaneous facial action recognition\nthrough AAM representations\n(Eds.)Handbook of Face Recognition2007, I-Tech Education and Publishing,\n2007, pp. 275\u2013286.\n[109] L.Z.McArthur,R.M.BaronToward\nperceptionPsychological Review 90 (3)1983 215\u2013238.\n[110] I. McCowan, S. Bengio, D. Gatica-Perez, G. Lathoud, F. Monay, D. Moore, P.\nWellner, H. Bourlard, Modeling human interaction in meetings, in:\nProceedings of IEEE International Conference on Acoustics, Speech and\nSignal Processing, 2003, pp. 748\u2013751.\n[111] I.McCowan, D.Gatica-Perez,\nD.ZhangAutomatic analysis of multimodal group actions in meetingsIEEE\nTransactions on Pattern Analysis and Machine Intelligence 27 (3)2005 305\u2013\n317.\n[112] D.McNeillHand and Mind: What Gestures Reveal about Thought1996,\nUniversity Of Chicago Press, IL, 1996.\n[113] A.Mehrabian, S.R.FerrisInference of attitude from nonverbal communication\nin two channelsJournal of Counseling Psychology 31 (3)1967 248\u2013252.\n[114] K. Mikolajczyk, C. Schmid, A. Zisserman, Human detection based on a\nprobabilistic assembly of robust part detectors, in: Proceedings of the\nEuropean Conference on Computer Vision, 2004, pp. 69\u201381.\n[115] T.B.Moeslund, E.GranumA survey of computer vision-based human motion\ncaptureComputer Vision and Image Understanding 81 (3)2001 231\u2013268.\n[116] T.B.Moeslund, A.Hilton, V.KrugerA survey of advances in vision-based human\nmotion capture and analysisComputer Vision and Image Understanding 104\n(2\u20133)2006 90\u2013126.\n[117] Y. Moh, P. Nguyen, J.C. Junqua, Towards domain independent speaker\nclustering, in: Proceedings of the IEEE International Conference on\nAcoustics, Speech, and Signal Processing, 2003, pp. 85\u201388.\n[118] S.M\u00f6ller, R.Sch\u00f6nweilerAnalysis of infant cries for the early detection of\nhearing impairmentSpeech Communication 28 (3)1999 175\u2013193.\n[119] P.R.Montague, G.S.Berns, J.D.Cohen, S.M.McClure, G.Pagnoni, M.Dhamala,\nM.C.Wiest,I.Karpov,R.D.King,\nsimultaneous fMRI during linked social interactionsNeuroimage 16 (4)2002\n1159\u20131164.\n[120] B.C.J.MooreAn Introduction to the Psychology of Hearing1982, Academic\nPress, New York, 1982.\n[121] N. Morgan, E. Fosler, N. Mirghafori. Speech recognition using on-line\nestimation of speaking rate, in: Proceedings of Eurospeech, 1997, pp. 2079\u2013\n2082.\n[122] N. Morgan, E. Fosler-Lussier, Combining multiple estimators of speaking rate,\nin: Proceedings of IEEE International Conference on Acoustics, Speech, and\nSignal Processing, 1998, pp. 729\u2013732.\n[123] D.MorrisPeoplewatching2007, Vintage, NY, 2007.\n[124] S. Mota, R.W. Picard, Automated posture analysis for detecting learners\ninterest level, in: Proceedings of Conference on Computer Vision and Pattern\nRecognition, 2003, pp. 49\u201356.\n[125] S. Mukhopadhyay, B. Smith, Passive capture and structuring of lectures, in:\nProceedings of the ACM International Conference on Multimedia, 1999, pp.\n477\u2013487.\n[126] K.P. Murphy, Dynamic Bayesian Networks: Representation, Inference and\nLearning, Ph.D. thesis, University of California Berkeley, 2002.\n[127] C.Nass, K.M.LeeDoes computer-synthesized speech manifest personality?\nExperimental tests of recognition, similarity-attraction, and consistency-\nattractionJournal of Experimental Psychology: Applied 7 (3)2001 171\u2013181.\n[128] C.Nass, J.SteuerComputers and social actorsHuman Communication Research\n19 (4)1993 504\u2013527.\n[129] I.Oikonomopoulos, I.Patras, M.PanticSpatiotemporal salient points for visual\nrecognition of human actionsIEEE Transactions on Systems, Man, and\nCybernetics \u2013 Part B 36 (3)2006 710\u2013719.\n[130] N.Oliver, A.Garg, E.HorvitzLayered representations for learning and inferring\noffice activity from multiple sensory channelsComputer Vision and Image\nUnderstanding 96 (2)2004 163\u2013180.\n[131] A.J.O\u2019Toole, T.Price, T.Vetter, J.C.Bartlett, V.Blanz3D shape and 2D surface\ntextures of human faces: the role of averages in attractiveness and ageImage\nand Vision Computing 18 (1)1999 9\u201319.\n[132] S.OviattUser-centeredmodeling\ninterfacesProceedings of the IEEE 912003 1457\u20131468.\n[133] S.Oviatt, C.Darves, R.CoulstonToward adaptive conversational interfaces:\nmodeling speech convergence with animated personasACM Transactions on\nComputer\u2013Human Interaction 11 (3)2004 300\u2013328.\n[134] P. Pal, A.N. Iyer, R.E. Yantorno, in: Proceedings of the IEEE International\nConference on Acoustics, Speech and Signal Processing, vol. 2, 2006, pp. 721\u2013\n724.\n[135] M.Pantic, M.S.BartlettMachine analysis of facial expressions, in: K.Delac,\nM.Grgic (Eds.)Handbook of Face Recognition2007, I-Tech Education and\nPublishing, 2007, pp. 377\u2013416.\n[136] M.Pantic, I.PatrasDynamics of facial expression: recognition of facial actions\nand their temporal segments from face profile image sequencesIEEE\nanalysisfor audioclassificationand\nof the face, in:K.Delac,M.Grgic\nan ecological theoryof social\nS.Bengio,G.Lathoud,M.Barnard,\nN.Apple, R.E.FisherHyperscanning:\nand evaluationof multimodal\nTransactions on Systems, Man, and Cybernetics \u2013 Part B: Cybernetics 36\n(2)2006 433\u2013449.\n[137] M.Pantic, A.Pentland, A.Nijholt, T.HuangHuman computing and machine\nunderstanding of human behavior: a surveyLecture Notes in Artificial\nIntelligence, vol. 44512007, Springer-Verlag, Berlin, 2007, pp. 47\u201371.\n[138] M.Pantic, A.Pentland, A.Nijholt, T.HuangHuman-centred intelligent human\u2013\ncomputer interaction (HCI2): how far are we from attaining it?International\nJournal of Autonomous and Adaptive Communications Systems 1 (2)2008\n168\u2013187\n[139] M.Pantic, L.J.M.RothkrantzAutomatic analysis of facial expressions: the state\nof the artIEEE Transactions on Pattern Analysis and Machine Intelligence 22\n(12)2000 1424\u20131445.\n[140] M.Pantic, L.J.M.RothkrantzToward an affect-sensitive multimodal human\u2013\ncomputer interactionProceedings of the IEEE 91 (9)2003 1370\u20131390.\n[141] C. Pelachaud, V. Carofiglio, B. De Carolis, F. de Rosis, I. Poggi, Embodied\ncontextual agent in information delivering application, in: Proceedings of the\nInternational Joint Conference on Autonomous Agents and Multiagent\nSystems, 2002, pp. 758\u2013765.\n[142] A. Pentland, Social dynamics: signals and behavior, in: International\nConference on Developmental Learning, 2004.\n[143] A.PentlandSocially aware computation and communicationIEEE Computer 38\n(3)2005 33\u201340.\n[144] A.PentlandAutomatic mapping and modeling of human networksPhysica A\n3782007 59\u201367.\n[145] A.PentlandSocial signal processingIEEE Signal Processing Magazine 24\n(4)2007 108\u2013111.\n[146] A.PentlandHonest Signals: How They Shape Our World2008, MIT Press,\nCambridge, MA, 2008.\n[147] S. Petridis, M. Pantic, Audiovisual discrimination between laughter and\nspeech, in: Proceedings of IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, 2008, pp. 5117\u20135121.\n[148] S. Petridis, M. Pantic, Audiovisual laughter detection based on temporal\nfeatures, in: Proceedings of IEEE International Conference on Multimodal\nInterfaces, 2008, pp. 37\u201344.\n[149] T. Pfau, G. Ruske, Estimating the speaking rate by vowel detection, in:\nProceedings of the IEEE International Conference on Acoustics, Speech, and\nSignal Processing, 1998, pp. 945\u2013948.\n[150] J.O.PicklesAn Introduction to the Physiology of Hearing1982, Academic Press,\nNew York, 1982.\n[151] F.E.Pollick, V.Lestou, J.Ryu, S.B.ChoEstimating the efficiency of recognizing\ngender and affect from biological motionVision Research 422002 2345\u20132355.\n[152] F.E.Pollick, H.M.Paterson, A.Bruderlin, A.J.SanfordPerceiving affect from arm\nmovementCognition 82 (2)2001 51\u201361.\n[153] R.PoppeVision-based human motion analysis: an overviewComputer Vision\nand Image Understanding 1082007 4\u201318.\n[154] G.PsathasConversation Analysis \u2013 The Study of Talk-in-Interaction1995, Sage\nPublications, Beverley Hills, 1995.\n[155] L. Rabiner, M. Sambur, Voiced-unvoiced-silence detection using the Itakura\nLPC distance measure, in: Proceedings of the IEEE International Conference\non Acoustics, Speech, and Signal Processing, 1977, pp. 323\u2013326.\n[156] L.R.Rabiner, R.W.SchaferDigital Processing of Speech Signals1978, Prentice-\nHall, Englewood Cliffs, NJ, 1978.\n[157] D.A. Reynolds, W. Campbell, T.T. Gleason, C. Quillen, D. Sturim, P. Torres-\nCarrasquillo, A. Adami, The 2004 MIT Lincoln laboratory speaker recognition\nsystem, in: Proceedings of IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, 2005, pp. 177\u2013180.\n[158] V.P.Richmond, J.C.McCroskeyNonverbal\nrelations1995, Allyn and Bacon, Bacon, NY, 1995.\n[159] R.Rienks, D.HeylenDominance detection in meetings using easily obtainable\nfeaturesLecture Notes in Computer Science, vol. 38692006, Springer, Berlin,\n2006, pp. 76\u201386.\n[160] R. Rienks, D. Zhang, D. Gatica-Perez, Detection and application of influence\nrankings in small group meetings, in: Proceedings of the International\nConference on Multimodal Interfaces, 2006, pp. 257\u2013264.\n[161] H.A.Rowley, S.Baluja, T.KanadeNeural network-based face detectionIEEE\nTransactions on Pattern Analysis and Machine Intelligence 20 (1)1998 23\u201338.\n[162] J.A.Russell, J.A.Bachorowski, J.M.Fernandez-DolsFacial and vocal expressions\nof emotionAnnual Reviews in Psychology 54 (1)2003 329\u2013349.\n[163] J.A.Russell,J.M.Fernandez-Dols\nExpression1997, Cambridge University Press, Cambridge, 1997.\n[164] N.RussoConnotation of seating arrangementsCornell Journal of Social\nRelations 2 (1)1967 37\u201344.\n[165] M.A.Sayette, D.W.Smith, M.J.Breiner, G.T.WilsonThe effect of alcohol on\nemotional response to a social stressorJournal of Studies on Alcohol 53\n(6)1992 541\u2013545.\n[166] A.E.ScheflenThe significance of posture in communication systemsPsychiatry\n271964 316\u2013331.\n[167] K.R.SchererPersonality markers in speech1979, Cambridge University Press,\nCambridge, 1979.\n[168] K.R.SchererVocalcommunication\nparadigmsSpeech Communication 40 (1\u20132)2003 227\u2013256.\n[169] H. Schneiderman, T. Kanade, A statistical model for 3D object detection\napplied to faces and cars, in: Proceedings of Conference on Computer Vision\nand Pattern Recognition, 2000, pp. 746\u2013751.\n[170] B. Schuller, R. M\u00fceller, B. H\u00f6ernler, A. H\u00f6ethker, H. Konosu, G. Rigoll,\nAudiovisual recognition of spontaneous interest within conversations, in:\nBehaviors in interpersonal\n(Eds.)The Psychologyof Facial\nof emotion:a reviewofresearch\n1758\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759"},{"page":17,"text":"Proceedings of the International Conference on Multimodal Interfaces, 2007,\npp. 30\u201337.\n[171] U.Segerstrale, P.Molnar (Eds.)Nonverbal Communication: Where Nature\nMeets Culture1997, Lawrence Erlbaum Associates, London, 1997.\n[172] A.Sepheri, Y.Yacoob, L.DavisEmploying the hand as an interface deviceJournal\nof Multimedia 1 (7)2006 18\u201329.\n[173] E.ShribergPhonetic consequences of speech disfluencyProceedings of the\nInternational Congress of Phonetic Sciences 11999 619\u2013622.\n[174] E. Shriberg, R. Bates, A. Stolcke, A prosody-only decision-tree model for\ndisfluency detection, in: Proceedings of Eurospeech, 1997, pp. 2383\u20132386.\n[175] E. Shriberg, A. Stolcke, D. Baron, Observations of overlap: findings and\nimplications for automatic processing of multiparty conversation, in:\nProceedings of Eurospeech, 2001, pp. 1359\u20131362.\n[176] P.E.Shrout, D.W.FiskeNonverbal behaviors and social evaluationJournal of\nPersonality 49 (2)1981 115\u2013128.\n[177] K. Sj\u00f6lander, J. Beskow, Wavesurfer \u2013 an open source speech tool, in:\nProceedings of International Conference on Spoken Language Processing,\n2000, pp. 464\u2013467.\n[178] A.F. Smeaton, P. Over, W. Kraaij, Evaluation campaigns and TRECVid, in:\nProceedings of the ACM International Workshop on Multimedia Information\nRetrieval, 2006, pp. 321\u2013330.\n[179] C.Sminchisescu, A.Kanaujia, D.MetaxasConditional models for contextual\nhuman motion recognitionComputer Vision and Image Understanding 104\n(2\u20133)2006 210\u2013220.\n[180] L.Smith-Lovin, C.BrodyInterruptions in group discussions: the effects of gender\nand group compositionAmerican Sociological Review 54 (3)1989 424\u2013435.\n[181] K.K.Sung, T.PoggioExample-based learning for view-based human face\ndetectionIEEE Transactions on Pattern Analysis and Machine Intelligence 20\n(1)1998 39\u201351.\n[182] E.L.ThorndikeIntelligence and its useHarper\u2019s Magazine 1401920 227\u2013235.\n[183] C.ThurauBehaviorhistogramsfor\ndetectionLecture Notes in Computer Science, vol. 48142007, Springer-\nVerlag, Berlin, 2007, pp. 271\u2013284.\n[184] C. Thurau, V. Hlavac, Pose primitive based human action recognition in videos\nor still images, in: Proceedings of the IEEE International Conference on\nComputer Vision and Pattern Recognition, 2008, pp. 1\u20136.\n[185] Y. Tian, T. Kanade, J.F. Cohn, Facial expression analysis, in: S.Z. Li, A.K. Jain\n(Eds.), Handbook of Face Recognition, 2005, pp. 247\u2013276.\n[186] H.L.TischlerIntroduction to Sociology1990, Harcourt Brace College Publishers,\nNew York, 1990.\n[187] Y.Tong, W.Liao, Q.JiFacial action unit recognition by exploiting their dynamic\nand semantic relationshipsIEEE Transactions on Pattern Analysis and\nMachine Intelligence 29 (10)2007 1683\u20131699.\n[188] D. Tran, A. Sorokin, D.A. Forsyth, Human activity recognition with metric\nlearning,in:Proceedings ofthe EuropeanConferenceonComputerVision,2008.\n[189] S.E.Tranter, D.A.ReynoldsAn overview of automatic speaker diarization\nsystemsIEEE Transactions on Audio, Speech, and Language Processing 14\n(5)2006 1557\u20131565.\n[190] H.C.TriandisCulture and Social Behavior1994, McGraw-Hill, New York, 1994.\n[191] K.P. Truong, D.A. Leeuwen, Automatic detection of laughter, in: Proceedings\nof the European Conference on Speech Communication and Technology,\n2005, pp. 485\u2013488.\n[192] K.P.Truong, D.A.van LeeuwenAutomatic discrimination between laughter and\nspeechSpeech Communication 49 (2)2007 144\u2013158.\n[193] L.Q.Uddin, M.Iacoboni, C.Lange, J.P.KeenanThe self and social cognition: the\nrole of cortical midline structures and mirror neuronsTrends in Cognitive\nSciences 11 (4)2007 153\u2013157.\n[194] A. Utsumi, N. Tetsutani, Human detection using geometrical pixel value\nstructures, in: Proceedings of the IEEE International Conference on Automatic\nFace and Gesture Recognition, 2002, pp. 34\u201339.\n[195] M.F. Valstar, H. Gunes, M. Pantic, How to distinguish posed from spontaneous\nsmiles using geometric features, in: Proceedings of the International\nConference on Multimodal Interfaces, 2007, pp. 38\u201345.\n[196] M.F. Valstar, M. Pantic, Fully automatic facial action unit detection and\ntemporal analysis, in: Proceedings of the International Conference on\nComputer Vision and Pattern Recognition Workshop, 2006, pp. 149\u2013150.\n[197] M.F. Valstar, M. Pantic, Z. Ambadar, J.F. Cohn, Spontaneous vs. posed facial\nbehavior: automatic analysis of brow actions, in: Proceedings of the\nInternational Conference on Multimodal Interfaces, 2006, pp. 162\u2013170.\naction recognition andhuman\n[198] J.Van den Stock, R.Righart, B.de GelderBody expressions influence recognition\nof emotions in the face and voiceEmotion 7 (3)2007 487\u2013494.\n[199] R. Vertegaal, R. Slagter, G. van der Veer, A. Nijholt, Eye gaze patterns in\nconversations: there is more to conversational agents than meets the eyes,\nin: Proceedings of the SIGCHI Conference on Human Factors in Computing\nSystems, 2001, pp. 301\u2013308.\n[200] A.VinciarelliSpeakers role recognition in multiparty audio recordings using\nsocial network analysis and duration distribution modelingIEEE Transactions\non Multimedia 9 (9)2007 1215\u20131226.\n[201] A.Vinciarelli, J.-M.OdobezApplication of information retrieval technologies to\npresentation slidesIEEE Transactions on Multimedia 8 (5)2006 981\u2013995.\n[202] A. Vinciarelli, M. Pantic, H. Bourlard, A. Pentland, Social signal processing:\nstate-of-the-art and future perspectives of an emerging domain, in:\nProceedings of the ACM International Conference on Multimedia, 2008, pp.\n1061\u20131070.\n[203] A. Vinciarelli, M. Pantic, H. Bourlard, A. Pentland, Social signals, their function,\nand automatic analysis: a survey, in: Proceedings of the ACM International\nConference on Multimodal Interfaces, 2008, pp. 61\u201368.\n[204] P.Viola, M.JonesRobust real-time face detectionComputer Vision 57 (2)2004\n137\u2013154.\n[205] A. Waibel, T. Schultz, M. Bett, M. Denecke, R. Malkin, I. Rogina, R. Stiefelhagen.\nSMaRT: the Smart Meeting Room task at ISL, in: Proceedings of IEEE\nInternational Conference on Acoustics, Speech, and Signal Processing, 2003,\npp. 752\u2013755.\n[206] L.Wang, W.Hu, T.TanRecent developments in human motion analysisPattern\nRecognition 36 (3)2003 585\u2013601.\n[207] P. Wang, Q. Ji, Multi-view face detection under complex scene based on\ncombined SVMs, in: Proceedings of International Conference on Pattern\nRecognition, 2004, pp. 179\u2013182.\n[208] R.M.Warner, D.B.SugarmanAttributions of personality based on physical\nappearance, speech, and handwritingJournal of Personality and Social\nPsychology 50 (4)1986 792\u2013799.\n[209] S.Wasserman,K.FaustSocialNetwork\nApplications1994, Cambridge University Press, Cambridge, 1994.\n[210] C.Y. Weng, W.T. Chu, J.L. Wu, Movie analysis based on roles social network,\nin: Proceedings of IEEE International Conference on Multimedia and Expo,\n2007, pp. 1403\u20131406.\n[211] J. Whitehill, J.R. Movellan, Personalized facial attractiveness prediction, in:\nProceedings of the IEEE International Conference on Automatic Face and\nGesture Recognition, 2008.\n[212] A.C.C.WilliamsFacial expression of pain: an evolutionary accountBehavioral\nand Brain Sciences 25 (4)2003 439\u2013455.\n[213] Y. Wu, T.S. Huang. Vision-based gesture recognition: a review, in:\nProceedings of the International Gesture Workshop, 1999, pp. 103\u2013109.\n[214] Y.Yacoob, L.DavisDetection and analysis of hairIEEE Transactions on Pattern\nAnalysis and Machine Intelligence 28 (7)2006 1164\u20131169.\n[215] M.H.Yang, D.Kriegman, N.AhujaDetecting faces in images: a surveyIEEE\nTransactions on Pattern Analysis and Machine Intelligence 24 (1)2002 34\u201358.\n[216] J. Yao, J.-M. Odobez, Fast human detection from videos using covariance\nfeatures, in: Proceedings of European Conference on Computer Vision Visual\nSurveillance Workshop, 2008.\n[217] G.YulePragmatics1996, Oxford University Press, Oxford, 1996.\n[218] M. Zancanaro, B. Lepri, F. Pianesi, Automatic detection of group functional\nroles in face to face interactions, in: Proceedings of the International\nConference on Multimodal Interfaces, 2006, pp. 28\u201334.\n[219] B.ZellnerPausesandthe temporal\n(Ed.)Fundamentals of Speech Synthesis and Speech Recognition1994, John\nWiley & Sons, 1994, pp. 41\u201362.\n[220] Z.Zeng, Y.Fu, G.I.Roisman, Z.Wen, Y.Hu, T.S.HuangSpontaneous emotional\nfacial expression detectionJournal of Multimedia 1 (5)2006 1\u20138.\n[221] Z.Zeng, M.Pantic, G.I.Roisman, T.H.HuangA survey of affect recognition\nmethods: audio, visual and spontaneous expressionsIEEE Transactions on\nPattern Analysis and Machine Intelligence 31 (1)2009 39\u201358.\n[222] Y.Zhang, Q.JiActive and dynamic information fusion for facial expression\nunderstanding from image sequencesIEEE Transactions on Pattern Analysis\nand Machine Intelligence 27 (5)2005 699\u2013714.\n[223] Q. Zhu, S. Avidan, M.C. Yeh, K.T. Cheng, Fast human detection using a\ncascadeofhistograms oforiented\nComputer Vision and Pattern Recognition, 2006, pp. 1491\u20131498.\nAnalysis:Methodsand\nstructureofspeech, in: E.Keller\ngradients,in: Proceedingsof\nA. Vinciarelli et al.\/Image and Vision Computing 27 (2009) 1743\u20131759\n1759"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710.pdf","widgetId":"rgw30_56ab9d884fd5f"},"id":"rgw30_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=222430190&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw31_56ab9d884fd5f"},"id":"rgw31_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=222430190&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":222430190,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":222430190,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2092800051,"url":"researcher\/2092800051_Sucheta_Ghosh","fullname":"Sucheta Ghosh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2092878740,"url":"researcher\/2092878740_Milos_Cernak","fullname":"Milos Cernak","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2024915222,"url":"researcher\/2024915222_Sarbani_Palit","fullname":"Sarbani Palit","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2087625501,"url":"researcher\/2087625501_B_B_Chaudhuri","fullname":"B. B. Chaudhuri","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings","usePlainButton":true,"publicationUid":289587864,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings","title":"An Analysis of Rhythmic Staccato-Vocalization Based on Frequency Demodulation for Laughter Detection in Conversational Meetings","displayTitleAsLink":true,"authors":[{"id":2092800051,"url":"researcher\/2092800051_Sucheta_Ghosh","fullname":"Sucheta Ghosh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2092878740,"url":"researcher\/2092878740_Milos_Cernak","fullname":"Milos Cernak","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2024915222,"url":"researcher\/2024915222_Sarbani_Palit","fullname":"Sarbani Palit","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2087625501,"url":"researcher\/2087625501_B_B_Chaudhuri","fullname":"B. B. Chaudhuri","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Human laugh is able to convey various kinds of meanings in human\ncommunications. There exists various kinds of human laugh signal, for example:\nvocalized laugh and non vocalized laugh. Following the theories of psychology,\namong all the vocalized laugh type, rhythmic staccato-vocalization\nsignificantly evokes the positive responses in the interactions. In this paper\nwe attempt to exploit this observation to detect human laugh occurrences, i.e.,\nthe laughter, in multiparty conversations from the AMI meeting corpus. First,\nwe separate the high energy frames from speech, leaving out the low energy\nframes through power spectral density estimation. We borrow the algorithm of\nrhythm detection from the area of music analysis to use that on the high energy\nframes. Finally, we detect rhythmic laugh frames, analyzing the candidate\nrhythmic frames using statistics. This novel approach for detection of\n`positive' rhythmic human laughter performs better than the standard laughter\nclassification baseline.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Milos_Cerak\/publication\/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings\/links\/56a66fa208ae44a674fe1e1d.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Milos_Cerak","sourceName":"Milo\u0161 Cer\u0148ak","hasSourceUrl":true},"publicationUid":289587864,"publicationUrl":"publication\/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings\/links\/56a66fa208ae44a674fe1e1d\/smallpreview.png","linkId":"56a66fa208ae44a674fe1e1d","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=289587864&reference=56a66fa208ae44a674fe1e1d&eventCode=&origin=publication_list","widgetId":"rgw35_56ab9d884fd5f"},"id":"rgw35_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=289587864&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"56a66fa208ae44a674fe1e1d","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":222430190,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289587864_An_Analysis_of_Rhythmic_Staccato-Vocalization_Based_on_Frequency_Demodulation_for_Laughter_Detection_in_Conversational_Meetings\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Human laugh is a crucial social signal due to the range of inner meanings it carries. This social signaling event [1] may denote the topical changes, communication synchrony and positive affect; on the other hand, it may also show disagreement or satirist views. Therefore, automatic human laugh occurrence or laughter detection in speech may have many applications in spoken dialog and discourse analysis. "],"widgetId":"rgw36_56ab9d884fd5f"},"id":"rgw36_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw34_56ab9d884fd5f"},"id":"rgw34_56ab9d884fd5f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=289587864&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":70071592,"url":"researcher\/70071592_Peter_Tu","fullname":"Peter Tu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2026807091,"url":"researcher\/2026807091_Jixu_Chen","fullname":"Jixu Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083964740,"url":"researcher\/2083964740_Ming-Ching_Chang","fullname":"Ming-Ching Chang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2084818394,"url":"researcher\/2084818394_Ting_Yu","fullname":"Ting Yu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":4,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods","usePlainButton":true,"publicationUid":283962644,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods","title":"Cross-cultural Training Analysis Via Social Science and Computer Vision Methods","displayTitleAsLink":true,"authors":[{"id":70071592,"url":"researcher\/70071592_Peter_Tu","fullname":"Peter Tu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2026807091,"url":"researcher\/2026807091_Jixu_Chen","fullname":"Jixu Chen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2083964740,"url":"researcher\/2083964740_Ming-Ching_Chang","fullname":"Ming-Ching Chang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084818394,"url":"researcher\/2084818394_Ting_Yu","fullname":"Ting Yu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084884006,"url":"researcher\/2084884006_Tai-Peng_Tian","fullname":"Tai-Peng Tian","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084683768,"url":"researcher\/2084683768_Gabriela_Rubin","fullname":"Gabriela Rubin","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084883118,"url":"researcher\/2084883118_Julia_Hockett","fullname":"Julia Hockett","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2069356946,"url":"researcher\/2069356946_Aubrey_Logan-Terry","fullname":"Aubrey Logan-Terry","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["12\/2015; 3:4068-4075. DOI:10.1016\/j.promfg.2015.07.978"],"abstract":"Computer Vision technology is an invaluable addition to cross-cultural communication training for military personnel. It allows trainers to assess trainees in real time and provide feedback grounded in social science research. The present study reports on a joint analysis of military cross-cultural training data by Computer Vision specialists from GE Global Research as well as analyses from Georgetown University's Social Interaction Research Group (SIRG). Data for this study were collected over 10 days at the Army Infantry Basic Officer Leaders Course (IBOLC). 80 lieutenants participated in classroom role-play scenarios designed to assess their ability to communicate cross-culturally. GE and SIRG researchers video-recorded interactions among the role players and Soldiers and correlations were observed between these automatic interpretations and those gleaned by the SIRG analysis team in order to augment understanding of the efficacy of the cross-cultural training. For the Computer Vision methods, Each person was represented as a stream of visual cues which include: position, articulated motion, facial expressions and gaze directions. The social science researchers conducted multimodal (including embodied elements such as eye gaze, hand gestures, and body positioning), mixed method (qualitative and quantitative) discourse analyses of the data. SIRG researchers developed a coding scheme, marking specific human behavioral features within each interaction. From such coding, SIRG identified key skills in cross-cultural interaction, including observation and adaptation to unfamiliar communicative norms, rapport building, and trouble recovery (for details see Logan-Terry & Damari, forthcoming). Various correlations between raw computer vision measurements and the social science coding scheme was observed. Such results represent a significant step towards establishing the efficacy of the joint analysis of automated Computer Vision and established social science methods with regards to complex social interaction analysis.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"publication\/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods?origin=publication_list","active":false,"primary":true,"extraClass":null,"icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS2351978915009798%2Fpdf%3Fmd5%3Df9cd871b7d6041095672d440d95c7122%26pid%3D1-s2.0-S2351978915009798-main.pdf","sourceName":"sciencedirect.com","hasSourceUrl":true},"publicationUid":283962644,"publicationUrl":"publication\/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods\/links\/5697cbdf08aea2d74375c381\/smallpreview.png","linkId":"5697cbdf08aea2d74375c381","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283962644&reference=5697cbdf08aea2d74375c381&eventCode=&origin=publication_list","widgetId":"rgw38_56ab9d884fd5f"},"id":"rgw38_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283962644&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":222430190,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283962644_Cross-cultural_Training_Analysis_Via_Social_Science_and_Computer_Vision_Methods\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["2 Based on these measurements, various aggregate statistics were computed on a frame by frame basis resulting in measures of four visual analytics, namely affect, proximity, engagement and body motion. These visual analytics are normalized to the scale of [0] [1]. "],"widgetId":"rgw39_56ab9d884fd5f"},"id":"rgw39_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw37_56ab9d884fd5f"},"id":"rgw37_56ab9d884fd5f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283962644&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2047898358,"url":"researcher\/2047898358_Zhanpeng_Zhang","fullname":"Zhanpeng Zhang","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A280325113565197%401443846123968_m"},{"id":2046395825,"url":"researcher\/2046395825_Ping_Luo","fullname":"Ping Luo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13950471,"url":"researcher\/13950471_Chen_Change_Loy","fullname":"Chen Change Loy","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39490087,"url":"researcher\/39490087_Xiaoou_Tang","fullname":"Xiaoou Tang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Sep 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281809119_Learning_Social_Relation_Traits_from_Face_Images","usePlainButton":true,"publicationUid":281809119,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/281809119_Learning_Social_Relation_Traits_from_Face_Images","title":"Learning Social Relation Traits from Face Images","displayTitleAsLink":true,"authors":[{"id":2047898358,"url":"researcher\/2047898358_Zhanpeng_Zhang","fullname":"Zhanpeng Zhang","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A280325113565197%401443846123968_m"},{"id":2046395825,"url":"researcher\/2046395825_Ping_Luo","fullname":"Ping Luo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13950471,"url":"researcher\/13950471_Chen_Change_Loy","fullname":"Chen Change Loy","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39490087,"url":"researcher\/39490087_Xiaoou_Tang","fullname":"Xiaoou Tang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Social relation defines the association, e.g, warm, friendliness, and\ndominance, between two or more people. Motivated by psychological studies, we\ninvestigate if such fine-grained and high-level relation traits can be\ncharacterised and quantified from face images in the wild. To address this\nchallenging problem we propose a deep model that learns a rich face\nrepresentation to capture gender, expression, head pose, and age-related\nattributes, and then performs pairwise-face reasoning for relation prediction.\nTo learn from heterogeneous attribute sources, we formulate a new network\narchitecture with a bridging layer to leverage the inherent correspondences\namong these datasets. It can also cope with missing target attribute labels.\nExtensive experiments show that our approach is effective for fine-grained\nsocial relation learning in images and videos.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281809119_Learning_Social_Relation_Traits_from_Face_Images","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Zhanpeng_Zhang2\/publication\/281809119_Learning_Social_Relation_Traits_from_Face_Images\/links\/5601394d08aeafc8ac8c8547.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Zhanpeng_Zhang2","sourceName":"Zhanpeng Zhang","hasSourceUrl":true},"publicationUid":281809119,"publicationUrl":"publication\/281809119_Learning_Social_Relation_Traits_from_Face_Images","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281809119_Learning_Social_Relation_Traits_from_Face_Images\/links\/5601394d08aeafc8ac8c8547\/smallpreview.png","linkId":"5601394d08aeafc8ac8c8547","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281809119&reference=5601394d08aeafc8ac8c8547&eventCode=&origin=publication_list","widgetId":"rgw41_56ab9d884fd5f"},"id":"rgw41_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281809119&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"5601394d08aeafc8ac8c8547","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":222430190,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281809119_Learning_Social_Relation_Traits_from_Face_Images\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Social signal processing. Understanding social relation is an important research topic in social signal processing [4] [29] [30] [36] [37], an important multidisciplinary problem that has attracted a surge of interest from computer vision community. Social signal processing mainly involves facial expression recognition [23] and affective behaviour analysis [28]. "],"widgetId":"rgw42_56ab9d884fd5f"},"id":"rgw42_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw40_56ab9d884fd5f"},"id":"rgw40_56ab9d884fd5f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281809119&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":222430190,"publicationLink":"publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw33_56ab9d884fd5f"},"id":"rgw33_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=222430190&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=298","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":true,"citationsCount":296,"hasIncomingCitations":true,"incomingCitationsCount":298,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw32_56ab9d884fd5f"},"id":"rgw32_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=222430190&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"541a1ae10cf2218008bfa710","name":"Alessandro Vinciarelli","date":"Sep 17, 2014 ","nameLink":"profile\/Alessandro_Vinciarelli2","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"3182bdbbc07708a39bf8a08bfa18058a","showFileSizeNote":false,"fileSize":"1.37 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"541a1ae10cf2218008bfa710","name":"Alessandro Vinciarelli","date":"Sep 17, 2014 ","nameLink":"profile\/Alessandro_Vinciarelli2","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"3182bdbbc07708a39bf8a08bfa18058a","showFileSizeNote":false,"fileSize":"1.37 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:9:rect:160.16,202.35,284.75,6.37","ordinal":"8"},"assetId":"AS:305289669693460@1449798137582"},{"props":{"position":"float","orientation":"portrait","coords":"pag:10:rect:32.71,420.96,520.02,6.37","ordinal":"9"},"assetId":"AS:305289669693461@1449798137652"}],"figureAssetIds":["AS:305289669693460@1449798137582","AS:305289669693461@1449798137652"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=cc-jHNdsyb7rSmbijv7lHzHShGh0VSmHRGy8f6jitAk-58mIrXxiUkyCaDqIR_to657kCYucN5KB5NzGt0EDpw.EWW68YL2ov6Py_yJj6-xXoV0Bd7WrV4Xk-G9tsAn2NpQwS_sQrqLA8SnMa3KEPkDdhJKJ5R3jk-MLof1Q6SUWw","clickOnPill":"publication.PublicationFigures.html?_sg=9zZSGIB_Vwi6Z_xuHCLPXoJAHRo7k97jToSTDuEGAL4R6jarjiAteLVdSoVKNWb_8TifcGL4pUzfhQ7ExEFEpw.4cJPg2bVAgY8TNYwPuG6VBzhFCNbvEuGsNoQ6dH-bIqtbU5IRkHWBacRgo_pMB4o_CVXajb8poceqjXoPSTHng"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAlessandro_Vinciarelli2%2Fpublication%2F222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain%2Flinks%2F541a1ae10cf2218008bfa710.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=4EMzgdBj3bSLcOnLhJ9koECnfUb-rkUe1KVPTaKHIYN9kyRkKc08JTtI_yPd3VMTMqF3Fv5GVm7m7WMxB4tkGA","urlHash":"9f6dc0ca0a5070d4e160df4bee9bd174","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=U-PZsdJz9t_vAD3wb3Hy5TQlGlSYX7ieumimSMhtOZlCATODY380pus-5At0UWl2reCtwXH-VJLSe1y8ceuVpG8XPUFLwmx-9aiTXksBMbk.FIw6Lqr1-igllklvC9eEP7saM2QOu9mkMZGDiHJjVIAJ-t_jPXvOAfT_iIjIlnGbRteUg2ZBuunZuzH0MZoeZQ.nYLTXi7vhHwcEIQ7H4T9LmyaTXjKMCY2VqE_jmFnX0Ob3HgzwS4BNwRxoaok84iL8bDW4m2C8SIeUlgKrp-TMg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"541a1ae10cf2218008bfa710","trackedDownloads":{"541a1ae10cf2218008bfa710":{"v":false,"d":false}},"assetId":"AS:142545740898306@1410996961733","readerDocId":"6894537","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":222430190,"commentCursorPromo":null,"widgetId":"rgw44_56ab9d884fd5f"},"id":"rgw44_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAlessandro_Vinciarelli2%2Fpublication%2F222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain%2Flinks%2F541a1ae10cf2218008bfa710.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A142545740898306%401410996961733&publicationUid=222430190&linkId=541a1ae10cf2218008bfa710&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Social Signal Processing: Survey of an Emerging Domain","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=3soyjaY4lSQL7-YKzi8fPI8ID9s6zmdGqYZxmH8JiGi_c6rh0haSYxHUtRzwW4I8Luj5xeIj6oTEzujJ_TGG7vdL3ygPd1lXKspnIpWzCRg.tNG_eMPvCkxK2Z69xVbc-Zw4lwt-Yx5hYEI1rCExnmzBMHCAe_vanlEMKllYa0OGTO0CJz6qW2rk2NmPcjI3wg.W3iAYJm0LgsO9HTo6IQC3mi-eTXbKI7CgscEeGQrKg1KyHJuFHKVrG27oG0tAFYsRPnf89FHSuWz8b3esyg6RQ","publicationUid":222430190,"trackedDownloads":{"541a1ae10cf2218008bfa710":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab9d884fd5f"},"id":"rgw46_56ab9d884fd5f","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab9d884fd5f"},"id":"rgw47_56ab9d884fd5f","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab9d884fd5f"},"id":"rgw48_56ab9d884fd5f","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw49_56ab9d884fd5f"},"id":"rgw49_56ab9d884fd5f","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw50_56ab9d884fd5f"},"id":"rgw50_56ab9d884fd5f","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw45_56ab9d884fd5f"},"id":"rgw45_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw43_56ab9d884fd5f"},"id":"rgw43_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9d884fd5f"},"id":"rgw2_56ab9d884fd5f","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":222430190},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=222430190&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9d884fd5f"},"id":"rgw1_56ab9d884fd5f","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"ewG4IWI2TAQHXH6KP24FA06ABSBreOU8ze5BpMmpW1mD0X7QSipcStRkfTSWEU8667cuHJTNwRzZwP02M75g3J3Jcw3v3k1RLaLtkXr4h6IacS8EiP6X7YXvifVMxEmpEKGOBEooIDQVOgnsINJLBkpLUBknkvzmCvw3vFUbGuroc4C4S6bwPzCERdRbBfZRk8D4G0URO+kxZIQje4\/MhbeIH8lFja9UEGFxcnOSYWIagEvnzfR8VxFscE4hHoCYVX4ivrPOtydtytWRwHMYEm7rpcMe86FXkMeK\/SKZj9E=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Social Signal Processing: Survey of an Emerging Domain\" \/>\n<meta property=\"og:description\" content=\"The ability to understand and manage social signals of a person we are communicating with is the core of social intelligence. Social intelligence is a facet of human intelligence that has been...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\" \/>\n<meta property=\"rg:id\" content=\"PB:222430190\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1016\/j.imavis.2008.11.007\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Social Signal Processing: Survey of an Emerging Domain\" \/>\n<meta name=\"citation_author\" content=\"Alessandro Vinciarelli\" \/>\n<meta name=\"citation_author\" content=\"Maja Pantic\" \/>\n<meta name=\"citation_author\" content=\"Herv\u00e9 Bourlard\" \/>\n<meta name=\"citation_publication_date\" content=\"2009\/11\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Image and Vision Computing\" \/>\n<meta name=\"citation_issn\" content=\"0262-8856\" \/>\n<meta name=\"citation_volume\" content=\"27\" \/>\n<meta name=\"citation_issue\" content=\"12\" \/>\n<meta name=\"citation_firstpage\" content=\"1743\" \/>\n<meta name=\"citation_lastpage\" content=\"1759\" \/>\n<meta name=\"citation_doi\" content=\"10.1016\/j.imavis.2008.11.007\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Alessandro_Vinciarelli2\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\/links\/541a1ae10cf2218008bfa710.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-788a2b08-9e7f-408c-8136-01b12f7e9921","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":565,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw51_56ab9d884fd5f"},"id":"rgw51_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-788a2b08-9e7f-408c-8136-01b12f7e9921", "4fe259b5b84d243806ca3fe9deb85467855abc56");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-788a2b08-9e7f-408c-8136-01b12f7e9921", "4fe259b5b84d243806ca3fe9deb85467855abc56");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw52_56ab9d884fd5f"},"id":"rgw52_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/222430190_Social_Signal_Processing_Survey_of_an_Emerging_Domain","requestToken":"jbOT4Z4X4OT0fa2oeaZL+hsLixUHPJQBHCmINeu5RqrcH43oIYpMBq4iOfrBtpxfp\/P\/lLJCub+837ozR3ScuAQG74o1i4szH4i5oS+gR5dA3DsqJNQ+VOOTZ+FtppQuVt2wA\/n4Ui1uBBSmdLDQuQE5DR9DZwfV6Fg0Vpfdxx40sP9V2GxKlUteDPAAifEEbAQV8HIWY6B3UpU+prZymL7mOdVn8EGHjGQh\/\/Q2ldx5X7VKvdzGcmdAhEUDmm8YvmNmkk4q954TtXh1Z2KLlUki8sBghryB84FuPoF6EEM=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=8GoM9E4BaDn4LKP2D3pu2bbUqB-_xEAQY5hEeMnELySSX_ldVsFjf7-U1yOBnUGD","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIyNDMwMTkwX1NvY2lhbF9TaWduYWxfUHJvY2Vzc2luZ19TdXJ2ZXlfb2ZfYW5fRW1lcmdpbmdfRG9tYWlu","signupCallToAction":"Join for free","widgetId":"rgw54_56ab9d884fd5f"},"id":"rgw54_56ab9d884fd5f","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw53_56ab9d884fd5f"},"id":"rgw53_56ab9d884fd5f","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw55_56ab9d884fd5f"},"id":"rgw55_56ab9d884fd5f","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
