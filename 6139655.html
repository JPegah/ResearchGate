<!DOCTYPE html> <html lang="en" class="" id="rgw55_56ab19c3f26f4"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="n+MNQaFsYC2P16xrE6ETKB50K1yebVgYdHbo7NXcGl5WmhjYYdTk4WtaCCXi4cNEIwxFXx2Bhp7Sa2glHYwbgNWUtfQVTIqgnAB3d3H96ATJYsKlmOTBZuyFSwzJWY/UqmWIfauu6u8vLuWPb4532pUsgddX3b8ZOc9zzl59CeNzc4246lrDMYpphyd3w0WNQLGMpEOxPNDFud+0qFGtMW+2bpYNJs1GQFj4a1lTXr9Hd0QQvsZKNFY/iZo6OPX4O2fSUtD3usBEe1QlryLKaTZG1ZTzDjQws1DVBsJVeIg="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-7b696229-7122-4f03-9be5-3f22e004405c",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/6139655_Hidden_Conditional_Random_Fields" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Hidden Conditional Random Fields" />
<meta property="og:description" content="We present a discriminative latent variable model for classification problems in structured domains where inputs can be represented by a graph of local observations. A hidden-state Conditional..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/6139655_Hidden_Conditional_Random_Fields/links/00b4951750bbf3fcea000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/6139655_Hidden_Conditional_Random_Fields" />
<meta property="rg:id" content="PB:6139655" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/TPAMI.2007.1124" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Hidden Conditional Random Fields" />
<meta name="citation_author" content="Ariadna Quattoni" />
<meta name="citation_author" content="Sybor Wang" />
<meta name="citation_author" content="Louis-Philippe Morency" />
<meta name="citation_author" content="Michael Collins" />
<meta name="citation_author" content="Trevor Darrell" />
<meta name="citation_pmid" content="17699927" />
<meta name="citation_publication_date" content="2007/11/01" />
<meta name="citation_journal_title" content="IEEE Transactions on Pattern Analysis and Machine Intelligence" />
<meta name="citation_issn" content="0162-8828" />
<meta name="citation_volume" content="29" />
<meta name="citation_issue" content="10" />
<meta name="citation_firstpage" content="1848" />
<meta name="citation_lastpage" content="53" />
<meta name="citation_doi" content="10.1109/TPAMI.2007.1124" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Ariadna_Quattoni/publication/6139655_Hidden_Conditional_Random_Fields/links/00b4951750bbf3fcea000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/6139655_Hidden_Conditional_Random_Fields" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/6139655_Hidden_Conditional_Random_Fields" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Hidden Conditional Random Fields (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Hidden Conditional Random Fields on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab19c3f26f4" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab19c3f26f4" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab19c3f26f4">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2FTPAMI.2007.1124&rft.atitle=Hidden%20Conditional%20Random%20Fields&rft.title=IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence&rft.jtitle=IEEE%20transactions%20on%20pattern%20analysis%20and%20machine%20intelligence&rft.volume=29&rft.issue=10&rft.date=2007&rft.pages=1848-53&rft.issn=0162-8828&rft.au=Ariadna%20Quattoni%2CSybor%20Wang%2CLouis-Philippe%20Morency%2CMichael%20Collins%2CTrevor%20Darrell&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Hidden Conditional Random Fields</h1> <meta itemprop="headline" content="Hidden Conditional Random Fields">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/6139655_Hidden_Conditional_Random_Fields/links/00b4951750bbf3fcea000000/smallpreview.png">  <div id="rgw8_56ab19c3f26f4" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab19c3f26f4" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Ariadna_Quattoni" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A273704424833026%401442267628404_m" title="Ariadna Quattoni" alt="Ariadna Quattoni" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ariadna Quattoni</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab19c3f26f4" data-account-key="Ariadna_Quattoni">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Ariadna_Quattoni"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A273704424833026%401442267628404_l" title="Ariadna Quattoni" alt="Ariadna Quattoni" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Ariadna_Quattoni" class="display-name">Ariadna Quattoni</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Polytechnic_University_of_Catalonia" title="Polytechnic University of Catalonia">Polytechnic University of Catalonia</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab19c3f26f4"> <a href="researcher/32689818_Sybor_Wang" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Sybor Wang" alt="Sybor Wang" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Sybor Wang</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab19c3f26f4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/32689818_Sybor_Wang"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Sybor Wang" alt="Sybor Wang" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/32689818_Sybor_Wang" class="display-name">Sybor Wang</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56ab19c3f26f4"> <a href="researcher/13852141_Louis-Philippe_Morency" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Louis-Philippe Morency" alt="Louis-Philippe Morency" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Louis-Philippe Morency</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56ab19c3f26f4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/13852141_Louis-Philippe_Morency"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Louis-Philippe Morency" alt="Louis-Philippe Morency" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/13852141_Louis-Philippe_Morency" class="display-name">Louis-Philippe Morency</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw15_56ab19c3f26f4"> <a href="researcher/48105240_Michael_Collins" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Michael Collins" alt="Michael Collins" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michael Collins</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw16_56ab19c3f26f4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/48105240_Michael_Collins"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Michael Collins" alt="Michael Collins" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/48105240_Michael_Collins" class="display-name">Michael Collins</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw17_56ab19c3f26f4"> <a href="researcher/8996478_Trevor_Darrell" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Trevor Darrell" alt="Trevor Darrell" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Trevor Darrell</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw18_56ab19c3f26f4">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8996478_Trevor_Darrell"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Trevor Darrell" alt="Trevor Darrell" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8996478_Trevor_Darrell" class="display-name">Trevor Darrell</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States </div>      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0162-8828_IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence"><span itemprop="name">IEEE Transactions on Pattern Analysis and Machine Intelligence</span></a> </span>    (Impact Factor: 5.78).     <meta itemprop="datePublished" content="2007-11">  11/2007;  29(10):1848-53.    DOI:&nbsp;10.1109/TPAMI.2007.1124           <div class="pub-source"> Source: <a href="http://www.ncbi.nlm.nih.gov/pubmed/17699927" rel="nofollow">PubMed</a> </div>  </div> <div id="rgw19_56ab19c3f26f4" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>We present a discriminative latent variable model for classification problems in structured domains where inputs can be represented by a graph of local observations. A hidden-state Conditional Random Field framework learns a set of latent variables conditioned on local features. Observations need not be independent and may overlap in space and time.</div> </p>  </div>   </div>     <div id="rgw20_56ab19c3f26f4" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/6139655_fig1_Figure-1-Encoding-Part-Dependencies-in-the-model-images-show-min-spanning-tree" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 1: Encoding Part Dependencies in the model: images show..." data-key="6139655_fig1_Figure-1-Encoding-Part-Dependencies-in-the-model-images-show-min-spanning-tree"> <img class="fig" src="https://www.researchgate.net/profile/Ariadna_Quattoni/publication/6139655/figure/fig1/Figure-1-Encoding-Part-Dependencies-in-the-model-images-show-min-spanning-tree_small.png" alt="Figure 1: Encoding Part Dependencies in the model: images show..." title="Figure 1: Encoding Part Dependencies in the model: images show..."/> </a> </li>  <li> <a href="/figure/6139655_fig2_Figure-4-Models-used-for-comparative-experiments-on-the-gesture-recognition-task-Y-is" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 4: Models used for comparative experiments on the gesture..." data-key="6139655_fig2_Figure-4-Models-used-for-comparative-experiments-on-the-gesture-recognition-task-Y-is"> <img class="fig" src="https://www.researchgate.net/profile/Ariadna_Quattoni/publication/6139655/figure/fig2/Figure-4-Models-used-for-comparative-experiments-on-the-gesture-recognition-task-Y-is_small.png" alt="Figure 4: Models used for comparative experiments on the gesture..." title="Figure 4: Models used for comparative experiments on the gesture..."/> </a> </li>  <li> <a href="/figure/6139655_fig3_Figure-5-Illustrations-of-the-six-gesture-classes-for-the-experiments-Below-each-image" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 5: Illustrations of the six gesture classes for the experiments...." data-key="6139655_fig3_Figure-5-Illustrations-of-the-six-gesture-classes-for-the-experiments-Below-each-image"> <img class="fig" src="https://www.researchgate.net/profile/Ariadna_Quattoni/publication/6139655/figure/fig3/Figure-5-Illustrations-of-the-six-gesture-classes-for-the-experiments-Below-each-image_small.png" alt="Figure 5: Illustrations of the six gesture classes for the experiments...." title="Figure 5: Illustrations of the six gesture classes for the experiments...."/> </a> </li>  <li> <a href="/figure/6139655_fig4_Figure-6-Graph-showing-the-distribution-of-the-hidden-states-for-each-gesture-class-The" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 6: Graph showing the distribution of the hidden states for each..." data-key="6139655_fig4_Figure-6-Graph-showing-the-distribution-of-the-hidden-states-for-each-gesture-class-The"> <img class="fig" src="https://www.researchgate.net/profile/Ariadna_Quattoni/publication/6139655/figure/fig4/Figure-6-Graph-showing-the-distribution-of-the-hidden-states-for-each-gesture-class-The_small.png" alt="Figure 6: Graph showing the distribution of the hidden states for each..." title="Figure 6: Graph showing the distribution of the hidden states for each..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw21_56ab19c3f26f4" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw35_56ab19c3f26f4">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw47_56ab19c3f26f4">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Ariadna_Quattoni/publication/6139655_Hidden_Conditional_Random_Fields/links/00b4951750bbf3fcea000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Ariadna_Quattoni">Ariadna Quattoni</a>   </span>  </div>  <div class="social-share-container"><div id="rgw49_56ab19c3f26f4" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw50_56ab19c3f26f4" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw51_56ab19c3f26f4" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw52_56ab19c3f26f4" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw53_56ab19c3f26f4" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw54_56ab19c3f26f4" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw48_56ab19c3f26f4" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAriadna_Quattoni%2Fpublication%2F6139655_Hidden_Conditional_Random_Fields%2Flinks%2F00b4951750bbf3fcea000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw34_56ab19c3f26f4"  itemprop="articleBody">  <p>Page 1</p> <p>Hidden-state Conditional Random Fields<br />A. Quattoni, S. Wang, L.-P Morency, M. Collins, T. Darrell; MIT CSAIL<br />Abstract<br />We present a discriminative latent variable model for classification problems in structured domains<br />where inputs can be represented by a graph of local observations. A hidden-state Conditional Random<br />Field framework learns a set of latent variables conditioned on local features. Observations need not be<br />independent and may overlap in space and time. We evaluate our model on object detection and gesture<br />recognition tasks.<br />1. Introduction<br />It is well known that models which include latent, or hidden-state, structure may be more expressive than<br />fully observable models, and can often find relevant substructure in a given domain. Hidden Markov<br />Models (HMMs) and Dynamic Bayesian Networks use hidden state to model observations, and have a<br />clear generative probabilistic formulation. In this paper we develop a hidden-state conditional random<br />field model, and demonstrate its ability to outperform generative hidden-state and discriminative fully-<br />observable models on object and gesture recognition tasks.<br />Conditional Random Fields have been shown to be powerful discriminative models because they can<br />incorporate essentially arbitrary feature-vector representations of the observed data points [13]. How-<br />ever, they are limited in that they cannot capture intermediate structures using hidden-state variables. In<br />this paper we propose a new model for classification based on CRFs augmented with latent state, which<br />1</p>  <p>Page 2</p> <p>we call Hidden-state Conditional Random Fields (HCRFs). While HMMs are the natural extension of<br />MRFs, HCRFs are the analogous extension for CRFs.<br />Figure 3 shows the difference between HCRFS and HMMS or Hidden markov random fields. Hidden<br />Markov random fields are directed graphical models [8], where a random variable h is modeled as a<br />markov process and it is assumed that the observation variable x is a deterministic or stochastic function<br />of h. One way of using HMMS for classification is to assume a hidden variable h for each category and<br />train a model for each of them independently. That is, given k categories and m samples (where Dlis<br />the training data for category l)in a maximum likelihood framework the parameters θlof each of the k<br />models are trained to maximize P(Dl|θl).<br />Differently, an HCRF models the distribution P(c,h|x) directly, where c is a category and h is an<br />intermediate hidden variable modeled as a markov random field globally conditioned on observation x.<br />The parameters θ of the model are trained discriminatively to optimize P(c|x).<br />There is an extensive literature dedicated to gesture recognition; for hand and arm gestures, a com-<br />prehensive survey was presented in Pavlovic et al. [20]. Generative models have been used successfully<br />to recognize arm gestures [2] and a number of sign languages [1, 24]. Kapoor and Picard presented a<br />HMM-based, real time head nod and head shake detector [9]. Fugie et al. also used HMMs to perform<br />head nod recognition [6].<br />The main limitation of latent generative approaches is that they require a model of local features given<br />underlying variables, and generally presume independence of the observations. Accurately specifying<br />such a generative model may be challenging, particulary in cases where we wish to incorporate long<br />range dependencies in the model and allow hidden variables to depend on several local features. These<br />observations led to the introduction of discriminative models for sequence labeling, including MEMM’s<br />[15], [22] and Conditional Random Fields (CRFs). CRFs were first introduced by Lafferty et al. [13]<br />and have been widely used since then in the natural language processing community for tasks such as<br />noun co-reference resolution [17], named entity recognition [16] and information extraction [3].<br />In computer vision, CRF’s have been applied to the task of detecting man-made structures in natural<br />images and have been shown to outperform Markov Random Fields (MRF) [12]. Sminchisescu [23]<br />2</p>  <p>Page 3</p> <p>applied CRFs to classify human motion activity and demonstrated their model was more accurate than<br />MEMMs and could discriminate subtle motion styles. Torralba et al. [25] introduced Boosted Random<br />Fields, a model that combines local and global image information for contextual object recognition.<br />Our hidden-state discriminative approach for object recognition is related to the work of Kumar and<br />Herbert [12], [11], who train a discriminative model using fully-labeled data where each image region is<br />assigned a part label from a discrete set of object parts. A CRF is trained and detection and segmentation<br />are performed by finding the most likely labeling of the image under the learned model. The main<br />difference between our approach and Kumar’s is that we do not assume that the part assignment variables<br />are fully observed and are instead regarded as latent variables. Incorporating hidden variables allows use<br />of training data not explicitly labeled with part (hidden-state) structure.<br />Another related model is presented in [26], which builds a discriminative classifier based on a part-<br />based feature representation. Such a representation is obtained by measuring the similarity between<br />image patches (detected with an interest point detector) to a pre-defined dictionary of parts. The dictio-<br />nary is built by extracting and clustering patches from a set of representative images of the target class.<br />Again, a significant difference between their approach and ours is that we do not perform a pre-selection<br />of discriminative parts, but rather incorporate such a step during training. In parallel to our work on ob-<br />ject recognition [21], [7] developed a hidden-state CRF model for phone recognition and demonstrated<br />the equivalence of HMM models to a subset of CRF models. Also, Koo and Collins [10] describe a<br />similar hidden state model applied to a reranking approach for natural language parsing.<br />In previous work on CRFs label sequences are typically taken to be fully observed on training ex-<br />amples. In our approach category labels are observed, but an additional layer of subordinate labels are<br />learned. These intermediate hidden variables model the latent structure of the input domain; our model<br />defines the joint probability of a class label and hidden state labels conditioned on the observations, with<br />dependencies between the hidden variables expressed by an undirected graph. The result is a model<br />where inference and parameter estimation can be carried out using standard graphical model algorithms<br />such as loopy belief propagation.<br />3</p>  <p>Page 4</p> <p>2. Hidden Conditional Random Fields<br />We presume a task where we wish to predict a label y given inputs. Each y is a member of a set Y of<br />possible labels and each vector x is a vector of local observations x = {x1,x2,...,xm}. The number<br />of local observations can vary across examples; for convenience of notation we omit dependence on the<br />example index and simply refer to the number of observations as m in each case. Each local observation<br />xjis represented by a feature vector φ(xj) ∈ ?d, where d is the dimensionality of the representation.<br />For our object recognition task this corresponds to an image patch descriptor, while for our gesture<br />recognition task this contains body motion observations. Our training set consists of labeled examples<br />(xi,yi) for i = 1...n, where each yi ∈ Y, and each xi = {xi,1,xi,2,...,xi,m}. For any example x<br />we also assume a vector of latent variables h = {h1,h2,...,hm}, which are not observed on training<br />examples, and where each hjis a member of H where H is a finite set of possible hidden labels in<br />the model. Intuitively, each hjcorresponds to a labeling of xjwith some member of H, which may<br />correspond to “part” or “sub-gesture” structure in an observation.<br />Given these definitions of labels y, observations x, and latent variables h, we define a conditional<br />probabilistic model:<br />Given a new test example x, and parameter values θ∗induced from a training set, we will take the<br />label for the example to be argmaxy∈YP(y | x,θ∗). Following previous work on CRFs [13, 12], we use<br />the following objective function to estimate the parameters:<br />L(θ) =<br />?<br />i<br />logP(yi| xi,θ) −<br />1<br />2σ2||θ||2<br />(1)<br />ThefirstterminEq.1isthelog-likelihoodofthedata. ThesecondtermisthelogofaGaussianpriorwith<br />variance σ2, i.e., P(θ) ∼ exp?<br />values, θ∗= argmaxθL(θ), under this criterion. As with other hidden state models (e.g., HMMs),<br />1<br />2σ2||θ||2?. We use gradient ascent to search for the optimal parameter<br />adding hidden state makes the optimization non-convex; we search for parameters by initializing from<br />multiple random start points and searching for the best local optimum.”<br />We encode structural constraints with an undirected graph structure, where the hidden variables<br />4</p>  <p>Page 5</p> <p>{h1,...,hm} correspond to vertices in the graph. The set of graph edges (j,k) ∈ E denotes links<br />between variables hjand hk. The graph E can be arbitrary; intuitively it should capture any domain<br />specific knowledge that we have about the structure of h. In our object recognition task it is a local mesh<br />that encodes spatial consistency between local appearance features, while in our gesture recognition task<br />it is a chain that captures temporal dynamics.<br />We define Ψ to take the following form:<br />Ψ(y,h,x;θ) =<br />m<br />?<br />j=1<br />?<br />l∈L1<br />f1<br />l(j,y,hj,x)θ1<br />l+<br />?<br />(j,k)∈E<br />?<br />l∈L2<br />f2<br />l(j,k,y,hj,hk,x)θ2<br />l<br />(2)<br />where L1is the set of node features, L2the set of edge features , f1<br />l,f2<br />lare functions defining the features<br />in the model, and θ1<br />l,θ2<br />lare the components of θ. The f1features depend on single hidden variable values<br />in the model; the f2features can depend on pairs of values. Note that Ψ is linear in the parameters θ,<br />and the model in Eq. ?? is a log-linear model. Moreover the features respect the structure of the graph,<br />in that no feature depends on more than two hidden variables hj,hk, and if a feature does depend on<br />variables hjand hkthere must be an edge (j,k) in the graph E.<br />Assuming that the edges in E form a tree, and that Ψ takes the form in Eq. 2, then exact methods<br />exist for inference and parameter estimation in the model. This follows because belief propagation can<br />be used to calculate the following quantities in O(|E||Y|) time:<br />∀y ∈ Y,Z(y | x,θ) =<br />?<br />h<br />exp{Ψ(y,h,x;θ)}<br />∀y ∈ Y,j ∈ 1...m,a ∈ H,P(hj= a | y,x,θ) =<br />?<br />h:hj=a<br />P(h | y,x,θ)<br />∀y ∈ Y,(j,k) ∈ E,a,b ∈ H,P(hj= a,hk= b | y,x,θ) =<br />?<br />h:hj=a,hk=b<br />P(h | y,x,θ)<br />The first term Z(y | x,θ) is a partition function defined by a summation over the h variables. Terms<br />of this form can be used to calculate P(y | x,θ) = Z(y | x,θ)/?<br />calculation of argmaxP(y | x,θ)— can be performed efficiently in the model. The second and third<br />y?Z(y?| x,θ). Hence inference—<br />5</p>  <p>Page 6</p> <p>terms are marginal distributions over individual variables hjor pairs of variables hj,hkcorresponding<br />to edges in the graph. The gradient of L(θ) can be defined in terms of these marginals, and hence can be<br />calculated efficiently. If E contains cycles then approximate methods, such as loopy belief-propagation,<br />may be necessary for inference and parameter estimation.<br />In brief, since P(h,y|x) = P(h|y,x)P(y|x) and since both terms on the right hand side can be<br />efficiently computed using belief propagation it follows that the joint distribution can also be computed<br />efficiently.”<br />We estimate parameters θ∗= argmaxL(θ) from a training set using a quasi-Newton method. The<br />gradient of L(θ) can be calculated efficiently using belief propogation update steps; the likelihood term<br />due to the i’th training example is:<br />Li(θ) = logP(yi| xi,θ) = log<br />??<br />heΨ(yi,h,xi;θ)<br />y?,heΨ(y?,h,xi;θ).<br />?<br />?<br />(3)<br />We first consider derivatives with respect to the parameters θ1<br />lcorresponding to features f1<br />l(j,y,hj,x)<br />that depend on single hidden variables. Taking derivatives gives<br />∂Li(θ)<br />∂θ1<br />l<br />=<br />?<br />h<br />P(h | yi,xi,θ)∂Ψ(yi,h,xi;θ)<br />∂θ1<br />l<br />−<br />?<br />P(y?,h | xi,θ)<br />y?,h<br />P(y?,h | xi,θ)∂Ψ(y?,h,xi;θ)<br />∂θ1<br />l<br />=<br />?<br />?<br />h<br />P(h | yi,xi,θ)<br />m<br />?<br />j=1<br />f1<br />l(j,yi,hj,xi) −<br />?<br />?<br />y?,h<br />m<br />?<br />j=1<br />f1<br />l(j,y?,hj,xi)<br />=<br />j,a<br />P(hj= a | yi,xi,θ)f1<br />l(j,yi,a,xi) −<br />y?,j,a<br />P(hj= a,y?| xi,θ)f1<br />l(j,y?,a,xi)<br />It follows that∂Li(θ)<br />∂θ1<br />l<br />can be expressed in terms of components P(hj= a | xi,θ) and P(y | xi,θ), which<br />can be calculated using belief propagation, provided that the graph E forms a tree structure. A similar<br />6</p>  <p>Page 7</p> <p>calculation gives<br />∂Li(θ)<br />∂θ2<br />l<br />=<br />?<br />(j,k)∈E,a,b<br />−<br />y?,(j,k)∈E,a,b<br />P(hj= a,hk= b | yi,xi,θ)f2<br />?<br />l(j,k,yi,a,b,xi)<br />P(hj= a,hk= b,y?| xi,θ)f2<br />l(j,k,y?,a,b,xi)<br />hence ∂Li(θ)/∂θ2<br />lcan also be expressed in terms of expressions that can be calculated using belief<br />propagation.<br />3. Experiments<br />We explored the performance of our HCRF model on both object and gesture recognition tasks, mea-<br />suring the effect of different degrees of connectivity in the mesh of local observations in the former task<br />and the chain of motion observations in the latter task.<br />In our experiments we use a restricted form of Ψ where observations interact only with the hidden<br />states:<br />Ψ(y,h,x;θ) =<br />?<br />j<br />φ(xj) · θ(hj) +<br />?<br />j<br />θ(y,hj) +<br />?<br />(j,k)∈E<br />θ(y,hj,hk)<br />(4)<br />where θ(hj) ∈ ?dfor hj∈ H is a parameter vector corresponding to the j’th latent variable. The inner-<br />product φ(xj) · θ(hj) can be interpreted as a measure of the compatibility between observation xjand<br />hidden-state hj, the parameter θ(y,hj) ∈ ? for hj∈ H, y ∈ Y can be interpreted as a measure of the<br />compatibility between latent variable hjand category label y, and each parameter θ(y,hi,hj) ∈ ? for<br />y ∈ Y, and hi,hj∈ H measures the compatibility between an edge with labels hiand hjand the label y.<br />For these experiments we chose the number of hidden states that minimized the training error,the same<br />is true for the number of hidden states and mixtures used in the HMMs.<br />In general however the number of hidden states could be better optimized through a held out valida-<br />tion set. To give an idea of the sensitivity of the results to this number [Table xxx nips] shows object<br />recognition results for different number of hidden states.<br />7</p>  <p>Page 8</p> <p>Figure 1: Encoding Part Dependencies in the model: images show min-spanning tree , 1-lattices (top) ,<br />2 , and 3-lattices (bottom) over detected features.<br />Data set5 parts10 parts<br />Car Side 9499<br />Car Rear<br />In the object recognition domain patches xi,jin each image are obtained using the SIFT detector [14]:<br />9191.7<br />each patch xi,jis then represented by a feature vector φ(xi,j) that incorporates a combination of SIFT<br />descriptor and relative location and scale features. We assume that parts conditioned on proximate<br />observations are likely to be dependent, as expressed in the neighborhood graph structure.<br />The graph E encodes the amount of connectivity between the hidden variables hj. Intuitively, E<br />determines the ability of our model to capture conditional dependencies between part assignments. Such<br />dependencies between hidden part assignments can be encoded using n-neighbor lattices over local<br />observations. However, increasing connectivity leads to an increase in the computational complexity of<br />performing inference in such models. If E has no connectivity (i.e E contains no edges) the potential<br />function for our model reduces to:<br />Ψ(y,h,x;θ) =<br />?<br />j<br />φ(xj) · θ(hj) +<br />?<br />j<br />θ(y,hj)<br />(5)<br />This graph may be too poor to capture important dependencies between part assignments, especially<br />given that our observations often contain overlapping image patches. Another option for defining E is to<br />use a minimum spanning tree where the weights on the edges are the distances between the correspond-<br />8</p>  <p>Page 9</p> <p>00.10.2<br />False Positives<br />0.3 0.4 0.5<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />0.9<br />1<br />R0C Side Data Set<br />Correct Positives<br />8<br />888<br />118<br />666<br />666<br />666<br />118<br />777<br />666<br />222<br />666<br />777<br />666<br />222<br />888<br />888<br />188<br />222<br />666<br />222<br />888<br />888<br />777<br />777<br />118<br />88<br />20 40 6080 100120140 160180200<br />20<br />40<br />60<br />80<br />2<br />8<br />2<br />2<br />8<br />8<br />8822<br />6<br />6<br />6<br />6<br />6<br />9<br />8<br />2<br />6<br />6666<br />6<br />8<br />8<br />2<br />4<br />6666<br />8<br />2<br />2<br />6<br />8<br />6<br />8<br />8999<br />8888<br />6622<br />6666<br />6622<br />8888<br />8888<br />6666<br />2666<br />6666<br />9999<br />6666<br />8811<br />2222<br />2221<br />2222<br />8888<br />8888<br />9991<br />9999<br />2222<br />111<br />20406080100120 140160180200<br />20<br />40<br />60<br />80<br />Figure 2: ROC curves for the 4 variants of the model: the red curve corresponds to a model with no<br />connectivity, the green curve to a model with minimum spanning tree connectivity, the blue curve to a<br />model with 2-Lattice connectivity and the yellow curve to a model with 3-Lattice connectivity; Viterbi<br />assignments of hidden states to local image patches for min spanning tree and unconnected model, center<br />and right respectively.<br />ing image patches. Distances could in general be based on any aspect of the location or feature space; in<br />our experiments below we relied on distance in the image plane. Note that the structure of E will vary<br />across different images. The advantage of using such a graph is that, as we mentioned earlier, when E<br />contains no cycles, and Ψ takes the form in Eq. 2, we can perform exact inference on E, using belief<br />propagation in time O(|E||Y|)2.<br />More generally, we define E to be an n-Lattice over the local observations. We build an n-neighbor<br />lattice by linking every node to its n closest nodes, (i.e. the nodes that correspond to the n closest local<br />observations). When E contains cycles computing exact inference becomes untractable, and we need to<br />resort to approximate methods using loopy-belief propagation techniques.<br />We evaluated the effect of different neighborhood structures on recognition performance in a simple<br />object category recognition task. Here for brevity we report results only for the well-known UIUC car<br />side dataset. Given a neighborhood structure for our model we trained a binary classifier to distinguish<br />between a category and a background set formed from the remaining UIUC images. The data set was<br />split into 3 data sets: a training data set of 200 images, a validation set of 100 images and a testing set<br />of 100 images. The validation set was used to select the regularization term (i.e. the variance of the<br />Gaussian prior) and as a stopping criteria for the gradient ascent.<br />9</p>  <p>Page 10</p> <p>Data set<br />Car Side<br />Car Rear<br />Face<br />Plane<br />Motorbike<br />Our Model<br />99 %<br />94.6 %<br />99 %<br />96 %<br />95 %<br />Others [1]<br />-<br />90.3 %<br />96.4 %<br />90.2 %<br />92.5 %<br />Figure 3: Comparison with state of the art approach for object recognition (Equal Error Rates)<br />For the first experiment we defined E to be an unconnected graph, for the second a minimum spanning<br />tree, for the third a 2-lattice, and for the fourth a 3-lattice, as shown in Figure 1. For the first and second<br />experiments gradient ascent was initialized randomly while for the third and fourth experiments we used<br />the minimum spanning tree solution as initial parameters. Figure 2 shows the ROC curves for the 4<br />variants of the model: the red curve corresponds to a model with no connectivity, the green curve to a<br />model with minimum spanning tree connectivity, the blue curve to a model with 2-Lattice connectivity<br />and the yellow curve to a model with 3-Lattice connectivity.<br />From this Figure we observe a significant improvement in performance when the model incorporates<br />some degree of dependency between the latent variables (Figure 1).<br />Figure 2 shows the most likely assignment of parts to features for the min-spanning tree model and the<br />unconnected model for an example in which the former gives a correct classification but the latter fails<br />to do so. Notice that both models give smooth part assignments, which is expected as the normalized<br />location is a feature of the patch representation. In some cases the model relies more heavily on relative<br />location than appearance, labeling a part based on its location and thus learning the shape of the object<br />rather than its appearance. A possible reason for this is that the appearance information might not be<br />very useful for discriminating between classes when the image resolution is too low; as it is the case for<br />the car dataset.<br />For this type of task the min-spanning tree model shows equivalent recognition performance to the<br />models that use more densely connected graphs. Thus it is clear that the minimum-spanning tree can<br />encode sufficient dependency constraints for certain categories. Table [NIPS table] shows a compari-<br />son between our model (with minimum spanning tree connectivity)and previous approaches to object<br />10</p>  <p>Page 11</p> <p>Figure 4: Models used for comparative experiments on the gesture recognition task, Y is the gesture<br />label and S the hidden state labels. The left most figure shows a ’stack of HMMs’ model where a<br />separate HMM is trained for each gesture class, the middle figure shows a CRF model and the right<br />most figure the proposed HCRF model.<br />recognition [5] for a standard dataset (Calteq 4).<br />While results using local appearance-based feature descriptors have been promising, our model is not<br />limited to such features and could, for example, be defined on a region-based appearance model.<br />We also explored our HCRF model on body and head gesture recognition, using a chain of observed<br />motion features as the input representation. We evaluated HCRFs with varying levels of long range<br />dependencies, and compared performance to baseline CRF and HMM models. Figure 4 shows graphical<br />representations of the HCRF, HMM, and CRF models used in our experiments.<br />For each gesture class, we first trained a separate HCRF model to discriminate the gesture class<br />from other classes. For a given test sequence, we compared the probabilities given by each of the two-<br />class HCRFs, and the highest scoring model was selected as the recognized gesture. Next, we trained<br />a single joint multi-class HCRF to recognize all classes. Test sequences were run with this model<br />and the gesture class with the highest probability was selected as the recognized gesture. Finally, we<br />conducted experiments that incorporated different long range dependencies. To incorporate long range<br />dependencies in the CRF and HCRF models, we modify the potential function to include a window<br />parameter ω that defines the amount of past and future history to be used when predicting the state at<br />time t. (ω = 0 indicates only the current observation is used).<br />The HMM models were trained using maximum likelihood; the number of Gaussian mixtures and<br />states were set by minimizing the error on the training data; in general this parameters can be optimized<br />with a held out dataset, we didn’t use a held-out validation because we had a small dataset available<br />11</p>  <p>Page 12</p> <p>Figure 5: Illustrations of the six gesture classes for the experiments. Below each image is the abbrevi-<br />ation for the gesture class. The green arrows are the motion trajectory of the fingertip and the numbers<br />next to the arrows symbolizes the order of these arrows<br />for training. The CRF was trained as a multi-way classifier where each state in the model represented<br />one gesture class. Six hidden states were used for the one-vs-all HCRFs, 12 for the multi-class HCRFS,<br />these states where shared among all the classes. For the HMM model we used 4 hidden states for each<br />class, these states were not shared among the different classes.<br />We ran experiments in two domains: arm and head gestures. In the arm gesture domain, we used a<br />dataset of gestures defined for a virtual manipulation task (see Figure 5). There were six gestures in the<br />dataset. In the Expand Horizontally (EH) arm gesture, the user starts with both arms close to the hips,<br />moves both arms laterally apart and retracts back to the resting position. In the Expand Vertically (EV)<br />arm gesture the arms move vertically apart and return to the resting position. In the Shrink Vertically<br />(SV) gesture both arms begin from the hips, move vertically together and back to the hips. In the Point<br />and Back (PB) gesture the user points with one hand and beckons with the other. In the Double Back<br />(DB) gesture, both arms beckon towards the user. Lastly in the Flip Back (FB) gesture, the user simulates<br />holding a book with one hand while the other hand makes a flipping motion, to mimic flipping the pages<br />of the book.<br />Users were asked to perform these gestures in front of a stereo camera. From each image frame, a<br />3D cylindrical body model, consisting of a head, torso, arms and forearms was estimated using a stereo-<br />tracking algorithm [4]. From these body models, both the joint angles and the relative co-ordinates of<br />the joints of the arms are used as observations for our experiments. Thirteen users were asked to perform<br />12</p>  <p>Page 13</p> <p>Arm Gesture<br />HMM ω = 0<br />CRF ω = 0<br />CRF ω = 1<br />HCRF (one-vs-all) ω = 0<br />HCRF (multiclass) ω = 0<br />HCRF (multiclass) ω = 1<br />HCRF (multiclass) ω = 2<br />HCRF (multiclass) ω = 3<br />Avg. Accuracy(%)<br />84.22<br />86.03<br />81.75<br />87.49<br />91.64<br />93.81<br />93.07<br />92.50<br />Table 1: Comparison of recognition performance (percentage accuracy) for body poses estimated from<br />image sequences on 6-way classification task.<br />these six gestures; an average of 90 gestures per class were collected.<br />Table 1 summarizes results for the arm gesture recognition experiments. In these experiments the CRF<br />performed better than HMMs at window size zero. At window size one, however, the CRF performance<br />was poorer; this may be due to overfitting when training the CRF model parameters. Both multi-class<br />and one-vs-all HCRFs perform better than HMMs and CRFs. The most significant improvement in<br />performance was obtained when we used a multi-class HCRF, suggesting that it is important to jointly<br />learn the best discriminative structure.<br />Figure 6 shows the distribution of states for different gesture classes learned by the best performing<br />model (multi-class HCRF). This graph was obtained by computing the Viterbi path for each sequence<br />(i.e. the most likely assignment for the hidden state variables) and counting the number of times that a<br />given state occurred among those sequences. As we can see, the model has found a unique distribution of<br />hidden states for each gesture, and there is a significant amount of state sharing among different gesture<br />classes.<br />From the results in table 1, we can see that incorporating some degree of long range dependencies<br />is important, since the HCRF performance improved when the window size was increased from 0 to 1.<br />However, we also see that further increasing the window size did not improve performance.<br />We also conducted experiments with a head gesture datase obtained using the pose tracking system<br />of [18]. The fast Fourier transform of the 3D angular velocities were used as input features. The<br />13</p>  <p>Page 14</p> <p>Models<br />HMM ω = 0<br />CRF ω = 0<br />CRF ω = 1<br />Accuracy (%)<br />65.33<br />66.53<br />68.24<br />71.88<br />85.25<br />HCRF (multi-class) ω = 0<br />HCRF (multi-class) ω = 1<br />Table 2: Comparison of recognition performance for head gestures.<br />Figure 6: Graph showing the distribution of the hidden states for each gesture class. The numbers in<br />each pie represent the hidden state label, and the area enclosed by the number represents the proportion.<br />data consisted of interactions between human participants and a robotic character [19]. A total of 16<br />participants interacted with a robot, with each interaction lasting between 2 to 5 minutes. A total of 152<br />head nods, 11 head shakes and 159 junk sequences were extracted based on ground truth labels. The<br />junk class had sequences that did not contain any head nods or head shakes during the interactions with<br />the robot. For all experiments in this paper, we separated the data such that the testing dataset had no<br />participants from the training set.<br />Table 2 summarizes the results for the head gesture experiments. The multi-class HCRF model per-<br />forms better than the HMM and CRF models at a window size of zero. The CRF has slightly better<br />performance than the HMMs for the head gesture task, and this performance improved with increased<br />window sizes. The HCRF multi-class model made a significant improvement when the window size was<br />increased, which indicates that incorporating long range dependencies was useful.<br />Notice that for the CRF model increasing the window size from 0 to 1 degrades its performance. This<br />seems surprising since one would expect that adding contextual features could never harm the predic-<br />tive power of the model. It is very likely that this degrade in performance is caused by over-fitting;<br />14</p>  <p>Page 15</p> <p>since adding contextual features increases the complexity of the model. However, we do not observe<br />a performance drop for the HCRF model which would seem to suggest that this model is less suscep-<br />tible to over-fitting; perhaps the presence of local minima prevents the model from over-optimizing its<br />parameters.<br />4. Summary and Conclusions<br />Wehavedevelopedadiscriminativehidden-statemodelanddemonstrateditsutilityonvisualrecognition<br />tasks. Our model combines the ability of CRFs to use dependent input features and the ability of HMMs<br />to learn latent structure; we train a single joint model which shares hidden states for all classes. Our<br />results have shown that our HCRFs outperform both CRFs and HMMs for certain gesture recognition<br />tasks. For arm gestures, the multi-class HCRF model outperforms HMMs and CRFs even when long<br />range dependencies are not used, demonstrating the advantages of joint discriminative learning. For<br />the object recognition dataset our results have shown that incorporating dependencies between latent<br />variables is important and that the minimum-spanning tree formulation can be a good approximation to<br />more highly connected models.<br />References<br />[1] M. Assan and K. Groebel. Video-based sign language recognition using hidden markov models. In Int’l Gest<br />Wksp: Gest. and Sign Lang., 1997.<br />[2] M. Brand, N. Oliver, and A. Pentland. Coupled hidden markov models for complex action recognition. In<br />CVPR, 1996.<br />[3] A. Culotta and P. V. amd A. Callum. Interactive information extraction with constrained conditional random<br />fields. In AAAI, 2004.<br />[4] D. Demirdjian and T. Darrell. 3-d articulated pose tracking for untethered deictic reference. In Int’l Conf. on<br />Multimodal Interfaces, 2002.<br />15</p>  <p>Page 16</p> <p>[5] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning.<br />In CVPR, 2003.<br />[6] S. Fujie, Y. Ejiri, K. Nakajima, Y. Matsusaka, and T. Kobayashi. A conversation robot using head gesture<br />recognition as para-linguistic information. In Proceedings of 13th IEEE International Workshop on Robot<br />and Human Communication, RO-MAN 2004, pages 159–164, September 2004.<br />[7] A. Gunawardana, M. Mahajan, A. Acero, and J. C. Platt. Hidden conditional random fields for phone<br />classification. In INTERSPEECH, 2005.<br />[8] A. K. H.Kuensch, S. Geman. Hidden markov random fields. In Annals of Appied Probability, Vol.5., 2005.<br />[9] A. Kapoor and R. Picard. A real-time head nod and shake detector. In Proceedings from the Workshop on<br />Perspective User Interfaces, November 2001.<br />[10] T. Koo and M. Collins. Hidden-variable models for discriminative reranking. In In proceedings of EMNLP<br />2005., 2005.<br />[11] S. Kumar and M. Hebert. Multiclass discriminative fields for parts-based object detection. In Snowbird<br />Learning Workshop, 2004.<br />[12] S. Kumar and M. Herbert. Discriminative random fields: A framework for contextual interaction in classifi-<br />cation. In ICCV, 2003.<br />[13] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: probabilistic models for segmenting<br />and labelling sequence data. In ICML, 2001.<br />[14] D. Lowe. Object recognition from local scale-invariant features. InIEEE Int Conference on Computer Vision,<br />1999.<br />[15] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy markov models for information extraction and<br />segmentation. In ICML, 2000.<br />[16] A. McCallum and W. Li. Early results for named entity recognition with conditional random fields, feature<br />induction and web-enhanced lexicons. In CoNLL, 2003.<br />[17] A. McCallum and B. Wellner. Toward conditional models of identity uncertainty with application to proper<br />noun coreference. In IJCAI Workshop on Information Integration on the Web, 2003.<br />[18] L.-P. Morency, A. Rahimi, and T. Darrell. Adaptive view-based appearance model. In CVPR, 2003.<br />16</p>  <p>Page 17</p> <p>[19] L.-P. Morency, C. Sidner, C. Lee, and T. Darrell. Contextual recognition of head gestures. In ICMI, 2005.<br />[20] V. I. Pavlovic, R. Sharma, and T. S. Huang. Visual interpretation of hand gestures for human-computer<br />interaction. In PAMI, volume 19, pages 677–695, 1997.<br />[21] A. Quattoni, M. Collins, and T. Darrell. Conditional random fields for object recognition. In NIPS, 2004.<br />[22] A. Ratnaparkhi. A maximum entropy part-of-speech tagger. In EMNLP, 1996.<br />[23] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Conditional models for contextual human motion<br />recognition. In Int’l Conf. on Computer Vision, 2005.<br />[24] T. Starner and A. Pentland. Real-time asl recognition from video using hidden markov models. In ISCV,<br />1995.<br />[25] A. Torralba, K. Murphy, and W. Freeman. Contextual models for object detection using boosted random<br />fields. In NIPS, 2004.<br />[26] M. Yang, D. Roth, and N. Ahuja. Learning to recognize 3d objects with snow. In Proceedings of the Sixth<br />European Conference on Computer Vision, 2000.<br />17</p>  <a href="https://www.researchgate.net/profile/Ariadna_Quattoni/publication/6139655_Hidden_Conditional_Random_Fields/links/00b4951750bbf3fcea000000.pdf">Download full-text</a> </div> <div id="rgw26_56ab19c3f26f4" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw27_56ab19c3f26f4">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw28_56ab19c3f26f4"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Ariadna_Quattoni/publication/6139655_Hidden_Conditional_Random_Fields/links/00b4951750bbf3fcea000000.pdf" class="publication-viewer" title="download.pdf">download.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Ariadna_Quattoni">Ariadna Quattoni</a> &middot; Jan 22, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw29_56ab19c3f26f4"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.3643&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Hidden Conditional Random Fields">Hidden Conditional Random Fields</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.3643&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw36_56ab19c3f26f4" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (213) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw37_56ab19c3f26f4" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw38_56ab19c3f26f4" >  <div class="indent-left">  <div id="rgw39_56ab19c3f26f4" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fwww.sersc.org%2Fjournals%2FIJMUE%2Fvol10_no10_2015%2F10.pdf" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: sersc.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw40_56ab19c3f26f4">  <li class="citation-context-item"> "Because of these good characteristics, CRF has been widely used in the vision community [7] [8]. Quattoni [9] introduced hidden variable into CRF, which is Hidden Conditional Random Fields Model (HCRF). The structure of HCRF may be more expressive than CRF which is completely observable [10]. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model"> <span class="publication-title js-publication-title">Abnormal Crowd Motion Detection with Hidden Conditional Random Fields Model</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2046559467_Dongping_Zhang" class="authors js-author-name ga-publications-authors">Dongping Zhang</a> &middot;     <a href="researcher/2084335562_Kaihang_Xu" class="authors js-author-name ga-publications-authors">Kaihang Xu</a> &middot;     <a href="researcher/2046573791_Yafei_Lu" class="authors js-author-name ga-publications-authors">Yafei Lu</a> &middot;     <a href="researcher/2084323252_Chen_Pan" class="authors js-author-name ga-publications-authors">Chen Pan</a> &middot;     <a href="researcher/2046585097_Huailiang_Peng" class="authors js-author-name ga-publications-authors">Huailiang Peng</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract"></a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract"></a><br/>   </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Oct 2015  &middot; International Journal of Multimedia and Ubiquitous Engineering  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw41_56ab19c3f26f4" >  <div class="indent-left">  <div id="rgw42_56ab19c3f26f4" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/273327811_Latent_Hierarchical_Model_for_Activity_Recognition">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Zhongyu_Lou" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Zhongyu Lou </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw43_56ab19c3f26f4">  <li class="citation-context-item"> "Linear-chain CRFs are efficient models because the exact inference is tractable. However, these models are limited because they cannot capture the intermediate structures within the target states [32]. By adding an extra layer of latent variables, the model allows for more flexibility and therefore can be used for modeling more complex data. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/273327811_Latent_Hierarchical_Model_for_Activity_Recognition"> <span class="publication-title js-publication-title">Latent Hierarchical Model for Activity Recognition</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2046381832_Ninghang_Hu" class="authors js-author-name ga-publications-authors">Ninghang Hu</a> &middot;     <a href="researcher/69914905_Gwenn_Englebienne" class="authors js-author-name ga-publications-authors">Gwenn Englebienne</a> &middot;     <a href="researcher/2024863605_Zhongyu_Lou" class="authors js-author-name ga-publications-authors">Zhongyu Lou</a> &middot;     <a href="researcher/20411091_Ben_Kroese" class="authors js-author-name ga-publications-authors">Ben Kröse</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We present a novel hierarchical model for human activity recognition. In
contrast to approaches that successively recognize actions and activities, our
approach jointly models actions and activities in a unified framework, and
their labels are simultaneously predicted. The model is embedded with a latent
layer that is able to capture a richer class of contextual information in both
state-state and observation-state pairs. Although loops are present in the
model, the model has an overall linear-chain structure, where the exact
inference is tractable. Therefore, the model is very efficient in both
inference and learning. The parameters of the graphical model are learned with
a Structured Support Vector Machine (Structured-SVM). A data-driven approach is
used to initialize the latent variables; therefore, no manual labeling for the
latent states is required. The experimental results from using two benchmark
datasets show that our model outperforms the state-of-the-art approach, and our
model is computationally more efficient. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Oct 2015  &middot; IEEE Transactions on Robotics  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Zhongyu_Lou/publication/273327811_Latent_Hierarchical_Model_for_Activity_Recognition/links/551a827a0cf26cbb81a2e524.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw44_56ab19c3f26f4" >  <div class="indent-left">  <div id="rgw45_56ab19c3f26f4" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/276211625_Semantic_Part_Segmentation_with_Deep_Learning">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1505.02438" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw46_56ab19c3f26f4">  <li class="citation-context-item"> "Before describing the energy function E(y, h|v; W ) in detail note that (i) the DCNN-based quantities v are always observed and the model does not describe their distribution; in other words, we construct a conditional model of y Lafferty et al. (2001); He et al. (2004); (ii) unlike common CRFs, there are also hidden variables h, which results in a Hidden Conditional Random Fields (HCRFs) Quattoni et al. (2007); Murphy (2012); (iii) however, unlike the loopy graphs used in generic HCRFs, the factor graph in this model is bipartite, which makes block Gibbs sampling possible (Sec. 4.1). " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/276211625_Semantic_Part_Segmentation_with_Deep_Learning"> <span class="publication-title js-publication-title">Semantic Part Segmentation with Deep Learning</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2076242094_S_Tsogkas" class="authors js-author-name ga-publications-authors">S. Tsogkas</a> &middot;     <a href="researcher/34263429_I_Kokkinos" class="authors js-author-name ga-publications-authors">I. Kokkinos</a> &middot;     <a href="researcher/2047911997_G_Papandreou" class="authors js-author-name ga-publications-authors">G. Papandreou</a> &middot;     <a href="researcher/70223017_A_Vedaldi" class="authors js-author-name ga-publications-authors">A. Vedaldi</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> In this work we address the task of segmenting an object into its parts, or
semantic part segmentation. We start by adapting a state-of-the-art semantic
segmentation system to this task, and show that a combination of a
fully-convolutional Deep CNN system coupled with Dense CRF labelling provides
excellent results for a broad range of object categories. Still, this approach
remains agnostic to high-level constraints between object parts. We introduce
such prior information by means of the Restricted Boltzmann Machine, adapted to
our task and train our model in an entirely discriminative fashion, as a hidden
CRF, demonstrating that prior information can yield additional improvements. We
evaluate the performance of our approach on the Penn-Fudan and LFW datasets for
the tasks of pedestrian parsing and face labelling respectively. We show
superior performance with respect to competitive methods that have been
extensively engineered on these benchmarks, as well as realistic qualitative
results on part segmentation, even in challenging cases, such as occluded or
deformable objects. We also provide quantitative and extensive qualitative
results on three classes from the PASCAL Parts dataset. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; May 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw31_56ab19c3f26f4" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw32_56ab19c3f26f4">  </ul> </div> </div>   <div id="rgw22_56ab19c3f26f4" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw23_56ab19c3f26f4"> <div> <h5> <a href="publication/224762134_Maximum_Conditional_Likelihood_Linear_Regression_and_Maximum_A_Posteriori_for_Hidden_Conditional_Random_Fields_speaker_adaptation" class="color-inherit ga-similar-publication-title"><span class="publication-title">Maximum Conditional Likelihood Linear Regression and Maximum A Posteriori for Hidden Conditional Random Fields speaker adaptation</span></a>  </h5>  <div class="authors"> <a href="researcher/33214774_Yun-Hsuan_Sung" class="authors ga-similar-publication-author">Yun-Hsuan Sung</a>, <a href="researcher/9097289_Constantinos_Boulis" class="authors ga-similar-publication-author">Constantinos Boulis</a>, <a href="researcher/8803979_Dan_Jurafsky" class="authors ga-similar-publication-author">Dan Jurafsky</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw24_56ab19c3f26f4"> <div> <h5> <a href="publication/259510760_Joint_semi-supervised_learning_of_Hidden_Conditional_Random_Fields_and_Hidden_Markov_Models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Joint semi-supervised learning of Hidden Conditional Random Fields and Hidden Markov Models</span></a>  </h5>  <div class="authors"> <a href="researcher/70046697_Yann_Soullard" class="authors ga-similar-publication-author">Yann Soullard</a>, <a href="researcher/78569844_Martin_Saveski" class="authors ga-similar-publication-author">Martin Saveski</a>, <a href="researcher/13364132_Thierry_Artieres" class="authors ga-similar-publication-author">Thierry Artières</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw25_56ab19c3f26f4"> <div> <h5> <a href="publication/265950996_Tone_modeling_based_on_hidden_conditional_random_fields_and_discriminative_model_weight_training" class="color-inherit ga-similar-publication-title"><span class="publication-title">Tone modeling based on hidden conditional random fields and discriminative model weight training</span></a>  </h5>  <div class="authors"> <a href="researcher/2054552485_Hao_Huang" class="authors ga-similar-publication-author">Hao Huang</a>, <a href="researcher/2054545132_Jie_Zhu" class="authors ga-similar-publication-author">Jie Zhu</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw56_56ab19c3f26f4" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw57_56ab19c3f26f4">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw58_56ab19c3f26f4" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=8x6YjzNLzFomKPcs4DOh8o5Y0ul--FzufZRsKLLklL6gRbeGqu0lPRGxLPla_9PE" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="iIt7xWLb4VWaP+c5EVdgZOpNwdclo2L4R8OK7aoVdsEBuW1nOutV2GfAfn0rfRPRFUocTTyd53bevyuAQyOK7dG6JSumd2Oj7jXAt4aO7hA0D5mUKYzF7GV7EZypeSN//3/AilhfnQB8nB4I81wAcBDSBBJ6Zw8m6m7a9T8t/GnjkhLDVVJW7PGJkBiR4SWK+xyzMiQRWrk60ME3lOCjV3byI1xW0zVumH/4CvSZeA9dziuUoaK0Us2CKWUXyWJi5QsYVwSBu9gq/QIlBRYUuj9V1xGXtBmytcO12d1DplQ="/> <input type="hidden" name="urlAfterLogin" value="publication/6139655_Hidden_Conditional_Random_Fields"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNjEzOTY1NV9IaWRkZW5fQ29uZGl0aW9uYWxfUmFuZG9tX0ZpZWxkcw%3D%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNjEzOTY1NV9IaWRkZW5fQ29uZGl0aW9uYWxfUmFuZG9tX0ZpZWxkcw%3D%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNjEzOTY1NV9IaWRkZW5fQ29uZGl0aW9uYWxfUmFuZG9tX0ZpZWxkcw%3D%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw59_56ab19c3f26f4"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 591;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Ariadna Quattoni","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273704424833026%401442267628404_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Ariadna_Quattoni","institution":"Polytechnic University of Catalonia","institutionUrl":false,"widgetId":"rgw4_56ab19c3f26f4"},"id":"rgw4_56ab19c3f26f4","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=2946602","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab19c3f26f4"},"id":"rgw3_56ab19c3f26f4","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=6139655","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":6139655,"title":"Hidden Conditional Random Fields","journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalDetailsTooltip":{"data":{"journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalAbbrev":"IEEE T PATTERN ANAL","publisher":"IEEE Computer Society; Institute of Electrical and Electronics Engineers, Institute of Electrical and Electronics Engineers","issn":"0162-8828","impactFactor":"5.78","fiveYearImpactFactor":"7.76","citedHalfLife":">10.0","immediacyIndex":"0.71","eigenFactor":"0.05","articleInfluence":"3.31","widgetId":"rgw6_56ab19c3f26f4"},"id":"rgw6_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0162-8828","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":"Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, Massachusetts, United States","type":"Article","details":{"doi":"10.1109\/TPAMI.2007.1124","journalInfos":{"journal":"","publicationDate":"11\/2007;","publicationDateRobot":"2007-11","article":"29(10):1848-53.","journalTitle":"IEEE Transactions on Pattern Analysis and Machine Intelligence","journalUrl":"journal\/0162-8828_IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence","impactFactor":5.78}},"source":{"sourceUrl":"http:\/\/www.ncbi.nlm.nih.gov\/pubmed\/17699927","sourceName":"PubMed"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/TPAMI.2007.1124"},{"key":"rft.atitle","value":"Hidden Conditional Random Fields"},{"key":"rft.title","value":"IEEE transactions on pattern analysis and machine intelligence"},{"key":"rft.jtitle","value":"IEEE transactions on pattern analysis and machine intelligence"},{"key":"rft.volume","value":"29"},{"key":"rft.issue","value":"10"},{"key":"rft.date","value":"2007"},{"key":"rft.pages","value":"1848-53"},{"key":"rft.issn","value":"0162-8828"},{"key":"rft.au","value":"Ariadna Quattoni,Sybor Wang,Louis-Philippe Morency,Michael Collins,Trevor Darrell"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab19c3f26f4"},"id":"rgw7_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=6139655","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":6139655,"peopleItems":[{"data":{"authorNameOnPublication":"Ariadna Quattoni","accountUrl":"profile\/Ariadna_Quattoni","accountKey":"Ariadna_Quattoni","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273704424833026%401442267628404_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ariadna Quattoni","profile":{"professionalInstitution":{"professionalInstitutionName":"Polytechnic University of Catalonia","professionalInstitutionUrl":"institution\/Polytechnic_University_of_Catalonia"}},"professionalInstitutionName":"Polytechnic University of Catalonia","professionalInstitutionUrl":"institution\/Polytechnic_University_of_Catalonia","url":"profile\/Ariadna_Quattoni","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273704424833026%401442267628404_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Ariadna_Quattoni","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab19c3f26f4"},"id":"rgw10_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2946602&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Polytechnic University of Catalonia","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":5,"accountCount":1,"publicationUid":6139655,"widgetId":"rgw9_56ab19c3f26f4"},"id":"rgw9_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2946602&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=5&accountCount=1&publicationUid=6139655","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/32689818_Sybor_Wang","authorNameOnPublication":"Sybor Wang","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Sybor Wang","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/32689818_Sybor_Wang","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab19c3f26f4"},"id":"rgw12_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=32689818&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab19c3f26f4"},"id":"rgw11_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=32689818&authorNameOnPublication=Sybor%20Wang","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/13852141_Louis-Philippe_Morency","authorNameOnPublication":"Louis-Philippe Morency","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Louis-Philippe Morency","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/13852141_Louis-Philippe_Morency","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56ab19c3f26f4"},"id":"rgw14_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=13852141&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56ab19c3f26f4"},"id":"rgw13_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=13852141&authorNameOnPublication=Louis-Philippe%20Morency","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/48105240_Michael_Collins","authorNameOnPublication":"Michael Collins","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michael Collins","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/48105240_Michael_Collins","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw16_56ab19c3f26f4"},"id":"rgw16_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=48105240&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw15_56ab19c3f26f4"},"id":"rgw15_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=48105240&authorNameOnPublication=Michael%20Collins","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8996478_Trevor_Darrell","authorNameOnPublication":"Trevor Darrell","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Trevor Darrell","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8996478_Trevor_Darrell","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw18_56ab19c3f26f4"},"id":"rgw18_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8996478&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw17_56ab19c3f26f4"},"id":"rgw17_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8996478&authorNameOnPublication=Trevor%20Darrell","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab19c3f26f4"},"id":"rgw8_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=6139655&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":6139655,"abstract":"<noscript><\/noscript><div>We present a discriminative latent variable model for classification problems in structured domains where inputs can be represented by a graph of local observations. A hidden-state Conditional Random Field framework learns a set of latent variables conditioned on local features. Observations need not be independent and may overlap in space and time.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw19_56ab19c3f26f4"},"id":"rgw19_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=6139655","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig1\/Figure-1-Encoding-Part-Dependencies-in-the-model-images-show-min-spanning-tree.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig1\/Figure-1-Encoding-Part-Dependencies-in-the-model-images-show-min-spanning-tree_small.png","figureUrl":"\/figure\/6139655_fig1_Figure-1-Encoding-Part-Dependencies-in-the-model-images-show-min-spanning-tree","selected":false,"title":"Figure 1: Encoding Part Dependencies in the model: images show...","key":"6139655_fig1_Figure-1-Encoding-Part-Dependencies-in-the-model-images-show-min-spanning-tree"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig2\/Figure-4-Models-used-for-comparative-experiments-on-the-gesture-recognition-task-Y-is.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig2\/Figure-4-Models-used-for-comparative-experiments-on-the-gesture-recognition-task-Y-is_small.png","figureUrl":"\/figure\/6139655_fig2_Figure-4-Models-used-for-comparative-experiments-on-the-gesture-recognition-task-Y-is","selected":false,"title":"Figure 4: Models used for comparative experiments on the gesture...","key":"6139655_fig2_Figure-4-Models-used-for-comparative-experiments-on-the-gesture-recognition-task-Y-is"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig3\/Figure-5-Illustrations-of-the-six-gesture-classes-for-the-experiments-Below-each-image.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig3\/Figure-5-Illustrations-of-the-six-gesture-classes-for-the-experiments-Below-each-image_small.png","figureUrl":"\/figure\/6139655_fig3_Figure-5-Illustrations-of-the-six-gesture-classes-for-the-experiments-Below-each-image","selected":false,"title":"Figure 5: Illustrations of the six gesture classes for the experiments....","key":"6139655_fig3_Figure-5-Illustrations-of-the-six-gesture-classes-for-the-experiments-Below-each-image"},{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig4\/Figure-6-Graph-showing-the-distribution-of-the-hidden-states-for-each-gesture-class-The.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655\/figure\/fig4\/Figure-6-Graph-showing-the-distribution-of-the-hidden-states-for-each-gesture-class-The_small.png","figureUrl":"\/figure\/6139655_fig4_Figure-6-Graph-showing-the-distribution-of-the-hidden-states-for-each-gesture-class-The","selected":false,"title":"Figure 6: Graph showing the distribution of the hidden states for each...","key":"6139655_fig4_Figure-6-Graph-showing-the-distribution-of-the-hidden-states-for-each-gesture-class-The"}],"readerDocId":"2964090","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw20_56ab19c3f26f4"},"id":"rgw20_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=2964090&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw21_56ab19c3f26f4"},"id":"rgw21_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab19c3f26f4"},"id":"rgw5_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=6139655&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":33214774,"url":"researcher\/33214774_Yun-Hsuan_Sung","fullname":"Yun-Hsuan Sung","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9097289,"url":"researcher\/9097289_Constantinos_Boulis","fullname":"Constantinos Boulis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8803979,"url":"researcher\/8803979_Dan_Jurafsky","fullname":"Dan Jurafsky","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"May 2008","journal":"Acoustics, Speech, and Signal Processing, 1988. ICASSP-88., 1988 International Conference on","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/224762134_Maximum_Conditional_Likelihood_Linear_Regression_and_Maximum_A_Posteriori_for_Hidden_Conditional_Random_Fields_speaker_adaptation","usePlainButton":true,"publicationUid":224762134,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"4.63","url":"publication\/224762134_Maximum_Conditional_Likelihood_Linear_Regression_and_Maximum_A_Posteriori_for_Hidden_Conditional_Random_Fields_speaker_adaptation","title":"Maximum Conditional Likelihood Linear Regression and Maximum A Posteriori for Hidden Conditional Random Fields speaker adaptation","displayTitleAsLink":true,"authors":[{"id":33214774,"url":"researcher\/33214774_Yun-Hsuan_Sung","fullname":"Yun-Hsuan Sung","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9097289,"url":"researcher\/9097289_Constantinos_Boulis","fullname":"Constantinos Boulis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8803979,"url":"researcher\/8803979_Dan_Jurafsky","fullname":"Dan Jurafsky","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Acoustics, Speech and Signal Processing, 2008. ICASSP 2008. IEEE International Conference on; 05\/2008"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/224762134_Maximum_Conditional_Likelihood_Linear_Regression_and_Maximum_A_Posteriori_for_Hidden_Conditional_Random_Fields_speaker_adaptation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/224762134_Maximum_Conditional_Likelihood_Linear_Regression_and_Maximum_A_Posteriori_for_Hidden_Conditional_Random_Fields_speaker_adaptation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw23_56ab19c3f26f4"},"id":"rgw23_56ab19c3f26f4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=224762134","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70046697,"url":"researcher\/70046697_Yann_Soullard","fullname":"Yann Soullard","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78569844,"url":"researcher\/78569844_Martin_Saveski","fullname":"Martin Saveski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13364132,"url":"researcher\/13364132_Thierry_Artieres","fullname":"Thierry Arti\u00e8res","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2014","journal":"Pattern Recognition Letters","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/259510760_Joint_semi-supervised_learning_of_Hidden_Conditional_Random_Fields_and_Hidden_Markov_Models","usePlainButton":true,"publicationUid":259510760,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.55","url":"publication\/259510760_Joint_semi-supervised_learning_of_Hidden_Conditional_Random_Fields_and_Hidden_Markov_Models","title":"Joint semi-supervised learning of Hidden Conditional Random Fields and Hidden Markov Models","displayTitleAsLink":true,"authors":[{"id":70046697,"url":"researcher\/70046697_Yann_Soullard","fullname":"Yann Soullard","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78569844,"url":"researcher\/78569844_Martin_Saveski","fullname":"Martin Saveski","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":13364132,"url":"researcher\/13364132_Thierry_Artieres","fullname":"Thierry Arti\u00e8res","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Pattern Recognition Letters 02\/2014; 37(1):161\u2013171. DOI:10.1016\/j.patrec.2013.03.028"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/259510760_Joint_semi-supervised_learning_of_Hidden_Conditional_Random_Fields_and_Hidden_Markov_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/259510760_Joint_semi-supervised_learning_of_Hidden_Conditional_Random_Fields_and_Hidden_Markov_Models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw24_56ab19c3f26f4"},"id":"rgw24_56ab19c3f26f4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=259510760","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2054552485,"url":"researcher\/2054552485_Hao_Huang","fullname":"Hao Huang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2054545132,"url":"researcher\/2054545132_Jie_Zhu","fullname":"Jie Zhu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 2008","journal":"Transactions of Nanjing University of Aeronautics and Astronautics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/265950996_Tone_modeling_based_on_hidden_conditional_random_fields_and_discriminative_model_weight_training","usePlainButton":true,"publicationUid":265950996,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/265950996_Tone_modeling_based_on_hidden_conditional_random_fields_and_discriminative_model_weight_training","title":"Tone modeling based on hidden conditional random fields and discriminative model weight training","displayTitleAsLink":true,"authors":[{"id":2054552485,"url":"researcher\/2054552485_Hao_Huang","fullname":"Hao Huang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2054545132,"url":"researcher\/2054545132_Jie_Zhu","fullname":"Jie Zhu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Transactions of Nanjing University of Aeronautics and Astronautics 03\/2008; 25(1)."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/265950996_Tone_modeling_based_on_hidden_conditional_random_fields_and_discriminative_model_weight_training","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/265950996_Tone_modeling_based_on_hidden_conditional_random_fields_and_discriminative_model_weight_training\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw25_56ab19c3f26f4"},"id":"rgw25_56ab19c3f26f4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=265950996","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw22_56ab19c3f26f4"},"id":"rgw22_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=6139655&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":6139655,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":6139655,"publicationType":"article","linkId":"00b4951750bbf3fcea000000","fileName":"download.pdf","fileUrl":"profile\/Ariadna_Quattoni\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000.pdf","name":"Ariadna Quattoni","nameUrl":"profile\/Ariadna_Quattoni","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 22, 2016","fileSize":"361.01 KB","widgetId":"rgw28_56ab19c3f26f4"},"id":"rgw28_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=6139655&linkId=00b4951750bbf3fcea000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":6139655,"publicationType":"article","linkId":"0e5fb5a6f0c41c4932e99598","fileName":"Hidden Conditional Random Fields","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.87.3643&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.87.3643&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw29_56ab19c3f26f4"},"id":"rgw29_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=6139655&linkId=0e5fb5a6f0c41c4932e99598&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw27_56ab19c3f26f4"},"id":"rgw27_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=6139655&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":5225,"valueFormatted":"5,225","widgetId":"rgw30_56ab19c3f26f4"},"id":"rgw30_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=6139655","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw26_56ab19c3f26f4"},"id":"rgw26_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=6139655&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":6139655,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw32_56ab19c3f26f4"},"id":"rgw32_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=6139655&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":5225,"valueFormatted":"5,225","widgetId":"rgw33_56ab19c3f26f4"},"id":"rgw33_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=6139655","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw31_56ab19c3f26f4"},"id":"rgw31_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=6139655&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Hidden-state Conditional Random Fields\nA. Quattoni, S. Wang, L.-P Morency, M. Collins, T. Darrell; MIT CSAIL\nAbstract\nWe present a discriminative latent variable model for classification problems in structured domains\nwhere inputs can be represented by a graph of local observations. A hidden-state Conditional Random\nField framework learns a set of latent variables conditioned on local features. Observations need not be\nindependent and may overlap in space and time. We evaluate our model on object detection and gesture\nrecognition tasks.\n1. Introduction\nIt is well known that models which include latent, or hidden-state, structure may be more expressive than\nfully observable models, and can often find relevant substructure in a given domain. Hidden Markov\nModels (HMMs) and Dynamic Bayesian Networks use hidden state to model observations, and have a\nclear generative probabilistic formulation. In this paper we develop a hidden-state conditional random\nfield model, and demonstrate its ability to outperform generative hidden-state and discriminative fully-\nobservable models on object and gesture recognition tasks.\nConditional Random Fields have been shown to be powerful discriminative models because they can\nincorporate essentially arbitrary feature-vector representations of the observed data points [13]. How-\never, they are limited in that they cannot capture intermediate structures using hidden-state variables. In\nthis paper we propose a new model for classification based on CRFs augmented with latent state, which\n1"},{"page":2,"text":"we call Hidden-state Conditional Random Fields (HCRFs). While HMMs are the natural extension of\nMRFs, HCRFs are the analogous extension for CRFs.\nFigure 3 shows the difference between HCRFS and HMMS or Hidden markov random fields. Hidden\nMarkov random fields are directed graphical models [8], where a random variable h is modeled as a\nmarkov process and it is assumed that the observation variable x is a deterministic or stochastic function\nof h. One way of using HMMS for classification is to assume a hidden variable h for each category and\ntrain a model for each of them independently. That is, given k categories and m samples (where Dlis\nthe training data for category l)in a maximum likelihood framework the parameters \u03b8lof each of the k\nmodels are trained to maximize P(Dl|\u03b8l).\nDifferently, an HCRF models the distribution P(c,h|x) directly, where c is a category and h is an\nintermediate hidden variable modeled as a markov random field globally conditioned on observation x.\nThe parameters \u03b8 of the model are trained discriminatively to optimize P(c|x).\nThere is an extensive literature dedicated to gesture recognition; for hand and arm gestures, a com-\nprehensive survey was presented in Pavlovic et al. [20]. Generative models have been used successfully\nto recognize arm gestures [2] and a number of sign languages [1, 24]. Kapoor and Picard presented a\nHMM-based, real time head nod and head shake detector [9]. Fugie et al. also used HMMs to perform\nhead nod recognition [6].\nThe main limitation of latent generative approaches is that they require a model of local features given\nunderlying variables, and generally presume independence of the observations. Accurately specifying\nsuch a generative model may be challenging, particulary in cases where we wish to incorporate long\nrange dependencies in the model and allow hidden variables to depend on several local features. These\nobservations led to the introduction of discriminative models for sequence labeling, including MEMM\u2019s\n[15], [22] and Conditional Random Fields (CRFs). CRFs were first introduced by Lafferty et al. [13]\nand have been widely used since then in the natural language processing community for tasks such as\nnoun co-reference resolution [17], named entity recognition [16] and information extraction [3].\nIn computer vision, CRF\u2019s have been applied to the task of detecting man-made structures in natural\nimages and have been shown to outperform Markov Random Fields (MRF) [12]. Sminchisescu [23]\n2"},{"page":3,"text":"applied CRFs to classify human motion activity and demonstrated their model was more accurate than\nMEMMs and could discriminate subtle motion styles. Torralba et al. [25] introduced Boosted Random\nFields, a model that combines local and global image information for contextual object recognition.\nOur hidden-state discriminative approach for object recognition is related to the work of Kumar and\nHerbert [12], [11], who train a discriminative model using fully-labeled data where each image region is\nassigned a part label from a discrete set of object parts. A CRF is trained and detection and segmentation\nare performed by finding the most likely labeling of the image under the learned model. The main\ndifference between our approach and Kumar\u2019s is that we do not assume that the part assignment variables\nare fully observed and are instead regarded as latent variables. Incorporating hidden variables allows use\nof training data not explicitly labeled with part (hidden-state) structure.\nAnother related model is presented in [26], which builds a discriminative classifier based on a part-\nbased feature representation. Such a representation is obtained by measuring the similarity between\nimage patches (detected with an interest point detector) to a pre-defined dictionary of parts. The dictio-\nnary is built by extracting and clustering patches from a set of representative images of the target class.\nAgain, a significant difference between their approach and ours is that we do not perform a pre-selection\nof discriminative parts, but rather incorporate such a step during training. In parallel to our work on ob-\nject recognition [21], [7] developed a hidden-state CRF model for phone recognition and demonstrated\nthe equivalence of HMM models to a subset of CRF models. Also, Koo and Collins [10] describe a\nsimilar hidden state model applied to a reranking approach for natural language parsing.\nIn previous work on CRFs label sequences are typically taken to be fully observed on training ex-\namples. In our approach category labels are observed, but an additional layer of subordinate labels are\nlearned. These intermediate hidden variables model the latent structure of the input domain; our model\ndefines the joint probability of a class label and hidden state labels conditioned on the observations, with\ndependencies between the hidden variables expressed by an undirected graph. The result is a model\nwhere inference and parameter estimation can be carried out using standard graphical model algorithms\nsuch as loopy belief propagation.\n3"},{"page":4,"text":"2. Hidden Conditional Random Fields\nWe presume a task where we wish to predict a label y given inputs. Each y is a member of a set Y of\npossible labels and each vector x is a vector of local observations x = {x1,x2,...,xm}. The number\nof local observations can vary across examples; for convenience of notation we omit dependence on the\nexample index and simply refer to the number of observations as m in each case. Each local observation\nxjis represented by a feature vector \u03c6(xj) \u2208 ?d, where d is the dimensionality of the representation.\nFor our object recognition task this corresponds to an image patch descriptor, while for our gesture\nrecognition task this contains body motion observations. Our training set consists of labeled examples\n(xi,yi) for i = 1...n, where each yi \u2208 Y, and each xi = {xi,1,xi,2,...,xi,m}. For any example x\nwe also assume a vector of latent variables h = {h1,h2,...,hm}, which are not observed on training\nexamples, and where each hjis a member of H where H is a finite set of possible hidden labels in\nthe model. Intuitively, each hjcorresponds to a labeling of xjwith some member of H, which may\ncorrespond to \u201cpart\u201d or \u201csub-gesture\u201d structure in an observation.\nGiven these definitions of labels y, observations x, and latent variables h, we define a conditional\nprobabilistic model:\nGiven a new test example x, and parameter values \u03b8\u2217induced from a training set, we will take the\nlabel for the example to be argmaxy\u2208YP(y | x,\u03b8\u2217). Following previous work on CRFs [13, 12], we use\nthe following objective function to estimate the parameters:\nL(\u03b8) =\n?\ni\nlogP(yi| xi,\u03b8) \u2212\n1\n2\u03c32||\u03b8||2\n(1)\nThefirstterminEq.1isthelog-likelihoodofthedata. ThesecondtermisthelogofaGaussianpriorwith\nvariance \u03c32, i.e., P(\u03b8) \u223c exp?\nvalues, \u03b8\u2217= argmax\u03b8L(\u03b8), under this criterion. As with other hidden state models (e.g., HMMs),\n1\n2\u03c32||\u03b8||2?. We use gradient ascent to search for the optimal parameter\nadding hidden state makes the optimization non-convex; we search for parameters by initializing from\nmultiple random start points and searching for the best local optimum.\u201d\nWe encode structural constraints with an undirected graph structure, where the hidden variables\n4"},{"page":5,"text":"{h1,...,hm} correspond to vertices in the graph. The set of graph edges (j,k) \u2208 E denotes links\nbetween variables hjand hk. The graph E can be arbitrary; intuitively it should capture any domain\nspecific knowledge that we have about the structure of h. In our object recognition task it is a local mesh\nthat encodes spatial consistency between local appearance features, while in our gesture recognition task\nit is a chain that captures temporal dynamics.\nWe define \u03a8 to take the following form:\n\u03a8(y,h,x;\u03b8) =\nm\n?\nj=1\n?\nl\u2208L1\nf1\nl(j,y,hj,x)\u03b81\nl+\n?\n(j,k)\u2208E\n?\nl\u2208L2\nf2\nl(j,k,y,hj,hk,x)\u03b82\nl\n(2)\nwhere L1is the set of node features, L2the set of edge features , f1\nl,f2\nlare functions defining the features\nin the model, and \u03b81\nl,\u03b82\nlare the components of \u03b8. The f1features depend on single hidden variable values\nin the model; the f2features can depend on pairs of values. Note that \u03a8 is linear in the parameters \u03b8,\nand the model in Eq. ?? is a log-linear model. Moreover the features respect the structure of the graph,\nin that no feature depends on more than two hidden variables hj,hk, and if a feature does depend on\nvariables hjand hkthere must be an edge (j,k) in the graph E.\nAssuming that the edges in E form a tree, and that \u03a8 takes the form in Eq. 2, then exact methods\nexist for inference and parameter estimation in the model. This follows because belief propagation can\nbe used to calculate the following quantities in O(|E||Y|) time:\n\u2200y \u2208 Y,Z(y | x,\u03b8) =\n?\nh\nexp{\u03a8(y,h,x;\u03b8)}\n\u2200y \u2208 Y,j \u2208 1...m,a \u2208 H,P(hj= a | y,x,\u03b8) =\n?\nh:hj=a\nP(h | y,x,\u03b8)\n\u2200y \u2208 Y,(j,k) \u2208 E,a,b \u2208 H,P(hj= a,hk= b | y,x,\u03b8) =\n?\nh:hj=a,hk=b\nP(h | y,x,\u03b8)\nThe first term Z(y | x,\u03b8) is a partition function defined by a summation over the h variables. Terms\nof this form can be used to calculate P(y | x,\u03b8) = Z(y | x,\u03b8)\/?\ncalculation of argmaxP(y | x,\u03b8)\u2014 can be performed efficiently in the model. The second and third\ny?Z(y?| x,\u03b8). Hence inference\u2014\n5"},{"page":6,"text":"terms are marginal distributions over individual variables hjor pairs of variables hj,hkcorresponding\nto edges in the graph. The gradient of L(\u03b8) can be defined in terms of these marginals, and hence can be\ncalculated efficiently. If E contains cycles then approximate methods, such as loopy belief-propagation,\nmay be necessary for inference and parameter estimation.\nIn brief, since P(h,y|x) = P(h|y,x)P(y|x) and since both terms on the right hand side can be\nefficiently computed using belief propagation it follows that the joint distribution can also be computed\nefficiently.\u201d\nWe estimate parameters \u03b8\u2217= argmaxL(\u03b8) from a training set using a quasi-Newton method. The\ngradient of L(\u03b8) can be calculated efficiently using belief propogation update steps; the likelihood term\ndue to the i\u2019th training example is:\nLi(\u03b8) = logP(yi| xi,\u03b8) = log\n??\nhe\u03a8(yi,h,xi;\u03b8)\ny?,he\u03a8(y?,h,xi;\u03b8).\n?\n?\n(3)\nWe first consider derivatives with respect to the parameters \u03b81\nlcorresponding to features f1\nl(j,y,hj,x)\nthat depend on single hidden variables. Taking derivatives gives\n\u2202Li(\u03b8)\n\u2202\u03b81\nl\n=\n?\nh\nP(h | yi,xi,\u03b8)\u2202\u03a8(yi,h,xi;\u03b8)\n\u2202\u03b81\nl\n\u2212\n?\nP(y?,h | xi,\u03b8)\ny?,h\nP(y?,h | xi,\u03b8)\u2202\u03a8(y?,h,xi;\u03b8)\n\u2202\u03b81\nl\n=\n?\n?\nh\nP(h | yi,xi,\u03b8)\nm\n?\nj=1\nf1\nl(j,yi,hj,xi) \u2212\n?\n?\ny?,h\nm\n?\nj=1\nf1\nl(j,y?,hj,xi)\n=\nj,a\nP(hj= a | yi,xi,\u03b8)f1\nl(j,yi,a,xi) \u2212\ny?,j,a\nP(hj= a,y?| xi,\u03b8)f1\nl(j,y?,a,xi)\nIt follows that\u2202Li(\u03b8)\n\u2202\u03b81\nl\ncan be expressed in terms of components P(hj= a | xi,\u03b8) and P(y | xi,\u03b8), which\ncan be calculated using belief propagation, provided that the graph E forms a tree structure. A similar\n6"},{"page":7,"text":"calculation gives\n\u2202Li(\u03b8)\n\u2202\u03b82\nl\n=\n?\n(j,k)\u2208E,a,b\n\u2212\ny?,(j,k)\u2208E,a,b\nP(hj= a,hk= b | yi,xi,\u03b8)f2\n?\nl(j,k,yi,a,b,xi)\nP(hj= a,hk= b,y?| xi,\u03b8)f2\nl(j,k,y?,a,b,xi)\nhence \u2202Li(\u03b8)\/\u2202\u03b82\nlcan also be expressed in terms of expressions that can be calculated using belief\npropagation.\n3. Experiments\nWe explored the performance of our HCRF model on both object and gesture recognition tasks, mea-\nsuring the effect of different degrees of connectivity in the mesh of local observations in the former task\nand the chain of motion observations in the latter task.\nIn our experiments we use a restricted form of \u03a8 where observations interact only with the hidden\nstates:\n\u03a8(y,h,x;\u03b8) =\n?\nj\n\u03c6(xj) \u00b7 \u03b8(hj) +\n?\nj\n\u03b8(y,hj) +\n?\n(j,k)\u2208E\n\u03b8(y,hj,hk)\n(4)\nwhere \u03b8(hj) \u2208 ?dfor hj\u2208 H is a parameter vector corresponding to the j\u2019th latent variable. The inner-\nproduct \u03c6(xj) \u00b7 \u03b8(hj) can be interpreted as a measure of the compatibility between observation xjand\nhidden-state hj, the parameter \u03b8(y,hj) \u2208 ? for hj\u2208 H, y \u2208 Y can be interpreted as a measure of the\ncompatibility between latent variable hjand category label y, and each parameter \u03b8(y,hi,hj) \u2208 ? for\ny \u2208 Y, and hi,hj\u2208 H measures the compatibility between an edge with labels hiand hjand the label y.\nFor these experiments we chose the number of hidden states that minimized the training error,the same\nis true for the number of hidden states and mixtures used in the HMMs.\nIn general however the number of hidden states could be better optimized through a held out valida-\ntion set. To give an idea of the sensitivity of the results to this number [Table xxx nips] shows object\nrecognition results for different number of hidden states.\n7"},{"page":8,"text":"Figure 1: Encoding Part Dependencies in the model: images show min-spanning tree , 1-lattices (top) ,\n2 , and 3-lattices (bottom) over detected features.\nData set5 parts10 parts\nCar Side 9499\nCar Rear\nIn the object recognition domain patches xi,jin each image are obtained using the SIFT detector [14]:\n9191.7\neach patch xi,jis then represented by a feature vector \u03c6(xi,j) that incorporates a combination of SIFT\ndescriptor and relative location and scale features. We assume that parts conditioned on proximate\nobservations are likely to be dependent, as expressed in the neighborhood graph structure.\nThe graph E encodes the amount of connectivity between the hidden variables hj. Intuitively, E\ndetermines the ability of our model to capture conditional dependencies between part assignments. Such\ndependencies between hidden part assignments can be encoded using n-neighbor lattices over local\nobservations. However, increasing connectivity leads to an increase in the computational complexity of\nperforming inference in such models. If E has no connectivity (i.e E contains no edges) the potential\nfunction for our model reduces to:\n\u03a8(y,h,x;\u03b8) =\n?\nj\n\u03c6(xj) \u00b7 \u03b8(hj) +\n?\nj\n\u03b8(y,hj)\n(5)\nThis graph may be too poor to capture important dependencies between part assignments, especially\ngiven that our observations often contain overlapping image patches. Another option for defining E is to\nuse a minimum spanning tree where the weights on the edges are the distances between the correspond-\n8"},{"page":9,"text":"00.10.2\nFalse Positives\n0.3 0.4 0.5\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nR0C Side Data Set\nCorrect Positives\n8\n888\n118\n666\n666\n666\n118\n777\n666\n222\n666\n777\n666\n222\n888\n888\n188\n222\n666\n222\n888\n888\n777\n777\n118\n88\n20 40 6080 100120140 160180200\n20\n40\n60\n80\n2\n8\n2\n2\n8\n8\n8822\n6\n6\n6\n6\n6\n9\n8\n2\n6\n6666\n6\n8\n8\n2\n4\n6666\n8\n2\n2\n6\n8\n6\n8\n8999\n8888\n6622\n6666\n6622\n8888\n8888\n6666\n2666\n6666\n9999\n6666\n8811\n2222\n2221\n2222\n8888\n8888\n9991\n9999\n2222\n111\n20406080100120 140160180200\n20\n40\n60\n80\nFigure 2: ROC curves for the 4 variants of the model: the red curve corresponds to a model with no\nconnectivity, the green curve to a model with minimum spanning tree connectivity, the blue curve to a\nmodel with 2-Lattice connectivity and the yellow curve to a model with 3-Lattice connectivity; Viterbi\nassignments of hidden states to local image patches for min spanning tree and unconnected model, center\nand right respectively.\ning image patches. Distances could in general be based on any aspect of the location or feature space; in\nour experiments below we relied on distance in the image plane. Note that the structure of E will vary\nacross different images. The advantage of using such a graph is that, as we mentioned earlier, when E\ncontains no cycles, and \u03a8 takes the form in Eq. 2, we can perform exact inference on E, using belief\npropagation in time O(|E||Y|)2.\nMore generally, we define E to be an n-Lattice over the local observations. We build an n-neighbor\nlattice by linking every node to its n closest nodes, (i.e. the nodes that correspond to the n closest local\nobservations). When E contains cycles computing exact inference becomes untractable, and we need to\nresort to approximate methods using loopy-belief propagation techniques.\nWe evaluated the effect of different neighborhood structures on recognition performance in a simple\nobject category recognition task. Here for brevity we report results only for the well-known UIUC car\nside dataset. Given a neighborhood structure for our model we trained a binary classifier to distinguish\nbetween a category and a background set formed from the remaining UIUC images. The data set was\nsplit into 3 data sets: a training data set of 200 images, a validation set of 100 images and a testing set\nof 100 images. The validation set was used to select the regularization term (i.e. the variance of the\nGaussian prior) and as a stopping criteria for the gradient ascent.\n9"},{"page":10,"text":"Data set\nCar Side\nCar Rear\nFace\nPlane\nMotorbike\nOur Model\n99 %\n94.6 %\n99 %\n96 %\n95 %\nOthers [1]\n-\n90.3 %\n96.4 %\n90.2 %\n92.5 %\nFigure 3: Comparison with state of the art approach for object recognition (Equal Error Rates)\nFor the first experiment we defined E to be an unconnected graph, for the second a minimum spanning\ntree, for the third a 2-lattice, and for the fourth a 3-lattice, as shown in Figure 1. For the first and second\nexperiments gradient ascent was initialized randomly while for the third and fourth experiments we used\nthe minimum spanning tree solution as initial parameters. Figure 2 shows the ROC curves for the 4\nvariants of the model: the red curve corresponds to a model with no connectivity, the green curve to a\nmodel with minimum spanning tree connectivity, the blue curve to a model with 2-Lattice connectivity\nand the yellow curve to a model with 3-Lattice connectivity.\nFrom this Figure we observe a significant improvement in performance when the model incorporates\nsome degree of dependency between the latent variables (Figure 1).\nFigure 2 shows the most likely assignment of parts to features for the min-spanning tree model and the\nunconnected model for an example in which the former gives a correct classification but the latter fails\nto do so. Notice that both models give smooth part assignments, which is expected as the normalized\nlocation is a feature of the patch representation. In some cases the model relies more heavily on relative\nlocation than appearance, labeling a part based on its location and thus learning the shape of the object\nrather than its appearance. A possible reason for this is that the appearance information might not be\nvery useful for discriminating between classes when the image resolution is too low; as it is the case for\nthe car dataset.\nFor this type of task the min-spanning tree model shows equivalent recognition performance to the\nmodels that use more densely connected graphs. Thus it is clear that the minimum-spanning tree can\nencode sufficient dependency constraints for certain categories. Table [NIPS table] shows a compari-\nson between our model (with minimum spanning tree connectivity)and previous approaches to object\n10"},{"page":11,"text":"Figure 4: Models used for comparative experiments on the gesture recognition task, Y is the gesture\nlabel and S the hidden state labels. The left most figure shows a \u2019stack of HMMs\u2019 model where a\nseparate HMM is trained for each gesture class, the middle figure shows a CRF model and the right\nmost figure the proposed HCRF model.\nrecognition [5] for a standard dataset (Calteq 4).\nWhile results using local appearance-based feature descriptors have been promising, our model is not\nlimited to such features and could, for example, be defined on a region-based appearance model.\nWe also explored our HCRF model on body and head gesture recognition, using a chain of observed\nmotion features as the input representation. We evaluated HCRFs with varying levels of long range\ndependencies, and compared performance to baseline CRF and HMM models. Figure 4 shows graphical\nrepresentations of the HCRF, HMM, and CRF models used in our experiments.\nFor each gesture class, we first trained a separate HCRF model to discriminate the gesture class\nfrom other classes. For a given test sequence, we compared the probabilities given by each of the two-\nclass HCRFs, and the highest scoring model was selected as the recognized gesture. Next, we trained\na single joint multi-class HCRF to recognize all classes. Test sequences were run with this model\nand the gesture class with the highest probability was selected as the recognized gesture. Finally, we\nconducted experiments that incorporated different long range dependencies. To incorporate long range\ndependencies in the CRF and HCRF models, we modify the potential function to include a window\nparameter \u03c9 that defines the amount of past and future history to be used when predicting the state at\ntime t. (\u03c9 = 0 indicates only the current observation is used).\nThe HMM models were trained using maximum likelihood; the number of Gaussian mixtures and\nstates were set by minimizing the error on the training data; in general this parameters can be optimized\nwith a held out dataset, we didn\u2019t use a held-out validation because we had a small dataset available\n11"},{"page":12,"text":"Figure 5: Illustrations of the six gesture classes for the experiments. Below each image is the abbrevi-\nation for the gesture class. The green arrows are the motion trajectory of the fingertip and the numbers\nnext to the arrows symbolizes the order of these arrows\nfor training. The CRF was trained as a multi-way classifier where each state in the model represented\none gesture class. Six hidden states were used for the one-vs-all HCRFs, 12 for the multi-class HCRFS,\nthese states where shared among all the classes. For the HMM model we used 4 hidden states for each\nclass, these states were not shared among the different classes.\nWe ran experiments in two domains: arm and head gestures. In the arm gesture domain, we used a\ndataset of gestures defined for a virtual manipulation task (see Figure 5). There were six gestures in the\ndataset. In the Expand Horizontally (EH) arm gesture, the user starts with both arms close to the hips,\nmoves both arms laterally apart and retracts back to the resting position. In the Expand Vertically (EV)\narm gesture the arms move vertically apart and return to the resting position. In the Shrink Vertically\n(SV) gesture both arms begin from the hips, move vertically together and back to the hips. In the Point\nand Back (PB) gesture the user points with one hand and beckons with the other. In the Double Back\n(DB) gesture, both arms beckon towards the user. Lastly in the Flip Back (FB) gesture, the user simulates\nholding a book with one hand while the other hand makes a flipping motion, to mimic flipping the pages\nof the book.\nUsers were asked to perform these gestures in front of a stereo camera. From each image frame, a\n3D cylindrical body model, consisting of a head, torso, arms and forearms was estimated using a stereo-\ntracking algorithm [4]. From these body models, both the joint angles and the relative co-ordinates of\nthe joints of the arms are used as observations for our experiments. Thirteen users were asked to perform\n12"},{"page":13,"text":"Arm Gesture\nHMM \u03c9 = 0\nCRF \u03c9 = 0\nCRF \u03c9 = 1\nHCRF (one-vs-all) \u03c9 = 0\nHCRF (multiclass) \u03c9 = 0\nHCRF (multiclass) \u03c9 = 1\nHCRF (multiclass) \u03c9 = 2\nHCRF (multiclass) \u03c9 = 3\nAvg. Accuracy(%)\n84.22\n86.03\n81.75\n87.49\n91.64\n93.81\n93.07\n92.50\nTable 1: Comparison of recognition performance (percentage accuracy) for body poses estimated from\nimage sequences on 6-way classification task.\nthese six gestures; an average of 90 gestures per class were collected.\nTable 1 summarizes results for the arm gesture recognition experiments. In these experiments the CRF\nperformed better than HMMs at window size zero. At window size one, however, the CRF performance\nwas poorer; this may be due to overfitting when training the CRF model parameters. Both multi-class\nand one-vs-all HCRFs perform better than HMMs and CRFs. The most significant improvement in\nperformance was obtained when we used a multi-class HCRF, suggesting that it is important to jointly\nlearn the best discriminative structure.\nFigure 6 shows the distribution of states for different gesture classes learned by the best performing\nmodel (multi-class HCRF). This graph was obtained by computing the Viterbi path for each sequence\n(i.e. the most likely assignment for the hidden state variables) and counting the number of times that a\ngiven state occurred among those sequences. As we can see, the model has found a unique distribution of\nhidden states for each gesture, and there is a significant amount of state sharing among different gesture\nclasses.\nFrom the results in table 1, we can see that incorporating some degree of long range dependencies\nis important, since the HCRF performance improved when the window size was increased from 0 to 1.\nHowever, we also see that further increasing the window size did not improve performance.\nWe also conducted experiments with a head gesture datase obtained using the pose tracking system\nof [18]. The fast Fourier transform of the 3D angular velocities were used as input features. The\n13"},{"page":14,"text":"Models\nHMM \u03c9 = 0\nCRF \u03c9 = 0\nCRF \u03c9 = 1\nAccuracy (%)\n65.33\n66.53\n68.24\n71.88\n85.25\nHCRF (multi-class) \u03c9 = 0\nHCRF (multi-class) \u03c9 = 1\nTable 2: Comparison of recognition performance for head gestures.\nFigure 6: Graph showing the distribution of the hidden states for each gesture class. The numbers in\neach pie represent the hidden state label, and the area enclosed by the number represents the proportion.\ndata consisted of interactions between human participants and a robotic character [19]. A total of 16\nparticipants interacted with a robot, with each interaction lasting between 2 to 5 minutes. A total of 152\nhead nods, 11 head shakes and 159 junk sequences were extracted based on ground truth labels. The\njunk class had sequences that did not contain any head nods or head shakes during the interactions with\nthe robot. For all experiments in this paper, we separated the data such that the testing dataset had no\nparticipants from the training set.\nTable 2 summarizes the results for the head gesture experiments. The multi-class HCRF model per-\nforms better than the HMM and CRF models at a window size of zero. The CRF has slightly better\nperformance than the HMMs for the head gesture task, and this performance improved with increased\nwindow sizes. The HCRF multi-class model made a significant improvement when the window size was\nincreased, which indicates that incorporating long range dependencies was useful.\nNotice that for the CRF model increasing the window size from 0 to 1 degrades its performance. This\nseems surprising since one would expect that adding contextual features could never harm the predic-\ntive power of the model. It is very likely that this degrade in performance is caused by over-fitting;\n14"},{"page":15,"text":"since adding contextual features increases the complexity of the model. However, we do not observe\na performance drop for the HCRF model which would seem to suggest that this model is less suscep-\ntible to over-fitting; perhaps the presence of local minima prevents the model from over-optimizing its\nparameters.\n4. Summary and Conclusions\nWehavedevelopedadiscriminativehidden-statemodelanddemonstrateditsutilityonvisualrecognition\ntasks. Our model combines the ability of CRFs to use dependent input features and the ability of HMMs\nto learn latent structure; we train a single joint model which shares hidden states for all classes. Our\nresults have shown that our HCRFs outperform both CRFs and HMMs for certain gesture recognition\ntasks. For arm gestures, the multi-class HCRF model outperforms HMMs and CRFs even when long\nrange dependencies are not used, demonstrating the advantages of joint discriminative learning. For\nthe object recognition dataset our results have shown that incorporating dependencies between latent\nvariables is important and that the minimum-spanning tree formulation can be a good approximation to\nmore highly connected models.\nReferences\n[1] M. Assan and K. Groebel. Video-based sign language recognition using hidden markov models. In Int\u2019l Gest\nWksp: Gest. and Sign Lang., 1997.\n[2] M. Brand, N. Oliver, and A. Pentland. Coupled hidden markov models for complex action recognition. In\nCVPR, 1996.\n[3] A. Culotta and P. V. amd A. Callum. Interactive information extraction with constrained conditional random\nfields. In AAAI, 2004.\n[4] D. Demirdjian and T. Darrell. 3-d articulated pose tracking for untethered deictic reference. In Int\u2019l Conf. on\nMultimodal Interfaces, 2002.\n15"},{"page":16,"text":"[5] R. Fergus, P. Perona, and A. Zisserman. Object class recognition by unsupervised scale-invariant learning.\nIn CVPR, 2003.\n[6] S. Fujie, Y. Ejiri, K. Nakajima, Y. Matsusaka, and T. Kobayashi. A conversation robot using head gesture\nrecognition as para-linguistic information. In Proceedings of 13th IEEE International Workshop on Robot\nand Human Communication, RO-MAN 2004, pages 159\u2013164, September 2004.\n[7] A. Gunawardana, M. Mahajan, A. Acero, and J. C. Platt. Hidden conditional random fields for phone\nclassification. In INTERSPEECH, 2005.\n[8] A. K. H.Kuensch, S. Geman. Hidden markov random fields. In Annals of Appied Probability, Vol.5., 2005.\n[9] A. Kapoor and R. Picard. A real-time head nod and shake detector. In Proceedings from the Workshop on\nPerspective User Interfaces, November 2001.\n[10] T. Koo and M. Collins. Hidden-variable models for discriminative reranking. In In proceedings of EMNLP\n2005., 2005.\n[11] S. Kumar and M. Hebert. Multiclass discriminative fields for parts-based object detection. In Snowbird\nLearning Workshop, 2004.\n[12] S. Kumar and M. Herbert. Discriminative random fields: A framework for contextual interaction in classifi-\ncation. In ICCV, 2003.\n[13] J. Lafferty, A. McCallum, and F. Pereira. Conditional random fields: probabilistic models for segmenting\nand labelling sequence data. In ICML, 2001.\n[14] D. Lowe. Object recognition from local scale-invariant features. InIEEE Int Conference on Computer Vision,\n1999.\n[15] A. McCallum, D. Freitag, and F. Pereira. Maximum entropy markov models for information extraction and\nsegmentation. In ICML, 2000.\n[16] A. McCallum and W. Li. Early results for named entity recognition with conditional random fields, feature\ninduction and web-enhanced lexicons. In CoNLL, 2003.\n[17] A. McCallum and B. Wellner. Toward conditional models of identity uncertainty with application to proper\nnoun coreference. In IJCAI Workshop on Information Integration on the Web, 2003.\n[18] L.-P. Morency, A. Rahimi, and T. Darrell. Adaptive view-based appearance model. In CVPR, 2003.\n16"},{"page":17,"text":"[19] L.-P. Morency, C. Sidner, C. Lee, and T. Darrell. Contextual recognition of head gestures. In ICMI, 2005.\n[20] V. I. Pavlovic, R. Sharma, and T. S. Huang. Visual interpretation of hand gestures for human-computer\ninteraction. In PAMI, volume 19, pages 677\u2013695, 1997.\n[21] A. Quattoni, M. Collins, and T. Darrell. Conditional random fields for object recognition. In NIPS, 2004.\n[22] A. Ratnaparkhi. A maximum entropy part-of-speech tagger. In EMNLP, 1996.\n[23] C. Sminchisescu, A. Kanaujia, Z. Li, and D. Metaxas. Conditional models for contextual human motion\nrecognition. In Int\u2019l Conf. on Computer Vision, 2005.\n[24] T. Starner and A. Pentland. Real-time asl recognition from video using hidden markov models. In ISCV,\n1995.\n[25] A. Torralba, K. Murphy, and W. Freeman. Contextual models for object detection using boosted random\nfields. In NIPS, 2004.\n[26] M. Yang, D. Roth, and N. Ahuja. Learning to recognize 3d objects with snow. In Proceedings of the Sixth\nEuropean Conference on Computer Vision, 2000.\n17"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000.pdf","widgetId":"rgw34_56ab19c3f26f4"},"id":"rgw34_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=6139655&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw35_56ab19c3f26f4"},"id":"rgw35_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=6139655&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":6139655,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":6139655,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2046559467,"url":"researcher\/2046559467_Dongping_Zhang","fullname":"Dongping Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084335562,"url":"researcher\/2084335562_Kaihang_Xu","fullname":"Kaihang Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2046573791,"url":"researcher\/2046573791_Yafei_Lu","fullname":"Yafei Lu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2084323252,"url":"researcher\/2084323252_Chen_Pan","fullname":"Chen Pan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Oct 2015","journal":"International Journal of Multimedia and Ubiquitous Engineering","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model","usePlainButton":true,"publicationUid":283663670,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model","title":"Abnormal Crowd Motion Detection with Hidden Conditional Random Fields Model","displayTitleAsLink":true,"authors":[{"id":2046559467,"url":"researcher\/2046559467_Dongping_Zhang","fullname":"Dongping Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084335562,"url":"researcher\/2084335562_Kaihang_Xu","fullname":"Kaihang Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2046573791,"url":"researcher\/2046573791_Yafei_Lu","fullname":"Yafei Lu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2084323252,"url":"researcher\/2084323252_Chen_Pan","fullname":"Chen Pan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2046585097,"url":"researcher\/2046585097_Huailiang_Peng","fullname":"Huailiang Peng","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["International Journal of Multimedia and Ubiquitous Engineering 10\/2015; 10(10):91-98. DOI:10.14257\/ijmue.2015.10.10.10"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fwww.sersc.org%2Fjournals%2FIJMUE%2Fvol10_no10_2015%2F10.pdf","sourceName":"sersc.org","hasSourceUrl":true},"publicationUid":283663670,"publicationUrl":"publication\/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model\/links\/568902f908ae051f9af73af3\/smallpreview.png","linkId":"568902f908ae051f9af73af3","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=283663670&reference=568902f908ae051f9af73af3&eventCode=&origin=publication_list","widgetId":"rgw39_56ab19c3f26f4"},"id":"rgw39_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=283663670&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":6139655,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283663670_Abnormal_Crowd_Motion_Detection_with_Hidden_Conditional_Random_Fields_Model\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Because of these good characteristics, CRF has been widely used in the vision community [7] [8]. Quattoni [9] introduced hidden variable into CRF, which is Hidden Conditional Random Fields Model (HCRF). The structure of HCRF may be more expressive than CRF which is completely observable [10]. "],"widgetId":"rgw40_56ab19c3f26f4"},"id":"rgw40_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw38_56ab19c3f26f4"},"id":"rgw38_56ab19c3f26f4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=283663670&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2046381832,"url":"researcher\/2046381832_Ninghang_Hu","fullname":"Ninghang Hu","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278639032717326%401443444130274_m"},{"id":69914905,"url":"researcher\/69914905_Gwenn_Englebienne","fullname":"Gwenn Englebienne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2024863605,"url":"researcher\/2024863605_Zhongyu_Lou","fullname":"Zhongyu Lou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":20411091,"url":"researcher\/20411091_Ben_Kroese","fullname":"Ben Kr\u00f6se","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Oct 2015","journal":"IEEE Transactions on Robotics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/273327811_Latent_Hierarchical_Model_for_Activity_Recognition","usePlainButton":true,"publicationUid":273327811,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.43","url":"publication\/273327811_Latent_Hierarchical_Model_for_Activity_Recognition","title":"Latent Hierarchical Model for Activity Recognition","displayTitleAsLink":true,"authors":[{"id":2046381832,"url":"researcher\/2046381832_Ninghang_Hu","fullname":"Ninghang Hu","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278639032717326%401443444130274_m"},{"id":69914905,"url":"researcher\/69914905_Gwenn_Englebienne","fullname":"Gwenn Englebienne","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2024863605,"url":"researcher\/2024863605_Zhongyu_Lou","fullname":"Zhongyu Lou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":20411091,"url":"researcher\/20411091_Ben_Kroese","fullname":"Ben Kr\u00f6se","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Robotics 10\/2015;  DOI:10.1109\/TRO.2015.2495002"],"abstract":"We present a novel hierarchical model for human activity recognition. In\ncontrast to approaches that successively recognize actions and activities, our\napproach jointly models actions and activities in a unified framework, and\ntheir labels are simultaneously predicted. The model is embedded with a latent\nlayer that is able to capture a richer class of contextual information in both\nstate-state and observation-state pairs. Although loops are present in the\nmodel, the model has an overall linear-chain structure, where the exact\ninference is tractable. Therefore, the model is very efficient in both\ninference and learning. The parameters of the graphical model are learned with\na Structured Support Vector Machine (Structured-SVM). A data-driven approach is\nused to initialize the latent variables; therefore, no manual labeling for the\nlatent states is required. The experimental results from using two benchmark\ndatasets show that our model outperforms the state-of-the-art approach, and our\nmodel is computationally more efficient.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/273327811_Latent_Hierarchical_Model_for_Activity_Recognition","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Zhongyu_Lou\/publication\/273327811_Latent_Hierarchical_Model_for_Activity_Recognition\/links\/551a827a0cf26cbb81a2e524.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Zhongyu_Lou","sourceName":"Zhongyu Lou","hasSourceUrl":true},"publicationUid":273327811,"publicationUrl":"publication\/273327811_Latent_Hierarchical_Model_for_Activity_Recognition","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/273327811_Latent_Hierarchical_Model_for_Activity_Recognition\/links\/551a827a0cf26cbb81a2e524\/smallpreview.png","linkId":"551a827a0cf26cbb81a2e524","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=273327811&reference=551a827a0cf26cbb81a2e524&eventCode=&origin=publication_list","widgetId":"rgw42_56ab19c3f26f4"},"id":"rgw42_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=273327811&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"551a827a0cf26cbb81a2e524","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":6139655,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/273327811_Latent_Hierarchical_Model_for_Activity_Recognition\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Linear-chain CRFs are efficient models because the exact inference is tractable. However, these models are limited because they cannot capture the intermediate structures within the target states [32]. By adding an extra layer of latent variables, the model allows for more flexibility and therefore can be used for modeling more complex data. "],"widgetId":"rgw43_56ab19c3f26f4"},"id":"rgw43_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw41_56ab19c3f26f4"},"id":"rgw41_56ab19c3f26f4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=273327811&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2076242094,"url":"researcher\/2076242094_S_Tsogkas","fullname":"S. Tsogkas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":34263429,"url":"researcher\/34263429_I_Kokkinos","fullname":"I. Kokkinos","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2047911997,"url":"researcher\/2047911997_G_Papandreou","fullname":"G. Papandreou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70223017,"url":"researcher\/70223017_A_Vedaldi","fullname":"A. Vedaldi","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A285668166127616%401445120006312_m\/Andrea_Vedaldi.png"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"May 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/276211625_Semantic_Part_Segmentation_with_Deep_Learning","usePlainButton":true,"publicationUid":276211625,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/276211625_Semantic_Part_Segmentation_with_Deep_Learning","title":"Semantic Part Segmentation with Deep Learning","displayTitleAsLink":true,"authors":[{"id":2076242094,"url":"researcher\/2076242094_S_Tsogkas","fullname":"S. Tsogkas","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":34263429,"url":"researcher\/34263429_I_Kokkinos","fullname":"I. Kokkinos","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2047911997,"url":"researcher\/2047911997_G_Papandreou","fullname":"G. Papandreou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70223017,"url":"researcher\/70223017_A_Vedaldi","fullname":"A. Vedaldi","last":true,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A285668166127616%401445120006312_m\/Andrea_Vedaldi.png"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"In this work we address the task of segmenting an object into its parts, or\nsemantic part segmentation. We start by adapting a state-of-the-art semantic\nsegmentation system to this task, and show that a combination of a\nfully-convolutional Deep CNN system coupled with Dense CRF labelling provides\nexcellent results for a broad range of object categories. Still, this approach\nremains agnostic to high-level constraints between object parts. We introduce\nsuch prior information by means of the Restricted Boltzmann Machine, adapted to\nour task and train our model in an entirely discriminative fashion, as a hidden\nCRF, demonstrating that prior information can yield additional improvements. We\nevaluate the performance of our approach on the Penn-Fudan and LFW datasets for\nthe tasks of pedestrian parsing and face labelling respectively. We show\nsuperior performance with respect to competitive methods that have been\nextensively engineered on these benchmarks, as well as realistic qualitative\nresults on part segmentation, even in challenging cases, such as occluded or\ndeformable objects. We also provide quantitative and extensive qualitative\nresults on three classes from the PASCAL Parts dataset.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/276211625_Semantic_Part_Segmentation_with_Deep_Learning","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1505.02438","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":276211625,"publicationUrl":"publication\/276211625_Semantic_Part_Segmentation_with_Deep_Learning","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/276211625_Semantic_Part_Segmentation_with_Deep_Learning\/links\/565b972b08ae1ef92980f956\/smallpreview.png","linkId":"565b972b08ae1ef92980f956","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=276211625&reference=565b972b08ae1ef92980f956&eventCode=&origin=publication_list","widgetId":"rgw45_56ab19c3f26f4"},"id":"rgw45_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=276211625&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":6139655,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/276211625_Semantic_Part_Segmentation_with_Deep_Learning\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Before describing the energy function E(y, h|v; W ) in detail note that (i) the DCNN-based quantities v are always observed and the model does not describe their distribution; in other words, we construct a conditional model of y Lafferty et al. (2001); He et al. (2004); (ii) unlike common CRFs, there are also hidden variables h, which results in a Hidden Conditional Random Fields (HCRFs) Quattoni et al. (2007); Murphy (2012); (iii) however, unlike the loopy graphs used in generic HCRFs, the factor graph in this model is bipartite, which makes block Gibbs sampling possible (Sec. 4.1). "],"widgetId":"rgw46_56ab19c3f26f4"},"id":"rgw46_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw44_56ab19c3f26f4"},"id":"rgw44_56ab19c3f26f4","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=276211625&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":6139655,"publicationLink":"publication\/6139655_Hidden_Conditional_Random_Fields","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw37_56ab19c3f26f4"},"id":"rgw37_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=6139655&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=213","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":213,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw36_56ab19c3f26f4"},"id":"rgw36_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=6139655&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00b4951750bbf3fcea000000","name":"Ariadna Quattoni","date":null,"nameLink":"profile\/Ariadna_Quattoni","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"bec3ecfd56a8e517315ffbc739aa4c4e","showFileSizeNote":false,"fileSize":"361.01 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00b4951750bbf3fcea000000","name":"Ariadna Quattoni","date":null,"nameLink":"profile\/Ariadna_Quattoni","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"bec3ecfd56a8e517315ffbc739aa4c4e","showFileSizeNote":false,"fileSize":"361.01 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:8:rect:58.50,232.01,494.93,27.00","ordinal":"1"},"assetId":"AS:277798821351452@1443243808921"},{"props":{"position":"float","orientation":"portrait","coords":"pag:11:rect:58.50,179.28,494.99,55.89","ordinal":"4"},"assetId":"AS:277798821351455@1443243808964"},{"props":{"position":"float","orientation":"portrait","coords":"pag:12:rect:58.50,209.94,494.94,41.45","ordinal":"5"},"assetId":"AS:277798825545734@1443243809106"},{"props":{"position":"float","orientation":"portrait","coords":"pag:14:rect:58.50,318.56,494.93,27.01","ordinal":"6"},"assetId":"AS:277798825545740@1443243809131"}],"figureAssetIds":["AS:277798821351452@1443243808921","AS:277798821351455@1443243808964","AS:277798825545734@1443243809106","AS:277798825545740@1443243809131"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=3zG6GbRcLaGg1fGrjV0crdITPtv36vzqTmrIQ3GrbIlg-SOa9Top0UtKgLzrIgEsfsJ_ZtmbToJaPp6VGmRwaQ.lXzs5-0Q6sPyrwV2eJr4VFmqSR2euLAVvUk8ivSto5va45zHbD_rSYph0PBp93DJVewQojvKuKLthLrJbkExwA","clickOnPill":"publication.PublicationFigures.html?_sg=e17rE7pWrvRHk47fzz6wY089w_ctX3KhVljWLPEB6bGxS8pvvHZOG1PCqPoWj6eVDkrytHXXykkRi3z8DbMq-Q.jdz7KRw7vnw3anwkuseYxnvLhq8WU61TbA2SjbkicFDm22UCB4wv4x2PT7gaBU2DbZ0neX_QcFfrhbc24hj94g"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAriadna_Quattoni%2Fpublication%2F6139655_Hidden_Conditional_Random_Fields%2Flinks%2F00b4951750bbf3fcea000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=xCFWcEEktChuWPnMtiTaCsO4RpN3jaFiBuRam7zkd1XxZcbUMFt4IAbPZUXuz00XU_E_Ez6hIMXg0ZCha-FOsw","urlHash":"57f8c093a26c736b4962cefbcf790f0f","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=wljyuMF1ytmGuu3Bxj68T35jC0OwCknjOyqU_6IcahccL8yafjdfDbUXgh5bK3MIFgm8ddiydF58vp0nYpKAoxWH0QNL9rVVn6U2eAG9wt0.1b0DhffhL4efC9F3m3l9zfpZmDAX_6szsxT036drsWDicZ3ePSawjcNTKFYuKeDV1vb26h7T4293OWFr4VuIew.QZ_gc9yHgCTEC1jqg537wMSWg2jpCLkQfYjlpU3HtKTSffIyImB_LQedo75z7S80l6dNhMtpB2qBZ52Un2UsIg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00b4951750bbf3fcea000000","trackedDownloads":{"00b4951750bbf3fcea000000":{"v":false,"d":false}},"assetId":"AS:104045486280712@1401817785860","readerDocId":"2964090","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":6139655,"commentCursorPromo":null,"widgetId":"rgw48_56ab19c3f26f4"},"id":"rgw48_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAriadna_Quattoni%2Fpublication%2F6139655_Hidden_Conditional_Random_Fields%2Flinks%2F00b4951750bbf3fcea000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A104045486280712%401401817785860&publicationUid=6139655&linkId=00b4951750bbf3fcea000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Hidden Conditional Random Fields","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=vBag8KRJhrppURnhlkUE3HaitT0XT4JooNm6b177Ed2n8udiwpZskR0WkqS3SrgsvtxXnE6xqPOUDv0EnDMmRUQsrEPanw5qWmRneLB3XhM.opRjxQ8bZWfDUC9rfe73ISDspF0RvTZcMwd7YcIoDJhUuszIyrQRJhIaL1V5cqYmN8KZnm39EK33B15t_Lcfhg.wMBqnwRU_hKutIvzSznZaLNeEv1qc8e5Kpb8qCtU_Ru4LGxM-TamMYKfXiQNvlAeNM0MaIjQLhWC10Sd5HrpKg","publicationUid":6139655,"trackedDownloads":{"00b4951750bbf3fcea000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw50_56ab19c3f26f4"},"id":"rgw50_56ab19c3f26f4","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw51_56ab19c3f26f4"},"id":"rgw51_56ab19c3f26f4","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw52_56ab19c3f26f4"},"id":"rgw52_56ab19c3f26f4","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw53_56ab19c3f26f4"},"id":"rgw53_56ab19c3f26f4","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw54_56ab19c3f26f4"},"id":"rgw54_56ab19c3f26f4","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw49_56ab19c3f26f4"},"id":"rgw49_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw47_56ab19c3f26f4"},"id":"rgw47_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/6139655_Hidden_Conditional_Random_Fields","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab19c3f26f4"},"id":"rgw2_56ab19c3f26f4","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":6139655},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=6139655&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab19c3f26f4"},"id":"rgw1_56ab19c3f26f4","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"n+MNQaFsYC2P16xrE6ETKB50K1yebVgYdHbo7NXcGl5WmhjYYdTk4WtaCCXi4cNEIwxFXx2Bhp7Sa2glHYwbgNWUtfQVTIqgnAB3d3H96ATJYsKlmOTBZuyFSwzJWY\/UqmWIfauu6u8vLuWPb4532pUsgddX3b8ZOc9zzl59CeNzc4246lrDMYpphyd3w0WNQLGMpEOxPNDFud+0qFGtMW+2bpYNJs1GQFj4a1lTXr9Hd0QQvsZKNFY\/iZo6OPX4O2fSUtD3usBEe1QlryLKaTZG1ZTzDjQws1DVBsJVeIg=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/6139655_Hidden_Conditional_Random_Fields\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Hidden Conditional Random Fields\" \/>\n<meta property=\"og:description\" content=\"We present a discriminative latent variable model for classification problems in structured domains where inputs can be represented by a graph of local observations. A hidden-state Conditional...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/6139655_Hidden_Conditional_Random_Fields\" \/>\n<meta property=\"rg:id\" content=\"PB:6139655\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/TPAMI.2007.1124\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Hidden Conditional Random Fields\" \/>\n<meta name=\"citation_author\" content=\"Ariadna Quattoni\" \/>\n<meta name=\"citation_author\" content=\"Sybor Wang\" \/>\n<meta name=\"citation_author\" content=\"Louis-Philippe Morency\" \/>\n<meta name=\"citation_author\" content=\"Michael Collins\" \/>\n<meta name=\"citation_author\" content=\"Trevor Darrell\" \/>\n<meta name=\"citation_pmid\" content=\"17699927\" \/>\n<meta name=\"citation_publication_date\" content=\"2007\/11\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"IEEE Transactions on Pattern Analysis and Machine Intelligence\" \/>\n<meta name=\"citation_issn\" content=\"0162-8828\" \/>\n<meta name=\"citation_volume\" content=\"29\" \/>\n<meta name=\"citation_issue\" content=\"10\" \/>\n<meta name=\"citation_firstpage\" content=\"1848\" \/>\n<meta name=\"citation_lastpage\" content=\"53\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/TPAMI.2007.1124\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Ariadna_Quattoni\/publication\/6139655_Hidden_Conditional_Random_Fields\/links\/00b4951750bbf3fcea000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/6139655_Hidden_Conditional_Random_Fields\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/6139655_Hidden_Conditional_Random_Fields\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-7b696229-7122-4f03-9be5-3f22e004405c","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":575,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw55_56ab19c3f26f4"},"id":"rgw55_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-7b696229-7122-4f03-9be5-3f22e004405c", "fdaac4ba5b55f13be7af6bad52487b23024c2bd7");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-7b696229-7122-4f03-9be5-3f22e004405c", "fdaac4ba5b55f13be7af6bad52487b23024c2bd7");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw56_56ab19c3f26f4"},"id":"rgw56_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/6139655_Hidden_Conditional_Random_Fields","requestToken":"iIt7xWLb4VWaP+c5EVdgZOpNwdclo2L4R8OK7aoVdsEBuW1nOutV2GfAfn0rfRPRFUocTTyd53bevyuAQyOK7dG6JSumd2Oj7jXAt4aO7hA0D5mUKYzF7GV7EZypeSN\/\/3\/AilhfnQB8nB4I81wAcBDSBBJ6Zw8m6m7a9T8t\/GnjkhLDVVJW7PGJkBiR4SWK+xyzMiQRWrk60ME3lOCjV3byI1xW0zVumH\/4CvSZeA9dziuUoaK0Us2CKWUXyWJi5QsYVwSBu9gq\/QIlBRYUuj9V1xGXtBmytcO12d1DplQ=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=8x6YjzNLzFomKPcs4DOh8o5Y0ul--FzufZRsKLLklL6gRbeGqu0lPRGxLPla_9PE","encodedUrlAfterLogin":"cHVibGljYXRpb24vNjEzOTY1NV9IaWRkZW5fQ29uZGl0aW9uYWxfUmFuZG9tX0ZpZWxkcw%3D%3D","signupCallToAction":"Join for free","widgetId":"rgw58_56ab19c3f26f4"},"id":"rgw58_56ab19c3f26f4","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw57_56ab19c3f26f4"},"id":"rgw57_56ab19c3f26f4","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw59_56ab19c3f26f4"},"id":"rgw59_56ab19c3f26f4","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
