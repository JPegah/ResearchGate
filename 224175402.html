<!DOCTYPE html> <html lang="en" class="" id="rgw50_56ab1a0b99463"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="E1KIJVPbESCKhu/OUQKDMu2TX1JJkGvawwd4z+Le2u0gUY48KRTVjOZG8CbVwRL7pUK0XwTg3YQqVyDC3xqLBiegVplkfDW1NLd0bBPZYChSqCpY07+7aT0EIHvOXLjMQJlRQidcTJZScoMELPQ5gq70OxEf48yGACWRpDGlWpaDewu7JWGO5Y5crVv12hVuBtqvADVq0pwPUEd7e/FpaZ0+Jig0WFTZs9Z55UPG+tzWCw1APkv6umTWxhp5V9sQ9Mldwxi4TEdrOcjGVJbd2ydvRg0eqW8/nCwu8AlO3wE="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-edd61362-41ed-4713-b0b4-1e4eace14ca3",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Scaling the iHMM: Parallelization versus Hadoop" />
<meta property="og:description" content="This paper compares parallel and distributed implementations of an iterative, Gibbs sampling, machine learning algorithm. Distributed implementations run under Hadoop on facility computing clouds...." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop/links/00b4951b71b4e35e32000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop" />
<meta property="rg:id" content="PB:224175402" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/CIT.2010.223" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Scaling the iHMM: Parallelization versus Hadoop" />
<meta name="citation_author" content="Sébastien Bratières" />
<meta name="citation_author" content="Jurgen van Gael" />
<meta name="citation_author" content="Andreas Vlachos" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_conference_title" content="Computer and Information Technology (CIT), 2010 IEEE 10th International Conference on" />
<meta name="citation_publication_date" content="2010/08/01" />
<meta name="citation_firstpage" content="1235" />
<meta name="citation_lastpage" content="1240" />
<meta name="citation_doi" content="10.1109/CIT.2010.223" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Andreas_Vlachos3/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop/links/00b4951b71b4e35e32000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/215868066921738/styles/pow/publicliterature/FigureList.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Scaling the iHMM: Parallelization versus Hadoop (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Scaling the iHMM: Parallelization versus Hadoop on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1a0b99463" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1a0b99463" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1a0b99463">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2FCIT.2010.223&rft.atitle=Scaling%20the%20iHMM%3A%20Parallelization%20versus%20Hadoop&rft.date=2010&rft.pages=1235%20-%201240&rft.au=S%C3%A9bastien%20Brati%C3%A8res%2CJurgen%20van%20Gael%2CAndreas%20Vlachos%2CZoubin%20Ghahramani&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">Scaling the iHMM: Parallelization versus Hadoop</h1> <meta itemprop="headline" content="Scaling the iHMM: Parallelization versus Hadoop">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop/links/00b4951b71b4e35e32000000/smallpreview.png">  <div id="rgw7_56ab1a0b99463" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1a0b99463"> <a href="researcher/2021802159_Sebastien_Bratieres" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Sébastien Bratières" alt="Sébastien Bratières" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Sébastien Bratières</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56ab1a0b99463">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2021802159_Sebastien_Bratieres"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Sébastien Bratières" alt="Sébastien Bratières" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2021802159_Sebastien_Bratieres" class="display-name">Sébastien Bratières</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1a0b99463"> <a href="researcher/69831654_Jurgen_van_Gael" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Jurgen van Gael" alt="Jurgen van Gael" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Jurgen van Gael</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab1a0b99463">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69831654_Jurgen_van_Gael"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Jurgen van Gael" alt="Jurgen van Gael" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69831654_Jurgen_van_Gael" class="display-name">Jurgen van Gael</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1a0b99463" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Andreas_Vlachos3" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272390873677839%401441954453533_m" title="Andreas Vlachos" alt="Andreas Vlachos" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Andreas Vlachos</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56ab1a0b99463" data-account-key="Andreas_Vlachos3">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Andreas_Vlachos3"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272390873677839%401441954453533_l" title="Andreas Vlachos" alt="Andreas Vlachos" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Andreas_Vlachos3" class="display-name">Andreas Vlachos</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Cambridge" title="University of Cambridge">University of Cambridge</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56ab1a0b99463"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw15_56ab1a0b99463">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Dept. of Eng., Univ. of Cambridge, Cambridge, UK </div>      DOI:&nbsp;10.1109/CIT.2010.223     Conference: Computer and Information Technology (CIT), 2010 IEEE 10th International Conference on      <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/conf/IEEEcit/IEEEcit2010.html#BratieresGVG10" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw16_56ab1a0b99463" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>This paper compares parallel and distributed implementations of an iterative, Gibbs sampling, machine learning algorithm. Distributed implementations run under Hadoop on facility computing clouds. The probabilistic model under study is the infinite HMM, in which parameters are learnt using an instance blocked Gibbs sampling, with a step consisting of a dynamic program. We apply this model to learn part-of-speech tags from newswire text in an unsupervised fashion. However our focus here is on runtime performance, as opposed to NLP-relevant scores, embodied by iteration duration, ease of development, deployment and debugging.</div> </p>  </div>  </div>   </div>     <div id="rgw17_56ab1a0b99463" class="figure-carousel"> <div class="carousel-hd"> Figures in this publication </div> <div class="carousel-bd"> <ul class="clearfix">  <li> <a href="/figure/224175402_fig1_Figure-3-All-experiments-in-the-same-plot-The-general-scaling-trend-follows-that-of-the" class=" fig-frame js-click-link "  rel="tooltip" data-tooltip="Figure 3. All experiments in the same plot. The general scaling trend..." data-key="224175402_fig1_Figure-3-All-experiments-in-the-same-plot-The-general-scaling-trend-follows-that-of-the"> <img class="fig" src="https://www.researchgate.net/profile/Andreas_Vlachos3/publication/224175402/figure/fig1/Figure-3-All-experiments-in-the-same-plot-The-general-scaling-trend-follows-that-of-the_small.png" alt="Figure 3. All experiments in the same plot. The general scaling trend..." title="Figure 3. All experiments in the same plot. The general scaling trend..."/> </a> </li>  </ul> </div> </div> <div class="action-container"> <div id="rgw18_56ab1a0b99463" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw32_56ab1a0b99463">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw42_56ab1a0b99463">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Andreas_Vlachos3/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop/links/00b4951b71b4e35e32000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Andreas_Vlachos3">Andreas Vlachos</a>   </span>  </div>  <div class="social-share-container"><div id="rgw44_56ab1a0b99463" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw45_56ab1a0b99463" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw46_56ab1a0b99463" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw47_56ab1a0b99463" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw48_56ab1a0b99463" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw49_56ab1a0b99463" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw43_56ab1a0b99463" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndreas_Vlachos3%2Fpublication%2F224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop%2Flinks%2F00b4951b71b4e35e32000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw31_56ab1a0b99463"  itemprop="articleBody">  <p>Page 1</p> <p>Scaling the iHMM: Parallelization versus Hadoop<br />S´ ebastien Brati` eres, Jurgen Van Gael<br />Department of Engineering<br />University of Cambridge<br />United Kingdom<br />Email: sb358, jv279 @cam.ac.uk<br />Andreas Vlachos<br />Computer Laboratory<br />University of Cambridge<br />United Kingdom<br />Email: av308@cl.cam.ac.uk<br />Zoubin Ghahramani<br />Department of Engineering<br />University of Cambridge<br />United Kingdom<br />Email: zoubin@eng.cam.ac.uk<br />Abstract—This paper compares parallel and distributed<br />implementations of an iterative, Gibbs sampling, machine<br />learning algorithm. Distributed implementations run under<br />Hadoop on facility computing clouds. The probabilistic model<br />under study is the infinite HMM [1], in which parameters are<br />learnt using an instance blocked Gibbs sampling, with a step<br />consisting of a dynamic program. We apply this model to learn<br />part-of-speech tags from newswire text in an unsupervised<br />fashion. However our focus here is on runtime performance,<br />as opposed to NLP-relevant scores, embodied by iteration<br />duration, ease of development, deployment and debugging.<br />I. INTRODUCTION<br />Probabilistic models are at the core of many modern<br />machine learning algorithms. Generally probabilistic models<br />are specified up to a finite number of parameters which are<br />subsequently learned by fitting the model to data. These<br />models are generally called parametric models. Nonpara-<br />metric models extend parametric models by allowing the<br />number of parameters to grow when the number of data-<br />points increases. As more parameters generally implies more<br />flexible models, nonparametric models can easily adapt to<br />the amount of data without overfitting.<br />As nonparametric models have the most potential in a<br />context where much data is available, it is crucial that we<br />can learn the parameters of these models efficiently. In<br />this paper we compare two ways of scaling the training<br />algorithm for a nonparametric model: using parallelization<br />on a single machine versus using distributed computing on<br />top of Hadoop. Similar effort has been undertaken by the<br />Mahout community in order to build distributed versions of<br />the parallelized machine learning algorithms described by<br />Bradski et al. [2]<br />The broad context in which we will explore the applica-<br />bility of the different scalability options is natural language<br />processing (NLP). The explosion of the WWW has made<br />available increasing amounts of text in digital format, thus<br />presenting a growing challenge to the traditional single-<br />processor or parallel approaches. Furthermore, it is a suitable<br />domain for Bayesian non-parametric methods to demonstrate<br />their potential, as it is hard to define in advance the right<br />level of detail needed for varying amounts of text of different<br />genres.<br />Concretely, the contribution of our paper is 4-fold:<br />1) We experiment how to apply the map-reduce paradigm<br />to an iterative sampling algorithm,<br />2) We explore which paradigm (parallel/distributed) is<br />best in which situation,<br />3) We report the lessons learned from our distributed<br />implementation,<br />4) We will shortly release the Hadoop source code to<br />encourage research in this area of machine learning.<br />II. THE INFINITE HIDDEN MARKOV MODEL<br />The hidden Markov model (or HMM) is a probabilistic<br />modelling tool that is in wide use throughout machine<br />learning, signal processing, bio-informatics and many other<br />fields. The HMM describes a probability distribution over<br />a sequence of observations w1,w2,···,wT of length T.<br />The HMM assumes there exists a Markov chain denoted<br />by s1,s2,···,sT where each st is in one of K possible<br />states. The distribution of the state at time t only depends<br />on the states before it through the state at time t − 1. This<br />dependency is governed by a K by K stochastic transition<br />matrix π where πij = p(st = j|st−1 = i). This is the<br />Markov property which gives the HMM its middle name.<br />Generally, we do not directly observe the Markov chain, but<br />rather an observation wt whose distribution only depends<br />on the state st, an observation model F (e.g. a Normal<br />distribution) and a set of parameters θ (e.g. the mean and<br />variances of the normal distribution).<br />For a fixed K many techniques (e.g. expectation max-<br />imization [3]) can be used to learn the parameters θ of<br />the HMM. In this paper we focus on the issue of learning<br />K using a nonparametric Bayesian approach. The infinite<br />hidden Markov model (or iHMM), introduced in [1] is a<br />probabilistic model which defines a distribution over all<br />hidden Markov models with arbitrary large state space<br />K. For a dataset of size N at most N states can be<br />used. Nonetheless, the prior distribution puts most of its<br />mass on state spaces much smaller than N (e.g. logN or<br />loglogN). Incidentally, after conditioning the model on N<br />observations, the posterior will have most of its mass on<br />hidden Markov models with state spaces much smaller than<br />N. The nonparametric behavior of the iHMM stems from</p>  <p>Page 2</p> <p>the fact that the more data is available, the more fine grained<br />a state space it can learn if the data calls for a more complex<br />model. As this paper focusses on the computational aspects<br />of the iHMM, we point the interested reader to [1] and [4]<br />for more theoretical background on the subject.<br />At present, there are no known deterministic algorithms<br />for learning the parameters of the iHMM. In this paper<br />we build on a Gibbs sampling method known as the beam<br />sampler [5]. We refer to [5] for technical details of the beam<br />sampler but line-out the algorithm here. Each iteration of the<br />beam sampler consists of the following steps:<br />1) Compute the sufficient statistics for the transition and<br />emission parameters,<br />2) Sample auxilary variables to dynamically truncate the<br />infinite transition matrix,<br />3) Sample a finite representation of the transition and<br />emission parameters,<br />4) Run forward-filtering backward-sampling to resample<br />the hidden state sequence,<br />5) Resample any hyperparameters in the model.<br />In the application we describe below the sufficient statis-<br />tics for the transition and emission parameters are the<br />numbers nijwhich denote the number of transitions from i<br />to j in our dataset and the numbers eiw which denote the<br />number of observations of type w emitted from state i. In<br />other words, these numbers can be computed in time linear<br />in the number of datapoints.<br />There is one auxilary variable for each adjacent pair of<br />states in the dataset. Since sampling each auxilary variable<br />is a constant time operation, we can compute these number<br />in time linear in the number of datapoints.<br />Sampling the transition and emission parameters is gen-<br />erally very cheap: if K states are used in an iteration and<br />the emission distribution is multinomial with E outcomes,<br />K2entries in the transition matrix and KE entries in the<br />emission distribution need to be computed.<br />The forwardfilteringbackward-sampling<br />requires computingthe<br />dynamicprogramming, we<br />thisrecursively usingthe<br />p(wt|st)?<br />entry takes K operations to compute. In other words, the<br />forward-filtering takes time T K2to compute. Once we have<br />computed p(sT|w1:T) we can sample sTand then backtrack<br />to sample each other element in the state sequence. The<br />complexity of this procedure is T K2.<br />In other words, one iteration for the Gibbs sampler of the<br />iHMM comprises of steps which are all linear in the length<br />of the sequence T. Nonetheless, the additional K2factor for<br />the dynamic programming step swamps the complexity for<br />the other steps in the computation.<br />procedure<br />Using<br />compute<br />table<br />can<br />equality<br />p(st|w1:t).<br />efficiently<br />p(st|w1:t)<br />This<br />∝<br />st−1p(st|st−1)p(st−1|w1:t−1).<br />has size KT for a sequence of length T, while each<br />table<br />III. UNSUPERVISED POS TAGGING WITH THE IHMM<br />Part-of-Speech (PoS) tagging is a standard component in<br />NLP pipelines. PoS tags characterize words according to<br />their syntactic (and sometimes semantic) behaviour, which<br />allows us to perform syntactic parsing as well as use them<br />in a variety of tasks, such as named entity recognition or<br />determining intonation for text-to-speech systems [6].<br />Most of the work in PoS tagging has focused on the<br />use of supervised machine learning methods, which require<br />large amounts of labelled data. Using such methods (the<br />supervised HMM being a very common choice), PoS tagging<br />performance on English newswire text has reached high<br />levels. However, when moving to new domains or languages<br />which do not have labelled data readily available, such<br />methods are unable to adapt. Therefore, recent work has<br />focused on unsupervised methods that use unlabelled data<br />and is available in large quantities.<br />In previous work on unsupervised PoS tagging using<br />HMMs, a main question was how to set the number of<br />hidden states appropriately. In particular, Johnson [7] reports<br />results for different numbers of hidden states but it is<br />unclear how to make this choice a priori, while Goldwater<br />&amp; Griffiths [8] leave this question as future work. It must be<br />pointed here that this is a non-issue when using supervised<br />machine learning methods, since there the model predicts<br />a PoS tag from a fixed set that was provided with the<br />training data. However in unsupervised PoS tagging the<br />states learned by the iHMM do not correspond to PoS tags<br />from a labelled corpus, therefore it is counter-intuitive to<br />fix their number in advance. The fact that different authors<br />use different versions with different number of PoS tags<br />of the same dataset (e.g. Goldwater &amp; Griffiths [8] versus<br />Johnson [7]) supports this claim. To address the issue of<br />selecting the number of states in unsupervised PoS tagging<br />Van Gael et al. [9] applied the iHMM to the task and<br />obtained competitive performance while allowing the model<br />to pick the number of hidden states.<br />Evaluating unsupervised PoS tagging is rather difficult<br />mainly due to the fact that the output of such systems<br />are not actual PoS tags but state identifiers. Therefore it is<br />impossible to evaluate performance against a manually la-<br />belled dataset using accuracy, as in supervised PoS tagging.<br />Nevertheless, the state identifiers provide a clustering of<br />the instances which can be used for evaluation purposes by<br />treating the instances with same state identifier as belonging<br />to the same cluster.<br />Clustering evaluation measures assess and sometimes<br />combine the two desirable properties that a clustering should<br />have with respect to a manually labelled dataset: homo-<br />geneity and completeness. Homogeneity is the degree to<br />which each cluster contains instances from a single class.<br />Completeness is the degree to which each class is contained<br />in a single cluster. While an ideal clustering should have both</p>  <p>Page 3</p> <p>properties, naively improving one of them can be harmful for<br />the other. For example, one can achieve better homogeneity<br />by simply increasing the number of clusters discovered but<br />this is likely to reduce completeness.<br />The most common approach followed in previous work is<br />to evaluate unsupervised PoS tagging as clustering against a<br />manually labelled dataset is the Variation of Information (VI)<br />[10] which assesses homogeneity and completeness using<br />the quantities H(C|K) (the conditional entropy of the class<br />distribution in the manually labelled dataset given the clus-<br />tering) and H(K|C) (the conditional entropy of clustering<br />given the class distribution in the manually labelled dataset).<br />The lower these quantities are, the better the clustering<br />is with respect to the manually labelled dataset. The final<br />score is obtained by summing them, which means that lower<br />values are better.<br />While inducing a mapping between states and PoS tags<br />and the use accuracy is an option, the quality of the mapping<br />would affect the evaluation, which is undesirable. This is<br />also the case with the commonly used F-measure clustering<br />evaluation measure [11]. Information-theoretic measures like<br />VI neither need nor attempt to infer such mappings, therefore<br />they are more suitable to our purposes.<br />IV. EXPERIMENTS<br />As a rough performance indicator, we measured the<br />duration of a Gibbs iteration in different setups, one par-<br />allel, and three distributed Hadoop setups. In addition, we<br />are contrasting the different settings in terms of ease of<br />development, deployment, and debugging.<br />A. Algorithm and data<br />The implemented algorithm was the same in all settings.<br />In particular, the initial value for K was fixed to 100, a value<br />to which cluster number converges as reported in [9]. To<br />measure iteration duration, we averaged across 10 iterations<br />of the algorithm.<br />In all our experiments, the datasets were derived from<br />the Wall Street Journal (WSJ) part of the Penn Treebank,<br />which is one of the standard corpora used in NLP research.<br />It consists of 1 million tokens of financial newswire text and<br />it has been labelled manually with PoS tags.<br />We extracted subsets of the WSJ dataset of sizes 1e3, 1e4<br />and 1e5 tokens. Together with the full dataset of 1e6 tokens<br />and a dataset with all datapoints duplicated 10 times (10e7<br />tokens) we are covering an interesting range. Note that the<br />dataset with 1e7 is not interesting from a modelling point<br />of view as we duplicated all data 10 times; nonetheless the<br />computational analysis remains valid.<br />As a sanity check, we evaluated the output of our<br />distributed implementation on the 1e5 subset of the WSJ<br />and the performance in terms of VI was 4.5 bits, roughly<br />equivalent to the ones achieved by the parallel one in [9]. It<br />must be noted that these scores are not strictly comparable<br />due to differences in the dataset size, and we leave it to<br />future work to present a full NLP-oriented evaluation of our<br />distributed implementation.<br />B. Configurations<br />Parallel is an implementation of the iHMM in .NET<br />which uses multithreading on a quad core 2.4 GHz machine<br />with 8GB of RAM.<br />hadoop-1 is an implementation of the iHMM on Hadoop,<br />where each step of the Gibbs iteration is implemented<br />as map-reduce. “Each step” means each operation which<br />scales with the number of data points, K or V (as defined<br />above). This gives a total of 9 map-reduce jobs for each<br />iteration in hadoop-1. hadoop-2 is entirely like hadoop-1,<br />except that only the most CPU-intensive step, namely the<br />dynamic program, was implemented as map-reduce. hadoop-<br />3 is exactly like hadoop-2 from the software point of view.<br />The Hadoop experiments were implemented in Java using<br />the Hadoop map-reduce library. They ran on Amazon’s Elas-<br />tic MapReduce computing cloud. For hadoop-1 experiments,<br />they ran on clusters of different sizes: each cluster had one<br />master node (the job tracker, in map-reduce terminology)<br />and one or several slave nodes: 1, 2, 3, 4, 8 or 16 depending<br />on the experiment. For hadoop-2 experiments, the cluster<br />size was kept constant: one master and one slave node. For<br />both hadoop-1 and hadoop-2, we used nodes of the Amazon<br />“small” type, i.e. 32-bit platforms with one CPU equivalent<br />to a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor, 1.7<br />GB of memory, with 160 GB storage. For hadoop-3, and in<br />order to beat the parallel setting, we resorted to Amazon<br />“extra large” nodes, 64-bit platforms with 8 virtual cores,<br />each equivalent to 2.5 times the reference 1.0-1.2 GHz 2007<br />Opteron or 2007 Xeon processor, 7 GB of memory, 1690 GB<br />of storage.<br />C. Results<br />Figures 1 to 3 represent the iteration duration (in seconds)<br />across different settings against a range of data set sizes.<br />Data set size is measured in number of data points, that is,<br />numbers of tokens (words). In the plots, each point marker<br />represents an experiment with a given software and hardware<br />setting, and a given data set size. Iteration duration was<br />averaged over the 10 first iterations of the iHMM learning<br />algorithm. Lines connect point markers to denote that they<br />belong to the same experimental setting. In the legends, the<br />letter H stands for hadoop.<br />log-log representation implies that a linear increase in<br />computing cost with the number of data points should be<br />reflected in a line. This is indeed the general trend of<br />all experiments. The initially lower slope of some curves<br />reflects the overhead of parts of the algorithm, or parts of<br />the distribution process, which does not scale linearly with<br />the data size.</p>  <p>Page 4</p> <p>? ?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />? ?<br />104<br />105<br />106<br />107<br />? data points<br />500<br />1000<br />2000<br />5000<br />iteration runtime ?sec?<br />?<br />H1?16<br />?<br />H1?8<br />?H1?4<br />?H1?3<br />?<br />H1?2<br />?<br />H1?1<br />Figure 1. All hadoop-1 experiments. The name of each result set is hadoop-<br />1-*, with * indicating the number of slave nodes in the cluster. Iteration<br />duration scales only slightly with cluster size.<br />????<br />??<br />??<br />??<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />? ?<br />?<br />?<br />?<br />? ?<br />?<br />?<br />Α Α<br />Β<br />Β<br />104<br />105<br />106<br />107<br />? data points<br />10<br />100<br />1000<br />iteration runtime ?sec?<br />Β H3?4<br />Α H3?1<br />? H2?f<br />? H2?e<br />? H2?d<br />? H2?c<br />? H2?b<br />? H2?a<br />?? parallel<br />Figure 2.<br />side. All hadoop-2-* experiments use the same setting, and are just different<br />runs of the same experiment. This demonstrates that there is little variability<br />between runs. The hadoop-3 experiments demonstrate that scaling, and<br />resorting to a more powerful machine type, finally beats the parallel setup.<br />All hadoop-2, hadoop-3, and the parallel experiment side-by-<br />?? ??<br />??<br />??<br />??<br />? ?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />?<br />? ?<br />Α Α<br />?<br />?<br />?<br />?<br />?<br />? ?<br />?<br />?<br />?<br />? ?<br />?<br />?<br />Β<br />Β<br />104<br />105<br />106<br />107<br />? data points<br />10<br />100<br />1000<br />iteration runtime ?sec?<br />Β<br />H3?4<br />Α<br />H3?1<br />?<br />H2?f<br />?<br />H2?e<br />?<br />H2?d<br />? H2?c<br />?<br />H2?b<br />?<br />H2?a<br />?H1?16<br />?<br />H1?8<br />?<br />H1?4<br />?<br />H1?3<br />?<br />H1?2<br />?<br />H1?1<br />??<br />parallel<br />Figure 3.<br />follows that of the parallel setup. Increasing data size even further than what<br />was tested here, we expect that hadoop-1 and -2 setups will become faster<br />than the parallel setup. However, the point where e.g. lines for experiments<br />hadoop-1-8 and parallel intersect seems several orders of magnitude above<br />present experiments.<br />All experiments in the same plot. The general scaling trend<br />Figure 4. Experimental data for Figures 1-3. Iteration duration in seconds.<br />None of the hadoop-1 and hadoop-2 implementations<br />were faster, in absolute terms, than parallel for the data sizes<br />demonstrated here. This can be attributed to the different<br />performance of the CPUs used in the parallel case, and on<br />the Amazon cluster used for the distributed experiments.<br />The parallel implementations scales perfectly well with<br />data size, but cannot accommodate data sizes beyond those<br />shown, i.e. from 10 million data points onwards, because it<br />is entirely memory-based (and therefore incurs no disk I/O<br />costs). It is therefore necessary, for large datasets, to resort<br />to the distributed implementations.<br />hadoop-1’s cost is roughly stable for a wide range of<br />settings, and does not scale well with the number of nodes.<br />This implementation, where each Gibbs iteration contains<br />9 map-reduce jobs, is apparently badly suited to a Hadoop<br />implementation because of the heavy overhead each map-<br />reduce job incurs: about 30 seconds notwithstanding the<br />number of nodes it runs on.<br />hadoop-2’s cost is well beyond that of hadoop-1 and does<br />not suffer from the large overhead effect. It therefore scales<br />well with the amount of data.<br />The isolated hadoop-3 experiments used hardware which<br />is comparable with the parallel experiment, and demonstrate<br />a definite speedup when running from a cluster.<br />D. Qualitative comparison<br />This section completes the computing cost comparison<br />with lessons from the software development exercise of all<br />implementations.<br />1) Development: Using the parallel extensions under .Net<br />proved relatively easy, and since no file-level or data-<br />splitting is expected from the developer, was a one-off<br />change from a reference, non-parallel implementation.<br />Turning to Hadoop implied learning the framework<br />through tutorials and books, a much longer process. The<br />developer writes his own reader/writer for the file format<br />in which he intends to store map input, intermediate data,<br />and reduce output. Portions of code corresponding to the<br />reference implementation can be reused inside the relevant,<br />corresponding map-reduce job. Shared parameters represent<br />a special challenge in this shared-nothing framework, and<br />here they were written to disk. Map-reduce jobs which<br />needed to update them had to funnel all data processing</p>  <p>Page 5</p> <p>through a single reducer, in order to obtain a single updated<br />value for the parameter.<br />Quite some effort had to be expended in tuning, consider-<br />ing the large overhead that a Hadoop job setup incurs, which<br />is constant on each slave node. In particular, in hadoop-<br />1, independent map-reduce jobs were parallelized using the<br />JobControl feature of Hadoop. In spite of these efforts, the<br />job setup overhead remained large, with respect to the fact<br />that our algorithm is data-light but CPU intensive in only<br />one of its phases.<br />2) Deployment: Deployment of the parallel .Net version<br />presented no particular difficulties. Deployment on the Ama-<br />zon Elastic MapReduce (AEMR) platform proved accept-<br />ably easy once all the necessary admninistrative configu-<br />ration had been performed, and the command-line tool for<br />cluster startup and termination, job startup and termination<br />had been learnt.<br />3) Debugging: Debugging the .Net implementation con-<br />sisted of fixing dependencies on shared parameters, which<br />can be spotted in the source code.<br />Running on AEMR presented a number of difficulties.<br />Several bugs and crashes, some unsolved to day, which<br />appeared exclusively on AEMR, not on our development<br />cluster, made deployment hard. Diagnostic tools sum up<br />to console and log consultation on running clusters. The<br />algorithm received a large amount of logging statements,<br />and logging configuration itself had to be brought in agree-<br />ment with Hadoop requirements, to allow some amount of<br />debugging.<br />V. CONCLUSION<br />We have implemented an iterative learning algorithm un-<br />der four different settings, one parallel and three distributed<br />ones with Hadoop. The runtime duration of an iteration is<br />crucial for such a Gibbs sampling algorithm, since conver-<br />gence typically takes several thousands of iterations.<br />We presented the algorithm (beam sampling on the infinite<br />HMM) and the application (part-of-speech tagging) we were<br />applying it on. Iteration durations were compared for the<br />different settings, and the effort involved in developing the<br />implementations was contrasted.<br />The parallel deployment had better performance than the<br />distributed ones running on “small” cores; the overall best<br />performance is obtained on a distributed setup running the<br />“extra-large”, more powerful, machines. The parallel setup<br />would not scale up to larger data sizes, neither would it<br />scale very much with number of cores used. Therefore a<br />distributed implementations is required when turning to web-<br />scale data.<br />Our first experiment with Hadoop, in which all loops were<br />turned into map-reduce jobs, incurred high map-reduce job<br />setup overhead, in spite of the tuning we applied. Had it<br />been know to us that Hadoop is not well suited to iterative<br />applications containing several map-reduce jobs per itera-<br />tion, we would not have tried this; we hope this finding is a<br />contribution of this paper. Our Hadoop implementation with<br />only one map-reduce job per iteration had acceptable scaling<br />behaviour with data size. Ongoing experiments investigate<br />its scaling across number of nodes, which seems roughly<br />linear, making it a good candidate for Gibbs sampling on a<br />very large scale.<br />ACKNOWLEDGMENT<br />The authors would like to thank Amazon Education Ser-<br />vices for a generous grant to use the Amazon Web Services.<br />Jurgen Van Gael is supported by a Microsoft Research<br />Scholarhsip.<br />REFERENCES<br />[1] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen, “The Infi-<br />nite Hidden Markov Model,” Advances in Neural Information<br />Processing Systems, vol. 14, pp. 577 – 584, 2002.<br />[2] G. Bradski, C.-T. Chu, A. Ng, K. Olukotun, S. K. Kim, Y.-<br />A. Lin, and Y. Yu, “Map-reduce for machine learning on<br />multicore,” in NIPS, 2006.<br />[3] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, “A max-<br />imization technique occurring in the statistical analysis of<br />probabilistic functions of markov chains,” The Annals of<br />Mathematical Statistics, vol. 41, no. 1, pp. 164–171, 1970.<br />[4] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei,<br />“Hierarchical Dirichlet processes,” Journal of the American<br />Statistical Association, vol. 101, no. 476, pp. 1566–1581,<br />2006.<br />[5] J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani,<br />“Beam Sampling for the Infinite Hidden Markov Model,” in<br />Proceedings of the 25th International Conference on Machine<br />learning, Helsinki, 2008.<br />[6] C. D. Manning and H. Schuetze, Foundations of Statistical<br />Natural Language Processing, 1st ed.<br />1999.<br />The MIT Press, Jun.<br />[7] M. Johnson, “Why Doesn’t EM Find Good HMM POS-<br />Taggers?” in Proceedings of the 2007 Joint Conference on<br />Empirical Methods in Natural Language Processing and<br />Computational Natural Language Learning, 2007, pp. 296–<br />305.<br />[8] S. Goldwater and T. Griffiths, “A fully Bayesian approach to<br />unsupervised part-of-speech tagging,” in Proceedings of the<br />45th Annual Meeting of the Association of Computational<br />Linguistics. Prague, Czech Republic: Association for Com-<br />putational Linguistics, June 2007, pp. 744–751.<br />[9] J. Van Gael, A. Vlachos, and Z. Ghahramani, “The infinite<br />HMM for unsupervised PoS tagging,” in Proceedings of<br />2009 Conference on Empirical Methods in Natural Language<br />Processing, Singapore, 2009, pp. 678–687.<br />[10] M. Meil˘ a, “Comparing clusterings—an information based<br />distance,” Journal of Multivariate Analysis, vol. 98, no. 5,<br />pp. 873–895, 2007.</p>  <p>Page 6</p> <p>[11] B. C. M. Fung, K. Wang, and M. Ester, “Hierarchical doc-<br />ument clustering using frequent itemsets,” in Proceedings of<br />SIAM International Conference on Data Mining, 2003, pp.<br />59–70.</p>  <a href="https://www.researchgate.net/profile/Andreas_Vlachos3/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop/links/00b4951b71b4e35e32000000.pdf">Download full-text</a> </div> <div id="rgw23_56ab1a0b99463" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab1a0b99463">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw25_56ab1a0b99463"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Andreas_Vlachos3/publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop/links/00b4951b71b4e35e32000000.pdf" class="publication-viewer" title="smla10-embedded_fonts-acrobat8.pdf">smla10-embedded_fonts-acrobat8.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Andreas_Vlachos3">Andreas Vlachos</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw26_56ab1a0b99463"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://eprints.pascal-network.org/archive/00006994/01/smla10-embedded_fonts-acrobat8.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Scaling the iHMM: Parallelization versus Hadoop">Scaling the iHMM: Parallelization versus Hadoop</a> </div>  <div class="details">   Available from <a href="http://eprints.pascal-network.org/archive/00006994/01/smla10-embedded_fonts-acrobat8.pdf" target="_blank" rel="nofollow">pascal-network.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw33_56ab1a0b99463" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (3) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw34_56ab1a0b99463" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab1a0b99463" >  <div class="indent-left">  <div id="rgw36_56ab1a0b99463" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fexport.arxiv.org%2Fpdf%2F1206.4679" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: export.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab1a0b99463">  <li class="citation-context-item"> "The state-of-the-art inference of iHMMs proposed by van Gael et al. (2008) uses a beam sampling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al., 2002). Although the beam sampling technique considerably reduces the computational cost of MCMC inference, it is still higher than that of HMMs using variational non-parametric Bayesian inference (VBHMMs), while acceleration of iHMMs has been discussed from the viewpoints of parallelization (Bratieres et al., 2010). In addition, iHMMs have a few hyper-parameters which mildly control the number of hidden states, and determination of them requires further computational costs. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models"> <span class="publication-title js-publication-title">Factorized Asymptotic Bayesian Hidden Markov Models</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70663005_Ryohei_Fujimaki" class="authors js-author-name ga-publications-authors">Ryohei Fujimaki</a> &middot;     <a href="researcher/46930114_Kohei_Hayashi" class="authors js-author-name ga-publications-authors">Kohei Hayashi</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> This paper addresses the issue of model selection for hidden Markov models
(HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has
been recently developed for model selection on independent hidden variables
(i.e., mixture models), for time-dependent hidden variables. As with FAB in
mixture models, FAB for HMMs is derived as an iterative lower bound
maximization algorithm of a factorized information criterion (FIC). It
inherits, from FAB for mixture models, several desirable properties for
learning HMMs, such as asymptotic consistency of FIC with marginal
log-likelihood, a shrinkage effect for hidden state selection, monotonic
increase of the lower FIC bound through the iterative optimization. Further, it
does not have a tunable hyper-parameter, and thus its model selection process
can be fully automated. Experimental results shows that FAB outperforms
states-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in
terms of model selection accuracy and computational efficiency. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Jun 2012  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   "  id="rgw38_56ab1a0b99463" >  <div class="indent-left">  <div id="rgw39_56ab1a0b99463" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fmlg.eng.cam.ac.uk%2Fpub%2Fpdf%2FGha12.pdf" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: mlg.eng.cam.ac.uk </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling"> <span class="publication-title js-publication-title">Bayesian non-parametrics and the probabilistic approach to modelling</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Modelling is fundamental to many fields of science and engineering. A model can be thought of as a representation of possible data one could predict from a system. The probabilistic approach to modelling uses probability theory to express all aspects of uncertainty in the model. The probabilistic approach is synonymous with Bayesian modelling, which simply uses the rules of probability theory in order to make predictions, compare alternative models, and learn model parameters and structure from data. This simple and elegant framework is most powerful when coupled with flexible probabilistic models. Flexibility is achieved through the use of Bayesian non-parametrics. This article provides an overview of probabilistic modelling and an accessible survey of some of the main tools in Bayesian non-parametrics. The survey covers the use of Bayesian non-parametrics for modelling unknown functions, density estimation, clustering, time-series modelling, and representing sparsity, hierarchies, and covariance structure. More specifically, it gives brief non-technical overviews of Gaussian processes, Dirichlet processes, infinite hidden Markov models, Indian buffet processes, Kingman&#39;s coalescent, Dirichlet diffusion trees and Wishart processes. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Feb 2013  &middot; Philosophical Transactions of The Royal Society A Mathematical Physical and Engineering Sciences  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  " href="publication/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   "  id="rgw40_56ab1a0b99463" >  <div class="indent-left">  <div id="rgw41_56ab1a0b99463" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Yutian_Chen3" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Yutian Chen </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions"> <span class="publication-title js-publication-title">Subsampling-Based Approximate Monte Carlo for Discrete Distributions</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/69632150_Yutian_Chen" class="authors js-author-name ga-publications-authors">Yutian Chen</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Drawing a sample from a discrete distribution is one of the building
components for Monte Carlo methods. Like other sampling algorithms, discrete
sampling also suffers from high computational burden in large-scale inference
problems. We study the problem of sampling a discrete random variable with a
high degree of dependency that is typical in large-scale Bayesian inference and
graphical models, and propose an efficient approximate solution with a
subsampling approach. We make a novel connection between the discrete sampling
and Multi-Armed Bandits problems with a finite reward population and provide
three algorithms with theoretical guarantees. Empirical evaluations show the
robustness and efficiency of the approximate algorithms in both synthetic and
real-world large-scale problems. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Yutian_Chen3/publication/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions/links/565d728808aeafc2aac7970f.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw28_56ab1a0b99463" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw29_56ab1a0b99463">  </ul> </div> </div>   <div id="rgw19_56ab1a0b99463" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw20_56ab1a0b99463"> <div> <h5> <a href="publication/292142783_Use_of_machine_learning_approaches_for_novel_drug_discovery" class="color-inherit ga-similar-publication-title"><span class="publication-title">Use of machine learning approaches for novel drug discovery</span></a>  </h5>  <div class="authors"> <a href="researcher/84464439_Angelica_Nakagawa_Lima" class="authors ga-similar-publication-author">Angélica Nakagawa Lima</a>, <a href="researcher/84459855_Eric_Allison_Philot" class="authors ga-similar-publication-author">Eric Allison Philot</a>, <a href="researcher/2096216000_Gustavo_Henrique_Goulart_Trossini" class="authors ga-similar-publication-author">Gustavo Henrique Goulart Trossini</a>, <a href="researcher/2096219901_Luis_Paulo_Barbour_Scott" class="authors ga-similar-publication-author">Luis Paulo Barbour Scott</a>, <a href="researcher/2081579571_Vinicius_Goncalves_Maltarollo" class="authors ga-similar-publication-author">Vinícius Gonçalves Maltarollo</a>, <a href="researcher/2096218810_Kathia_Maria_Honorio" class="authors ga-similar-publication-author">Kathia Maria Honorio</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw21_56ab1a0b99463"> <div> <h5> <a href="publication/289584538_Machine_learning_in_decisional_process" class="color-inherit ga-similar-publication-title"><span class="publication-title">Machine learning in decisional process</span></a>  </h5>  <div class="authors"> <a href="researcher/2082393623_Teresa_Scantamburlo" class="authors ga-similar-publication-author">Teresa Scantamburlo</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw22_56ab1a0b99463"> <div> <h5> <a href="publication/291949356_Machine_learning_techniques_for_robust_classification_of_partial_discharges_in_oil-paper_insulation_systems" class="color-inherit ga-similar-publication-title"><span class="publication-title">Machine learning techniques for robust classification of partial discharges in oil–paper insulation systems</span></a>  </h5>  <div class="authors"> <a href="researcher/8954495_Ayman_El-Hag" class="authors ga-similar-publication-author">Ayman El-Hag</a>, <a href="researcher/2034435798_Mustafa_Harbaji" class="authors ga-similar-publication-author">Mustafa Harbaji</a>, <a href="researcher/2095898958_Wei_Lee_Woon" class="authors ga-similar-publication-author">Wei Lee Woon</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw51_56ab1a0b99463" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw52_56ab1a0b99463">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw53_56ab1a0b99463" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=tuPICfurXZjCn8g5NUNBNtrie7fMTFAhSxKJQRixeG5Yx5sevoL8356M9URe7T6h" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="lww/ZacrEx8GvkLznrZtreccqmPovM6IaYX3/ao09jHpkJMFpCwfGQgGMfyiPk3OOIRic6F8l5kPBEWafOze6BvWcl5pvX3qI5cslGw+co3oWy4jCWOQtv2FysKvH/rfdSLbhJub3mgxkifaPyhJm9mnv248ZcpkchhiU+pbvdghLRGpHcrPo4oKv4lIi9+GKbqZcmDQFWrLqYi+t2ibcJ/gt95MvqErv2jq613frgfWA7vYW15lX+6O6MacFLwXL+4dw3Ey6RF3v2V1e3b6m7Q1Y0Kh+eDSY4dQB+M43bg="/> <input type="hidden" name="urlAfterLogin" value="publication/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MTc1NDAyX1NjYWxpbmdfdGhlX2lITU1fUGFyYWxsZWxpemF0aW9uX3ZlcnN1c19IYWRvb3A%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MTc1NDAyX1NjYWxpbmdfdGhlX2lITU1fUGFyYWxsZWxpemF0aW9uX3ZlcnN1c19IYWRvb3A%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MTc1NDAyX1NjYWxpbmdfdGhlX2lITU1fUGFyYWxsZWxpemF0aW9uX3ZlcnN1c19IYWRvb3A%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw54_56ab1a0b99463"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 544;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FigureList","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Andreas Vlachos","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272390873677839%401441954453533_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Andreas_Vlachos3","institution":"University of Cambridge","institutionUrl":false,"widgetId":"rgw4_56ab1a0b99463"},"id":"rgw4_56ab1a0b99463","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1841678","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1a0b99463"},"id":"rgw3_56ab1a0b99463","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=224175402","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":224175402,"title":"Scaling the iHMM: Parallelization versus Hadoop","journalTitle":false,"journalDetailsTooltip":false,"affiliation":"Dept. of Eng., Univ. of Cambridge, Cambridge, UK","type":"Conference Paper","details":{"doi":"10.1109\/CIT.2010.223","conferenceInfos":"Conference: Computer and Information Technology (CIT), 2010 IEEE 10th International Conference on"},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/conf\/IEEEcit\/IEEEcit2010.html#BratieresGVG10","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/CIT.2010.223"},{"key":"rft.atitle","value":"Scaling the iHMM: Parallelization versus Hadoop"},{"key":"rft.date","value":"2010"},{"key":"rft.pages","value":"1235 - 1240"},{"key":"rft.au","value":"S\u00e9bastien Brati\u00e8res,Jurgen van Gael,Andreas Vlachos,Zoubin Ghahramani"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw6_56ab1a0b99463"},"id":"rgw6_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=224175402","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":224175402,"peopleItems":[{"data":{"authorUrl":"researcher\/2021802159_Sebastien_Bratieres","authorNameOnPublication":"S\u00e9bastien Brati\u00e8res","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"S\u00e9bastien Brati\u00e8res","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2021802159_Sebastien_Bratieres","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56ab1a0b99463"},"id":"rgw9_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2021802159&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1a0b99463"},"id":"rgw8_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2021802159&authorNameOnPublication=S%C3%A9bastien%20Brati%C3%A8res","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69831654_Jurgen_van_Gael","authorNameOnPublication":"Jurgen van Gael","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Jurgen van Gael","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69831654_Jurgen_van_Gael","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab1a0b99463"},"id":"rgw11_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69831654&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1a0b99463"},"id":"rgw10_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69831654&authorNameOnPublication=Jurgen%20van%20Gael","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Andreas Vlachos","accountUrl":"profile\/Andreas_Vlachos3","accountKey":"Andreas_Vlachos3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272390873677839%401441954453533_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Andreas Vlachos","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge"}},"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge","url":"profile\/Andreas_Vlachos3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272390873677839%401441954453533_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Andreas_Vlachos3","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56ab1a0b99463"},"id":"rgw13_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1841678&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Cambridge","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":4,"accountCount":1,"publicationUid":224175402,"widgetId":"rgw12_56ab1a0b99463"},"id":"rgw12_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1841678&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=4&accountCount=1&publicationUid=224175402","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw15_56ab1a0b99463"},"id":"rgw15_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw14_56ab1a0b99463"},"id":"rgw14_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1a0b99463"},"id":"rgw7_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=224175402&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":224175402,"abstract":"<noscript><\/noscript><div>This paper compares parallel and distributed implementations of an iterative, Gibbs sampling, machine learning algorithm. Distributed implementations run under Hadoop on facility computing clouds. The probabilistic model under study is the infinite HMM, in which parameters are learnt using an instance blocked Gibbs sampling, with a step consisting of a dynamic program. We apply this model to learn part-of-speech tags from newswire text in an unsupervised fashion. However our focus here is on runtime performance, as opposed to NLP-relevant scores, embodied by iteration duration, ease of development, deployment and debugging.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw16_56ab1a0b99463"},"id":"rgw16_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=224175402","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":{"data":{"figures":[{"imageUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402\/figure\/fig1\/Figure-3-All-experiments-in-the-same-plot-The-general-scaling-trend-follows-that-of-the.png","previewImageUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402\/figure\/fig1\/Figure-3-All-experiments-in-the-same-plot-The-general-scaling-trend-follows-that-of-the_small.png","figureUrl":"\/figure\/224175402_fig1_Figure-3-All-experiments-in-the-same-plot-The-general-scaling-trend-follows-that-of-the","selected":false,"title":"Figure 3. All experiments in the same plot. The general scaling trend...","key":"224175402_fig1_Figure-3-All-experiments-in-the-same-plot-The-general-scaling-trend-follows-that-of-the"}],"readerDocId":"6707665","linkBehaviour":"dialog","isDialog":true,"headerText":"Figures in this publication","isNewPublicationDesign":false,"widgetId":"rgw17_56ab1a0b99463"},"id":"rgw17_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/FigureList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FigureList.html?readerDocId=6707665&isDialog=1&linkBehaviour=dialog","viewClass":"views.publicliterature.FigureListView","yuiModules":["rg.views.publicliterature.FigureListView","css-pow-publicliterature-FigureList"],"stylesheets":["pow\/publicliterature\/FigureList.css"],"_isYUI":true},"previewImage":"https:\/\/i1.rgstatic.net\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw18_56ab1a0b99463"},"id":"rgw18_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1a0b99463"},"id":"rgw5_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=224175402&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":84464439,"url":"researcher\/84464439_Angelica_Nakagawa_Lima","fullname":"Ang\u00e9lica Nakagawa Lima","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":84459855,"url":"researcher\/84459855_Eric_Allison_Philot","fullname":"Eric Allison Philot","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096216000,"url":"researcher\/2096216000_Gustavo_Henrique_Goulart_Trossini","fullname":"Gustavo Henrique Goulart Trossini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2096219901,"url":"researcher\/2096219901_Luis_Paulo_Barbour_Scott","fullname":"Luis Paulo Barbour Scott","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Expert Opinion on Drug Discovery","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/292142783_Use_of_machine_learning_approaches_for_novel_drug_discovery","usePlainButton":true,"publicationUid":292142783,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"3.54","url":"publication\/292142783_Use_of_machine_learning_approaches_for_novel_drug_discovery","title":"Use of machine learning approaches for novel drug discovery","displayTitleAsLink":true,"authors":[{"id":84464439,"url":"researcher\/84464439_Angelica_Nakagawa_Lima","fullname":"Ang\u00e9lica Nakagawa Lima","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":84459855,"url":"researcher\/84459855_Eric_Allison_Philot","fullname":"Eric Allison Philot","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096216000,"url":"researcher\/2096216000_Gustavo_Henrique_Goulart_Trossini","fullname":"Gustavo Henrique Goulart Trossini","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096219901,"url":"researcher\/2096219901_Luis_Paulo_Barbour_Scott","fullname":"Luis Paulo Barbour Scott","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2081579571,"url":"researcher\/2081579571_Vinicius_Goncalves_Maltarollo","fullname":"Vin\u00edcius Gon\u00e7alves Maltarollo","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2096218810,"url":"researcher\/2096218810_Kathia_Maria_Honorio","fullname":"Kathia Maria Honorio","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Expert Opinion on Drug Discovery 01\/2016;  DOI:10.1517\/17460441.2016.1146250"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/292142783_Use_of_machine_learning_approaches_for_novel_drug_discovery","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/292142783_Use_of_machine_learning_approaches_for_novel_drug_discovery\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56ab1a0b99463"},"id":"rgw20_56ab1a0b99463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=292142783","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2082393623,"url":"researcher\/2082393623_Teresa_Scantamburlo","fullname":"Teresa Scantamburlo","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"ACM SIGCAS Computers and Society","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289584538_Machine_learning_in_decisional_process","usePlainButton":true,"publicationUid":289584538,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/289584538_Machine_learning_in_decisional_process","title":"Machine learning in decisional process","displayTitleAsLink":true,"authors":[{"id":2082393623,"url":"researcher\/2082393623_Teresa_Scantamburlo","fullname":"Teresa Scantamburlo","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["ACM SIGCAS Computers and Society 01\/2016; 45(3):218-224. DOI:10.1145\/2874239.2874270"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289584538_Machine_learning_in_decisional_process","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289584538_Machine_learning_in_decisional_process\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw21_56ab1a0b99463"},"id":"rgw21_56ab1a0b99463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=289584538","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":8954495,"url":"researcher\/8954495_Ayman_El-Hag","fullname":"Ayman El-Hag","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2034435798,"url":"researcher\/2034435798_Mustafa_Harbaji","fullname":"Mustafa Harbaji","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095898958,"url":"researcher\/2095898958_Wei_Lee_Woon","fullname":"Wei Lee Woon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"IET Science Measurement ? Technology","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291949356_Machine_learning_techniques_for_robust_classification_of_partial_discharges_in_oil-paper_insulation_systems","usePlainButton":true,"publicationUid":291949356,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.95","url":"publication\/291949356_Machine_learning_techniques_for_robust_classification_of_partial_discharges_in_oil-paper_insulation_systems","title":"Machine learning techniques for robust classification of partial discharges in oil\u2013paper insulation systems","displayTitleAsLink":true,"authors":[{"id":8954495,"url":"researcher\/8954495_Ayman_El-Hag","fullname":"Ayman El-Hag","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2034435798,"url":"researcher\/2034435798_Mustafa_Harbaji","fullname":"Mustafa Harbaji","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2095898958,"url":"researcher\/2095898958_Wei_Lee_Woon","fullname":"Wei Lee Woon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IET Science Measurement ? Technology 01\/2016;  DOI:10.1049\/iet-smt.2015.0076"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291949356_Machine_learning_techniques_for_robust_classification_of_partial_discharges_in_oil-paper_insulation_systems","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291949356_Machine_learning_techniques_for_robust_classification_of_partial_discharges_in_oil-paper_insulation_systems\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw22_56ab1a0b99463"},"id":"rgw22_56ab1a0b99463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291949356","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw19_56ab1a0b99463"},"id":"rgw19_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=224175402&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":224175402,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":224175402,"publicationType":"inProceedings","linkId":"00b4951b71b4e35e32000000","fileName":"smla10-embedded_fonts-acrobat8.pdf","fileUrl":"profile\/Andreas_Vlachos3\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000.pdf","name":"Andreas Vlachos","nameUrl":"profile\/Andreas_Vlachos3","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"337 KB","widgetId":"rgw25_56ab1a0b99463"},"id":"rgw25_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=224175402&linkId=00b4951b71b4e35e32000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":224175402,"publicationType":"inProceedings","linkId":"0f9822f70cf290067124ec7a","fileName":"Scaling the iHMM: Parallelization versus Hadoop","fileUrl":"http:\/\/eprints.pascal-network.org\/archive\/00006994\/01\/smla10-embedded_fonts-acrobat8.pdf","name":"pascal-network.org","nameUrl":"http:\/\/eprints.pascal-network.org\/archive\/00006994\/01\/smla10-embedded_fonts-acrobat8.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw26_56ab1a0b99463"},"id":"rgw26_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=224175402&linkId=0f9822f70cf290067124ec7a&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw24_56ab1a0b99463"},"id":"rgw24_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=224175402&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":76,"valueFormatted":"76","widgetId":"rgw27_56ab1a0b99463"},"id":"rgw27_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=224175402","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab1a0b99463"},"id":"rgw23_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=224175402&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":224175402,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw29_56ab1a0b99463"},"id":"rgw29_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=224175402&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":76,"valueFormatted":"76","widgetId":"rgw30_56ab1a0b99463"},"id":"rgw30_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=224175402","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw28_56ab1a0b99463"},"id":"rgw28_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=224175402&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Scaling the iHMM: Parallelization versus Hadoop\nS\u00b4 ebastien Brati` eres, Jurgen Van Gael\nDepartment of Engineering\nUniversity of Cambridge\nUnited Kingdom\nEmail: sb358, jv279 @cam.ac.uk\nAndreas Vlachos\nComputer Laboratory\nUniversity of Cambridge\nUnited Kingdom\nEmail: av308@cl.cam.ac.uk\nZoubin Ghahramani\nDepartment of Engineering\nUniversity of Cambridge\nUnited Kingdom\nEmail: zoubin@eng.cam.ac.uk\nAbstract\u2014This paper compares parallel and distributed\nimplementations of an iterative, Gibbs sampling, machine\nlearning algorithm. Distributed implementations run under\nHadoop on facility computing clouds. The probabilistic model\nunder study is the infinite HMM [1], in which parameters are\nlearnt using an instance blocked Gibbs sampling, with a step\nconsisting of a dynamic program. We apply this model to learn\npart-of-speech tags from newswire text in an unsupervised\nfashion. However our focus here is on runtime performance,\nas opposed to NLP-relevant scores, embodied by iteration\nduration, ease of development, deployment and debugging.\nI. INTRODUCTION\nProbabilistic models are at the core of many modern\nmachine learning algorithms. Generally probabilistic models\nare specified up to a finite number of parameters which are\nsubsequently learned by fitting the model to data. These\nmodels are generally called parametric models. Nonpara-\nmetric models extend parametric models by allowing the\nnumber of parameters to grow when the number of data-\npoints increases. As more parameters generally implies more\nflexible models, nonparametric models can easily adapt to\nthe amount of data without overfitting.\nAs nonparametric models have the most potential in a\ncontext where much data is available, it is crucial that we\ncan learn the parameters of these models efficiently. In\nthis paper we compare two ways of scaling the training\nalgorithm for a nonparametric model: using parallelization\non a single machine versus using distributed computing on\ntop of Hadoop. Similar effort has been undertaken by the\nMahout community in order to build distributed versions of\nthe parallelized machine learning algorithms described by\nBradski et al. [2]\nThe broad context in which we will explore the applica-\nbility of the different scalability options is natural language\nprocessing (NLP). The explosion of the WWW has made\navailable increasing amounts of text in digital format, thus\npresenting a growing challenge to the traditional single-\nprocessor or parallel approaches. Furthermore, it is a suitable\ndomain for Bayesian non-parametric methods to demonstrate\ntheir potential, as it is hard to define in advance the right\nlevel of detail needed for varying amounts of text of different\ngenres.\nConcretely, the contribution of our paper is 4-fold:\n1) We experiment how to apply the map-reduce paradigm\nto an iterative sampling algorithm,\n2) We explore which paradigm (parallel\/distributed) is\nbest in which situation,\n3) We report the lessons learned from our distributed\nimplementation,\n4) We will shortly release the Hadoop source code to\nencourage research in this area of machine learning.\nII. THE INFINITE HIDDEN MARKOV MODEL\nThe hidden Markov model (or HMM) is a probabilistic\nmodelling tool that is in wide use throughout machine\nlearning, signal processing, bio-informatics and many other\nfields. The HMM describes a probability distribution over\na sequence of observations w1,w2,\u00b7\u00b7\u00b7,wT of length T.\nThe HMM assumes there exists a Markov chain denoted\nby s1,s2,\u00b7\u00b7\u00b7,sT where each st is in one of K possible\nstates. The distribution of the state at time t only depends\non the states before it through the state at time t \u2212 1. This\ndependency is governed by a K by K stochastic transition\nmatrix \u03c0 where \u03c0ij = p(st = j|st\u22121 = i). This is the\nMarkov property which gives the HMM its middle name.\nGenerally, we do not directly observe the Markov chain, but\nrather an observation wt whose distribution only depends\non the state st, an observation model F (e.g. a Normal\ndistribution) and a set of parameters \u03b8 (e.g. the mean and\nvariances of the normal distribution).\nFor a fixed K many techniques (e.g. expectation max-\nimization [3]) can be used to learn the parameters \u03b8 of\nthe HMM. In this paper we focus on the issue of learning\nK using a nonparametric Bayesian approach. The infinite\nhidden Markov model (or iHMM), introduced in [1] is a\nprobabilistic model which defines a distribution over all\nhidden Markov models with arbitrary large state space\nK. For a dataset of size N at most N states can be\nused. Nonetheless, the prior distribution puts most of its\nmass on state spaces much smaller than N (e.g. logN or\nloglogN). Incidentally, after conditioning the model on N\nobservations, the posterior will have most of its mass on\nhidden Markov models with state spaces much smaller than\nN. The nonparametric behavior of the iHMM stems from"},{"page":2,"text":"the fact that the more data is available, the more fine grained\na state space it can learn if the data calls for a more complex\nmodel. As this paper focusses on the computational aspects\nof the iHMM, we point the interested reader to [1] and [4]\nfor more theoretical background on the subject.\nAt present, there are no known deterministic algorithms\nfor learning the parameters of the iHMM. In this paper\nwe build on a Gibbs sampling method known as the beam\nsampler [5]. We refer to [5] for technical details of the beam\nsampler but line-out the algorithm here. Each iteration of the\nbeam sampler consists of the following steps:\n1) Compute the sufficient statistics for the transition and\nemission parameters,\n2) Sample auxilary variables to dynamically truncate the\ninfinite transition matrix,\n3) Sample a finite representation of the transition and\nemission parameters,\n4) Run forward-filtering backward-sampling to resample\nthe hidden state sequence,\n5) Resample any hyperparameters in the model.\nIn the application we describe below the sufficient statis-\ntics for the transition and emission parameters are the\nnumbers nijwhich denote the number of transitions from i\nto j in our dataset and the numbers eiw which denote the\nnumber of observations of type w emitted from state i. In\nother words, these numbers can be computed in time linear\nin the number of datapoints.\nThere is one auxilary variable for each adjacent pair of\nstates in the dataset. Since sampling each auxilary variable\nis a constant time operation, we can compute these number\nin time linear in the number of datapoints.\nSampling the transition and emission parameters is gen-\nerally very cheap: if K states are used in an iteration and\nthe emission distribution is multinomial with E outcomes,\nK2entries in the transition matrix and KE entries in the\nemission distribution need to be computed.\nThe forwardfilteringbackward-sampling\nrequires computingthe\ndynamicprogramming, we\nthisrecursively usingthe\np(wt|st)?\nentry takes K operations to compute. In other words, the\nforward-filtering takes time T K2to compute. Once we have\ncomputed p(sT|w1:T) we can sample sTand then backtrack\nto sample each other element in the state sequence. The\ncomplexity of this procedure is T K2.\nIn other words, one iteration for the Gibbs sampler of the\niHMM comprises of steps which are all linear in the length\nof the sequence T. Nonetheless, the additional K2factor for\nthe dynamic programming step swamps the complexity for\nthe other steps in the computation.\nprocedure\nUsing\ncompute\ntable\ncan\nequality\np(st|w1:t).\nefficiently\np(st|w1:t)\nThis\n\u221d\nst\u22121p(st|st\u22121)p(st\u22121|w1:t\u22121).\nhas size KT for a sequence of length T, while each\ntable\nIII. UNSUPERVISED POS TAGGING WITH THE IHMM\nPart-of-Speech (PoS) tagging is a standard component in\nNLP pipelines. PoS tags characterize words according to\ntheir syntactic (and sometimes semantic) behaviour, which\nallows us to perform syntactic parsing as well as use them\nin a variety of tasks, such as named entity recognition or\ndetermining intonation for text-to-speech systems [6].\nMost of the work in PoS tagging has focused on the\nuse of supervised machine learning methods, which require\nlarge amounts of labelled data. Using such methods (the\nsupervised HMM being a very common choice), PoS tagging\nperformance on English newswire text has reached high\nlevels. However, when moving to new domains or languages\nwhich do not have labelled data readily available, such\nmethods are unable to adapt. Therefore, recent work has\nfocused on unsupervised methods that use unlabelled data\nand is available in large quantities.\nIn previous work on unsupervised PoS tagging using\nHMMs, a main question was how to set the number of\nhidden states appropriately. In particular, Johnson [7] reports\nresults for different numbers of hidden states but it is\nunclear how to make this choice a priori, while Goldwater\n& Griffiths [8] leave this question as future work. It must be\npointed here that this is a non-issue when using supervised\nmachine learning methods, since there the model predicts\na PoS tag from a fixed set that was provided with the\ntraining data. However in unsupervised PoS tagging the\nstates learned by the iHMM do not correspond to PoS tags\nfrom a labelled corpus, therefore it is counter-intuitive to\nfix their number in advance. The fact that different authors\nuse different versions with different number of PoS tags\nof the same dataset (e.g. Goldwater & Griffiths [8] versus\nJohnson [7]) supports this claim. To address the issue of\nselecting the number of states in unsupervised PoS tagging\nVan Gael et al. [9] applied the iHMM to the task and\nobtained competitive performance while allowing the model\nto pick the number of hidden states.\nEvaluating unsupervised PoS tagging is rather difficult\nmainly due to the fact that the output of such systems\nare not actual PoS tags but state identifiers. Therefore it is\nimpossible to evaluate performance against a manually la-\nbelled dataset using accuracy, as in supervised PoS tagging.\nNevertheless, the state identifiers provide a clustering of\nthe instances which can be used for evaluation purposes by\ntreating the instances with same state identifier as belonging\nto the same cluster.\nClustering evaluation measures assess and sometimes\ncombine the two desirable properties that a clustering should\nhave with respect to a manually labelled dataset: homo-\ngeneity and completeness. Homogeneity is the degree to\nwhich each cluster contains instances from a single class.\nCompleteness is the degree to which each class is contained\nin a single cluster. While an ideal clustering should have both"},{"page":3,"text":"properties, naively improving one of them can be harmful for\nthe other. For example, one can achieve better homogeneity\nby simply increasing the number of clusters discovered but\nthis is likely to reduce completeness.\nThe most common approach followed in previous work is\nto evaluate unsupervised PoS tagging as clustering against a\nmanually labelled dataset is the Variation of Information (VI)\n[10] which assesses homogeneity and completeness using\nthe quantities H(C|K) (the conditional entropy of the class\ndistribution in the manually labelled dataset given the clus-\ntering) and H(K|C) (the conditional entropy of clustering\ngiven the class distribution in the manually labelled dataset).\nThe lower these quantities are, the better the clustering\nis with respect to the manually labelled dataset. The final\nscore is obtained by summing them, which means that lower\nvalues are better.\nWhile inducing a mapping between states and PoS tags\nand the use accuracy is an option, the quality of the mapping\nwould affect the evaluation, which is undesirable. This is\nalso the case with the commonly used F-measure clustering\nevaluation measure [11]. Information-theoretic measures like\nVI neither need nor attempt to infer such mappings, therefore\nthey are more suitable to our purposes.\nIV. EXPERIMENTS\nAs a rough performance indicator, we measured the\nduration of a Gibbs iteration in different setups, one par-\nallel, and three distributed Hadoop setups. In addition, we\nare contrasting the different settings in terms of ease of\ndevelopment, deployment, and debugging.\nA. Algorithm and data\nThe implemented algorithm was the same in all settings.\nIn particular, the initial value for K was fixed to 100, a value\nto which cluster number converges as reported in [9]. To\nmeasure iteration duration, we averaged across 10 iterations\nof the algorithm.\nIn all our experiments, the datasets were derived from\nthe Wall Street Journal (WSJ) part of the Penn Treebank,\nwhich is one of the standard corpora used in NLP research.\nIt consists of 1 million tokens of financial newswire text and\nit has been labelled manually with PoS tags.\nWe extracted subsets of the WSJ dataset of sizes 1e3, 1e4\nand 1e5 tokens. Together with the full dataset of 1e6 tokens\nand a dataset with all datapoints duplicated 10 times (10e7\ntokens) we are covering an interesting range. Note that the\ndataset with 1e7 is not interesting from a modelling point\nof view as we duplicated all data 10 times; nonetheless the\ncomputational analysis remains valid.\nAs a sanity check, we evaluated the output of our\ndistributed implementation on the 1e5 subset of the WSJ\nand the performance in terms of VI was 4.5 bits, roughly\nequivalent to the ones achieved by the parallel one in [9]. It\nmust be noted that these scores are not strictly comparable\ndue to differences in the dataset size, and we leave it to\nfuture work to present a full NLP-oriented evaluation of our\ndistributed implementation.\nB. Configurations\nParallel is an implementation of the iHMM in .NET\nwhich uses multithreading on a quad core 2.4 GHz machine\nwith 8GB of RAM.\nhadoop-1 is an implementation of the iHMM on Hadoop,\nwhere each step of the Gibbs iteration is implemented\nas map-reduce. \u201cEach step\u201d means each operation which\nscales with the number of data points, K or V (as defined\nabove). This gives a total of 9 map-reduce jobs for each\niteration in hadoop-1. hadoop-2 is entirely like hadoop-1,\nexcept that only the most CPU-intensive step, namely the\ndynamic program, was implemented as map-reduce. hadoop-\n3 is exactly like hadoop-2 from the software point of view.\nThe Hadoop experiments were implemented in Java using\nthe Hadoop map-reduce library. They ran on Amazon\u2019s Elas-\ntic MapReduce computing cloud. For hadoop-1 experiments,\nthey ran on clusters of different sizes: each cluster had one\nmaster node (the job tracker, in map-reduce terminology)\nand one or several slave nodes: 1, 2, 3, 4, 8 or 16 depending\non the experiment. For hadoop-2 experiments, the cluster\nsize was kept constant: one master and one slave node. For\nboth hadoop-1 and hadoop-2, we used nodes of the Amazon\n\u201csmall\u201d type, i.e. 32-bit platforms with one CPU equivalent\nto a 1.0-1.2 GHz 2007 Opteron or 2007 Xeon processor, 1.7\nGB of memory, with 160 GB storage. For hadoop-3, and in\norder to beat the parallel setting, we resorted to Amazon\n\u201cextra large\u201d nodes, 64-bit platforms with 8 virtual cores,\neach equivalent to 2.5 times the reference 1.0-1.2 GHz 2007\nOpteron or 2007 Xeon processor, 7 GB of memory, 1690 GB\nof storage.\nC. Results\nFigures 1 to 3 represent the iteration duration (in seconds)\nacross different settings against a range of data set sizes.\nData set size is measured in number of data points, that is,\nnumbers of tokens (words). In the plots, each point marker\nrepresents an experiment with a given software and hardware\nsetting, and a given data set size. Iteration duration was\naveraged over the 10 first iterations of the iHMM learning\nalgorithm. Lines connect point markers to denote that they\nbelong to the same experimental setting. In the legends, the\nletter H stands for hadoop.\nlog-log representation implies that a linear increase in\ncomputing cost with the number of data points should be\nreflected in a line. This is indeed the general trend of\nall experiments. The initially lower slope of some curves\nreflects the overhead of parts of the algorithm, or parts of\nthe distribution process, which does not scale linearly with\nthe data size."},{"page":4,"text":"? ?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? ?\n104\n105\n106\n107\n? data points\n500\n1000\n2000\n5000\niteration runtime ?sec?\n?\nH1?16\n?\nH1?8\n?H1?4\n?H1?3\n?\nH1?2\n?\nH1?1\nFigure 1. All hadoop-1 experiments. The name of each result set is hadoop-\n1-*, with * indicating the number of slave nodes in the cluster. Iteration\nduration scales only slightly with cluster size.\n????\n??\n??\n??\n?\n?\n?\n?\n?\n?\n?\n? ?\n?\n?\n?\n? ?\n?\n?\n\u0391 \u0391\n\u0392\n\u0392\n104\n105\n106\n107\n? data points\n10\n100\n1000\niteration runtime ?sec?\n\u0392 H3?4\n\u0391 H3?1\n? H2?f\n? H2?e\n? H2?d\n? H2?c\n? H2?b\n? H2?a\n?? parallel\nFigure 2.\nside. All hadoop-2-* experiments use the same setting, and are just different\nruns of the same experiment. This demonstrates that there is little variability\nbetween runs. The hadoop-3 experiments demonstrate that scaling, and\nresorting to a more powerful machine type, finally beats the parallel setup.\nAll hadoop-2, hadoop-3, and the parallel experiment side-by-\n?? ??\n??\n??\n??\n? ?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n?\n? ?\n\u0391 \u0391\n?\n?\n?\n?\n?\n? ?\n?\n?\n?\n? ?\n?\n?\n\u0392\n\u0392\n104\n105\n106\n107\n? data points\n10\n100\n1000\niteration runtime ?sec?\n\u0392\nH3?4\n\u0391\nH3?1\n?\nH2?f\n?\nH2?e\n?\nH2?d\n? H2?c\n?\nH2?b\n?\nH2?a\n?H1?16\n?\nH1?8\n?\nH1?4\n?\nH1?3\n?\nH1?2\n?\nH1?1\n??\nparallel\nFigure 3.\nfollows that of the parallel setup. Increasing data size even further than what\nwas tested here, we expect that hadoop-1 and -2 setups will become faster\nthan the parallel setup. However, the point where e.g. lines for experiments\nhadoop-1-8 and parallel intersect seems several orders of magnitude above\npresent experiments.\nAll experiments in the same plot. The general scaling trend\nFigure 4. Experimental data for Figures 1-3. Iteration duration in seconds.\nNone of the hadoop-1 and hadoop-2 implementations\nwere faster, in absolute terms, than parallel for the data sizes\ndemonstrated here. This can be attributed to the different\nperformance of the CPUs used in the parallel case, and on\nthe Amazon cluster used for the distributed experiments.\nThe parallel implementations scales perfectly well with\ndata size, but cannot accommodate data sizes beyond those\nshown, i.e. from 10 million data points onwards, because it\nis entirely memory-based (and therefore incurs no disk I\/O\ncosts). It is therefore necessary, for large datasets, to resort\nto the distributed implementations.\nhadoop-1\u2019s cost is roughly stable for a wide range of\nsettings, and does not scale well with the number of nodes.\nThis implementation, where each Gibbs iteration contains\n9 map-reduce jobs, is apparently badly suited to a Hadoop\nimplementation because of the heavy overhead each map-\nreduce job incurs: about 30 seconds notwithstanding the\nnumber of nodes it runs on.\nhadoop-2\u2019s cost is well beyond that of hadoop-1 and does\nnot suffer from the large overhead effect. It therefore scales\nwell with the amount of data.\nThe isolated hadoop-3 experiments used hardware which\nis comparable with the parallel experiment, and demonstrate\na definite speedup when running from a cluster.\nD. Qualitative comparison\nThis section completes the computing cost comparison\nwith lessons from the software development exercise of all\nimplementations.\n1) Development: Using the parallel extensions under .Net\nproved relatively easy, and since no file-level or data-\nsplitting is expected from the developer, was a one-off\nchange from a reference, non-parallel implementation.\nTurning to Hadoop implied learning the framework\nthrough tutorials and books, a much longer process. The\ndeveloper writes his own reader\/writer for the file format\nin which he intends to store map input, intermediate data,\nand reduce output. Portions of code corresponding to the\nreference implementation can be reused inside the relevant,\ncorresponding map-reduce job. Shared parameters represent\na special challenge in this shared-nothing framework, and\nhere they were written to disk. Map-reduce jobs which\nneeded to update them had to funnel all data processing"},{"page":5,"text":"through a single reducer, in order to obtain a single updated\nvalue for the parameter.\nQuite some effort had to be expended in tuning, consider-\ning the large overhead that a Hadoop job setup incurs, which\nis constant on each slave node. In particular, in hadoop-\n1, independent map-reduce jobs were parallelized using the\nJobControl feature of Hadoop. In spite of these efforts, the\njob setup overhead remained large, with respect to the fact\nthat our algorithm is data-light but CPU intensive in only\none of its phases.\n2) Deployment: Deployment of the parallel .Net version\npresented no particular difficulties. Deployment on the Ama-\nzon Elastic MapReduce (AEMR) platform proved accept-\nably easy once all the necessary admninistrative configu-\nration had been performed, and the command-line tool for\ncluster startup and termination, job startup and termination\nhad been learnt.\n3) Debugging: Debugging the .Net implementation con-\nsisted of fixing dependencies on shared parameters, which\ncan be spotted in the source code.\nRunning on AEMR presented a number of difficulties.\nSeveral bugs and crashes, some unsolved to day, which\nappeared exclusively on AEMR, not on our development\ncluster, made deployment hard. Diagnostic tools sum up\nto console and log consultation on running clusters. The\nalgorithm received a large amount of logging statements,\nand logging configuration itself had to be brought in agree-\nment with Hadoop requirements, to allow some amount of\ndebugging.\nV. CONCLUSION\nWe have implemented an iterative learning algorithm un-\nder four different settings, one parallel and three distributed\nones with Hadoop. The runtime duration of an iteration is\ncrucial for such a Gibbs sampling algorithm, since conver-\ngence typically takes several thousands of iterations.\nWe presented the algorithm (beam sampling on the infinite\nHMM) and the application (part-of-speech tagging) we were\napplying it on. Iteration durations were compared for the\ndifferent settings, and the effort involved in developing the\nimplementations was contrasted.\nThe parallel deployment had better performance than the\ndistributed ones running on \u201csmall\u201d cores; the overall best\nperformance is obtained on a distributed setup running the\n\u201cextra-large\u201d, more powerful, machines. The parallel setup\nwould not scale up to larger data sizes, neither would it\nscale very much with number of cores used. Therefore a\ndistributed implementations is required when turning to web-\nscale data.\nOur first experiment with Hadoop, in which all loops were\nturned into map-reduce jobs, incurred high map-reduce job\nsetup overhead, in spite of the tuning we applied. Had it\nbeen know to us that Hadoop is not well suited to iterative\napplications containing several map-reduce jobs per itera-\ntion, we would not have tried this; we hope this finding is a\ncontribution of this paper. Our Hadoop implementation with\nonly one map-reduce job per iteration had acceptable scaling\nbehaviour with data size. Ongoing experiments investigate\nits scaling across number of nodes, which seems roughly\nlinear, making it a good candidate for Gibbs sampling on a\nvery large scale.\nACKNOWLEDGMENT\nThe authors would like to thank Amazon Education Ser-\nvices for a generous grant to use the Amazon Web Services.\nJurgen Van Gael is supported by a Microsoft Research\nScholarhsip.\nREFERENCES\n[1] M. J. Beal, Z. Ghahramani, and C. E. Rasmussen, \u201cThe Infi-\nnite Hidden Markov Model,\u201d Advances in Neural Information\nProcessing Systems, vol. 14, pp. 577 \u2013 584, 2002.\n[2] G. Bradski, C.-T. Chu, A. Ng, K. Olukotun, S. K. Kim, Y.-\nA. Lin, and Y. Yu, \u201cMap-reduce for machine learning on\nmulticore,\u201d in NIPS, 2006.\n[3] L. E. Baum, T. Petrie, G. Soules, and N. Weiss, \u201cA max-\nimization technique occurring in the statistical analysis of\nprobabilistic functions of markov chains,\u201d The Annals of\nMathematical Statistics, vol. 41, no. 1, pp. 164\u2013171, 1970.\n[4] Y. W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei,\n\u201cHierarchical Dirichlet processes,\u201d Journal of the American\nStatistical Association, vol. 101, no. 476, pp. 1566\u20131581,\n2006.\n[5] J. Van Gael, Y. Saatci, Y. W. Teh, and Z. Ghahramani,\n\u201cBeam Sampling for the Infinite Hidden Markov Model,\u201d in\nProceedings of the 25th International Conference on Machine\nlearning, Helsinki, 2008.\n[6] C. D. Manning and H. Schuetze, Foundations of Statistical\nNatural Language Processing, 1st ed.\n1999.\nThe MIT Press, Jun.\n[7] M. Johnson, \u201cWhy Doesn\u2019t EM Find Good HMM POS-\nTaggers?\u201d in Proceedings of the 2007 Joint Conference on\nEmpirical Methods in Natural Language Processing and\nComputational Natural Language Learning, 2007, pp. 296\u2013\n305.\n[8] S. Goldwater and T. Griffiths, \u201cA fully Bayesian approach to\nunsupervised part-of-speech tagging,\u201d in Proceedings of the\n45th Annual Meeting of the Association of Computational\nLinguistics. Prague, Czech Republic: Association for Com-\nputational Linguistics, June 2007, pp. 744\u2013751.\n[9] J. Van Gael, A. Vlachos, and Z. Ghahramani, \u201cThe infinite\nHMM for unsupervised PoS tagging,\u201d in Proceedings of\n2009 Conference on Empirical Methods in Natural Language\nProcessing, Singapore, 2009, pp. 678\u2013687.\n[10] M. Meil\u02d8 a, \u201cComparing clusterings\u2014an information based\ndistance,\u201d Journal of Multivariate Analysis, vol. 98, no. 5,\npp. 873\u2013895, 2007."},{"page":6,"text":"[11] B. C. M. Fung, K. Wang, and M. Ester, \u201cHierarchical doc-\nument clustering using frequent itemsets,\u201d in Proceedings of\nSIAM International Conference on Data Mining, 2003, pp.\n59\u201370."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000.pdf","widgetId":"rgw31_56ab1a0b99463"},"id":"rgw31_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=224175402&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw32_56ab1a0b99463"},"id":"rgw32_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=224175402&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":224175402,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":224175402,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":70663005,"url":"researcher\/70663005_Ryohei_Fujimaki","fullname":"Ryohei Fujimaki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":46930114,"url":"researcher\/46930114_Kohei_Hayashi","fullname":"Kohei Hayashi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2012","journal":null,"showEnrichedPublicationItem":false,"citationCount":4,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models","usePlainButton":true,"publicationUid":227716500,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models","title":"Factorized Asymptotic Bayesian Hidden Markov Models","displayTitleAsLink":true,"authors":[{"id":70663005,"url":"researcher\/70663005_Ryohei_Fujimaki","fullname":"Ryohei Fujimaki","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":46930114,"url":"researcher\/46930114_Kohei_Hayashi","fullname":"Kohei Hayashi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"This paper addresses the issue of model selection for hidden Markov models\n(HMMs). We generalize factorized asymptotic Bayesian inference (FAB), which has\nbeen recently developed for model selection on independent hidden variables\n(i.e., mixture models), for time-dependent hidden variables. As with FAB in\nmixture models, FAB for HMMs is derived as an iterative lower bound\nmaximization algorithm of a factorized information criterion (FIC). It\ninherits, from FAB for mixture models, several desirable properties for\nlearning HMMs, such as asymptotic consistency of FIC with marginal\nlog-likelihood, a shrinkage effect for hidden state selection, monotonic\nincrease of the lower FIC bound through the iterative optimization. Further, it\ndoes not have a tunable hyper-parameter, and thus its model selection process\ncan be fully automated. Experimental results shows that FAB outperforms\nstates-of-the-art variational Bayesian HMM and non-parametric Bayesian HMM in\nterms of model selection accuracy and computational efficiency.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fexport.arxiv.org%2Fpdf%2F1206.4679","sourceName":"export.arxiv.org","hasSourceUrl":true},"publicationUid":227716500,"publicationUrl":"publication\/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models\/links\/03071f0d0cf27751242d0a96\/smallpreview.png","linkId":"03071f0d0cf27751242d0a96","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=227716500&reference=03071f0d0cf27751242d0a96&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1a0b99463"},"id":"rgw36_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=227716500&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224175402,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/227716500_Factorized_Asymptotic_Bayesian_Hidden_Markov_Models\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The state-of-the-art inference of iHMMs proposed by van Gael et al. (2008) uses a beam sampling technique which is more efficient than well-known Gibbs sampling techniques (Beal et al., 2002). Although the beam sampling technique considerably reduces the computational cost of MCMC inference, it is still higher than that of HMMs using variational non-parametric Bayesian inference (VBHMMs), while acceleration of iHMMs has been discussed from the viewpoints of parallelization (Bratieres et al., 2010). In addition, iHMMs have a few hyper-parameters which mildly control the number of hidden states, and determination of them requires further computational costs. "],"widgetId":"rgw37_56ab1a0b99463"},"id":"rgw37_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw35_56ab1a0b99463"},"id":"rgw35_56ab1a0b99463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=227716500&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[[]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Feb 2013","journal":"Philosophical Transactions of The Royal Society A Mathematical Physical and Engineering Sciences","showEnrichedPublicationItem":false,"citationCount":10,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling","usePlainButton":true,"publicationUid":234019992,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.15","url":"publication\/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling","title":"Bayesian non-parametrics and the probabilistic approach to modelling","displayTitleAsLink":true,"authors":[{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Philosophical Transactions of The Royal Society A Mathematical Physical and Engineering Sciences 02\/2013; 371(1984):20110553. DOI:10.1098\/rsta.2011.0553"],"abstract":"Modelling is fundamental to many fields of science and engineering. A model can be thought of as a representation of possible data one could predict from a system. The probabilistic approach to modelling uses probability theory to express all aspects of uncertainty in the model. The probabilistic approach is synonymous with Bayesian modelling, which simply uses the rules of probability theory in order to make predictions, compare alternative models, and learn model parameters and structure from data. This simple and elegant framework is most powerful when coupled with flexible probabilistic models. Flexibility is achieved through the use of Bayesian non-parametrics. This article provides an overview of probabilistic modelling and an accessible survey of some of the main tools in Bayesian non-parametrics. The survey covers the use of Bayesian non-parametrics for modelling unknown functions, density estimation, clustering, time-series modelling, and representing sparsity, hierarchies, and covariance structure. More specifically, it gives brief non-technical overviews of Gaussian processes, Dirichlet processes, infinite hidden Markov models, Indian buffet processes, Kingman's coalescent, Dirichlet diffusion trees and Wishart processes.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"publication\/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling?origin=publication_list","active":false,"primary":true,"extraClass":null,"icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fmlg.eng.cam.ac.uk%2Fpub%2Fpdf%2FGha12.pdf","sourceName":"mlg.eng.cam.ac.uk","hasSourceUrl":true},"publicationUid":234019992,"publicationUrl":"publication\/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling\/links\/02b40ae60cf2b4facd54f410\/smallpreview.png","linkId":"02b40ae60cf2b4facd54f410","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=234019992&reference=02b40ae60cf2b4facd54f410&eventCode=&origin=publication_list","widgetId":"rgw39_56ab1a0b99463"},"id":"rgw39_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=234019992&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224175402,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/234019992_Bayesian_non-parametrics_and_the_probabilistic_approach_to_modelling\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw38_56ab1a0b99463"},"id":"rgw38_56ab1a0b99463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=234019992&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":69632150,"url":"researcher\/69632150_Yutian_Chen","fullname":"Yutian Chen","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[[]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions","usePlainButton":true,"publicationUid":279633530,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions","title":"Subsampling-Based Approximate Monte Carlo for Discrete Distributions","displayTitleAsLink":true,"authors":[{"id":69632150,"url":"researcher\/69632150_Yutian_Chen","fullname":"Yutian Chen","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Drawing a sample from a discrete distribution is one of the building\ncomponents for Monte Carlo methods. Like other sampling algorithms, discrete\nsampling also suffers from high computational burden in large-scale inference\nproblems. We study the problem of sampling a discrete random variable with a\nhigh degree of dependency that is typical in large-scale Bayesian inference and\ngraphical models, and propose an efficient approximate solution with a\nsubsampling approach. We make a novel connection between the discrete sampling\nand Multi-Armed Bandits problems with a finite reward population and provide\nthree algorithms with theoretical guarantees. Empirical evaluations show the\nrobustness and efficiency of the approximate algorithms in both synthetic and\nreal-world large-scale problems.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Yutian_Chen3\/publication\/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions\/links\/565d728808aeafc2aac7970f.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Yutian_Chen3","sourceName":"Yutian Chen","hasSourceUrl":true},"publicationUid":279633530,"publicationUrl":"publication\/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions\/links\/565d728808aeafc2aac7970f\/smallpreview.png","linkId":"565d728808aeafc2aac7970f","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=279633530&reference=565d728808aeafc2aac7970f&eventCode=&origin=publication_list","widgetId":"rgw41_56ab1a0b99463"},"id":"rgw41_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=279633530&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"565d728808aeafc2aac7970f","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224175402,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/279633530_Subsampling-Based_Approximate_Monte_Carlo_for_Discrete_Distributions\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw40_56ab1a0b99463"},"id":"rgw40_56ab1a0b99463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=279633530&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":224175402,"publicationLink":"publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop","hasShowMore":false,"newOffset":3,"pageSize":10,"widgetId":"rgw34_56ab1a0b99463"},"id":"rgw34_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=224175402&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=3","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":3,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw33_56ab1a0b99463"},"id":"rgw33_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=224175402&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"00b4951b71b4e35e32000000","name":"Andreas Vlachos","date":null,"nameLink":"profile\/Andreas_Vlachos3","filename":"smla10-embedded_fonts-acrobat8.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2257a4b9120c61f2a8f74056af8a6406","showFileSizeNote":false,"fileSize":"337 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"00b4951b71b4e35e32000000","name":"Andreas Vlachos","date":null,"nameLink":"profile\/Andreas_Vlachos3","filename":"smla10-embedded_fonts-acrobat8.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"2257a4b9120c61f2a8f74056af8a6406","showFileSizeNote":false,"fileSize":"337 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Conference Paper","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[{"props":{"position":"float","orientation":"portrait","coords":"pag:4:rect:54.00,660.47,243.00,54.44","ordinal":"3"},"assetId":"AS:302777566810116@1449199205164"}],"figureAssetIds":["AS:302777566810116@1449199205164"],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=QkPYEA45FaPwxU4PIAWf8pee205mWJV-nUqB2LFpLG-a2_TPbAn1Ag--Ufy2Zz1R3qqYMLk0eq6Lu2QMvrvqSA.MsAlPmkVyl6zRsXJAi_PKXDBvpck9JknVEpeBadbcTvxaNEuo8DePWTzcuw1dYeZQ1QrlCCXMx6h-hdLPsDSbA","clickOnPill":"publication.PublicationFigures.html?_sg=NJ76akhOVOClSG8srY9_TPTnOwa2WktKX_Wp5Mgr5sf406zPsKOJNEZI8frIk-I8FMnRFNrXfKdXHdz0cvMRYw.Vw9yCmGEl-lElfnX64tQMs1cIje7UvF5_vYqq5bNjuaVSGD7AwAI0mkw-zHTnCMMIiQhgBzSSLAj95cPEIP0OQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndreas_Vlachos3%2Fpublication%2F224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop%2Flinks%2F00b4951b71b4e35e32000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=AoIM1z_H_i-4xBkV01LGxGII-ocd-lB537ay1Ug4U85wCpQabPvP-YtCMAjdVinzcbfOfZWpiBrTXhVJNOvK8A","urlHash":"e0aa5c087b9f8da17c0adaead2b57460","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=WocDlVVCcBbXWkp5pQSy2-HnQr7zLeen3Nh08g8Z6Rx2PIslB2kPHCMIbgjLEZdfx1Q0CcN3OWH7-TeHuvD5iouNLA4QbDGRJAOko5PbH58.Ss7mbcxjYfMjmfPz3y7ayhDVq0Uwgk5NAOZe45T14pc40qx55EBaL0lPBjzZDcpZcwz-tGZlryWB4cDYjNq9Qw._asTM1GKL7s17EoeFO_SmvHcCPg8jieUQmHIEcGLr9rY5Fl6gtzKrUKxA-_p_22mgdfIBqU95WeDnWCWMix6fg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"00b4951b71b4e35e32000000","trackedDownloads":{"00b4951b71b4e35e32000000":{"v":false,"d":false}},"assetId":"AS:98664194248714@1400534785447","readerDocId":"6707665","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":224175402,"commentCursorPromo":null,"widgetId":"rgw43_56ab1a0b99463"},"id":"rgw43_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndreas_Vlachos3%2Fpublication%2F224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop%2Flinks%2F00b4951b71b4e35e32000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A98664194248714%401400534785447&publicationUid=224175402&linkId=00b4951b71b4e35e32000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Scaling the iHMM: Parallelization versus Hadoop","publicationType":"Conference Paper","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=rnCG0leB4-4X1YKMW4qy7R7DGztEIccVNCTBDoaLck9RjC7IpkY84kXxyRUinxbO83jK327UsqDLKDkgr8Gb16R6Mq6ThYXKiSdYE4YU3ws.VasWZqgMcXiZ3qxh4ZHZNnsuUdnfb2hSBgXwIo9S07BMgQFH4ugnT_V1wkYlJoYZ81IGxELtNWBbGcWX6i-7oQ.re1xdj8B72upbYe5VrE2yJHTETfKsCarfoP-VyH5D3vvFffdeszKMgZ-uyS_hAg7XCSU4qkLbrWMhMKHtkvwCw","publicationUid":224175402,"trackedDownloads":{"00b4951b71b4e35e32000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw45_56ab1a0b99463"},"id":"rgw45_56ab1a0b99463","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab1a0b99463"},"id":"rgw46_56ab1a0b99463","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab1a0b99463"},"id":"rgw47_56ab1a0b99463","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1a0b99463"},"id":"rgw48_56ab1a0b99463","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw49_56ab1a0b99463"},"id":"rgw49_56ab1a0b99463","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw44_56ab1a0b99463"},"id":"rgw44_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw42_56ab1a0b99463"},"id":"rgw42_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1a0b99463"},"id":"rgw2_56ab1a0b99463","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":224175402},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=224175402&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1a0b99463"},"id":"rgw1_56ab1a0b99463","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"E1KIJVPbESCKhu\/OUQKDMu2TX1JJkGvawwd4z+Le2u0gUY48KRTVjOZG8CbVwRL7pUK0XwTg3YQqVyDC3xqLBiegVplkfDW1NLd0bBPZYChSqCpY07+7aT0EIHvOXLjMQJlRQidcTJZScoMELPQ5gq70OxEf48yGACWRpDGlWpaDewu7JWGO5Y5crVv12hVuBtqvADVq0pwPUEd7e\/FpaZ0+Jig0WFTZs9Z55UPG+tzWCw1APkv6umTWxhp5V9sQ9Mldwxi4TEdrOcjGVJbd2ydvRg0eqW8\/nCwu8AlO3wE=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Scaling the iHMM: Parallelization versus Hadoop\" \/>\n<meta property=\"og:description\" content=\"This paper compares parallel and distributed implementations of an iterative, Gibbs sampling, machine learning algorithm. Distributed implementations run under Hadoop on facility computing clouds....\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\" \/>\n<meta property=\"rg:id\" content=\"PB:224175402\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/CIT.2010.223\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Scaling the iHMM: Parallelization versus Hadoop\" \/>\n<meta name=\"citation_author\" content=\"S\u00e9bastien Brati\u00e8res\" \/>\n<meta name=\"citation_author\" content=\"Jurgen van Gael\" \/>\n<meta name=\"citation_author\" content=\"Andreas Vlachos\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_conference_title\" content=\"Computer and Information Technology (CIT), 2010 IEEE 10th International Conference on\" \/>\n<meta name=\"citation_publication_date\" content=\"2010\/08\/01\" \/>\n<meta name=\"citation_firstpage\" content=\"1235\" \/>\n<meta name=\"citation_lastpage\" content=\"1240\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/CIT.2010.223\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Andreas_Vlachos3\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\/links\/00b4951b71b4e35e32000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/215868066921738\/styles\/pow\/publicliterature\/FigureList.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-edd61362-41ed-4713-b0b4-1e4eace14ca3","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":529,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw50_56ab1a0b99463"},"id":"rgw50_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-edd61362-41ed-4713-b0b4-1e4eace14ca3", "c5754075e76d0c302b82f1cbef5cab9a6439a39a");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-edd61362-41ed-4713-b0b4-1e4eace14ca3", "c5754075e76d0c302b82f1cbef5cab9a6439a39a");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw51_56ab1a0b99463"},"id":"rgw51_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/224175402_Scaling_the_iHMM_Parallelization_versus_Hadoop","requestToken":"lww\/ZacrEx8GvkLznrZtreccqmPovM6IaYX3\/ao09jHpkJMFpCwfGQgGMfyiPk3OOIRic6F8l5kPBEWafOze6BvWcl5pvX3qI5cslGw+co3oWy4jCWOQtv2FysKvH\/rfdSLbhJub3mgxkifaPyhJm9mnv248ZcpkchhiU+pbvdghLRGpHcrPo4oKv4lIi9+GKbqZcmDQFWrLqYi+t2ibcJ\/gt95MvqErv2jq613frgfWA7vYW15lX+6O6MacFLwXL+4dw3Ey6RF3v2V1e3b6m7Q1Y0Kh+eDSY4dQB+M43bg=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=tuPICfurXZjCn8g5NUNBNtrie7fMTFAhSxKJQRixeG5Yx5sevoL8356M9URe7T6h","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjI0MTc1NDAyX1NjYWxpbmdfdGhlX2lITU1fUGFyYWxsZWxpemF0aW9uX3ZlcnN1c19IYWRvb3A%3D","signupCallToAction":"Join for free","widgetId":"rgw53_56ab1a0b99463"},"id":"rgw53_56ab1a0b99463","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw52_56ab1a0b99463"},"id":"rgw52_56ab1a0b99463","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw54_56ab1a0b99463"},"id":"rgw54_56ab1a0b99463","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
