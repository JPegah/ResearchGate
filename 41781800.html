<!DOCTYPE html> <html lang="en" class="" id="rgw37_56ab9ff619504"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="zs+qzA/4i6AaF7N/X3mnlr1Z4nhV5yqyOZbgRWDVxQHvM1W9j2epsJJSS5pZRb0pegg5x7PnZEO5jk0PqfH1cwIsbJWj33Z27uoYJQpCkR4WLIfWI2RQsDyVhYQU/hScsNOJ7ua8ukPRklosK6EsnYH2pQ/2ObFxvR22h3Yp4ajgSvCXzCB/MegEZk+PXfPEo9ZNN00xBW5R+Nehr4C6cniQ4qmRKKoCEHepoQSk4KjnffTaMaueM3qfUg0KK9YrpPqpu3FrSWGCL3DgYGb1RH2Yz/JSvEVq2gXWsC0f0F8="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-4c772283-4f02-481f-b4d3-df18d3d6a7f0",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Approximations for Binary Gaussian Process Classification" />
<meta property="og:description" content="We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification/links/0c96051a5e4f9e85d3000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification" />
<meta property="rg:id" content="PB:41781800" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Approximations for Binary Gaussian Process Classification" />
<meta name="citation_author" content="Hannes Nickisch" />
<meta name="citation_author" content="Carl Edward Rasmussen" />
<meta name="citation_publication_date" content="2008/10/01" />
<meta name="citation_journal_title" content="Journal of Machine Learning Research" />
<meta name="citation_issn" content="1533-7928" />
<meta name="citation_volume" content="9" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Hannes_Nickisch/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification/links/0c96051a5e4f9e85d3000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Approximations for Binary Gaussian Process Classification (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Approximations for Binary Gaussian Process Classification on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9ff619504" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9ff619504" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab9ff619504">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Approximations%20for%20Binary%20Gaussian%20Process%20Classification&rft.title=Journal%20of%20Machine%20Learning%20Research%2C%20v.9%2C%202035-2078%20(2008)&rft.jtitle=Journal%20of%20Machine%20Learning%20Research%2C%20v.9%2C%202035-2078%20(2008)&rft.volume=9&rft.date=2008&rft.issn=1533-7928&rft.au=Hannes%20Nickisch%2CCarl%20Edward%20Rasmussen&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Approximations for Binary Gaussian Process Classification</h1> <meta itemprop="headline" content="Approximations for Binary Gaussian Process Classification">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification/links/0c96051a5e4f9e85d3000000/smallpreview.png">  <div id="rgw8_56ab9ff619504" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56ab9ff619504" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Hannes_Nickisch" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272392765309003%401441954904967_m/Hannes_Nickisch.png" title="Hannes Nickisch" alt="Hannes Nickisch" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Hannes Nickisch</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56ab9ff619504" data-account-key="Hannes_Nickisch">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Hannes_Nickisch"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272392765309003%401441954904967_l/Hannes_Nickisch.png" title="Hannes Nickisch" alt="Hannes Nickisch" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Hannes_Nickisch" class="display-name">Hannes Nickisch</a>    </h5> <div class="truncate-single-line meta">    <span class="meta">Philips Research Hamburg</span>    </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56ab9ff619504"> <a href="researcher/43277170_Carl_Edward_Rasmussen" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Carl Edward Rasmussen" alt="Carl Edward Rasmussen" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Carl Edward Rasmussen</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab9ff619504">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/43277170_Carl_Edward_Rasmussen"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Carl Edward Rasmussen" alt="Carl Edward Rasmussen" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/43277170_Carl_Edward_Rasmussen" class="display-name">Carl Edward Rasmussen</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1533-7928_Journal_of_Machine_Learning_Research"><span itemprop="name">Journal of Machine Learning Research</span></a> </span>    (Impact Factor: 2.47).     <meta itemprop="datePublished" content="2008-10">  10/2008;  9.             <div class="pub-source"> Source: <a href="http://edoc.mpg.de/419931" rel="nofollow">OAI</a> </div>  </div> <div id="rgw13_56ab9ff619504" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw14_56ab9ff619504" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw28_56ab9ff619504">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw29_56ab9ff619504">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Hannes_Nickisch/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification/links/0c96051a5e4f9e85d3000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Hannes_Nickisch">Hannes Nickisch</a>   </span>  </div>  <div class="social-share-container"><div id="rgw31_56ab9ff619504" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw32_56ab9ff619504" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw33_56ab9ff619504" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw34_56ab9ff619504" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw35_56ab9ff619504" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw36_56ab9ff619504" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw30_56ab9ff619504" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FHannes_Nickisch%2Fpublication%2F41781800_Approximations_for_Binary_Gaussian_Process_Classification%2Flinks%2F0c96051a5e4f9e85d3000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw27_56ab9ff619504"  itemprop="articleBody">  <p>Page 1</p> <p>Journal of Machine Learning Research 9 (2008) 2035-2078Submitted 8/07; Revised 4/08; Published 10/08<br />Approximations for Binary Gaussian Process Classification<br />Hannes Nickisch<br />Max Planck Institute for Biological Cybernetics<br />Spemannstraße 38<br />72076 Tübingen, Germany<br />HN@TUEBINGEN.MPG.DE<br />Carl Edward Rasmussen∗<br />Department of Engineering<br />University of Cambridge<br />Trumpington Street<br />Cambridge, CB2 1PZ, UK<br />CER54@CAM.AC.UK<br />Editor: Carlos Guestrin<br />Abstract<br />We provide a comprehensive overview of many recent algorithms for approximate inference in<br />Gaussian process models for probabilistic binary classification. The relationships between several<br />approaches are elucidated theoretically, and the properties of the different algorithms are corrobo-<br />rated by experimental results. We examine both 1) the quality of the predictive distributions and<br />2) the suitability of the different marginal likelihood approximations for model selection (selecting<br />hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods<br />produce good predictive distributions although their marginal likelihood approximations are poor.<br />Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost<br />always the method of choice unless the computational budget is very tight. We also extend existing<br />methods in various ways, and provide unifying code implementing all approaches.<br />Keywords:<br />Gaussian process priors, probabilistic classification, Laplaces’s approximation, ex-<br />pectation propagation, variational bounding, mean field methods, marginal likelihood evidence,<br />MCMC<br />1. Introduction<br />Gaussian processes (GPs) can conveniently be used to specify prior distributions for Bayesian infer-<br />ence. In the case of regression with Gaussian noise, inference can be done simply in closed form,<br />since the posterior is also a GP. For non-Gaussian likelihoods, such as e.g., in binary classification,<br />exact inference is analytically intractable.<br />One prolific line of attack is based on approximating the non-Gaussian posterior with a tractable<br />Gaussian distribution. One might think that finding such an approximating GP is a well-defined<br />problem with a largely unique solution. However, we find no less than three different types of solu-<br />tion in the recent literature: Laplace Approximation (LA) (Williams and Barber, 1998), Expectation<br />Propagation (EP) (Minka, 2001a) and Kullback-Leibler divergence (KL) minimization (Opper and<br />Archambeau, 2008) comprising Variational Bounding (VB) (Gibbs and MacKay, 2000) as a special<br />∗. Also at Max Planck Institute for Biological Cybernetics, Spemannstraße 38, 72076 Tübingen, Germany.<br />c ?2008 Hannes Nickisch and Carl Edward Rasmussen.</p>  <p>Page 2</p> <p>NICKISCH AND RASMUSSEN<br />case. Another approach is based on a factorial approximation, rather than a Gaussian (Csató et al.,<br />2000).<br />Practical applications reflect the richness of approximate inference methods: LA has been used<br />for sequence annotation (Altun et al., 2004) and prostate cancer prediction (Chu et al., 2005), EP for<br />affect recognition (Kapoor and Picard, 2005), VB for weld cracking prognosis (Gibbs and MacKay,<br />2000), Label Regression (LR) serves for object categorization (Kapoor et al., 2007) and MCMC<br />sampling is applied to rheuma diagnosis (Schwaighofer et al., 2002). Brain computer interfaces<br />(Zhong et al., 2008) even rely on several (LA, EP, VB) methods.<br />In this paper, we compare these different approximations and provide insights into the strengths<br />and weaknesses of each method, extending the work of Kuss and Rasmussen (2005) in several di-<br />rections: We cover many more approximation methods (VB,KL,FV,LR), put all of them in common<br />framework and provide generic implementations dealing with both the logistic and the cumula-<br />tive Gaussian likelihood functions and clarify the aspects of the problem causing difficulties for<br />each method. We derive Newton’s method for KL and VB. We show how to accelerate MCMC<br />simulations. We highlight numerical problems, comment on computational complexity and supply<br />runtime measurements based on experiments under a wide range of conditions, including different<br />likelihood and different covariance functions. We provide deeper insights into the methods behavior<br />by systematically linking them to each other. Finally, we review the tight connections to methods<br />from the literature on Statistical Physics, including the TAP approximation and TAPnaive.<br />The quantities of central importance are the quality of the probabilistic predictions and the suit-<br />ability of the approximate marginal likelihood for selecting parameters of the covariance function<br />(hyperparameters). The marginal likelihood for any Gaussian approximate posterior can be lower<br />bounded using Jensen’s inequality, but the specific approximation schemes also come with their<br />own marginal likelihood approximations.<br />We are able to draw clear conclusions. Whereas every method has good performance under<br />some circumstances, only a single method gives consistently good results. We are able to theoreti-<br />cally corroborate our experimental findings; together this provides solid evidence and guidelines for<br />choosing an approximation method in practice.<br />2. Gaussian Processes for Binary Classification<br />We describe probabilistic binary classification based on Gaussian processes in this section. For<br />a graphical model representation see Figure 1 and for a 1d pictorial description consult Figure 2.<br />Given data points xifrom a domain X with corresponding class labels yi∈ {−1,+1}, one would<br />like to predict the class membership probability for a test point x∗. This is achieved by using a<br />latent function f whose value is mapped into the unit interval by means of a sigmoid function<br />sig:R→[0,1] such that the class membership probability P(y = +1|x) can be written as sig(f(x)).<br />The class membership probability must normalize ∑yP(y|x)=1, which leads to P(y = +1|x)=1−<br />P(y = −1|x). If the sigmoid function satisfies the point symmetry condition sig(t) = 1−sig(−t),<br />the likelihood can be compactly written as<br />P(y|x)=<br />sig(y· f(x)).<br />2036</p>  <p>Page 3</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />In this paper, two point symmetric sigmoids are considered<br />siglogit(t)<br />:=<br />1<br />1+e−t<br />Zt<br />sigprobit(t)<br />:=<br />−∞N (τ|0,1)dτ.<br />The two functions are very similar at the origin (showing locally linear behavior around sig(0)=<br />1/2 with slope 1/4 for siglogitand 1/√2π for sigprobit) but differ in how fast they approach 0/1 when<br />t goes to infinity. For large negative values of t, we have the asymptotics<br />siglogit(t) ≈ exp(−t)<br />Linear decay of ln(siglogit) corresponds to a weaker penalty for wrongly classified examples than<br />the quadratic decay of ln(sigprobit) .<br />For notational convenience, the following shorthands are used: The matrix X = [x1,...,xn] of<br />size n×d collects the training points, the vector y = [y1,...,yn]?of size n×1 collects the target<br />values and latent function values are summarized by f=[f1,..., fn]?with fi= f(xi). Observed data<br />is written as D = {(xi,yi)|i = 1,...,n} = (X,y). Quantities carrying an asterisk refer to test points,<br />that is, f∗contains the latent function values for test points [x∗,1,...,x∗,m] = X∗⊂ X. Covariances<br />between latent values f and f∗at data points x and x∗follow the same notation, namely [K∗∗]ij=<br />k(x∗,i,x∗,j), [K∗]ij= k(xi,x∗,j), [k∗]i= k(xi,x∗) and k∗∗= k(x∗,x∗), where [A]ijdenotes the entry<br />Aijof the matrix A.<br />Given the latent function f, the class labels are assumed to be Bernoulli distributed and inde-<br />pendent random variables, which gives rise to a factorial likelihood, factorizing over data points<br />(see Figure 1)<br />andsigprobit(t) ≈ exp(−1<br />2t2+0.158t −1.78), for t ? 0.<br />P(y|f)=<br />P(y|f) =<br />n<br />∏<br />i=1<br />P(yi|fi) =<br />n<br />∏<br />i=1<br />sig(yifi).<br />(1)<br />A GP (Rasmussen and Williams, 2006) is a stochastic process fully specified by a mean function<br />m(x) = E[f(x)] and a positive definite covariance function k(x,x?) = V[f(x), f(x?)]. This means<br />that a random variable f(x) is associated to every x ∈ X , such that for any set of inputs X ⊂ X,<br />the joint distribution P(f|X,θ) = N (f|m0,K) is Gaussian with mean vector m0and covariance<br />matrix K. The mean function and covariance functions may depend on additional hyperparameters<br />θ. For notational convenience we will assume m(x) ≡ 0 throughout. Thus, the elements of K are<br />Kij= k(xi,xj,θ).<br />By application of Bayes’ rule, one gets an expression for the posterior distribution over the<br />latent values f<br />P(f|y,X,θ)=<br />P(y|f)P(f|X,θ)<br />RP(y|f)P(f|X,θ)df=N (f|0,K)<br />P(y|X,θ)<br />n<br />∏<br />i=1<br />sig(yifi),<br />(2)<br />where Z =P(y|X,θ)=RP(y|f)P(f|X,θ)df denotes the marginal likelihood or evidence for the hy-<br />inputs is<br />perparameter θ. The joint prior over training and test latent values f and f∗given the corresponding<br />2037</p>  <p>Page 4</p> <p>NICKISCH AND RASMUSSEN<br />P(f∗,f|X∗,X,θ)= N<br />??<br />f<br />f∗<br />?????0,<br />Z<br />?<br />K<br />K?<br />K∗<br />K∗∗<br />∗<br />??<br />.<br />When making predictions, we marginalize over the training set latent variables<br />Z<br />where the joint posterior is factored into the product of the posterior and the conditional prior<br />?<br />Finally, the predictive class membership probability p∗:= P(y∗= 1|x∗,y,X,θ) is obtained by aver-<br />aging out the test set latent variables<br />P(f∗|X∗,y,X,θ) =<br />P(f∗,f|X∗,y,X,θ)df =<br />P(f∗|f,X∗,X,θ)P(f|y,X,θ)df,<br />(3)<br />P(f∗|f,X∗,X,θ)= N<br />f∗|K?<br />∗K−1f,K∗∗−K?<br />∗K−1K∗<br />?<br />.<br />P(y∗|x∗,y,X,θ) =<br />Z<br />P(y∗|f∗)P(f∗|x∗,y,X,θ)d f∗ =<br />Z<br />sig(y∗f∗)P(f∗|x∗,y,X,θ)df∗.<br />(4)<br />The integral is analytically tractable for sigprobit(Rasmussen and Williams, 2006, Ch. 3.9) and can<br />be efficiently approximated for siglogit(Williams and Barber, 1998, App. A).<br />Figure 1: Graphical Model for binary Gaussian process classification: Circles represent unknown<br />quantities, squares refer to observed variables. The horizontal thick line means fully<br />connected latent variables. An observed label yiis conditionally independent of all other<br />nodes given the corresponding latent variable fi. Labels yiand latent function values<br />fiare connected through the sigmoid likelihood; all latent function values fiare fully<br />connected, since they are drawn from the same GP. The labels yiare binary, whereas the<br />prediction p∗is a probability and can thus have values from the whole interval [0,1].<br />2.1 Stationary Covariance Functions<br />In preparation for the analysis of the approximation schemes described in this paper, we investigate<br />some simple properties of the posterior for stationary covariance functions in different regimes<br />2038</p>  <p>Page 5</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />encountered in classification. Stationary covariances of the form k(x,x?,θ) = σ2<br />g : R → R a monotonously decreasing function1and θ = {σf,?} are widely used. The following<br />section supplies a geometric intuition of that specific prior in the classification scenario by analyzing<br />the limiting behavior of the covariance matrix K as a function of the length scale ? and the limiting<br />behavior of the likelihood as a function of the latent function scale σf. A pictorial illustration of the<br />setting is given in Figure 3.<br />fg(|x−x?|/?) with<br />2.1.1 LENGTH SCALE<br />Two limiting cases of “ignorance with respect to the data” with marginal likelihood Z = 2−ncan be<br />distinguished, where<br />= [1,...1]?and I is the identity matrix (see Appendix B.1)<br />?<br />lim<br />?→0K<br />lim<br />=<br />σ2<br />fI,<br />?→∞K<br />=<br />σ2<br />f<br />????.<br />For very small length scales (? → 0), the prior is simply isotropic as all points are deemed to be<br />far away from each other and the whole model factorizes. Thus, the (identical) posterior moments<br />can be calculated dimension-wise. (See Figure 3, regimes 1, 4 and 7.)<br />For very long length scales (? → ∞), the prior becomes degenerate as all datapoints are deemed<br />to be close to each other and takes the form of a cigar along the hyper-diagonal. (See Figure 3,<br />regimes 3, 6 and 9.) A 1d example of functions drawn from GP priors with different lengthscales ?<br />is shown in Figure 2 on the left. The lengthscale has to be suited to the data; if chosen too small, we<br />will overfit, if chosen too high underfitting will occur.<br />2.1.2 LATENT FUNCTION SCALE<br />The sigmoid likelihood function sig(yifi) measures the agreement of the signs of the latent function<br />and the label in a smooth way, that is, values close to one if the signs of yiand fiare the same and |fi|<br />is large, and values close to zero if the signs are different and |fi| is large. The latent function scale<br />σfof the data can be moved into the likelihood˜ sigσf(t)=sig(σ2<br />the likelihood and finally the smoothness of the agreement by interpolation between the two limiting<br />cases “ignorant” and “hard cut”<br />ft), thus σfmodels the steepness of<br />lim<br />σf→0sig(t)<br />lim<br />≡<br />≡<br />1<br />2<br />step(t) :=?<br />“ignorant&quot;,<br />σf→∞sig(t)<br />0, t &lt; 0;<br />1<br />2, t = 0;1, 0 &lt;t<br />“hard cut&quot;.<br />In the case of very small latent scales (σf→ 0), the likelihood is flat causing the posterior to<br />equal the prior. The marginal likelihood is again Z = 2−n. (See Figure 3, regimes 7, 8 and 9.)<br />In the case of large latent scales (σf? 1), the likelihood approaches the step function. (See<br />Figure 3, regimes 1, 2 and 3.) A further increase of the latent scale does not change the model<br />anymore. The model is effectively the same for all σfabove a threshold.<br />1. Furthermore, we require g(0) = 1 and limt→∞g(t) = 0.<br />2039</p>  <p>Page 6</p> <p>NICKISCH AND RASMUSSEN<br />02468 10<br />−4<br />−2<br />0<br />2<br />4<br />a) Prior lengthscales<br />02468 10<br />−4<br />−2<br />0<br />2<br />4<br />b) f~Prior<br />0246810<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />c) sig(f), f~Prior<br />02468 10<br />−4<br />−2<br />0<br />2<br />4<br />d) f~Posterior, n=7<br />0246810<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />e) sig(f), n=7<br />0246810<br />−4<br />−2<br />0<br />2<br />4<br />f) f~Posterior, n=20<br />02468 10<br />0<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />g) sig(f), n=20<br />Figure 2: Pictorial illustration of binary Gaussian process classification in 1d: Plot a) shows 3 sam-<br />ple functions drawn from GPs with different lengthscales ?. Then, three pairs of plots<br />show distributions over functions f : R → R and sig(f) : R → [0,1] occurring in GP clas-<br />sification. b+c) the prior, d+e) a posterior with n = 7 observations and f+g) a posterior<br />withn=20observationsalongwiththenobservationswithbinarylabels. Thethickblack<br />line is the mean, the gray background is the ± standard deviation and the thin lines are<br />sample functions. With more and more data points observed, the uncertainty is gradually<br />shrunk. At the decision boundary the uncertainty is smallest.<br />2.2 Gaussian Approximations<br />Unfortunately, the posterior over the latent values (Equation 2) is not Gaussian due to the non-<br />Gaussian likelihood (Equation 1). Therefore, the latent distribution (Equation 3), the predictive<br />distribution (Equation 4) and the marginal likelihood Z cannot be written as analytical expressions.<br />To obtain exact answers, one can resort to sampling algorithms (MCMC). However, if sig is con-<br />cave in the logarithmic domain, the posterior can be shown to be unimodal motivating Gaussian<br />approximations to the posterior. Five different Gaussian approximations corresponding to methods<br />explained later onwards in the paper are depicted in Figure 4.<br />A quadratic approximation to the log likelihood φ(fi) := lnP(yi|fi) at˜fi<br />φ(˜fi)+φ?(˜fi)(fi−˜fi)+1<br />φ(fi)<br />≈<br />2φ??(˜fi)(fi−˜fi)2= −1<br />2wif2<br />i+bifi+constfi<br />motivates the following approximate posterior Q(f|y,X,θ)<br />lnP(f|y,X,θ)<br />(2)<br />=<br />−1<br />−1<br />−1<br />lnN (f|m,V) =: lnQ(f|y,X,θ),<br />2f?K−1f+<br />2f?K−1f−1<br />2(f−m)??K−1+W?(f−m)+constf<br />n<br />∑<br />i=1<br />lnP(yi|fi)+constf<br />quad. approx.<br />≈<br />m:=(K−1+W)−1b<br />=<br />2f?Wf+b?f+constf<br />=<br />(5)<br />2040</p>  <p>Page 7</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />Prior<br />l2 small<br />Prior<br />l2 medium<br />Prior<br />l2 large<br />Lik.<br />σf<br />2 large<br />Lik.<br />σf<br />2 medium<br />Lik.<br />σf<br />2 small<br />123<br />456<br />789<br />Figure 3: Gaussian Process Classification: Prior, Likelihood and exact Posterior: Nine num-<br />bered quadrants show posterior obtained by multiplication of different priors and like-<br />lihoods. The leftmost column illustrates the likelihood function for three different<br />steepness parameters σf and the upper row depicts the prior for three different length<br />scales ?. Here, we use σf as a parameter of the likelihood. Alternatively, rows cor-<br />respond to “degree of Gaussianity” and columns stand for “degree of isotropy“. The<br />axes show the latent function values f1= f(x1) and f2= f(x2). A simple toy exam-<br />ple employing the cumulative Gaussian likelihood and a squared exponential covariance<br />k(x,x?) = σ2<br />tion scales lnσf= {−1.5,0,1.5} is used. Two data points x1=√2, x2= −√2 with<br />corresponding labels y1= 1, y2= −1 form the data set.<br />fexp(−?x−x??2/2?2) with length scales ln? = {0,1,2.5} and latent func-<br />where V−1= K−1+W and W denotes the precision of the effective likelihood (see Equation 7). It<br />turns out that the methods discussed in the following sections correspond to particular choices of m<br />and V.<br />Let us assume, we have found such a Gaussian approximation to the posterior with mean m<br />and (co)variance V. Consequently, the latent distribution for a test point becomes a tractable one-<br />dimensional Gaussian P(f∗|x∗,y,X,θ) = N (f∗|µ∗,σ2<br />and Williams, 2006, p. 44 and 56):<br />∗) with the following moments (Rasmussen<br />µ∗<br />σ2<br />=<br />=<br />k?<br />k∗∗−k?<br />∗K−1m = k?<br />∗<br />∗α,α = K−1m,<br />k∗∗−k?<br />∗<br />?K−1−K−1VK−1?k∗<br />=<br />∗<br />?K+W−1?−1k∗.<br />(6)<br />Since Gaussians are closed under multiplication, one can—given the Gaussian prior P(f|X,θ)<br />and the Gaussian approximation to the posterior Q(f|y,X,θ)—deduce the Gaussian factor Q(y|f)<br />such that Q(f|y,X,θ) ∝ Q(y|f)P(f|X,θ). Consequently, this Gaussian factor can be thought of as<br />an effective likelihood. Five different effective likelihoods, corresponding to methods discussed sub-<br />2041</p>  <p>Page 8</p> <p>NICKISCH AND RASMUSSEN<br />best Gaussian posterior, KL=0.118<br />−50510<br />−10<br />−5<br />0<br />5<br />LA posterior, KL=0.557<br />−50510<br />−10<br />−5<br />0<br />5<br />EP posterior, KL=0.118<br />−50510<br />−10<br />−5<br />0<br />5<br />VB posterior, KL=3.546<br />−50510<br />−10<br />−5<br />0<br />5<br />KL posterior, KL=0.161<br />−50510<br />−10<br />−5<br />0<br />5<br />Figure 4: Five Gaussian Approximations to the Posterior (exact Posterior and mode in gray): Dif-<br />ferent Gaussian approximations to the exact posterior using the regime 2 setting of Figure<br />3 are shown. The exact posterior is represented in gray by a cross at the mode and a sin-<br />gle equiprobability contour line. From left to right: The best Gaussian approximation<br />(intractable) matches the moments of the true posterior, the Laplace approximation does<br />a Taylor expansion around the mode, the EP approximation iteratively matches marginal<br />moments, the variational method maximizes a lower bound on the marginal likelihood<br />and the KL method minimizes the Kullback-Leibler to the exact posterior. The axes show<br />the latent function values f1= f(x1) and f2= f(x2).<br />sequently in the paper, are depicted in Figure 5. By “dividing” the approximate Gaussian posterior<br />(see Appendix B.2) by the true Gaussian prior we find the contribution of the effective likelihood<br />Q(y|f):<br />Q(y|f) ∝N (f|m,V)<br />N (f|0,K)<br />We see (also from Equation 5) that W models the precision of the effective likelihood. In general, W<br />is a full matrix containing n2parameters.2However, all algorithms maintaining a Gaussian posterior<br />approximationworkwithadiagonal Wtoenforcetheeffectivelikelihoodtofactorizeoverexamples<br />(as the true likelihood does, see Figure 1) in order to reduce the number of parameters. We are not<br />aware of work quantifying the error made by this assumption.<br />∝ N<br />?<br />f|(KW)−1m+m,W−1?<br />.<br />(7)<br />2.3 Log Marginal Likelihood<br />Prior knowledge over the latent function f is encoded in the choice of a covariance function k con-<br />taining hyperparameters θ. In principle, one can do inference jointly over f and θ e.g., by sampling<br />techniques. Another approach to model selection is maximum likelihood type II also known as<br />the evidence framework (MacKay, 1992), where the hyperparameters θ are chosen to maximize<br />the marginal likelihood or evidence P(y|X,θ). In other words, one maximizes the agreement be-<br />tween observed data and the model. Therefore, one has a strong motivation to estimate the marginal<br />likelihood.<br />Geometrically, the marginal likelihood measures the volume of the prior times the likelihood.<br />High volume implies a strong consensus between our initial belief and our observations. In GP clas-<br />sification, each data point xigives rise to a dimension fiin latent space. The likelihood implements<br />a mechanism, for smoothly restricting the posterior along the axis of fito the side corresponding<br />?<br />2. Numerical moment matching with K =<br />7<br />6<br />6<br />7<br />?<br />, y1= y2= 1 and sigprobitleads to W =<br />?<br />0.142<br />−0.017<br />−0.017<br />0.142<br />?<br />.<br />2042</p>  <p>Page 9</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />best Gaussian likelihood<br />−50510<br />−10<br />−5<br />0<br />5<br />LA likelihood<br />−50510<br />−10<br />−5<br />0<br />5<br />EP likelihood<br />−50510<br />−10<br />−5<br />0<br />5<br />VB likelihood<br />−50510<br />−10<br />−5<br />0<br />5<br />KL likelihood<br />−50510<br />−10<br />−5<br />0<br />5<br />Figure 5: Five Effective Likelihoods (exact Prior/Likelihood in gray): A Gaussian approximation<br />to the posterior induces a Gaussian effective likelihood (Equation 7). Different effective<br />likelihoods are shown; order and setting are the same as described in Figure 4. The axes<br />show the latent function values f1= f(x1) and f2= f(x2). The effective likelihood re-<br />places the non-Gaussian likelihood (indicated by three gray lines). A good replacement<br />behaves like the exact likelihood in regions of high prior density (indicated by gray el-<br />lipses). EP and KL yield a good coverage of that region. However LA and VB yield too<br />concentrated replacements.<br />to the sign of yi. Thus, the latent space Rnis softly cut down to the orthant given by the values in<br />y. The log marginal likelihood measures, what fraction of the prior lies in that orthant. Finally, the<br />value Z = 2−ncorresponds to the case, where half of the prior lies on either side along each axis in<br />latent space. Consequently, successful inference is characterized by Z &gt; 2−n.<br />Some posterior approximations (Sections 3 and 4) provide an approximation to the marginal<br />likelihood, other methods provide a lower bound (Sections 5 and 6). Any Gaussian approximation<br />Q(f|θ) = N (f|m,V) to the posterior P(f|y,X,θ) gives rise to a lower bound ZBto the marginal<br />likelihood Z by application of Jensen’s inequality. This bound has been used in the context of<br />sparse approximations (Seeger, 2003).<br />lnZ = lnP(y|X,θ)=<br />ln<br />Z<br />Q(f|θ)lnP(y|f)P(f|X,θ)<br />P(y|f)P(f|X,θ)df = ln<br />Z<br />df =: lnZB.<br />Q(f|θ)P(y|f)P(f|X,θ)<br />Q(f|θ)<br />df<br />Jensen<br />≥<br />Z<br />Q(f|θ)<br />(8)<br />Some algebra (Appendix B.3) leads to the following expression for lnZB:<br />n<br />∑<br />i=1<br />?<br />Model selection means maximization of lnZB. Term 1) is a sum of one-dimensional Gaussian<br />integrals of sigmoid functions in the logarithmic domain with adjustable offset and steepness. The<br />integrals can be numerically computed in an efficient way using Gauss-Hermite quadrature (Press<br />et al., 1993, §4.5). As the sigmoid in the log domain takes only negative values, the first term will<br />be negative. That means, maximization of the first term is done by shifting the log-sigmoid such<br />that the high-density region of the Gaussian is multiplied by small values. Term 2) is the equivalent<br />Z<br />N (f|,0,1)lnsig?yi<br />?√Viif +mi<br />??df<br />???<br />1) data fit<br />+1<br />2[n−m?K−1m<br />????<br />2) data fit<br />+ln??VK−1??−tr?VK−1?<br />????<br />3) regularizer<br />].<br />(9)<br />2043</p>  <p>Page 10</p> <p>NICKISCH AND RASMUSSEN<br />of the data-fit term in GP regression (Rasmussen and Williams, 2006, Ch. 5.4.1). Thus, the first<br />and the second term encourage fitting the data by favouring small variances Viiand large means mi<br />having the same sign as yi. The third term can be rewritten as −ln|I+KW|−tr?(I+KW)−1?and<br />eigenvalues of KW small, thereby favouring a smaller class of functions—this can be seen as an<br />instance of Occam’s razor.<br />Furthermore, the bound<br />yields −∑n<br />i=1ln(1+λi)+<br />1<br />1+λiwith λi≥ 0 being the eigenvalues of KW. Thus, term 3) keeps the<br />lnZB<br />=<br />Z<br />Q(f|θ)lnP(f|y,X,θ)P(y|X)<br />Q(f|θ)<br />df = lnZ−KL(Q(f|θ) ? P(f|y,X,θ))<br />(10)<br />can be decomposed into the exact marginal likelihood minus the Kullback-Leibler (KL) diver-<br />gence between the exact posterior and the approximate posterior. Thus by maximizing the lower<br />bound lnZBon lnZ, we effectively minimize the KL-divergence between P(f|y,X,θ) and Q(f|θ)=<br />N (f|m,V). The bound is tight if and only if Q(f|θ) = P(f|y,X,θ).<br />3. Laplace Approximation (LA)<br />A second order Taylor expansion around the posterior mode m leads to a natural way of constructing<br />a Gaussian approximation to the log-posterior Ψ(f) = lnP(f|y,X,θ) (Williams and Barber, 1998;<br />Rasmussen and Williams, 2006, Ch. 3). The mode m is taken as the mean of the approximate<br />Gaussian. Linear terms of Ψ vanish because the gradient at the mode is zero. The quadratic term of<br />Ψ is given by the negative Hessian W, which - due to the likelihood’s factorial structure - turns out<br />to be diagonal. The mode m is found by Newton’s method.<br />3.1 Posterior<br />P(f|y,X,θ)<br />≈ N (f|m,V) =N<br />=<br />argmax<br />f∈Rn<br />−∂2lnP(y|f)<br />∂f∂f?<br />?<br />f|m,?K−1+W?−1?<br />?<br />,<br />m<br />P(y|f)P(f|X,θ),<br />????f=m<br />W<br />=<br />= −<br />∂2lnP(yi|fi)<br />∂f2<br />i<br />????<br />fi=mi<br />?<br />ii<br />.<br />3.2 Log Marginal Likelihood<br />The unnormalized posterior P(y|f)P(f|X,θ) has its maximum h = exp(Ψ(m)) at its mode m,<br />wherethegradientvanishes. ATaylorexpansionofΨisthengivenbyΨ(f)≈h−1<br />W)(f−m). Consequently, the log marginal likelihood can be approximated by plugging in the ap-<br />proximation of Ψ(f).<br />2(f−m)?(K−1+<br />lnZ = lnP(y|X,θ)=<br />ln<br />Z<br />P(y|f)P(f|X,θ)df = ln<br />Z<br />lnP(y|m)−1<br />Z<br />exp(Ψ(f))df<br />≈<br />lnh+lnexp<br />?<br />−1<br />2(f−m)??K−1+W?(f−m)<br />2m?K−1m+1<br />?<br />df<br />=<br />2ln|I+KW|.<br />2044</p>  <p>Page 11</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />4. Expectation Propagation (EP)<br />EP (Minka, 2001b) is an iterative method to find approximations based on approximate marginal<br />moments, which can be applied to Gaussian processes. See (Rasmussen and Williams, 2006, Ch. 3)<br />for details. The individual likelihood terms are replaced by site functions ti(fi) being unnormalized<br />Gaussians<br />P(yi|fi) ≈ ti<br />such that the approximate marginal moments of Q(fi) :=RN (f|0,K)∏n<br />agree with the marginals of<br />based on the exact likelihood term P(yj|fj). That means, there are 3n quantities µi, σ2<br />to be iteratively optimized. Convergence of EP is not generally guaranteed, but there always exists<br />a fixed-point for the EP updates in GP classification (Minka, 2001a). If the EP iterations converge,<br />the solution obtained is a saddle point of a special energy function (Minka, 2001a). However, an<br />EP update does not necessarily imply a decrease in energy. For our case of log-concave likelihood<br />functions, we always observed convergence, but we are not aware of a formal proof.<br />?fi,µi,σ2<br />i,Zi<br />?:= ZiN?fi|µi,σ2<br />?<br />i<br />?<br />j=1ZjN<br />df¬iof the approximation<br />?<br />fj|µj,σ2<br />j<br />?<br />df¬i<br />RN (f|0,K)P(yi|fi)∏j?=iZjN<br />fj|µj,σ2<br />j<br />?<br />iand Zi<br />4.1 Posterior<br />Based on these local approximations, the approximate posterior can be written as:<br />P(f|y,X,θ)<br />≈ N (f|m,V) =N<br />=<br />i<br />=<br />VWµ =<br />?<br />f|m,?K−1+W?−1?<br />I−K?K+W−1?−1?<br />,<br />W<br />?σ−2<br />?<br />ii,<br />m<br />?<br />KWµ, µ = (µ1,...,µn)?.<br />4.2 Log Marginal Likelihood<br />&gt;From the likelihood approximations, one can directly obtain an expression for the approximate log<br />marginal likelihood<br />lnZ = lnP(y|X,θ)=<br />ln<br />Z<br />Z<br />P(y|f)P(f|X,θ)df<br />n<br />∏<br />i=1<br />lnZi−1<br />Zi<br />√2π−1<br />≈<br />ln<br />t?fi,µi,σ2<br />2µ??K+W−1?−1µ−1<br />2m??K−1+K−1W−1K−1?m−1<br />i,Zi<br />?P(f|X,θ)df<br />=<br />n<br />∑<br />i=1<br />n<br />∑<br />i=1<br />2ln??K+W−1??−n<br />2ln2π<br />=<br />ln<br />2ln??K+W−1??=: lnZEP.<br />The lower bound provided by Jensen’s inequality ZB(Equation 9) is known to be below the approx-<br />imation ZEPobtained by EP (Opper and Winther, 2005, page 2183). From ZEP≥ ZBand Z ≥ ZBit<br />is not clear, which value one should use. In principle, ZEPcould be a bad approximation. However,<br />our experimental findings and extensive Monte Carlo simulations suggest that ZEPis very accurate.<br />2045</p>  <p>Page 12</p> <p>NICKISCH AND RASMUSSEN<br />4.3 Thouless, Anderson &amp; Palmer method (TAP)<br />Based on ideas rooted in Statistical Physics, one can approach the problem from a slightly different<br />angle(OpperandWinther,2000). IndividualGaussianapproximationsN (fi|µ¬i,σ2<br />to predictive distributions P?fi|xi,y\i,X\i,θ?for data points xithat have been previously removed<br />parameters of interest.<br />¬i)areonlymade<br />from the training set. Based on µ¬iand σ2<br />¬ione can derive explicit expressions for (α,W<br />1<br />2), our<br />αi<br />≈<br />R<br />∂<br />∂fiP(yi|fi)N (fi|µ¬i,σ2<br />RP(yi|fi)N (fi|µ¬i,σ2<br />σ2<br />¬i<br />αi[Kα]i<br />¬i)dfi<br />¬i)dfi<br />,<br />?W−1?<br />ii<br />≈<br />?<br />1<br />−1<br />?<br />.<br />(11)<br />In turn, the 2n parameters (µ¬i,σ2<br />¬i) can be expressed as a function of α, K and W<br />??K+W−1?−1?<br />=[Kα]i−σ2<br />1<br />2.<br />σ2<br />¬i<br />=<br />1/<br />ii−?W−1?<br />ii,<br />µ¬i<br />¬iαi.<br />(12)<br />As a result, a system (Equations 11/12) of nonlinear equations in µ¬iand σ2<br />by iteration. Each step involves a matrix inversion of cubic complexity. A faster “naïve” variant<br />updating only n parameters has also been proposed (Opper and Winther, 2000) but it does not lead<br />to the same fixed point. As in the FV algorithm (Section 7), a formal complex transformation leads<br />to a simplified version by fixing σ2<br />Finally, for prediction, the predictive posterior P(f∗|x∗,y,X,θ) is approximated by a Gaussian<br />N (f∗|µ∗,σ2<br />A fixed-point of the TAP mean-field equations is also a fixed-point of the EP algorithm (Minka,<br />2001a). This theoretical result was confirmed in our numerical simulations. However, the EP algo-<br />rithm is more practical and typically much faster. For this reason, we are not going to treat the TAP<br />method as an independent algorithm in this paper.<br />¬ihas to be solved<br />¬i= Kii, called (TAPnaive) in the sequel.<br />∗) at a test point x∗based on the parameters (α,W<br />1<br />2) and according to equation (6).<br />5. KL-Divergence Minimization (KL)<br />In principle, we simply want to minimize a dissimilarity measure between the approximate posterior<br />Q(f|θ) = N (f|m,V) and the exact posterior P(f|y,X,θ). One quantity to minimize is the KL-<br />divergence<br />KL(P(f|y,X,θ) ? Q(f|θ))=<br />Z<br />P(f|y,X,θ)lnP(f|y,X,θ)<br />Q(f|θ)<br />df.<br />Unfortunately, this expression is intractable. If instead, we measure the reverse KL-divergence, we<br />regain tractability<br />KL(Q(f|θ) ? P(f|y,X,θ))=<br />Z<br />N (f|m,V)lnN (f|m,V)<br />P(f|y,X,θ)df =: KL(m,V).<br />2046</p>  <p>Page 13</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />A similar approach has been followed for regression with Laplace or Cauchy noise (Opper and<br />Archambeau, 2008). Finally, we minimize the following objective (see Appendix B.3) with respect<br />to the variables m and V. Constant terms have been dropped from the expression:<br />KL(m,V)<br />c= −<br />Z<br />N (f)<br />?<br />n<br />∑<br />i=1<br />lnsig(√viiyif +miyi)<br />?<br />df −1<br />2ln|V|+1<br />2m?K−1m+1<br />2tr?K−1V?.<br />We refer to the first term of KL(m,V) as a(m,V) to keep the expressions short. We calculate first<br />derivatives and equate them with zero to obtain necessary conditions that have to be fulfilled at a<br />local optimum (m∗,V∗)<br />∂KL<br />∂m<br />∂KL<br />∂V<br />=<br />∂a<br />∂m−K−1m = 0 ⇒ K−1m =∂a<br />∂a<br />∂V+1<br />∂m= α,<br />?<br />=<br />2V−1−1<br />2K−1= 0 ⇒ V =<br />K−1−2∂a<br />∂V<br />?−1<br />=?K−1−2Λ?−1<br />which defines Λ. If the approximate posterior is parametrized by (m,V), there are in principle in<br />the order of n2parameters. But if the necessary conditions for a local minimum are fulfilled (i.e., the<br />derivatives ∂KL/∂m and ∂KL/∂V vanish), the problem can be re-parametrized in terms of (α,Λ).<br />Since Λ = ∂a/∂V is a diagonal matrix (see Equation 17), the optimum is characterized 2n free<br />parameters. This fact was already pointed out by Manfred Opper (personal communication) and<br />Matthias Seeger (Seeger, 1999, Ch. 5.21, Eq. 5.3). Thus, a minimization scheme based on Newton<br />iterations on the joint vector ξ :=[α?,Λii]?takesO(8·n3) operations. Details about the derivatives<br />∂KL/∂ξ and ∂2KL/∂ξ∂ξ?are provided in Appendix A.2.<br />5.1 Posterior<br />Based on these local approximations, the approximate posterior can be written as:<br />P(f|y,X,θ)<br />≈ N (f|m,V) =N<br />=<br />−2Λ,<br />=<br />Kα.<br />?<br />f|m,?K−1+W?−1?<br />,<br />W<br />m<br />5.2 Log Marginal Likelihood<br />Since the method inherently maximizes a lower bound on the marginal likelihood, this bound (Equa-<br />tion 9) is used as approximation to the marginal likelihood.<br />6. Variational Bounds (VB)<br />The following variational bounding method (Gibbs and MacKay, 2000) is a special case of the KL<br />method. Instead of optimizing a bound on the joint (Eq. 8), they impose the bounding condition on<br />each likelihood term individually. Here, we treat parametrization based on quadratic lower bounds<br />on the individual likelihoods in the logarithmic domain. We first derive all calculations based on<br />2047</p>  <p>Page 14</p> <p>NICKISCH AND RASMUSSEN<br />general likelihoods. Individual likelihood bounds<br />P(yi|fi)<br />⇒ P(y|f)<br />≥<br />≥<br />exp?aif2<br />exp<br />i+biyifi+ci<br />f?Af+(b?y)?f+c?<br />?, ∀fi∈ R∀i<br />?<br />??<br />=: Q(y|f,A,b,c), ∀f ∈ R<br />are defined in terms of coefficients ai,biand ci, where ? denotes the element-wise product of two<br />vectors. This lower bound on the likelihood induces a lower bound on the marginal likelihood.<br />Z<br />Carrying out the Gaussian integral<br />Z<br />leads to (see Appendix B.4)<br />+1<br />Z =<br />P(f|X)P(y|f)df<br />≥<br />Z<br />P(f|X)Q(y|f,A,b,c)df = ZB.<br />ZB<br />=<br />N (f|0,K)exp<br />?<br />f?Af+(b?y)?f+c?<br />??<br />df<br />lnZB<br />=<br />c?<br />?<br />2(b?y)??K−1−2A?−1(b?y)−1<br />2ln|I−2AK|<br />(13)<br />which can now be maximized with respect to the coefficients ai,biand ci. In order to get an efficient<br />algorithm, one has to calculate the first and second derivatives ∂lnZB/∂ς, ∂2lnZB/∂ς∂ς?(as done<br />in Appendix A.1). Hyperparameters can be optimized using the gradient ∂lnZB/∂θ.<br />6.1 Logit Bound<br />Optimizing the logistic likelihood function (Gibbs and MacKay, 2000), we obtain the necessary<br />conditions<br />Aς<br />:=<br />−Λς,<br />1<br />2<br />ς2<br />bς<br />:=<br />?,<br />cς,i<br />:=<br />iλ(ςi)−1<br />2ςi+lnsiglogit(ςi)<br />where we define λ(ςi) =?2siglogit(ςi)−1?/(4ςi) and Λς= [λ(ςi)]ii. This shows, that we only have<br />is symmetric and tight at f = ±ς.<br />to optimize with respect to n parameters ς. We apply Newton’s method for this purpose. The bound<br />6.2 Probit Bound<br />Forreasonsofcompleteness, wederivesimilarexpressions(AppendixB.5)forthecumulativeGaus-<br />sian likelihood sigprobit(fi) with necessary conditions<br />−1<br />2<br />N (ςi)<br />sigprobit(ςi),<br />?ςi<br />which again depend only on a single vector of parameters we optimize using Newton’s method. The<br />bound is tight for f = ς.<br />aς<br />:=<br />?,<br />(14)<br />bς,i<br />:=<br />ςi+<br />cς,i<br />:=<br />2−bi<br />?<br />ςi+ln?sigprobit(ςi)?<br />2048</p>  <p>Page 15</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />6.3 Posterior<br />Based on these local approximations, the approximate posterior can be written as<br />P(f|y,X,θ)<br />≈ N (f|m,V) =N<br />=<br />−2Aς,<br />=<br />?<br />f|m,?K−1+W?−1?<br />?−1(y?bς),<br />,<br />W<br />mV(y?bς) =?K−1−2Aς<br />where we have expressed the posterior parameters directly as a function of the coefficients. Finally,<br />we deal with an approximate posterior Q(f|θ) = N (f|mς,Vς) only depending on a vector ς of<br />n variational parameters and a mapping ς ?→ (mς,Vς). In the KL method, every combination of<br />values m and W is allowed, in the VB method, mςand Vςcannot be chosen independently, since<br />the have to be compatible with the bounding requirements. Therefore, the variational posterior is<br />more constrained than the general Gaussian posterior and thus easier to optimize.<br />6.4 Log Marginal Likelihood<br />It turns out, that the approximation to the marginal likelihood (Equation 13) is often quite poor and<br />the more general Jensen bound approach (Equation 9) is much tighter. In practice, one would have<br />to evaluate both of them and keep the maximum value.<br />7. Factorial Variational Method (FV)<br />Instead of approximating the posterior P(f|y,X,θ) by the closest Gaussian distribution, one can use<br />the closest factorial distribution Q(f|y,X,θ)=∏iQ(fi), also called ensemble learning (Csató et al.,<br />2000). Another kind of factorial approximation Q(f) = Q(f+)Q(f−)—a posterior factorizing over<br />classes—is used in multi-class classification (Girolami and Rogers, 2006).<br />7.1 Posterior<br />Asaresultoffree-formminimizationoftheKullback-LeiblerdivergenceKL(Q(f|y,X,θ) ? P(f|y,X,θ))<br />by equating its functional derivative δKL/δQ(fi) with the zero function (Appendix B.6), one finds<br />the best approximation to be of the following form:<br />??µi,σ2<br />σ2<br />i<br />=<br />Z<br />In fact, the best product distribution consists of a factorial Gaussian times the original likelihood.<br />The Gaussian has the same moments as the Leave-One-Out prediction (Sundararajan and Keerthi,<br />2001). Since the posterior is factorial, the effective likelihood of the factorial approximation has an<br />odd shape. It effectively has to annihilate the correlations in the prior, and these correlations are<br />usually what allows learning to happen in the first place. However, the best fitting factorial is still<br />able to ensure that the latent means have the right signs. Even though all correlations are neglected,<br />Q(fi)<br />∝ N?fi<br />?K−1?−1<br />=<br />i<br />?P(yi|fi),<br />µi<br />=<br />mi−σ2<br />i<br />?K−1m?<br />ii,<br />fiQ(fi)dfi.<br />i= [Kα]i−σ2<br />iαi,<br />mi<br />(15)<br />2049</p>  <p>Page 16</p> <p>NICKISCH AND RASMUSSEN<br />it is still possible that the model picks up the most important structure, since the expectations are<br />coupled. Of course, at test time, it is essential that correlations are taken into account again using<br />Equation 6, as it would otherwise be impossible to inject any knowledge into the predictive dis-<br />tribution. For predictions we use the Gaussian N (f|m,Dg(v)) instead of Q(f). This is a further<br />approximation, but it allows to stay inside the Gaussian framework.<br />Parameters µiand miare found by the following algorithm. Starting from m = 0, iterate the<br />following until convergence; (1) compute µi, (2) update miby taking a step in the direction towards<br />mias given by Equation 15. Stepsizes are adapted.<br />7.2 Log Marginal Likelihood<br />Surprisingly, one can obtain a lower bound on the marginal likelihood (Csató et al., 2000):<br />n<br />∑<br />i=1<br />lnZ<br />≥<br />lnsig<br />?yimi<br />σi<br />?<br />−1<br />2α??<br />K−Dg(?σ2<br />1,...,σ2<br />n<br />??)<br />?<br />α−1<br />2ln|K|+<br />n<br />∑<br />i=1<br />lnσi.<br />8. Label Regression Method (LR)<br />Classification has also been treated using label regression or least squares classification (Rifkin and<br />Klautau, 2004). In its simplest form, this method simply ignores the discreteness of the class labels<br />at the cost of not being able to provide proper probabilistic predictions. However, we treat LR<br />as a heuristic way of choosing α and W, which allows us to think of it as yet another Gaussian<br />approximation to the posterior allowing for valid predictions of class probabilities.<br />8.1 Posterior<br />After inference, according to Equation 6, the moments of the (Gaussian approximation to the) pos-<br />terior GP can be written as µ∗= k?<br />W−1= σ2<br />and<br />we obtain GP regression from data points xi∈ X to real labels yi∈ R with noise of variance σ2<br />as a special case. In regression, the posterior moments are given by µ∗= k?<br />σ2<br />∗<br />y can be absorbed by the hyperparameters. There is an additional parameter σn, describing the width<br />of the effective likelihood. In experiments, we selected σn∈ [0.5,2] to maximise the log marginal<br />likelihood.<br />∗α and σ2<br />α =?K+W−1?−1?K+W−1?α =?K+W−1?−1y,<br />∗= k∗∗−k?<br />∗<br />?K+W−1?−1k∗. Fixing<br />nI<br />n<br />∗<br />?K+σ2<br />nI?−1y and<br />∗= k∗∗−k?<br />?K+σ2<br />nI?−1k∗(Rasmussen and Williams, 2006). The arbitrary scale of the discrete<br />8.2 Log Marginal Likelihood<br />There are two ways of obtaining an estimate of the log marginal likelihood. One can simply ignore<br />the binary nature and use the regression marginal likelihood lnZregas proxy for lnZ—an approach<br />we only mention but not use in the experiments<br />lnZreg<br />=<br />−1<br />2α??K+σ2<br />nI?α−1<br />2ln??K+σ2<br />nI??−n<br />2ln2π.<br />Alternatively, the Jensen bound (8) yields a lower bound lnZ ≥ lnZB—which seems more in line<br />with the classification scenario than lnZreg.<br />2050</p>  <p>Page 17</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />9. Relations Between the Methods<br />All considered approximations can be separated into local and global methods. Local methods<br />exploit properties (such as derivatives) of the posterior at a special location only. Global methods<br />minimize the KL-divergence KL(Q||P) =RQ(f)lnQ(f)/P(f)df between the posterior P(f) and a<br />algorithm.<br />tractable family of distributions Q(f). Often this methodology is also referred to as a variational<br />assumptionrelationconditionsapprox. posterior Q(f)<br />name<br />Q(f) =N (f|m,V)<br />→<br />m<br />W<br />=<br />=<br />argmaxfP(f)<br />−∂2lnP(y|f)<br />∂f∂f?<br />N (f|m,(K−1+W)−1)<br />LA<br />Q(f) = ∏iqi(fi)<br />→<br />?<br />δKL<br />δqi(fi)≡ 0<br />qi(fi)=?fd<br />∂KL<br />∂V,m= 0<br />∏iN (fi|µi,σ2<br />N?f|m,(K−1+W)−1?<br />N?f|m,(K−1+W)−1?<br />N?f|mς∗,(K−1+Wς∗)−1?<br />N (f|m,(K−1+σ−2<br />i)P(yi|fi)<br />FV<br />?fd<br />i<br />?<br />i<br />?<br />Q(fi)<br />EP<br />?<br />→<br />?<br />→<br />Q(f) =N (f|m,V)<br />KL<br />P(yi|fi) ≥N (fi|µςi,σ2<br />ςi)<br />∂KL<br />∂ς∗= 0<br />VB<br />P(yi|fi) :=N (fi|yi,σ2<br />n)<br />→<br />m = (I+σ2<br />nK−1)−1y<br />nI)−1)<br />LR<br />The only local method considered is the LA approximation matching curvature at the posterior<br />mode. Common tractable distributions for global methods include factorial and Gaussian distri-<br />butions. They have their direct correspondent in the FV method and the KL method. Individual<br />likelihood bounds make the VB method a more constrained and easier-to-optimize version of the<br />KL method. Interestingly, EP can be seen in some sense as a hybrid version of FV and KL, com-<br />bining the advantages of both methods. Within the Expectation Consistence framework (Opper and<br />Winther,2005), EPcanbethoughtofasanalgorithmthatimplicitlyworkswithtwodistributions—a<br />factorial and a Gaussian—having the same marginal moments?fd<br />In the divergence measure and message passing framework (Minka, 2005), EP is cast as a mes-<br />sage passing algorithm template: Iterative minimization of local divergences to a tractable family<br />of distributions yields a small global divergence. From that viewpoint, FV and KL are considered<br />as special cases with divergence measure KL(Q||P) combined with factorial and Gaussian distribu-<br />tions.<br />i<br />?. By means of iterative updates,<br />one keeps these expectations consistent and produces a posterior approximation.<br />There is also a link between local and global methods, namely from the KL to the LA method.<br />The necessary conditions for the LA method do hold on average for the KL method (Opper and<br />Archambeau, 2008).<br />Finally, LR neither qualifies as local nor global—it is just a heuristic way of setting m and W.<br />2051</p>  <p>Page 18</p> <p>NICKISCH AND RASMUSSEN<br />10. Markov Chain Monte Carlo (MCMC)<br />The only way of getting a handle on the ground truth for the moments Z, m and V is by applying<br />sampling techniques. In the limit of long runs, one is guaranteed to get the right answer. But in<br />practice, these methods can be very slow, compared to analytic approximations discussed previ-<br />ously. MCMC runs are rather supposed to provide a gold standard for the comparison of the other<br />methods.<br />It turns out to be most challenging to obtain reliable marginal likelihood estimates as it is equiv-<br />alent to solving the free energy problem in physics. We employ Annealed Importance Sampling<br />(AIS) and thermodynamic integration to yield the desired marginal likelihoods. Instead of starting<br />annealing from the prior distribution, we propose to directly start from an approximate posterior in<br />order to speed up the sampling process.<br />Accurate estimates of the first and second moments can be obtained by sampling directly from<br />the (unnormalized) posterior using Hybrid Monte Carlo methods (Neal, 1993).<br />10.1 Thermodynamic Integration<br />The goal is to calculate the marginal likelihood Z =RP(y|f)P(f|X)df. AIS (Neal, 1993, 2001)<br />denotes an inverse temperature schedule with the properties τ(0) = 0, τ(T) = 1 and τ(t +1) ≥ τ(t)<br />leading to Z0=RP(f|X)df = 1 and ZT= Z.<br />Zt/Zt−1can be approximated by importance sampling with samples fsfrom the “intermediate pos-<br />terior” P(f|y,X,t −1) := P(y|f)τ(t−1)P(f|X)/Zt−1at time t.<br />Zt<br />Zt−1<br />=<br />P(y|f)∆τ(t)P(f|y,X,t −1)df<br />1<br />S<br />∑<br />s=1<br />This works fine for small temperature changes ∆τ(t) := τ(t)−τ(t −1). In the limit, we smoothly<br />interpolate between P(y|f)0P(f|X) and P(y|f)1P(f|X), that is, we start by sampling from the prior<br />and finally approach the posterior. Note that sampling is algorithmically possible even though the<br />distribution is only known up to a constant factor.<br />works with intermediate quantities Zt:=RP(y|f)τ(t)P(f|X)df. Here, τ : N ⊃ [0,T] → [0,1] ⊂ R<br />On the other hand, we have Z = ZT/Z0= ∏T<br />t=1Zt/Zt−1—an expanded fraction. Each factor<br />=<br />RP(y|f)τ(t)P(f|X)df<br />Zt−1<br />Z<br />S<br />P(y|fs)∆τ(t),<br />=<br />Z<br />P(y|f)τ(t)<br />P(y|f)τ(t−1)<br />P(y|f)τ(t−1)P(f|X)<br />Zt−1<br />df<br />≈<br />fs∼ P(f|y,X,t −1).<br />10.2 Amelioration Using an Approximation to the Posterior<br />In practice, the posterior can be quite different from the prior. That means that individual fractions<br />Zt/Zt−1may be difficult to estimate. One can make these fractions more similar by increasing the<br />number of steps T or by “starting” from a distribution close to the posterior rather than from the<br />prior. Let Q(f) = N (f|m,V) ≈ P(f|y,X,T) = P(y|f)P(f|X)/ZTdenote an approximation to the<br />posterior. Setting N (f|m,V) = Q(y|f)P(f|X), one can calculate the effective likelihood Q(y|f) by<br />division (see Appendix B.2).<br />For the integration we use Zt=RP(y|f)τ(t)Q(y|f)1−τ(t)P(f|X)df where Z0=RQ(y|f)P(f|X)df<br />can be computed analytically. Again, each factor<br />Zt<br />Zt−1of the expanded fraction can be approximated<br />2052</p>  <p>Page 19</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />by importance sampling from the modified intermediate posterior:<br />P(f|y,X,t −1)=<br />P(y|f)τ(t−1)Q(y|f)1−τ(t−1)P(f|X)/Zt−1<br />?P(y|f)<br />=<br />Q(y|f)<br />?τ(t−1)<br />Q(y|f)P(f|X)/Zt−1,<br />Zt<br />Zt−1<br />=<br />RP(y|f)τ(t)Q(y|f)1−τ(t)P(f|X)df<br />Zt−1<br />Z<br />Z ?P(y|f)<br />1<br />S<br />∑<br />s=1<br />Q(y|fs)<br />=<br />P(y|f)τ(t)Q(y|f)1−τ(t)<br />P(y|f)τ(t−1)Q(y|f)1−τ(t−1)<br />?∆τ(t)<br />S<br />?P(y|fs)<br />P(y|f)τ(t−1)Q(y|f)1−τ(t−1)P(f|X)<br />Zt−1<br />df<br />=<br />Q(y|f)<br />P(f|y,X,t −1)df<br />?∆τ(t)<br />≈<br />,<br />fs∼ P(f|y,X,t −1).<br />ThechoiceofQ(f)tobeagoodapproximationtothetrueposteriormakesthefractionP(y|f)/Q(y|f)<br />as constant as possible, which in turn reduces the error due to the finite step size in thermodynamical<br />integration.<br />10.3 Algorithm<br />If only one sample ftis used per temperature τ(t), the value of the entire fraction is obtained as<br />ln<br />Zt<br />Zt−1<br />= ∆τ(t)[lnP(y|ft)−lnQ(y|ft)]<br />which gives rise to the full estimate<br />lnZ ≈<br />T<br />∑<br />t=1<br />ln<br />Zt<br />Zt−1<br />= lnZQ+<br />T<br />∑<br />t=1<br />∆τ(t)<br />?<br />lnP(y|ft)+1<br />2(ft− ˜ m)?W(ft− ˜ m)<br />?<br />for a single run r. The finite temperature change bias can be removed by combining results Zrfrom<br />R different runs by their arithmetic mean1<br />R∑rZr(Neal, 2001)<br />lnZ = ln<br />Z<br />P(y|f)P(f|X)df ≈ ln<br />?<br />1<br />R<br />R<br />∑<br />r=1<br />Zr<br />?<br />.<br />Finally, the only primitive needed to obtain MCMC estimates of Z, m and V is an efficient<br />sampler for the “intermediate” posterior P(f|y,X,t −1). We use Hybrid Monte Carlo sampling<br />(Neal, 1993).<br />10.4 Results<br />If the posterior is very close to the prior (as in regimes 7-9 of Figure 3), it does not make a dif-<br />ference, which we start from. However, if the posterior can be well approximated by a Gaussian<br />2053</p>  <p>Page 20</p> <p>NICKISCH AND RASMUSSEN<br />(regimes 4-6), but is sufficiently different from the prior, then the method decreases variance and<br />consequently improves runtimes of AIS. Different approximation methods lead also to differences<br />in the improvement. Namely, the Laplace approximation performs worse than the approximation<br />found by Expectation Propagation because Laplace’s method approximates around the mode which<br />can be far away from the mean.<br />For our evaluations of the approximations to the marginal likelihood, however we started the<br />algorithmfromtheprior. Otherwise, onemightbeworriedofbiasingtheMCMCsimulationtowards<br />the initial distribution in cases where the chain fails to mix properly.<br />11. Implementation<br />Implementationsofallmethodsdiscussedareprovidedat http://www.kyb.mpg.de/~hn/approxXX.<br />tar.gz. The code is designed as an extension to the Gaussian Processes for Machine Learning<br />(GPML) (Rasmussen and Williams, 2006) Matlab Code.3Approximate inference for Gaussian<br />processes is done by the binaryGP.m function, which takes as arguments the covariance func-<br />tion, the likelihood function and the approximation method. The existing GPML package provides<br />approxLA.m for Laplace’s method and approxEP.m for Expectation Propagation. These implemen-<br />tations are generic to the likelihood function. We provide cumGauss.m and logistic.m that were<br />designed to avoid numerical problems. In the extension, approxKL.m, approxVB.m, approxFV.m<br />and approxTAP.m are included, among others not discussed here, for example sparse and online<br />methods outside the scope of the current investigation. The implementations are straight-forward,<br />although special care has been taken to avoid numerical problems e.g., situations where K is close<br />to singular. More concretely, we use the well-conditioned matrix4B = W<br />its Cholesky decomposition to calculate V =?K−1+W?−1or k?<br />Especially LA and EP show a high level of robustness along the full spectrum of possible hyper-<br />parameters. KL uses Gauss-Hermite quadrature; we did not notice problems stemming therefrom.<br />The FV and TAP methods work very reliably, although, we had to add a small (10−6) ridge for FV<br />to regularize K. As a general statement, we did not observe any numerical problems for a wide<br />range of hyperparameters reaching from reasonable values to very extreme scales.<br />In addition to the code for the algorithms, we provide also a tarball containing all necessary<br />scripts to reproduce the figures of the paper. We offer two versions: The first version contains only<br />the code for running the experiments and drawing the figures.5The second version additionally<br />includes the results of the experiments.6<br />1<br />2KW<br />1<br />2+I = LL?and<br />∗<br />?K+W−1?−1k∗. The posterior<br />mean is represented in terms of α to avoid multiplications with K−1and facilitate predictions.<br />12. Experiments<br />The purpose of the experiments is to illustrate the strengths and weaknesses of the different approxi-<br />mation methods. First of all, the quality of the approximation itself in terms of posterior moments Z,<br />3. The package is available at http://www.gaussianprocess.org/gpml/code.<br />4. All eigenvalues λ of B satisfy 1 ≤ λ ≤ 1+n<br />5. The code base (∼ 9Mb) can be obtained from http://www.kyb.mpg.de/~hn/supplement_code.tar.gz.<br />6. The complete code base (∼ 400Mb) including all simulation results and scripts to generate figures is stored at<br />http://www.kyb.mpg.de/~hn/supplement_all.tar.gz.<br />4maxijKij, thus B−1and |B| can be safely computed.<br />2054</p>  <p>Page 21</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />m and V is studied. At a second level, building on the “low-level” features, we compare predictive<br />performance in terms of the predictive probability p∗given by (Equations 4 and 6):<br />p∗:= P(y∗= 1|x∗,y,X,θ)<br />≈<br />Z<br />sig(f∗)N?f∗|µ∗,σ2<br />∗<br />?df∗.<br />(16)<br />On a third level, we assess higher order properties such as the information score, describing how<br />much information the model managed to extract about the target labels, and the error rate—a binary<br />measure of whether a test input is assigned the right class. Uncertainty predictions provided by the<br />model are not captured by the error rate.<br />Accurate marginal likelihood estimates Z are a key to hyperparameter learning. In that respect,<br />Z can be seen as a high-level feature and as the “zeroth” posterior moment at the same time.<br />A summary of the whole section is provided in Table 1.<br />12.1 Data Sets<br />One main idea of the paper is to study the general behavior of approximate GP classification. Our<br />results for the different approximation methods are not specific to a particular data set but apply to a<br />wide range of application domains. This is reflected by the choice of our reference data sets, widely<br />used in the machine learning literature. Due to limited space, we don’t include the full experiments<br />on all data sets in this paper. However, we have verified that the same qualitative conclusions hold<br />for all the data sets considered. The full results are available via the web.7<br />Data set<br />Breast<br />Crabs<br />Ionosphere<br />Pima<br />Sonar<br />USPS 3 vs. 5<br />ntrain<br />300<br />100<br />200<br />350<br />108<br />767<br />ntest<br />383<br />100<br />151<br />418<br />100<br />773<br />d<br />9<br />6<br />34<br />8<br />60<br />256<br />Brief description of problem domain<br />Breast cancer8<br />Sex of Leptograpsus crabs9<br />Classification of radar returns from the ionosphere10<br />Diabetes in Pima Indians11<br />Sonar signals bounced by a metal or rock cylinder12<br />Binary sub-problem of the USPS handwritten digit data set13<br />12.2 Results<br />In the following, we report our experimental results covering posterior moments and predictive per-<br />formance. Findings for all 5 methods are provided to make the methods as comparable as possible.<br />7. See links in Footnotes 5 and 6.<br />8. Data set at http://mlearn.ics.uci.edu/databases/breast-cancer-wisconsin/.<br />9. Data set at http://www.stats.ox.ac.uk/pub/PRNN/.<br />10. Data set at http://mlearn.ics.uci.edu/databases/ionosphere/.<br />11. Data set at http://mlearn.ics.uci.edu/databases/pima-indians-diabetes/.<br />12. Datasetat<br />ftp://ftp.ics.uci.edu/pub/machine-learning-databases/undocumented/<br />connectionist-bench/sonar/.<br />13. Data set at http://www.gaussianprocess.org/gpml/data/.<br />2055</p>  <p>Page 22</p> <p>NICKISCH AND RASMUSSEN<br />Training marginals<br />−2000 200<br />−200<br />0<br />200<br />µ for LA<br />−2000200<br />−200<br />0<br />200<br />µ for EP<br />−2000 200<br />−200<br />0<br />200<br />µ for VB<br />−2000200<br />−200<br />0<br />200<br />µ for KL<br />−2000 200<br />−200<br />0<br />200<br />µ for FV<br />0 20 40<br />0<br />20<br />40<br />σ for LA<br />020 40<br />0<br />20<br />40<br />σ for EP<br />0 2040<br />0<br />20<br />40<br />σ for VB<br />02040<br />0<br />20<br />40<br />σ for KL<br />02040<br />0<br />20<br />40<br />σ for FV<br />0 0.51<br />0<br />0.5<br />1<br />p for LA<br />00.51<br />0<br />0.5<br />1<br />p for EP<br />0 0.51<br />0<br />0.5<br />1<br />p for VB<br />00.5 1<br />0<br />0.5<br />1<br />p for KL<br />0 0.51<br />0<br />0.5<br />1<br />p for FV<br />Test marginals<br />−2000200<br />−200<br />0<br />200<br />µ for LA<br />−2000200<br />−200<br />0<br />200<br />µ for EP<br />−2000200<br />−200<br />0<br />200<br />µ for VB<br />−2000 200<br />−200<br />0<br />200<br />µ for KL<br />−2000200<br />−200<br />0<br />200<br />µ for FV<br />020 40<br />0<br />20<br />40<br />σ for LA<br />020 40<br />0<br />20<br />40<br />σ for EP<br />02040<br />0<br />20<br />40<br />σ for VB<br />02040<br />0<br />20<br />40<br />σ for KL<br />02040<br />0<br />20<br />40<br />σ for FV<br />0 0.51<br />0<br />0.5<br />1<br />p for LA<br />00.5 1<br />0<br />0.5<br />1<br />p for EP<br />00.51<br />0<br />0.5<br />1<br />p for VB<br />00.51<br />0<br />0.5<br />1<br />p for KL<br />00.51<br />0<br />0.5<br />1<br />p for FV<br />Figure 6: Marginals of USPS 3 vs. 5 for a highly non-Gaussian posterior: Each row consists of<br />five plots showing MCMC ground truth on the x-axis and LA, EP, VB, KL and FV on<br />the y-axis. Based on the logistic likelihood function and the squared exponential covari-<br />ance function with parameters ln? = 2.25 and lnσf= 4.25 we plot the marginal means,<br />standard deviations and resulting predictive probabilities in rows 1-3. We are working<br />in regime 2 of Figure 3 that means the posterior is highly non-Gaussian. The upper part<br />shows marginals of training points and the lower part shows test point marginals.<br />2056</p>  <p>Page 23</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />LA EP*VB<br />logit|probit<br />lower bound<br />on indiv.<br />likelihoods<br />KLFVMCMC<br />idea<br />quadratic<br />expansion<br />around the<br />mode<br />Newton steps<br />marginal<br />moment<br />matching<br />KL minim.,<br />average w.r.t.<br />wrong Q(f)<br />best<br />free-form<br />factorial<br />sampling,<br />thermo-<br />dynamic<br />integration<br />Hybrid MC,<br />AIS<br />O(n3)<br />very slow<br />&gt;500<br />algorithm<br />iterative<br />matching<br />O(n3)<br />fast<br />10<br />Newton stepsNewton stepsfixed-point<br />iteration<br />O(n3)<br />very fast<br />4<br />complexity<br />speed<br />running<br />time<br />likelihood<br />properties<br />evidence Z<br />mean m<br />covariance<br />V<br />information<br />I<br />PRO<br />O(n3)<br />very fast<br />1<br />O(n3)<br />fast<br />8<br />O(8n3)<br />slow<br />150<br />1st-3rd log.<br />derivative<br />–<br />– –<br />–<br />N -integralslower boundsimple<br />evaluation<br />–<br />+<br />–<br />N -integrals1st log<br />derivative<br />=<br />=<br />=<br />≈<br />≈<br />≈<br />– –– – –<br />–<br />– –<br />++| – –<br />– –<br />–<br />≈ ≈| –<br />≈<br />–=<br />speedpractical<br />accuracy<br />speed<br />principled<br />method<br />overconfidence<br />speedtheoretical<br />accuracy<br />very slow<br />CON<br />mean?=mode,<br />low info I<br />strong over-<br />confidence<br />factorizing<br />approxima-<br />tion<br />Table 1: Feature summary of the considered algorithms: For each of the six algorithms under con-<br />sideration, the major properties are listed in the above table. The basic idea of the method<br />along with its computational algorithm and complexity is summarized, the requirements to<br />the likelihood functions are given, the accuracy of evidence and moment estimates as well<br />as information is outlined and some striking advantages and drawbacks are compared. Six<br />relations characterize accuracy: – – – extreme underestimation, – – heavy underestimation,<br />– underestimation, = ground truth, ≈ good approximation, + overestimation and ++ heavy<br />overestimation. Running times were calculated by running each algorithm for 9 different<br />hyperparameter regimes and both likelihoods on all data sets. An average running time<br />per data set was calculated for each method and scaled to yield 1 for LA. In the table, the<br />average of these numbers are shown. We are well aware of the fact, that these numbers<br />also depend on our Matlab implementations and choices of convergence thresholds.<br />12.2.1 MEAN m AND (CO)VARIANCE V<br />The posterior process, or equivalently the posterior distribution over the latent values f, is deter-<br />mined by its location parameter m and its width parameter V. In that respect, these two low-level<br />quantities are the basis for all further calculations. In general, one can say that the methods show<br />2057</p>  <p>Page 24</p> <p>NICKISCH AND RASMUSSEN<br />050100150<br />0<br />0.005<br />0.01<br />0.015<br />0.02<br />0.025<br />0.03<br />0.035<br />marginal # 353<br /> <br /> <br />best N(µ,σ2)<br />MC<br />LA<br />EP<br />KL<br />VB<br />Figure 7: Marginals USPS 3 vs. 5 for digit #353 ≡<br />point from Figure 6 is shown. Ground truth in terms of true marginal and best Gaus-<br />sian marginal (matching the moments of the true marginal) are plotted in gray, Gaussian<br />approximations are visualized as lines. For multivariate Gaussians N (m,V), the i-th<br />marginal is given by N ([m]i,[V]ii). Thus, the mode miof marginal i coincides with the i-<br />th coordinate of the mode of the joint [m]i. This relation does not hold for general skewed<br />distribution. Therefore, the marginal given by the Laplace approximation is not centered<br />at the mode of the true marginal.<br />: Posterior marginals for one special training<br />significant differences in the case of highly non-Gaussian posteriors (regimes 1-5 of Figure 3). Even<br />in the two-dimensional toy example of Figures 4 and 5, significant differences are apparent. The<br />means are inaccurate for LA and VB; whereas the variances are somewhat underestimated by LA<br />and KL and severely so by VB. Marginal means m and variances dg(V) for USPS 3 vs. 5 are<br />shown in Figure 6; an exemplary marginal is pictured in Figure 7 for all approximate methods and<br />the MCMC estimate. Along the same lines, a close-to-Gaussian posterior is illustrated in Figure 8.<br />We chose the hyperparameters for the non Gaussian case of Figure 6 to maximize the EP marginal<br />likelihood (see Figure 9), whereas the hyperparameters of Figure 8 were selected to yield a posterior<br />that is almost Gaussian but still has reasonable predictive performance.<br />TheLAmethodhastheprincipledweaknessofexpandingaroundthemode. Inhigh-dimensional<br />spaces, the mode can be very far away from the mean (Kuss and Rasmussen, 2005). The absolute<br />value of the mean is strongly underestimated. Furthermore, the posterior is highly curved at its<br />mode which leads to an underestimated variance, too. These effects can be seen in the first column<br />of Figures 6 and 7, although in the close-to-Gaussian regime LA works well, Figure 8. For large<br />latent function scales σ2<br />proaches the origin and the curvature at the mode becomes larger. Thus the approximate posterior<br />as found by LA becomes a zero-mean Gaussian which is much too narrow.<br />The EP method almost perfectly agrees with the MCMC estimates, second column of Figure<br />6. That means, iterative matching of approximate marginal moments leads to accurate marginal<br />moments of the posterior.<br />The KL method minimizes the KL-divergence KL(Q(f) ? P(f)) =RQ(f)lnQ(f)<br />P(f) is very small, Q(f) has to be very small as well. In the limit that means P(f) = 0 ⇒ Q(f) = 0.<br />f, in the limit σ2<br />f→ ∞, the likelihood becomes a step function, the mode ap-<br />P(f)df with the av-<br />erage taken to the approximate distribution Q(f). The method is zero-forcing i.e., in regions where<br />2058</p>  <p>Page 25</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />Training ≈ Test marginals<br />µ for EP<br />−4 −2024<br />−4<br />−2<br />0<br />2<br />4<br />µ for LA<br />−4 −2024<br />−4<br />−2<br />0<br />2<br />4<br />−4 −2024<br />−4<br />−2<br />0<br />2<br />4<br />µ for VB<br />−4 −2024<br />−4<br />−2<br />0<br />2<br />4<br />µ for KL<br />−4 −2024<br />−4<br />−2<br />0<br />2<br />4<br />µ for FV<br />00.2 0.4 0.6<br />0<br />0.2<br />0.4<br />0.6<br />σ for LA<br />00.2 0.4 0.6<br />0<br />0.2<br />0.4<br />0.6<br />σ for EP<br />00.2 0.4 0.6<br />0<br />0.2<br />0.4<br />0.6<br />σ for VB<br />00.2 0.4 0.6<br />0<br />0.2<br />0.4<br />0.6<br />σ for KL<br />00.2 0.4 0.6<br />0<br />0.2<br />0.4<br />0.6<br />σ for FV<br />0 0.51<br />0<br />0.5<br />1<br />p for LA<br />00.51<br />0<br />0.5<br />1<br />p for EP<br />00.51<br />0<br />0.5<br />1<br />p for VB<br />00.51<br />0<br />0.5<br />1<br />p for KL<br />00.51<br />0<br />0.5<br />1<br />p for FV<br />Figure 8: Marginals of USPS 3 vs. 5 for a close-to-Gaussian posterior: Using the squared ex-<br />ponential covariance and the logistic likelihood function with parameters ln? = 3 and<br />lnσf= 0.5, we plot the marginal means, standard deviations and resulting predictive<br />probabilities in rows 1-3. Only the quantities for the trainings set are shown, because the<br />test set results are very similar. We are working in regime 8 of Figure 3 that means the<br />posterior is of rather Gaussian shape. Each row consists of five plots showing MCMC<br />ground truth on the x-axis and LA, EP, VB, KL and FV on the y-axis.<br />Thus, the support of Q(f) is smaller than the support of P(f) and hence the variance is underesti-<br />mated. Typically, the posterior has a long tail away from zero as seen in Figure 3 regimes 1-5. The<br />zero forcing property shifts the mean of the approximation away from the origin, which results in a<br />slightly overestimated mean, fourth column of Figure 6.<br />Finally, the VB method can be seen as a more constrained version of the KL method with<br />deteriorated approximation properties. The variance underestimation and mean overestimation is<br />magnified, third column of Figure 6. Due to the required lower bounding property of each individual<br />likelihood term, the approximate posterior has to obey severe restrictions. Especially, the lower<br />bound to the cumulative Gaussian cannot adjust its width since the asymptotic behavior does not<br />depend on the variational parameter (Equation 14).<br />The FV method has a special rôle because it does not lead to a Gaussian approximation to<br />the posterior but to the closest (in terms of KL-divergence) factorial distribution. If the prior is<br />quite isotropic (regimes 1,4 and 7 of Figure 3), the factorial approximation provides a reasonable<br />approximation. If the latent function values are correlated, the approximation fails. Because of<br />the zero forcing property, mentioned in the discussion of the KL method, both the means and the<br />variances are underestimated. Since a factorial distribution cannot capture correlations, the effect<br />can be severe. It is worth mentioning that there is no difference whether the posterior is close to a<br />2059</p>  <p>Page 26</p> <p>NICKISCH AND RASMUSSEN<br />Gaussian or not. In that respect, the FV method complements the LA method, which has difficulties<br />in regimes 1, 2 and 4 of Figure 3.<br />12.2.2 PREDICTIVE PROBABILITY p∗AND INFORMATION SCORE I<br />Low-level features like posterior moments are not a goal per se, they are only needed for the purpose<br />of calculating predictive probabilities. Figures 4 and 6 show predictive probabilities in the last row.<br />In principle, a bad approximation in terms of posterior moments can still provide reasonable<br />predictions. Consider the predictive probability from Equation 16 using a cumulative Gaussian<br />likelihood<br />Z<br />It is easy to see that the predictive probability p∗is constant if µ∗/?1+σ2∗is constant. That<br />while keeping the sign of µ∗fixed, does not affect the probabilistic prediction. In the limit of large<br />µ∗and large σ∗, rescaling does not change the prediction.<br />Summarizing all predictive probabilities piwe consider the scaled information score I. As a<br />baseline model we use the best model ignoring the inputs xi. This model simply returns predictions<br />matching the class frequencies of the training set<br />p∗<br />=<br />sigprobit(f∗)N (f∗|µ∗,σ2<br />∗)df∗= sigprobit(µ∗/<br />?<br />1+σ2∗).<br />means, moving mean µ∗and standard deviation σ∗along the hyperbolic curve µ2<br />∗/C2−σ2<br />∗= 1,<br />B<br />=<br />−<br />∑<br />y={+1,−1}<br />ny<br />test<br />n+1<br />test+n−1<br />test<br />log2<br />ny<br />train<br />n+1<br />train+n−1<br />train<br />≤ 1[bit].<br />We take the difference between the baseline B (entropy) and the average negative log predictive<br />probabilities log2P(y∗|x∗,y,X) to obtain the information score<br />I<br />=<br />B+<br />1<br />2ntest<br />ntest<br />∑<br />i=1<br />(1+yi)log2(pi)+(1−yi)log2(1− pi),<br />which is 1[bit] for perfect (and confident) prediction and 0[bits] for random guessing (for equiprob-<br />able classes). Figures 9(c), 10(middle) and 11(c) contain information scores for 5 different approx-<br />imation methods on two different data sets as a function of the hyperparameters of the covariance<br />function. According to the EP and KL plots (most prominently in Figure 11(c)), there are two<br />strategies for a model to achieve good predictive performance:<br />• Find a good length scale ? (e.g., ln? ≈ 2) and choose a latent function scale σfabove some<br />threshold (e.g., lnσf&gt; 3).<br />• Start from a good set of hyperparameters (e.g., ln? ≈ 2, lnσf≈ 2) and compensate a harder<br />cutting likelihood (σ2<br />f↑) by making the data points more similar to each other (?2↑).<br />The LA method heavily underestimates the marginal means in the non-Gaussian regime (regimes<br />1-5 of Figure 3). As a consequence, the predictive probabilities are strongly under-confident in the<br />non-Gaussian regime, first column of Figure 6. The information score’s value is too small in the<br />non-Gaussian regime, Figures 9(c) and 11(c).<br />2060</p>  <p>Page 27</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />−130<br />−130<br />−115<br />−115<br />−105<br />−105<br />−100<br />−200<br />−200<br />−160<br />−160<br />log Evidence for LA<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−130<br />−130<br />−115<br />−115<br />−105<br />−105<br />−100<br />−100<br />−95<br />−92<br />−200<br />−200<br />−160<br />−160<br />log Evidence for EP<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−130<br />−130<br />−115<br />−115<br />−160<br />−105<br />−105<br />−100<br />−200<br />−200<br />−160<br />log Evidence for KL<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−200<br />−200<br />−160<br />−160<br />log Evidence for VB<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />(a) Evidence<br />−130<br />−130<br />−115<br />−115<br />−200<br />−200<br />−200<br />−160<br />−160<br />−160<br />log Evidence for LA<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−130<br />−130<br />−160<br />−115<br />−115<br />−105<br />−105<br />−100<br />−200<br />−200<br />−160<br />log Evidence for EP<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−130<br />−130<br />−115<br />−115<br />−105<br />−105<br />−100<br />−200<br />−200<br />−160<br />−160<br />log Evidence for KL<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−130<br />−130<br />−200<br />−200<br />−200<br />−160<br />−160<br />log Evidence for VB<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />(b) Lower bound on evidence<br />0.7<br />0.7<br />0.7<br />0.8<br />0.8<br />0.84<br />0.25<br />0.25<br />0.5<br />0.5<br />0.5<br />Information [bits] for LA<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />0.7<br />0.7<br />0.8<br />0.8<br />0.84<br />0.84<br />0.86<br />0.86<br />0.88<br />0.25<br />0.5<br />0.5<br />Information [bits] for EP<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />0.7<br />0.7<br />0.8<br />0.8<br />0.84<br />0.84<br />0.86<br />0.86<br />0.88<br />0.89<br />0.25<br />0.5<br />1<br />0.5<br />Information [bits] for KL<br />ln(l)<br />ln(σf)<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />0.7<br />0.7<br />0.7<br />0.8<br />0.8<br />0.8<br />0.84<br />0.84<br />0.84<br />0.86<br />0.86<br />0.88<br />0.88<br />0.89<br />0.25<br />0.5<br />0.5<br />Information [bits] for VB<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />(c) Information in bits<br />16<br />18<br />18<br />18<br />20<br />20<br />20<br />25<br />25<br />25<br />30<br />30<br />30<br />30<br />35<br />35<br />35<br />40<br />45<br />40<br />45<br />50<br />50<br />No test errors for LA<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />18<br />18<br />18<br />18<br />20<br />20<br />25<br />30<br />25<br />30<br />35<br />40<br />3<br />45<br />50<br />No test errors for EP<br />ln(l)<br />ln(σf)<br />1245<br />0<br />1<br />2<br />3<br />4<br />5<br />18<br />18<br />18<br />18<br />18<br />20<br />25<br />25<br />30<br />30<br />35<br />40<br />45<br />50<br />No test errors for KL<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />16<br />18<br />20<br />20<br />25<br />25<br />30<br />30<br />30<br />30<br />35<br />35<br />40<br />45<br />50<br />4<br />No test errors for VB<br />ln(l)<br />ln(σf)<br />1235<br />0<br />1<br />2<br />3<br />4<br />5<br />(d) Number of errors<br />Figure 9: Evidence and classification performance for LA, EP, KL &amp; VB on USPS 3 vs. 5: The<br />length scale ? and the latent scale σfdetermine the working regime (1-9) of the Gaussian<br />Process as drafted in Figure 3. We use the logistic likelihood and the squared exponential<br />covariance function to classify handwritten digits. The four panels illustrate the model<br />performance in terms of evidence, information and classification errors over the space<br />of hyperparameters (?,σf). For better visibility we choose a logarithmic scale of the<br />axes. Panel (a) shows the inherent evidence approximation of the four methods and panel<br />(b) contains the Jensen lower bound (Equation 9) on the evidence used in KL method.<br />Both panels share the same contour levels for all four methods. Note that for the VB<br />method, the general lower bound is a better evidence estimate than the bound provided<br />by the method itself. Panel (c) and (d) show the information score and the number of<br />misclassifications. One can read-off the divergence between posterior and approximation<br />by recalling KL(Q||P) = lnZ −lnZBfrom Equation 10 and assuming lnZEP≈ lnZ. In<br />the figure this corresponds to subtracting Subplots (b, LA-VB) from Subplots (a, EP).<br />Obviously, the divergence vanishes for close-to-Gaussian posteriors (regimes 3,5-6,7-9).<br />2061</p>  <p>Page 28</p> <p>NICKISCH AND RASMUSSEN<br />−250<br />−750<br />−500<br />−400<br />−300<br />log Evidence for FV<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />0.7<br />0.7<br />0.8<br />0.84<br />0.8<br />0.86<br />0.88<br />0.25<br />0.5<br />2<br />Information [bits] for FV<br />ln(l)<br />ln(σf)<br />1345<br />0<br />1<br />2<br />3<br />4<br />5<br />16<br />16<br />18<br />18<br />18<br />20<br />25<br />25<br />30<br />35<br />4<br />30<br />30<br />35<br />30<br />35<br />40<br />3<br />40<br />45<br />50<br />45<br />50<br />No test errors for FV<br />ln(l)<br />ln(σf)<br />125<br />0<br />1<br />2<br />3<br />4<br />5<br />Figure 10: Evidence and classification performance for FV on USPS 3 vs. 5: The plots are a sup-<br />plement to Figure 9 in that they make the factorial variational method comparable, even<br />though we use the cumulative Gaussian likelihood. The levels of the contour lines for<br />the information score and the number of misclassifications are the same as in Figure 9.<br />For the marginal likelihood other contours are shown, since it has significantly different<br />values.<br />Since the EP algorithm yields marginal moments very close to the MCMC estimates (second<br />column of Figure 6), its predictive probabilities and information score is consequently also very<br />accurate, Figures 9(c) and 11(c). The plots corresponding to EP can be seen as the quasi gold<br />standard (Kuss and Rasmussen, 2005, Figures 4 and 5).<br />The KL method slightly underestimates the variance and slightly overestimates the mean which<br />leads to slightly overconfident predictions, fourth column of Figure 6. Overconfidence, in general,<br />leads to a degradation of the information score, however in this example, the information score is<br />very close to the EP values and at the peak it is even slightly (0.01[bits]) higher, Figures 9(c) and<br />11(c).<br />The VB method, again, has the same problems as the KL method only amplified. The predic-<br />tions are overconfident, third column of Figure 6. Consequently, the information measured score<br />in the non-Gaussian regime is too small. The logistic likelihood function (Figure 9(c)) yields much<br />better results than the cumulative Gaussian likelihood function (Figure 11(c)).<br />Finally, as the FV method is accurate if the prior is isotropic, predictive probabilities and in-<br />formation scores are very high in regimes 1, 4 and 7 of Figure 3. For correlated priors, the FV<br />method achieves only low information scores, Figure 10(middle). The method seems to benefit<br />from the “hyperbolic scaling invariance” of the predictive probabilities mentioned earlier in that<br />section because both the mean and the variance are strongly underestimated.<br />12.2.3 NUMBER OF ERRORS E<br />If one is only interested in the actual class and not in the associated confidence level, one can simply<br />measure the number of misclassifications. Results for 5 approximation methods and 2 data sets are<br />shown in Figures 9(d), 10(right) and 11(d).<br />Interestingly, all four Gaussian approximation have very similar error rates. The reason is<br />mainly due to the fact that all methods manage to compute the right sign of the marginal mean.<br />Only the FV method with cumulative Gaussian likelihood seems a bit problematic, even though the<br />2062</p>  <p>Page 29</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />−65<br />−65<br />−65<br />−70<br />−60<br />−60<br />−60<br />−80<br />−75<br />−75<br />−70<br />−70<br />log Evidence for LA<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−65<br />−65<br />−60<br />−60<br />−60<br />−55<br />−55<br />−75<br />−70<br />−70<br />log Evidence for EP<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−65<br />−65<br />−65<br />−60<br />−60<br />−60<br />−55<br />−80<br />−75<br />−75<br />−70<br />−70<br />−70<br />log Evidence for KL<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−80<br />−75<br />−80<br />−75<br />−70<br />−70<br />−75<br />log Evidence for VB<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />(a) Evidence<br />−65<br />−65<br />−65<br />−60<br />−60<br />−60<br />−80<br />−80<br />−75<br />−75<br />−75<br />−70<br />−70<br />−70<br />log Evidence for LA<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−65<br />−65<br />−65<br />−70<br />−60<br />−60<br />−60<br />−80<br />−75<br />−70<br />−80<br />−75<br />−70<br />−75<br />log Evidence for EP<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−65<br />−65<br />−65<br />−60<br />−60<br />−60<br />−55<br />−80<br />−75<br />−75<br />−70<br />−70<br />−70<br />log Evidence for KL<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />−65<br />−65<br />−65<br />−70<br />−80<br />−70<br />−80<br />−70<br />−75<br />−75<br />−75<br />log Evidence for VB<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />(b) Lower bound on evidence<br />0.3<br />0.3<br />0.3<br />0.05<br />0.05<br />0.1<br />0.1<br />0.2<br />0.2<br />0.2<br />Information [bits] for LA<br />ln(l)<br />ln(σf)<br />12345<br />0<br />1<br />2<br />3<br />4<br />5<br />0.3<br />0.3<br />0.3<br />0.4<br />0.4<br />0.05<br />4<br />0.1<br />4<br />0.1<br />0.2<br />0.2<br />0.2<br />Information [bits] for EP<br />ln(l)<br />ln(σf)<br />1235<br />0<br />1<br />2<br />3<br />5<br />0.3<br />0.3<br />0.2<br />0.3<br />0.4<br />0.4<br />0.5<br />0.05<br />0.1<br />1<br />0.1<br />0.1<br />0.2<br />0.2<br />Information [bits] for KL<br />ln(l)<br />ln(σf)<br />12345<br />0<br />2<br />3<br />4<br />5<br />0.3<br />0.3<br />0.3<br />0.4<br />0.05<br />0.05<br />0.05<br />0.1<br />0.1<br />0.1<br />0.2<br />1<br />0.2<br />0.2<br />Information [bits] for VB<br />ln(l)<br />ln(σf)<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />(c) Information in bits<br />13<br />15<br />15<br />17<br />17<br />20<br />1<br />20<br />25<br />25<br />25<br />25<br />25<br />30<br />35<br />No test errors for LA<br />ln(l)<br />ln(σf)<br />2345<br />0<br />1<br />2<br />3<br />4<br />5<br />15<br />17<br />17<br />20<br />25<br />20<br />20<br />25<br />25<br />30<br />2<br />30<br />35<br />No test errors for EP<br />ln(l)<br />ln(σf)<br />1345<br />0<br />1<br />2<br />3<br />4<br />5<br />15<br />17<br />20<br />25<br />20<br />25<br />30<br />25<br />30<br />2<br />30<br />35<br />No test errors for KL<br />ln(l)<br />ln(σf)<br />1345<br />0<br />1<br />2<br />3<br />4<br />5<br />13<br />15<br />15<br />17<br />17<br />20<br />1<br />20<br />25<br />25<br />25<br />25<br />25<br />30<br />2<br />35<br />No test errors for VB<br />ln(l)<br />ln(σf)<br />345<br />0<br />1<br />2<br />3<br />4<br />5<br />(d) Number of errors<br />Figure 11: Evidence and classification performance for LA, EP, KL &amp; VB on Sonar: We show the<br />same quantities as in Figure 9, only for the Sonar Mines versus Rocks data set and using<br />the cumulative Gaussian likelihood function.<br />difference is only very small. Small error rates do not imply high information scores, it is rather the<br />other way round. In Figure 9(d) at ln? = 2 and lnσf= 4 only 16 errors are made by the LA method<br />while the information score (Figure 9(c)) is only of 0.25[bits].<br />Even the FV method yields very accurate classes, having only small error rates.<br />2063</p>  <p>Page 30</p> <p>NICKISCH AND RASMUSSEN<br />12.2.4 MARGINAL LIKELIHOOD Z<br />Agreement of model and data is typically measured by the marginal likelihood Z. Hyperparameters<br />can conveniently be optimized using Z not least because the gradient∂lnZ<br />efficiently computed for all methods. Formally, the marginal likelihood is the volume of the product<br />of prior and likelihood. In classification, the likelihood is a product of sigmoid functions (Figure<br />3), so that only the orthant {f|f?y ≥ 0 ∈ Rn} contains values P(f|y) ≥1<br />are bounded by lnZ ≤ 0 where lnZ = 0 corresponds to a perfect model. As pointed out in Section<br />2.1.1, the marginal likelihood for a model ignoring the data and having equiprobable targets has the<br />value lnZ = −nln2, which serves as a baseline.<br />Evidences provided by LA, EP and VB for two data sets are shown in Figures 9(a), 10(left) and<br />11(a). As the Jensen bound can be applied to any Gaussian approximation of the posterior, we also<br />report it in Figures 9(b) and 11(b).<br />The LA method strongly underestimates the evidence in the non-Gaussian regime, because it is<br />forced to center its approximation at the mode, Figures 9(a) and 11(a). Nevertheless, there is a good<br />agreement between the value of the marginal likelihood and the corresponding information score.<br />The Jensen lower bound is not tight for the LA approximation, Figures 9(b) and 11(b).<br />The EP method yields the highest values among all other methods. As described in Section<br />2.1.2, for high latent function scales σ2<br />behavior is only to be seen for the EP method, Figures 9(a) and 11(a). Again, the Jensen bound<br />is not tight for the EP method, Figures 9(b) and 11(b). The difference between EP and MCMC<br />marginal likelihood estimate is vanishingly small (Kuss and Rasmussen, 2005, Figures 4 and 5).<br />The KL method directly uses the Jensen bound (Equation 8) which can only be tight for Gaus-<br />sian posterior distributions. If the posterior is very skew, the bound inherently underestimates the<br />marginal likelihood. Therefore, Figures 9(a) and 9(b) and Figures 11(a) and 11(b) show the same<br />values. The disagreement between information score and marginal likelihood makes hyperparame-<br />ter selection based on the KL method problematic.<br />The VB method’s lower bound on the evidence turns out to be very loose, Figures 9(a) and<br />11(a). Theoretically, it cannot be better than the more general Jensen bound due to the additional<br />constraints imposed by the individual bound on each likelihood factor, Figures 9(b) and 11(b). In<br />practice, one uses the Jensen bound for hyperparameter selection. Again, the maximum of the<br />bound to the evidence is not very helpful for finding regions of high information score.<br />Finally, the FV method only yields a poor approximation to the marginal likelihood due to the<br />factorial approximation, Figure 10. The more isotropic the model becomes (small ?), the tighter<br />is the bound. For strongly correlated priors (large ?) the evidence drops even below the baseline<br />lnZ =−nln2. Thus, the bound is not adequate to do hyperparameter selection as its maximum does<br />not lie in regions with high information score.<br />∂θcan be analytically and<br />2. In principle, evidences<br />f, the model becomes effectively independent of σ2<br />f. This<br />12.2.5 CHOICE OF LIKELIHOOD<br />In the experiments, we worked with two different likelihood functions, namely the logistic and<br />the cumulative Gaussian likelihood. The two functions differ in their slope at the origin and their<br />asymptotic behavior. We did not find empirical evidence supporting the use of either likelihood.<br />Theoretically, the cumulative Gaussian likelihood should be less robust against outliers due to the<br />quadratic asymptotics. Practically, the different slopes result in a shift of the latent function length<br />scale in the order of ln1<br />4−ln<br />1<br />√2π≈ 0.46 on a log scale in that the logistic likelihood prefers a<br />2064</p>  <p>Page 31</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />bigger latent scale. Only for the VB method, differences were significant because the logistic bound<br />is more concise. Numerically, however the cumulative Gaussian is preferable.<br />12.3 Results Across Data Sets<br />We conclude with a quantitative summary of experiments conducted on 6 data sets (breast, crabs,<br />ionosphere, diabetes, sonar, USPS 3 vs. 5), two different likelihoods (cumulative Gaussian, logistic)<br />and 8 covariance functions (linear, polynomial of degree 1-3, Matérn ν ∈ {3<br />tial and neural network) resulting in 96 trials. All 7 approximate classification methods were trained<br />on a 16×16 grid of hyperparameters to compare their behavior under a wide range of conditions.<br />We calculated the maximum (over the hyperparameter grid) amount of information, every algorithm<br />managed to extract from the data in each of the 96 trials. The table shows the number of trials, where<br />the respective algorithm had a maximum information score that was above the mean/median (over<br />the 7 methods).<br />2,5<br />2}, squared exponen-<br />Test \ Method<br /># trials, information below mean<br /># trials, information below median<br />LA<br />31<br />54<br />EP<br />0<br />0<br />KL<br />0<br />0<br />VB<br />6<br />15<br />FV<br />34<br />48<br />LR<br />92<br />96<br />TAPnaive<br />31<br />51<br />13. Conclusions<br />In the present paper we provide a comprehensive overview of methods for approximate Gaussian<br />process classification. We present an exhaustive analysis of the considered algorithms using the-<br />oretical arguments. We deliver thorough empirical evidence supporting our insights revealing the<br />strengthsandweaknessesofthealgorithms. Finally, wemakeaunifiedandmodularimplementation<br />of all methods available to the research community.<br />We are able to conclude that the Expectation Propagation algorithm is, in terms of accuracy,<br />always the method of choice, except when you cannot afford the slightly longer running time com-<br />pared to the Laplace approximation.<br />Our comparisons include the Laplace approximation and the Expectation Propagation algorithm<br />(Kuss and Rasmussen, 2005). We extend the latter to the logistic likelihood. We apply Kullback-<br />Leibler divergence minimization to Gaussian process classification and derive an efficient Newton<br />algorithm. Although the principles behind this method have been known for some time, we are<br />unaware that this method has been previously implemented for GPs in practise. The existing varia-<br />tional method (Gibbs and MacKay, 2000) is extended by a lower bound on the cumulative Gaussian<br />likelihood and we provide an implementation based on Newton’s method. Furthermore, we give a<br />detailed analysis of the Factorial Variational method (Csató et al., 2000).<br />All methods are considered in a common framework, approximation quality is assessed, predic-<br />tive performance is measured and model selection is benchmarked.<br />In practice, an approximation method has to satisfy a wide range of requirements. If runtime<br />is the major concern or one is interested in error rate only, the Laplace approximation or label<br />regression should be considered. Only Expectation Propagation and—although a lot slower—the<br />KL-method deliver accurate marginals as well as reliable class probabilities and allow for faithful<br />model selection.<br />If an application demands a non-standard likelihood function, this also affects the choice of<br />the algorithm: The Laplace approximation requires derivatives, Expectation Propagation and the<br />2065</p>  <p>Page 32</p> <p>NICKISCH AND RASMUSSEN<br />Factorial Variational method need integrability with respect to Gaussian measures. However, the<br />KL-method simply needs to evaluate the likelihood and known lower bounds naturally lead to the<br />VB algorithm.<br />Finally, if the classification problem contains a lot of label noise (σfis small), the exact pos-<br />terior distribution is effectively close to Gaussian. In that case, the choice of the approximation<br />method is not crucial since in the Gaussian regime, they will give the same answer. For weakly<br />coupled training data, the Factorial Variational method can lead to quite reasonable approximations.<br />As a future goal remains an in-depth understanding of the properties of sparse and online ap-<br />proximations to the posterior and a coverage of a broader range of covariance functions. Also, the<br />approximation techniques discussed can be applied to other non-Gaussian inference problems be-<br />sides the narrow applications to binary GP classification discussed here, and there is hope that some<br />of the insights presented may be useful more generally.<br />Acknowledgments<br />Thanks to Manfred Opper for pointing us initially to the practical possibility of the KL method and<br />the three anonymous reviewers.<br />Appendix A. Derivatives<br />In the following, we provide the expressions for the derivatives needed to implement the VB and<br />the KL method.<br />A.1 Derivatives for VB<br />Some notational remarks. Partial derivatives w.r.t. one single parameter such as<br />matrices or vectors, respectively. Lowercase letters {a,b,c}ςindicate vectors, upper case letters<br />{A,B,C}ςstand for the corresponding diagonal matrices with the vector as diagonal. The dot<br />notation applies to both lower and uppercase letters and denote derivatives w.r.t. the variational<br />parameter vector ς<br />∂Aς<br />∂ςior∂bς<br />∂ςistay<br />˙ aς<br />:=<br />?∂aςi<br />?∂2aςi<br />Dg(˙ aς).<br />∂ςi<br />?<br />i<br />=∂aς<br />∂ς, vector,<br />=∂2aς<br />∂ς2, vector,<br />¨ aς<br />:=<br />∂ς2<br />i<br />?<br />i<br />˙Aς<br />:=<br />The operators Dg : Rn→ Rn×nand dg : Rn×n→ Rnmanipulate matrix diagonals. The result of<br />Dg(x) is a diagonal matrix X containing x as diagonal, whereas dg(X) returns the diagonal of X as<br />a vector. Hence, we have Dg(dg(x)) = x, but in general dg(Dg(X)) = X does only hold true for<br />diagonal matrices.<br />2066</p>  <p>Page 33</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />A.1.1 SOME SHORTCUTS USED LATER ONWARDS<br />˜Kς<br />:=<br />?K−1−2Aς<br />Dg(y)bς= y?bς,<br />˜Kς˜bς=?K−1−2Aς<br />˜Kς<br />∂ςj<br />˜KςK−1∂K<br />∂θiK−1˜Kς(y?bς),<br />∂lς<br />∂ς?=˜Kς<br />˙bς?y?lς+dg<br />˙bς?y?lς+lς?lς? ˙ aς,<br />y?lς?∂˙bς<br />∂ςj<br />∂rς<br />∂ς?= Dg?y?˙bς+2lς? ˙ aς<br />?−1 condK small<br />=<br />K−K<br />?<br />K−1<br />2A−1<br />ς<br />?−1<br />K,<br />˜bς<br />lς<br />∂lς<br />∂ςj<br />∂lς<br />∂θi<br />˙Lς<br />:=<br />:=<br />?−1(y?bς),<br />∂ςj<br />=<br />?<br />2∂Aς<br />lς+y?∂bς<br />?<br />,<br />=<br />:=<br />?2Dg(lς)˙Aς+Dg(y)˙Bς<br />lςl?<br />?,<br />rς<br />:=<br />?<br />ς˙Aς<br />?<br />∂ςj+2lς? ˙ aς?∂lς<br />?˙Lς+Dg?lς??y?¨bς+lς? ¨ aς<br />=<br />∂rς<br />∂ςj<br />=+˙bς?y?∂lς<br />∂ςj+lς?lς?∂˙ aς<br />∂ςj,<br />˙Rς<br />:=<br />??<br />=<br />Dg?y?˙bς+2lς? ˙ aς<br />?˜KςDg?y?˙bς+2lς? ˙ aς<br />?+Dg?lς??y?¨bς+lς? ¨ aς<br />??.<br />A.1.2 FIRST DERIVATIVES W.R.T. VARIATIONAL PARAMETERS ςiYIELDING THE GRADIENT<br />lnZB<br />=<br />c?<br />ς<br />∂ci<br />∂ςi+˜b?<br />∂ci<br />∂ςi+l?<br />?∂ci<br />?∂ci<br />?∂ci<br />˙ cς+lς??˙bς?y+lς? ˙ aς<br />?<br />+1<br />2<br />˜b?<br />ς˜Kς˜bς−1<br />?<br />y?∂bς<br />∂ςi<br />+˙bς?y??˜Kς˜bς<br />+˙bς?y?lς+dg<br />+rς+dg?˜Kς˙Aς<br />2ln|I−2AςK|,<br />y?∂bς<br />∂ςi<br />+∂Aς<br />∂ςi<br />∂lnZB<br />∂ςi<br />=<br />ς˜Kς<br />?<br />+∂Aς<br />∂ςi<br />˜Kς˜bς<br />?<br />˜Kς∂Aς<br />+tr<br />?<br />(I−2AςK)−?K∂Aς<br />?<br />ς˜Kς˙Aς<br />∂ςi<br />?<br />lς,˜Kς<br />=<br />ς<br />lς<br />?<br />+tr<br />?<br />˜Kς˜bς˜b?<br />∂ςi<br />,<br />∂lnZB<br />∂ς<br />=<br />∂ςi<br />?<br />?<br />?<br />i<br />?+dg<br />?<br />?<br />??<br />?<br />+dg?˜Kς˙Aς<br />?<br />lς=<br />∂ςi<br />i<br />lςl?<br />ς˙Aς<br />?<br />+dg?˜Kς˙Aς<br />rς=<br />∂ςi<br />i<br />=<br />?+dg?˜Kς<br />2067<br />?? ˙ aς.</p>  <p>Page 34</p> <p>NICKISCH AND RASMUSSEN<br />A.1.3 SECOND DERIVATIVES W.R.T. VARIATIONAL PARAMETERS ςiYIELDING THE HESSIAN<br />?<br />∂2lnZB<br />∂ς∂ς?<br />∂ς2<br />i<br />ii<br />=<br />∂2lnZB<br />∂ςj∂ςi<br />=<br />∂2ci<br />∂ςj∂ςi+∂rς,i<br />?∂2ci<br />¨Cς+˙Rς+2?˜Kς˙Aς<br />∂ςj<br />∂ς?+2?˜Kς˙Aς<br />+tr2˜Kς∂Aς<br />∂ςj<br />???˜Kς˙Aς<br />???˜Kς˙Aς<br />˜Kς∂Aς<br />∂ςi<br />+˜Kς<br />∂2Aς<br />∂ςj∂ςi<br />?<br />,<br />=<br />?<br />+∂rς<br />??+Dg?dg(˜Kς)? ¨ aς<br />??+Dg?dg(˜Kς)? ¨ aς<br />?<br />?.<br />A.1.4 MIXED DERIVATIVES W.R.T. HYPER- θiAND VARIATIONAL PARAMETERS ςi<br />∂2lnZB<br />∂θi∂ς<br />=<br />˙ aς?<br />∂<br />∂θi<br />?<br />?lς?lς+dg?˜Kς<br />2lς?∂lς<br />??+˙bς?y?∂lς<br />˜KςK−1∂K<br />∂θiK−1˜Kς<br />∂θi<br />=<br />˙ aς?<br />∂θi+dg<br />???<br />+˙bς?y?∂lς<br />∂θi.<br />A.1.5 FIRST DERIVATIVES W.R.T. HYPERPARAMETERS θi:<br />For a gradient optimization with respect to θ, we need the gradient of the objective ∂lnZB/∂θ.<br />Naïvely, the gradient is given by:<br />∂lnZB<br />∂θi<br />=<br />1<br />2<br />1<br />2l?<br />˜b?<br />ς˜KςK−1∂K<br />∂θiK−1˜Kς˜bς+tr<br />?<br />(I−2AςK)−?Aς∂K<br />(I−2AςK)−?Aς∂K<br />∂θi<br />?<br />lς=<br />ςK−1∂K<br />∂θiK−1lς+tr<br />?<br />∂θi<br />?<br />.<br />However, the optimal variational parameter ς∗depends implicitly on the actual choice of θ and one<br />has to account for that in the derivative by adding an extra “implicit” term<br />????ς=ς∗<br />theorem for continuous and differentiable functions F:<br />∂lnZB(θ,ς)<br />∂θi<br />=<br />∂lnZB(θ,ς∗)<br />∂θi<br />+<br />n<br />∑<br />j=1<br />∂lnZB(θ,ς∗)<br />∂ς∗<br />j<br />∂ς∗<br />∂θi.<br />j<br />The question of how to find an expression for∂ς∗<br />∂θcan be solved by means of the implicit function<br />F : Rp×Rn→ Rn,<br />Setting F(x,y) ≡∂lnZB<br />F(x,y) = 0<br />⇒<br />∂y<br />∂x(x) = −<br />?∂F<br />∂y(x,y(x))<br />?−1∂F<br />∂x(x,y(x))ifF(x,y(x)) = 0.<br />∂ς(θ,ς) leads to<br />∂ς∗<br />∂θ?<br />θ<br />=<br />−<br />?∂2lnZB(θ,ς∗<br />θ)<br />∂ς∂ς?<br />?−1∂2lnZB(θ,ς∗<br />θ)<br />∂θ?∂ς<br />and in turn combines to<br />∂lnZB<br />∂θi<br />????ς=ς∗<br />=<br />∂lnZB<br />∂θi<br />−<br />?∂lnZB<br />∂ς<br />???∂2lnZB<br />∂ς∂ς?<br />?−1∂2lnZB<br />∂θi∂ς<br />where all terms are known.<br />2068</p>  <p>Page 35</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />A.2 Derivatives for KL<br />The lower bound lnZBto the log marginal likelihood lnZ is given by Equation 9 as<br />lnZ<br />≥<br />= lnZB(m,V) = a(y,m,V)+1<br />2ln??VK−1??+n<br />RN (fi|mi,vii)lnsig(yifi)dfi. As a first step, we calcu-<br />2−1<br />2m?K−1m−1<br />2tr?VK−1?<br />where we used the shortcut a(y,m,V)=∑n<br />late the first derivatives of lnZBwith respect to the posterior moments m and V to derive necessary<br />conditions for the optimum by equating them with zero:<br />i=1<br />∂lnZB<br />∂V<br />=∂a(y,m,V)<br />∂V<br />∂lnZB<br />∂m<br />+1<br />2V−1−1<br />2K−1 != 0<br />⇒<br />V =<br />?<br />K−1−2Dgdg∂a<br />m = K∂a<br />∂m.<br />∂V<br />?−1<br />,<br />=∂a(y,m,V)<br />∂m<br />−K−1m<br />!= 0<br />⇒<br />These two expressions are plugged in the original expression for lnZBusing A = (I−2KΛ)−1and<br />Λ = Dgdg∂a<br />∂Vto yield:<br />lnZB(α,Λ)=<br />a?y,Kα,(K−1−2Λ)−1?+1<br />2ln|A|−1<br />2trA+n<br />2−1<br />2α?Kα.<br />Our algorithm uses the parameters α, Λ, so we calculate first and second derivatives to implement<br />Newton’s method.<br />A.2.1 FIRST DERIVATIVES W.R.T. PARAMETERS α, Λ YIELDING THE GRADIENT<br />∂lnZB<br />∂λ<br />=∂a<br />∂λ+dg(V)−dg(VA?)<br />and<br />∂lnZB<br />∂α<br />=∂a<br />∂α−Kα.<br />Only the terms containing derivatives of a need further attention, namely<br />∂a<br />∂α= K∂a<br />∂m<br />and<br />d(dgV)=<br />dg<br />?<br />d?K−1−2Λ?−1?<br />2(V?V)dλ ⇒∂dgV<br />2(V?V)∂a(y,m,V)<br />= 2dg[VdΛV] = 2dg<br />?<br />∑<br />k<br />vkv?<br />kdλk<br />?<br />= 2∑<br />k<br />(vk?vk)dλk<br />=<br />∂λ?= 2V?V,<br />∂a<br />∂λ<br />=<br />∂dgV<br />.<br />As a last step, the derivatives w.r.t. m and the diagonal part of V yield<br />2069</p>  <p>Page 36</p> <p>NICKISCH AND RASMUSSEN<br />∂a<br />∂mi<br />=<br />Z∂N (f|mi,vii)<br />1<br />√vii<br />∂mi<br />f ·N (f)lnsig(√viiyif +miyi)df,<br />lnsig(yif)df =<br />Z<br />f −mi<br />vii<br />N (f|mi,vii)lnsig(yif)df<br />=<br />Z<br />∂a<br />∂vii<br />=<br />Z∂N (f|mi,vii)<br />1<br />2vii<br />∂vii<br />lnsig(yif)df =<br />Z<br /><br />(f −mi)2<br />v<br />3<br />2<br />ii<br />−<br />1<br />√vii<br /><br />N (f|mi,vii)lnsig(yif)df<br />=<br />Z?f2−1?·N (f)lnsig(√viiyif +miyi)df.<br />A.2.2 SECOND DERIVATIVES W.R.T. PARAMETERS α, Λ YIELDING THE HESSIAN<br />Again, we proceed in two steps, calculating derivatives w.r.t. α and Λ and by the chain rule compute<br />those w.r.t. m and V.<br />∂2lnZB<br />∂α∂α?<br />=<br />∂2a<br />∂α∂α?+K =<br />∂<br />∂α<br />∂m?<br />∂2a<br />∂m∂m?K+K,<br />∂2a<br />∂λ∂α?=<br />∂<br />∂α<br />?<br />∂a<br />∂m?<br />∂m<br />∂α?<br />∂<br />∂m<br />?<br />?<br />+K =<br />∂<br />∂α<br />?<br />∂a<br />∂m?K<br />?<br />+K<br />=<br />?<br />∂a<br />?<br />K+K =∂m?<br />∂α<br />∂a<br />∂m?<br />?<br />K+K<br />=<br />K<br />∂2lnZB<br />∂λ∂α?<br />=<br />∂<br />∂λ<br />∂2a<br />?<br />∂a<br />∂m?<br />?<br />K =∂(dgV)?<br />∂λ<br />∂<br />∂dgV<br />?<br />∂a<br />∂m?<br />?<br />K<br />=<br />2V?V<br />∂2a<br />∂λ∂λ?+R,<br />2∂<br />∂λ<br />∂dgV∂m?K,<br />∂2lnZB<br />∂λ∂λ?<br />=<br />R := 2V?(V−AV?−VA?)<br />?<br />?<br />∂(dgV)?<br />∂2a<br />∂dgV∂(dgV)?V?V+4<br />=<br />?<br />∂a<br />∂(dgV)?V?V<br />∂2a<br />∂λ∂(dgV)?V?V+2<br />+R<br />=<br />2<br />∂a<br />∂V?V<br />∂λi<br />?<br />∂(dgV)?<br />∂a<br />∂(dgV)?<br />?<br />i<br />+R<br />=<br />2∂(dgV)?<br />∂λ<br />∂a<br />?<br />V?∂V<br />∂λi<br />??<br />???<br />i<br />+R<br />=<br />4V?V<br />∂2a<br />∂dgV∂(dgV)?V?V+8<br />?<br />?<br />V?<br />?<br />viv?<br />i<br />i<br />+R.<br />In the following, we abbreviate N (f|mi,vii) by Ni.<br />2070</p>  <p>Page 37</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />∂2a<br />∂m2<br />i<br />=<br />Z∂2Ni<br />1<br />vii<br />Z<br />1<br />3<br />2<br />ii<br />Z∂2Ni<br />1<br />4v2<br />ii<br />∂m2<br />Z?f2−1?·N (f)lnsig(√viiyif +miyi)df,<br />∂2Ni<br />∂vii∂milnsig(yif)df =<br />Z?f3−3f?·N (f)lnsig(√viiyif +miyi)df,<br />lnsig(yif)df =<br />i<br />lnsig(yif)df =<br />Z(f −mi)2−cii<br />v2<br />ii<br />Nilnsig(yif)df<br />=<br />∂2a<br />∂cii∂mi<br />=<br />Z(f −mi)3−3(f −mi)vii<br />2v3<br />ii<br />Nilnsig(yif)df<br />=<br />2v<br />∂2a<br />∂v2<br />ii<br />=<br />∂v2<br />Z?f4−6f2+3?·N (f)lnsig(√viiyif +miyi)df.<br />A.2.3 FIRST DERIVATIVES W.R.T. HYPERPARAMETERS θi:<br />ii<br />Z(f −mi)4−6vii(f −mi)2+3v2<br />ii<br />4v4<br />ii<br />Nilnsig(yif)df<br />=<br />The direct gradient is given by the following equation where we have marked the dependency of the<br />covariance K on θiby subscripts<br />∂lnZB(α,Λ)<br />∂θi<br />=<br />α?∂Kθ<br />?<br />∂θi<br />A?Λ∂Kθ<br />∂a(y,m,V)<br />∂m<br />+dg<br />?<br />?<br />A∂Kθ<br />∂θi<br />A?<br />?<br />??∂a(y,m,V)<br />−1<br />∂dgV<br />+tr<br />∂θi<br />?<br />−tr<br />A∂Kθ<br />∂θi<br />ΛA<br />2α?∂Kθ<br />∂θi<br />α.<br />Again we have would have to add an implicit term to the gradient, but in our implementation, we<br />forbore from doing so.<br />Appendix B. Auxiliary Calculations<br />In the following, we enumerate some calculations we removed from the main text in order to im-<br />prove on readability.<br />B.1 Limits of the Covariance Matrix and Corresponding Marginal Likelihood<br />We investigate the behavior of the covariance matrix K for extreme lengthscales ?. The matrix is<br />given by [K]ij= σ2<br />with g(0) = 1 and limt→∞g(t) = 0. &gt;From this definition we have [K]ii= σ2<br />|xi−xj|/? &gt; 0 for i ?= j. From<br />fg(|xi−xj|/?) where g : R → R is monotonously decreasing and continuous<br />f. We define ∆ij:=<br />lim<br />?→0[K]ij<br />lim<br />i?=j<br />=<br />lim<br />?→0σ2<br />lim<br />fg(|xi−xj|/?) = σ2<br />flim<br />∆ij→∞g(∆ij) = 0,<br />flim<br />?→∞[K]ij<br />i?=j<br />=<br />?→∞σ2<br />fg(|xi−xj|/?) = σ2<br />∆ij→0g(∆ij) = 1<br />we conclude<br />2071</p>  <p>Page 38</p> <p>NICKISCH AND RASMUSSEN<br />lim<br />?→0K<br />lim<br />=<br />σ2<br />fI,<br />?→∞K<br />=<br />σ2<br />f<br />????.<br />The sigmoids are normalized sig(−fi)+sig(fi) = 1 and the Gaussian is symmetric N (fi) =<br />N (−fi). Consequently, we have<br />Z<br />sig(yifi)N (fi|0,σ2<br />f)dfi<br />=<br />Z<br />Z0<br />Z∞<br />Z∞<br />Z∞<br />sig(fi)N (fi|0,σ2<br />f)dfi<br />=<br />−∞sig(fi)N (fi|0,σ2<br />sig(−fi)N (−fi|0,σ2<br />[sig(−fi)+sig(fi)]N (fi|0,σ2<br />1·N (fi|0,σ2<br />f)dfi+<br />Z∞<br />0<br />sig(fi)N (fi|0,σ2<br />Z∞<br />f)dfi<br />f)dfi<br />=<br />0<br />f)dfi+<br />0<br />sig(fi)N (fi|0,σ2<br />f)dfi<br />=<br />0<br />=<br />0<br />f)dfi=1<br />2.<br />The marginal likelihood is given by<br />Z<br />=<br />Z<br />Z<br />P(y|f)P(f|X,θ)df<br />n<br />∏<br />i=1<br />=<br />sig(yifi)|2πK|−1<br />2exp(−1<br />2f?K−1f)df.<br />B.1.1 LENGTHSCALE TO ZERO<br />For K = σ2<br />fI the prior factorizes and we get<br />Z?→0<br />=<br />n<br />∏<br />i=1<br />Z<br />sig(yifi)<br />1<br />?<br />2πσ2<br />f<br />exp(−f2<br />i<br />2σ2<br />f<br />)dfi<br />(17)<br />=<br />n<br />∏<br />i=1<br />1<br />2= 2−n.<br />2072</p>  <p>Page 39</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />B.1.2 LENGTHSCALE TO INFINITY<br />To get K → σ2<br />position of K is written as K = ∑n<br />f<br />????we write K = σ2<br />f1+ε2I with 1 =<br />i=1uiu?<br />????and let ε → 0. The eigenvalue decom-<br />1<br />√n<br />iλiwith u1=<br />?, λ1= σ2<br />f+ε2and all other λi= ε2<br />Z 1<br />ε<br />K=UΛU?<br />=<br />Z<br />Z<br />Z<br />Z<br />Z<br />2−n+1<br />n<br />∏<br />i=1<br />n<br />∏<br />i=1<br />n<br />∏<br />i=1<br />sig(yifi)|2πΛ|−1<br />?<br />sig<br />yi<br />?<br />n<br />?σf<br />Z<br />2−n+1<br />sig<br />2exp(−1<br />?<br />?<br /><br />2f?UΛ−1U?f)df<br />t=Λ−1<br />2U?f<br />=<br />sig<br />yi<br />?<br />?<br />f+ε2<br />λi·t?ui<br />|2πΛ|−1<br />2exp(−1<br />2t?t)<br />???Λ<br />1<br />2<br />???dt<br />=<br />?<br />λi·t?ui<br />N (ti)dt<br />=<br />sig<br /><br /><br />σ2<br />·t?<br />?<br />N (t1)<br />n<br />∏<br />i=2<br />n<br />∏<br />i=2<br />?<br />?<br />sig<br />?<br />ε·t?ui<br />??<br />N (ti)dt,<br />Z?→∞= lim<br />ε→0Z<br />=<br />sig<br />√n·t?<br />??<br />N (t1)<br />?1<br />2<br />N (ti)dt<br />(17)<br />=<br />sig<br />?σf<br />?σf<br />√n·t?<br />??<br />N (r)dr<br />N (t)dt<br />r=t?<br />=<br />?<br />Z<br />√n·r<br />?<br />(17)<br />=<br />2−n.<br />B.1.3 LATENT SCALE TO ZERO<br />We define σ2<br />f˜K = K and σf˜f = f and derive<br />Zσf<br />=<br />Z<br />Z<br />Z<br />Z<br />Z<br />n<br />∏<br />i=1<br />n<br />∏<br />i=1<br />n<br />∏<br />i=1<br />n<br />∏<br />i=1<br />n<br />∏<br />i=1<br />sig(yifi)|2πK|−1<br />2exp(−1<br />2f?K−1f)df<br />=<br />sig?yiσf˜fi<br />sig?yiσf˜fi<br />?sig?yiσf˜fi<br />?1<br />?|2πK|−1<br />???2πσ2<br />??N?˜f|0,˜K?d˜f,<br />N?˜f|0,˜K?d˜f = 2−n.<br />2exp(−σ2<br />f<br />2<br />˜f?K−1˜f)σn<br />fd˜f<br />=<br />f˜K??−1<br />2exp(−σ2<br />f<br />2<br />˜f?σ−2<br />f<br />˜K−1˜f)σn<br />fd˜f<br />=<br />Zσf→0= lim<br />σf→0Z<br />=<br />2<br />?<br />Note that the functions, we are using are all well-behaved, such that the limits do exist.<br />2073</p>  <p>Page 40</p> <p>NICKISCH AND RASMUSSEN<br />B.2 Posterior Divided by Prior = Effective Likelihood<br />Q(y|f)=<br />N (f|m,V)<br />P(f|X)<br />N?f| ˜ m,W−1?<br />(2π)−n<br />=<br />N<br />?<br />f|m,?K−1+W?−1?<br />N (f|0,K)<br />˜ m = (KW)−1m+m<br />?<br />2exp<br />−1<br />−1<br />−1<br />2(f− ˜ m)?W(f− ˜ m)<br />2˜ m??K+W−1?−1˜ m−1<br />=<br />N ( ˜ m|0,K+W−1),<br />2??W−1??−1<br />=<br />2exp<br />−1<br />?<br />2(f− ˜ m)?W(f− ˜ m)<br />2˜ m?(K+W−1)−1˜ m<br />?<br />(2π)−n<br />2|K+W−1|−1<br />exp<br />?<br />=<br />?<br />1<br />ZQexp<br />−1<br />|KW+I|<br />?<br />2(f− ˜ m)?W(f− ˜ m)<br />2˜ m?(K+W−1)−1˜ m<br />?<br />exp<br />?<br />?<br />=:<br />?<br />−1<br />?<br />,<br />lnZQ<br />=<br />2ln|KW+I|<br />B.3 Kullback-Leibler Divergence for KL method<br />We wish to calculate the divergence between the approximate posterior, a Gaussian, and the true<br />posterior<br />KL(Q(f|θ) ? P(f|y,X,θ))=<br />Z<br />Z<br />lnZ+<br />Z<br />Z<br />N (f|m,V)lnN (f|m,V)<br />P(f|y,X,θ)df<br />Z·N (f|m,V)<br />N (f|m,V)∏n<br />N (f|m,V)lnN (f|m,V)df<br />n<br />∏<br />i=1<br />N (f|m,V)lnN (f|0,K)df.<br />(2)<br />=<br />N (f|m,V)ln<br />Z<br />N (f|m,V)ln<br />i=1P(yi|fi)df<br />=<br />−<br />P(yi|fi)df<br />−<br />There are three Gaussian integrals to evaluate; the entropy of the approximate posterior and two<br />other expectations<br />KL(Q(f|θ) ? P(f|y,X,θ))=<br />lnZ−1<br />Z<br />+n<br />2ln|V|−n<br />?<br />2ln2π+1<br />2−n<br />lnsig(√viiyif +miyi)<br />2ln2π<br />−<br />N (f)<br />n<br />∑<br />i=1<br />2ln|K|+1<br />?<br />2tr?K−1V?.<br />df<br />(17)<br />2m?K−1m+1<br />2074</p>  <p>Page 41</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />Summing up and dropping the constant (w.r.t. m and V) terms, we arrive at<br />KL(m,V)<br />c= −<br />Z<br />N (f)<br />?<br />n<br />∑<br />i=1<br />lnsig(√viiyif +miyi)<br />?<br />df −1<br />2ln|V|+1<br />2m?K−1m+1<br />2tr?K−1V?.<br />B.4 Gaussian Integral for VB Lower Bound<br />ZB<br />=<br />Z<br />P(f|X)Q(y|f,A,b,c)df =<br />exp?c?<br />exp?c?<br />exp?c?<br />c?<br />Z<br />N (f|0,K)exp<br />?<br />f?Af+(b?y)?f+c?<br />?<br />??<br />df<br />=<br />? ?<br />?(2π)n|K|<br />?(2π)n|K|<br />?|I−2AK|exp<br />Z<br />?<br />exp<br />?<br />−1<br />2f??K−1−2A?f+(b?y)?f<br />(2π)n<br />|K−1−2A|exp<br />?1<br />2(b?y)??K−1−2A?−1(b?y)−1<br />df<br />=<br />? ?<br />?1<br />2(b?y)??K−1−2A?−1(b?y)<br />2(b?y)??K−1−2A?−1(b?y)<br />?<br />=<br />? ?<br />?<br />,<br />lnZB<br />=<br />?<br />+1<br />2ln|I−2AK|.<br />B.5 Lower Bound for the Cumulative Gaussian Likelihood<br />A lower bound<br />sigprobit(yifi)<br />≥<br />Q(yi|fi,ςi) = aif2<br />i+bifi+ci<br />for the cumulative Gaussian likelihood function is derived by matching the function at one point ς<br />Q(yi= +1|fi,ςi)=<br />sigprobit(ςi), ∀i<br />and by matching the first derivative<br />∂<br />∂filnQ(yi= +1|fi,ςi)<br />????ςi<br />=<br />∂lnsigprobit(yifi)<br />∂fi<br />=<br />N (ςi)<br />sigprobit(ςi), ∀i<br />at this point for a tight approximation. Solving for these constraints leads to the coefficients<br />asymptotic behavior ⇒ ai<br />=<br />−1<br />2,<br />first derivative ⇒ bi<br />=<br />ςi+<br />?ςi<br />N (ςi)<br />sigprobit(ςi),<br />?<br />point matching ⇒ ci<br />=<br />2−bi<br />ςi+logsigprobit(ςi).<br />2075</p>  <p>Page 42</p> <p>NICKISCH AND RASMUSSEN<br />B.6 Free Form Optimization for FV<br />We make a factorial approximation P(f|y,X) ≈ Q(f) := ∏iQ(fi) to the posterior by minimizing<br />KL[Q(f)||P(f)]=<br />Z<br />n<br />∏<br />i=1<br />Z<br />Q(fi)ln<br />Z·∏n<br />i=1Q(fi)<br />i=1P(yi|fi)df<br />Z<br />N (f|m,V)∏n<br />Q(fi)<br />P(yi|fi)dfi+1<br />= ∑<br />i<br />Q(fi)ln<br />2<br />n<br />∏<br />i=1<br />Q(fi)f?K−1fdf+constf.<br />Free-form optimization proceeds by equating the functional derivative with zero<br />δKL<br />δQ(fi)<br />=<br />lnQ(fi)+1−lnP(yi|fi)+1<br />2<br />δ<br />δQ(fi)<br />Z<br />n<br />∏<br />i=1<br />Q(fi)f?K−1fdf.<br />(18)<br />We abbreviate the integral in the last term with ξ and rewrite it in terms of simple one-dimensional<br />integrals ml=RflQ(fl)dfland vl=Rf2<br />ξ<br />=<br />∏<br />i<br />j,k<br />Z<br />i?=l<br />j?=l<br /><br /><br />=<br />j?=l<br />=<br />induction over l<br />= ∑<br />lj&lt;l<br />lQ(fl)dfl−m2<br />l<br />Z<br />Qi∑<br />fj<br />?K−1?<br />Ql<br />jkfkdf<br />=<br />∏<br />Qi<br />?Z<br />?<br />f2<br />l<br />?K−1?<br />ll+2fl∑<br />fj<br />?K−1?<br />jl+ ∑<br />j?=l,k?=l<br />fj<br />?K−1?<br />jkfk<br />?<br />dfl<br />?<br />df¬l<br />=<br />Z<br />∏<br />i?=l<br />Qi<br /><br />?K−1?<br />ll<br />Z<br />?<br />f2<br />lQldfl<br />??<br />mj<br />?<br />vl+m2<br />l<br />+2(∑<br />j?=l<br />fj<br />?K−1?<br />Z<br />jl)<br />Z<br />?<br />flQldfl<br />???<br />ml<br />+ ∑<br />j?=l,k?=l<br />fj<br />?K−1?<br />jkfk<br /><br />df¬l<br /><br />?K−1?<br />?K−1?<br />ll(vl+m2<br />l)+2∑<br />?K−1?<br />?K−1?<br />δRfp<br />jlml+<br />∏<br />i?=l<br />Qi ∑<br />j?=l,k?=l<br />fj<br />?K−1?<br />jkfkdf¬l<br />ll(vl+m2<br />l)+2∑<br />mj<br />jlml.<br />Plugging this into Equation 18 and using<br />lQ(fl)dfl<br />δQ(fl)<br />= fp<br />l, we find<br />δKL<br />δQ(fi)<br />=<br />lnQ(fi)+1−lnP(yi|fi)+1<br />?<br />?<br />2fi<br />?K−1?<br />?K−1?<br />?<br />iifi+ fi∑<br />?<br />l<br />?K−1?<br />ilml<br />!≡ 0<br />⇒ Q(fi)<br />∝<br />exp<br />−1<br />?????mi−<br />2fi<br />?K−1?<br />?K−1m?<br />iifi− fi∑<br />l?=i<br />ilml<br />P(yi|fi)<br />⇒ Q(fi)<br />∝ N<br />fi<br />i<br />[K−1]ii<br />,?K−1?−1<br />ii<br />P(yi|fi)<br />as the functional form of the best possible factorial approximation, namely a product of the true<br />likelihood times a Gaussian with the same precision as the prior marginal.<br />2076</p>  <p>Page 43</p> <p>APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION<br />References<br />Yasemin Altun, Thomas Hofmann, and Alex Smola. Gaussian process classification for segmenting<br />and annotating sequences. In International Conference on Machine Learning, 2004.<br />Wei Chu, Zoubin Ghahramani, Francesco Falciani, and David L. Wild. Biomarker discovery in<br />microarray gene expression data with gaussian processes. Bioinformatics, 21:3385–3393, 2005.<br />Lehel Csató, Ernest Fokoué, Manfred Opper, and Bernhard Schottky. Efficient Approaches to Gaus-<br />sian Process Classification. In Neural Information Processing Systems 12, pages 251–257. MIT<br />Press, 2000.<br />Mark N. Gibbs and David J. C. MacKay. Variational Gaussian Process Classifiers. IEEE Transac-<br />tions on Neural Networks, 11(6):1458–1464, 2000.<br />Mark Girolami and Simon Rogers. Variational Bayesian Multinomial Probit Regression with Gaus-<br />sian Process Priors. Neural Computation, 18:1790–1817, 2006.<br />Ashish Kapoor and Rosalind W. Picard. Multimodal affect recognition in learning environments.<br />In ACM international conference on Multimedia, 2005.<br />Ashish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaus-<br />sian processes for object categorization. In ICCV, 2007.<br />Malte Kuss and Carl Edward Rasmussen. Assessing Approximate Inference for Binary Gaussian<br />Process Classification. Journal of Machine Learning Research, 6:1679 – 1704, 10 2005.<br />David J. C. MacKay. Bayesian Interpolation. Neural Computation, 4(3):415–447, 1992.<br />Thomas P. Minka. Expectation Propagation for Approximate Bayesian Inference. In UAI, pages<br />362–369. Morgan Kaufmann, 2001a.<br />Thomas P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, De-<br />partment of Electrical Engineering and Computer Science, MIT, 2001b.<br />Tom Minka. Divergence Measures and Message Passing. Technical report, Microsoft Research,<br />2005.<br />Radford M. Neal. Annealed Importance Sampling. Statistics and Computing, 11:125–139, 2001.<br />Radford M. Neal. Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical<br />ReportCRG-TR-93-1, DepartmentofComputerScience, UniversityofToronto, September1993.<br />Manfred Opper and Cédric Archambeau. The Variational Gaussian Approximation Revisited. Neu-<br />ral Computation, accepted, 2008.<br />Manfred Opper and Ole Winther. Gaussian Processes for Classification: Mean Field Algorithms.<br />Neural Computation, 12(11):2655–2684, 2000.<br />Manfred Opper and Ole Winther. Expectation Consistent Approximate Inference. Journal of Ma-<br />chine Learning Research, 6:2177–2204, 2005.<br />2077</p>  <p>Page 44</p> <p>NICKISCH AND RASMUSSEN<br />William H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical<br />Recipes in C. Cambridge University Press, 2nd edition, February 1993.<br />CarlEdwardRasmussenandChristopherK.I.Williams. GaussianProcessesforMachineLearning.<br />The MIT Press, Cambridge, MA, 2006.<br />Ryan Rifkin and Aldebaro Klautau. In defense of one-vs-all classification. JMLR, 5:101–141, 2004.<br />Anton Schwaighofer, Volker Tresp, Peter Mayer, Alexander K. Scheel, and Gerhard Müller. The<br />RA scanner: Prediction of rheumatoid joint inflammation based on laser imaging. In NIPS, 2002.<br />Matthias Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds<br />and Sparse Approximations. PhD thesis, University of Edinburgh, 2003.<br />Matthias Seeger. Bayesian Methods for Support Vector Machines and Gaussian Processes. Master’s<br />thesis, Universität Karlsruhe, 1999.<br />S. Sundararajan and S. S. Keerthi. Predictive Approaches for Choosing Hyperparameters in Gaus-<br />sian Processes. Neural Computation, 13:1103–1118, 2001.<br />Christopher K. I. Williams and David Barber. Bayesian Classification with Gaussian Processes.<br />IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(20):1342–1351, 1998.<br />Mingjun Zhong, Fabien Lotte, Mark Girolami, and Anatole Lécuyer. Classifying eeg for brain<br />computer interfaces using gaussian processes. Pattern Recognition Letters, 29:354–359, 2008.<br />2078</p>  <a href="https://www.researchgate.net/profile/Hannes_Nickisch/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification/links/0c96051a5e4f9e85d3000000.pdf">Download full-text</a> </div> <div id="rgw19_56ab9ff619504" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab9ff619504">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab9ff619504"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Hannes_Nickisch/publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification/links/0c96051a5e4f9e85d3000000.pdf" class="publication-viewer" title="download.pdf">download.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Hannes_Nickisch">Hannes Nickisch</a> &middot; Jan 21, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab9ff619504"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.8505&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Approximations for Binary Gaussian Process Classification">Approximations for Binary Gaussian Process Classif...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.165.8505&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw24_56ab9ff619504" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw25_56ab9ff619504">  </ul> </div> </div>   <div id="rgw15_56ab9ff619504" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56ab9ff619504"> <div> <h5> <a href="publication/291953420_Variable_selection_in_finite_mixture_of_semi-parametric_regression_models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Variable selection in finite mixture of semi-parametric regression models</span></a>  </h5>  <div class="authors"> <a href="researcher/2068873402_Ehsan_Ormoz" class="authors ga-similar-publication-author">Ehsan Ormoz</a>, <a href="researcher/2068918282_Farzad_Eskandari" class="authors ga-similar-publication-author">Farzad Eskandari</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab9ff619504"> <div> <h5> <a href="publication/276528436_A_Robust_Extreme_Learning_Machine_for_Pattern_Classification_with_Outliers" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Robust Extreme Learning Machine for Pattern Classification with Outliers</span></a>  </h5>  <div class="authors"> <a href="researcher/9068101_Guilherme_A_Barreto" class="authors ga-similar-publication-author">Guilherme A. Barreto</a>, <a href="researcher/2058019307_Ana_Luiza_BP_Barros" class="authors ga-similar-publication-author">Ana Luiza B.P. Barros</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab9ff619504"> <div> <h5> <a href="publication/291436888_Model_uncertainty_and_reference_value_of_the_Planck_constant" class="color-inherit ga-similar-publication-title"><span class="publication-title">Model uncertainty and reference value of the Planck constant</span></a>  </h5>  <div class="authors"> <a href="researcher/2095246602_Giovanni_Mana" class="authors ga-similar-publication-author">Giovanni Mana</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw38_56ab9ff619504" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw39_56ab9ff619504">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw40_56ab9ff619504" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=E1DTcL2ARtLD20Hq6Qb5IyRxuwpaj5dWQn8KvwrMreYZM_O02LZsZ1KPM8O1tebL" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="fkPz19ElCCZX9y3LVhKzopFTKem7apDY8Drvxoe2L6WjmTEOwF8/cNlBGYCn0wQxHIFeT2FI2K0Nu9kE6YmAnfi8w+2C+SZJAa2EKax2djpvzY91NCmgwVpfiCcND2o6/Jv8XL7nMnAYZ06AupJNJ5uH2GpFw8xEzv/fdNrC8H/IjFHltm5yLcvP1cCCvmp85aT9C9HWMESJNGb3DVxKQATcseqXW1ZjVBE6ZEFlxE/hpecFvYpb3wIwcQWQZgGqMdJq9cFhpRfFusChCOjf4wXg+CWIZbNxC353htGqSTA="/> <input type="hidden" name="urlAfterLogin" value="publication/41781800_Approximations_for_Binary_Gaussian_Process_Classification"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vNDE3ODE4MDBfQXBwcm94aW1hdGlvbnNfZm9yX0JpbmFyeV9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9u"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vNDE3ODE4MDBfQXBwcm94aW1hdGlvbnNfZm9yX0JpbmFyeV9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9u"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vNDE3ODE4MDBfQXBwcm94aW1hdGlvbnNfZm9yX0JpbmFyeV9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9u"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw41_56ab9ff619504"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 507;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Hannes Nickisch","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272392765309003%401441954904967_m\/Hannes_Nickisch.png","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Hannes_Nickisch","institution":"Philips Research Hamburg","institutionUrl":false,"widgetId":"rgw4_56ab9ff619504"},"id":"rgw4_56ab9ff619504","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1853859","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab9ff619504"},"id":"rgw3_56ab9ff619504","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=41781800","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":41781800,"title":"Approximations for Binary Gaussian Process Classification","journalTitle":"Journal of Machine Learning Research","journalDetailsTooltip":{"data":{"journalTitle":"Journal of Machine Learning Research","journalAbbrev":"J MACH LEARN RES","publisher":false,"issn":"1533-7928","impactFactor":"2.47","fiveYearImpactFactor":"4.77","citedHalfLife":"8.30","immediacyIndex":"0.31","eigenFactor":"0.03","articleInfluence":"3.23","widgetId":"rgw6_56ab9ff619504"},"id":"rgw6_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1533-7928","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"10\/2008;","publicationDateRobot":"2008-10","article":"9.","journalTitle":"Journal of Machine Learning Research","journalUrl":"journal\/1533-7928_Journal_of_Machine_Learning_Research","impactFactor":2.47}},"source":{"sourceUrl":"http:\/\/edoc.mpg.de\/419931","sourceName":"OAI"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Approximations for Binary Gaussian Process Classification"},{"key":"rft.title","value":"Journal of Machine Learning Research, v.9, 2035-2078 (2008)"},{"key":"rft.jtitle","value":"Journal of Machine Learning Research, v.9, 2035-2078 (2008)"},{"key":"rft.volume","value":"9"},{"key":"rft.date","value":"2008"},{"key":"rft.issn","value":"1533-7928"},{"key":"rft.au","value":"Hannes Nickisch,Carl Edward Rasmussen"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56ab9ff619504"},"id":"rgw7_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=41781800","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":41781800,"peopleItems":[{"data":{"authorNameOnPublication":"Hannes Nickisch","accountUrl":"profile\/Hannes_Nickisch","accountKey":"Hannes_Nickisch","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272392765309003%401441954904967_m\/Hannes_Nickisch.png","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Hannes Nickisch","profile":{"professionalInstitution":{"professionalInstitutionName":"Philips Research Hamburg","professionalInstitutionUrl":false}},"professionalInstitutionName":"Philips Research Hamburg","professionalInstitutionUrl":false,"url":"profile\/Hannes_Nickisch","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272392765309003%401441954904967_l\/Hannes_Nickisch.png","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Hannes_Nickisch","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56ab9ff619504"},"id":"rgw10_56ab9ff619504","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1853859&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Philips Research Hamburg","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":41781800,"widgetId":"rgw9_56ab9ff619504"},"id":"rgw9_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1853859&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=41781800","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/43277170_Carl_Edward_Rasmussen","authorNameOnPublication":"Carl Edward Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Carl Edward Rasmussen","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/43277170_Carl_Edward_Rasmussen","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab9ff619504"},"id":"rgw12_56ab9ff619504","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=43277170&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab9ff619504"},"id":"rgw11_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=43277170&authorNameOnPublication=Carl%20Edward%20Rasmussen","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab9ff619504"},"id":"rgw8_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=41781800&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":41781800,"abstract":"<noscript><\/noscript><div>We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several approaches are elucidated theoretically, and the properties of the different algorithms are corroborated by experimental results. We examine both 1) the quality of the predictive distributions and 2) the suitability of the different marginal likelihood approximations for model selection (selecting hyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods produce good predictive distributions although their marginal likelihood approximations are poor. Strong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost always the method of choice unless the computational budget is very tight. We also extend existing methods in various ways, and provide unifying code implementing all approaches.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw13_56ab9ff619504"},"id":"rgw13_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=41781800","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw14_56ab9ff619504"},"id":"rgw14_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab9ff619504"},"id":"rgw5_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=41781800&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2068873402,"url":"researcher\/2068873402_Ehsan_Ormoz","fullname":"Ehsan Ormoz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2068918282,"url":"researcher\/2068918282_Farzad_Eskandari","fullname":"Farzad Eskandari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Communication in Statistics- Theory and Methods","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291953420_Variable_selection_in_finite_mixture_of_semi-parametric_regression_models","usePlainButton":true,"publicationUid":291953420,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.27","url":"publication\/291953420_Variable_selection_in_finite_mixture_of_semi-parametric_regression_models","title":"Variable selection in finite mixture of semi-parametric regression models","displayTitleAsLink":true,"authors":[{"id":2068873402,"url":"researcher\/2068873402_Ehsan_Ormoz","fullname":"Ehsan Ormoz","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2068918282,"url":"researcher\/2068918282_Farzad_Eskandari","fullname":"Farzad Eskandari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Communication in Statistics- Theory and Methods 02\/2016; 45(3):695-711. DOI:10.1080\/03610926.2013.835413"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291953420_Variable_selection_in_finite_mixture_of_semi-parametric_regression_models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291953420_Variable_selection_in_finite_mixture_of_semi-parametric_regression_models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab9ff619504"},"id":"rgw16_56ab9ff619504","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291953420","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":9068101,"url":"researcher\/9068101_Guilherme_A_Barreto","fullname":"Guilherme A. Barreto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2058019307,"url":"researcher\/2058019307_Ana_Luiza_BP_Barros","fullname":"Ana Luiza B.P. Barros","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Feb 2016","journal":"Neurocomputing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/276528436_A_Robust_Extreme_Learning_Machine_for_Pattern_Classification_with_Outliers","usePlainButton":true,"publicationUid":276528436,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.08","url":"publication\/276528436_A_Robust_Extreme_Learning_Machine_for_Pattern_Classification_with_Outliers","title":"A Robust Extreme Learning Machine for Pattern Classification with Outliers","displayTitleAsLink":true,"authors":[{"id":9068101,"url":"researcher\/9068101_Guilherme_A_Barreto","fullname":"Guilherme A. Barreto","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2058019307,"url":"researcher\/2058019307_Ana_Luiza_BP_Barros","fullname":"Ana Luiza B.P. Barros","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Neurocomputing 02\/2016; 176:3-13. DOI:10.1016\/j.neucom.2014.10.095"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/276528436_A_Robust_Extreme_Learning_Machine_for_Pattern_Classification_with_Outliers","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/276528436_A_Robust_Extreme_Learning_Machine_for_Pattern_Classification_with_Outliers\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab9ff619504"},"id":"rgw17_56ab9ff619504","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=276528436","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2095246602,"url":"researcher\/2095246602_Giovanni_Mana","fullname":"Giovanni Mana","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291436888_Model_uncertainty_and_reference_value_of_the_Planck_constant","usePlainButton":true,"publicationUid":291436888,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291436888_Model_uncertainty_and_reference_value_of_the_Planck_constant","title":"Model uncertainty and reference value of the Planck constant","displayTitleAsLink":true,"authors":[{"id":2095246602,"url":"researcher\/2095246602_Giovanni_Mana","fullname":"Giovanni Mana","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291436888_Model_uncertainty_and_reference_value_of_the_Planck_constant","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291436888_Model_uncertainty_and_reference_value_of_the_Planck_constant\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab9ff619504"},"id":"rgw18_56ab9ff619504","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291436888","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56ab9ff619504"},"id":"rgw15_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=41781800&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":41781800,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":41781800,"publicationType":"article","linkId":"0c96051a5e4f9e85d3000000","fileName":"download.pdf","fileUrl":"profile\/Hannes_Nickisch\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000.pdf","name":"Hannes Nickisch","nameUrl":"profile\/Hannes_Nickisch","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 21, 2016","fileSize":"2.38 MB","widgetId":"rgw21_56ab9ff619504"},"id":"rgw21_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=41781800&linkId=0c96051a5e4f9e85d3000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":41781800,"publicationType":"article","linkId":"0e608559f0c46d4f0acd3dbd","fileName":"Approximations for Binary Gaussian Process Classification","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.165.8505&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.165.8505&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw22_56ab9ff619504"},"id":"rgw22_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=41781800&linkId=0e608559f0c46d4f0acd3dbd&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56ab9ff619504"},"id":"rgw20_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=41781800&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":40,"valueFormatted":"40","widgetId":"rgw23_56ab9ff619504"},"id":"rgw23_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=41781800","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab9ff619504"},"id":"rgw19_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=41781800&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":41781800,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw25_56ab9ff619504"},"id":"rgw25_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=41781800&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":40,"valueFormatted":"40","widgetId":"rgw26_56ab9ff619504"},"id":"rgw26_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=41781800","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw24_56ab9ff619504"},"id":"rgw24_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=41781800&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Journal of Machine Learning Research 9 (2008) 2035-2078Submitted 8\/07; Revised 4\/08; Published 10\/08\nApproximations for Binary Gaussian Process Classification\nHannes Nickisch\nMax Planck Institute for Biological Cybernetics\nSpemannstra\u00dfe 38\n72076 T\u00fcbingen, Germany\nHN@TUEBINGEN.MPG.DE\nCarl Edward Rasmussen\u2217\nDepartment of Engineering\nUniversity of Cambridge\nTrumpington Street\nCambridge, CB2 1PZ, UK\nCER54@CAM.AC.UK\nEditor: Carlos Guestrin\nAbstract\nWe provide a comprehensive overview of many recent algorithms for approximate inference in\nGaussian process models for probabilistic binary classification. The relationships between several\napproaches are elucidated theoretically, and the properties of the different algorithms are corrobo-\nrated by experimental results. We examine both 1) the quality of the predictive distributions and\n2) the suitability of the different marginal likelihood approximations for model selection (selecting\nhyperparameters) and compare to a gold standard based on MCMC. Interestingly, some methods\nproduce good predictive distributions although their marginal likelihood approximations are poor.\nStrong conclusions are drawn about the methods: The Expectation Propagation algorithm is almost\nalways the method of choice unless the computational budget is very tight. We also extend existing\nmethods in various ways, and provide unifying code implementing all approaches.\nKeywords:\nGaussian process priors, probabilistic classification, Laplaces\u2019s approximation, ex-\npectation propagation, variational bounding, mean field methods, marginal likelihood evidence,\nMCMC\n1. Introduction\nGaussian processes (GPs) can conveniently be used to specify prior distributions for Bayesian infer-\nence. In the case of regression with Gaussian noise, inference can be done simply in closed form,\nsince the posterior is also a GP. For non-Gaussian likelihoods, such as e.g., in binary classification,\nexact inference is analytically intractable.\nOne prolific line of attack is based on approximating the non-Gaussian posterior with a tractable\nGaussian distribution. One might think that finding such an approximating GP is a well-defined\nproblem with a largely unique solution. However, we find no less than three different types of solu-\ntion in the recent literature: Laplace Approximation (LA) (Williams and Barber, 1998), Expectation\nPropagation (EP) (Minka, 2001a) and Kullback-Leibler divergence (KL) minimization (Opper and\nArchambeau, 2008) comprising Variational Bounding (VB) (Gibbs and MacKay, 2000) as a special\n\u2217. Also at Max Planck Institute for Biological Cybernetics, Spemannstra\u00dfe 38, 72076 T\u00fcbingen, Germany.\nc ?2008 Hannes Nickisch and Carl Edward Rasmussen."},{"page":2,"text":"NICKISCH AND RASMUSSEN\ncase. Another approach is based on a factorial approximation, rather than a Gaussian (Csat\u00f3 et al.,\n2000).\nPractical applications reflect the richness of approximate inference methods: LA has been used\nfor sequence annotation (Altun et al., 2004) and prostate cancer prediction (Chu et al., 2005), EP for\naffect recognition (Kapoor and Picard, 2005), VB for weld cracking prognosis (Gibbs and MacKay,\n2000), Label Regression (LR) serves for object categorization (Kapoor et al., 2007) and MCMC\nsampling is applied to rheuma diagnosis (Schwaighofer et al., 2002). Brain computer interfaces\n(Zhong et al., 2008) even rely on several (LA, EP, VB) methods.\nIn this paper, we compare these different approximations and provide insights into the strengths\nand weaknesses of each method, extending the work of Kuss and Rasmussen (2005) in several di-\nrections: We cover many more approximation methods (VB,KL,FV,LR), put all of them in common\nframework and provide generic implementations dealing with both the logistic and the cumula-\ntive Gaussian likelihood functions and clarify the aspects of the problem causing difficulties for\neach method. We derive Newton\u2019s method for KL and VB. We show how to accelerate MCMC\nsimulations. We highlight numerical problems, comment on computational complexity and supply\nruntime measurements based on experiments under a wide range of conditions, including different\nlikelihood and different covariance functions. We provide deeper insights into the methods behavior\nby systematically linking them to each other. Finally, we review the tight connections to methods\nfrom the literature on Statistical Physics, including the TAP approximation and TAPnaive.\nThe quantities of central importance are the quality of the probabilistic predictions and the suit-\nability of the approximate marginal likelihood for selecting parameters of the covariance function\n(hyperparameters). The marginal likelihood for any Gaussian approximate posterior can be lower\nbounded using Jensen\u2019s inequality, but the specific approximation schemes also come with their\nown marginal likelihood approximations.\nWe are able to draw clear conclusions. Whereas every method has good performance under\nsome circumstances, only a single method gives consistently good results. We are able to theoreti-\ncally corroborate our experimental findings; together this provides solid evidence and guidelines for\nchoosing an approximation method in practice.\n2. Gaussian Processes for Binary Classification\nWe describe probabilistic binary classification based on Gaussian processes in this section. For\na graphical model representation see Figure 1 and for a 1d pictorial description consult Figure 2.\nGiven data points xifrom a domain X with corresponding class labels yi\u2208 {\u22121,+1}, one would\nlike to predict the class membership probability for a test point x\u2217. This is achieved by using a\nlatent function f whose value is mapped into the unit interval by means of a sigmoid function\nsig:R\u2192[0,1] such that the class membership probability P(y = +1|x) can be written as sig(f(x)).\nThe class membership probability must normalize \u2211yP(y|x)=1, which leads to P(y = +1|x)=1\u2212\nP(y = \u22121|x). If the sigmoid function satisfies the point symmetry condition sig(t) = 1\u2212sig(\u2212t),\nthe likelihood can be compactly written as\nP(y|x)=\nsig(y\u00b7 f(x)).\n2036"},{"page":3,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nIn this paper, two point symmetric sigmoids are considered\nsiglogit(t)\n:=\n1\n1+e\u2212t\nZt\nsigprobit(t)\n:=\n\u2212\u221eN (\u03c4|0,1)d\u03c4.\nThe two functions are very similar at the origin (showing locally linear behavior around sig(0)=\n1\/2 with slope 1\/4 for siglogitand 1\/\u221a2\u03c0 for sigprobit) but differ in how fast they approach 0\/1 when\nt goes to infinity. For large negative values of t, we have the asymptotics\nsiglogit(t) \u2248 exp(\u2212t)\nLinear decay of ln(siglogit) corresponds to a weaker penalty for wrongly classified examples than\nthe quadratic decay of ln(sigprobit) .\nFor notational convenience, the following shorthands are used: The matrix X = [x1,...,xn] of\nsize n\u00d7d collects the training points, the vector y = [y1,...,yn]?of size n\u00d71 collects the target\nvalues and latent function values are summarized by f=[f1,..., fn]?with fi= f(xi). Observed data\nis written as D = {(xi,yi)|i = 1,...,n} = (X,y). Quantities carrying an asterisk refer to test points,\nthat is, f\u2217contains the latent function values for test points [x\u2217,1,...,x\u2217,m] = X\u2217\u2282 X. Covariances\nbetween latent values f and f\u2217at data points x and x\u2217follow the same notation, namely [K\u2217\u2217]ij=\nk(x\u2217,i,x\u2217,j), [K\u2217]ij= k(xi,x\u2217,j), [k\u2217]i= k(xi,x\u2217) and k\u2217\u2217= k(x\u2217,x\u2217), where [A]ijdenotes the entry\nAijof the matrix A.\nGiven the latent function f, the class labels are assumed to be Bernoulli distributed and inde-\npendent random variables, which gives rise to a factorial likelihood, factorizing over data points\n(see Figure 1)\nandsigprobit(t) \u2248 exp(\u22121\n2t2+0.158t \u22121.78), for t ? 0.\nP(y|f)=\nP(y|f) =\nn\n\u220f\ni=1\nP(yi|fi) =\nn\n\u220f\ni=1\nsig(yifi).\n(1)\nA GP (Rasmussen and Williams, 2006) is a stochastic process fully specified by a mean function\nm(x) = E[f(x)] and a positive definite covariance function k(x,x?) = V[f(x), f(x?)]. This means\nthat a random variable f(x) is associated to every x \u2208 X , such that for any set of inputs X \u2282 X,\nthe joint distribution P(f|X,\u03b8) = N (f|m0,K) is Gaussian with mean vector m0and covariance\nmatrix K. The mean function and covariance functions may depend on additional hyperparameters\n\u03b8. For notational convenience we will assume m(x) \u2261 0 throughout. Thus, the elements of K are\nKij= k(xi,xj,\u03b8).\nBy application of Bayes\u2019 rule, one gets an expression for the posterior distribution over the\nlatent values f\nP(f|y,X,\u03b8)=\nP(y|f)P(f|X,\u03b8)\nRP(y|f)P(f|X,\u03b8)df=N (f|0,K)\nP(y|X,\u03b8)\nn\n\u220f\ni=1\nsig(yifi),\n(2)\nwhere Z =P(y|X,\u03b8)=RP(y|f)P(f|X,\u03b8)df denotes the marginal likelihood or evidence for the hy-\ninputs is\nperparameter \u03b8. The joint prior over training and test latent values f and f\u2217given the corresponding\n2037"},{"page":4,"text":"NICKISCH AND RASMUSSEN\nP(f\u2217,f|X\u2217,X,\u03b8)= N\n??\nf\nf\u2217\n?????0,\nZ\n?\nK\nK?\nK\u2217\nK\u2217\u2217\n\u2217\n??\n.\nWhen making predictions, we marginalize over the training set latent variables\nZ\nwhere the joint posterior is factored into the product of the posterior and the conditional prior\n?\nFinally, the predictive class membership probability p\u2217:= P(y\u2217= 1|x\u2217,y,X,\u03b8) is obtained by aver-\naging out the test set latent variables\nP(f\u2217|X\u2217,y,X,\u03b8) =\nP(f\u2217,f|X\u2217,y,X,\u03b8)df =\nP(f\u2217|f,X\u2217,X,\u03b8)P(f|y,X,\u03b8)df,\n(3)\nP(f\u2217|f,X\u2217,X,\u03b8)= N\nf\u2217|K?\n\u2217K\u22121f,K\u2217\u2217\u2212K?\n\u2217K\u22121K\u2217\n?\n.\nP(y\u2217|x\u2217,y,X,\u03b8) =\nZ\nP(y\u2217|f\u2217)P(f\u2217|x\u2217,y,X,\u03b8)d f\u2217 =\nZ\nsig(y\u2217f\u2217)P(f\u2217|x\u2217,y,X,\u03b8)df\u2217.\n(4)\nThe integral is analytically tractable for sigprobit(Rasmussen and Williams, 2006, Ch. 3.9) and can\nbe efficiently approximated for siglogit(Williams and Barber, 1998, App. A).\nFigure 1: Graphical Model for binary Gaussian process classification: Circles represent unknown\nquantities, squares refer to observed variables. The horizontal thick line means fully\nconnected latent variables. An observed label yiis conditionally independent of all other\nnodes given the corresponding latent variable fi. Labels yiand latent function values\nfiare connected through the sigmoid likelihood; all latent function values fiare fully\nconnected, since they are drawn from the same GP. The labels yiare binary, whereas the\nprediction p\u2217is a probability and can thus have values from the whole interval [0,1].\n2.1 Stationary Covariance Functions\nIn preparation for the analysis of the approximation schemes described in this paper, we investigate\nsome simple properties of the posterior for stationary covariance functions in different regimes\n2038"},{"page":5,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nencountered in classification. Stationary covariances of the form k(x,x?,\u03b8) = \u03c32\ng : R \u2192 R a monotonously decreasing function1and \u03b8 = {\u03c3f,?} are widely used. The following\nsection supplies a geometric intuition of that specific prior in the classification scenario by analyzing\nthe limiting behavior of the covariance matrix K as a function of the length scale ? and the limiting\nbehavior of the likelihood as a function of the latent function scale \u03c3f. A pictorial illustration of the\nsetting is given in Figure 3.\nfg(|x\u2212x?|\/?) with\n2.1.1 LENGTH SCALE\nTwo limiting cases of \u201cignorance with respect to the data\u201d with marginal likelihood Z = 2\u2212ncan be\ndistinguished, where\n= [1,...1]?and I is the identity matrix (see Appendix B.1)\n?\nlim\n?\u21920K\nlim\n=\n\u03c32\nfI,\n?\u2192\u221eK\n=\n\u03c32\nf\n????.\nFor very small length scales (? \u2192 0), the prior is simply isotropic as all points are deemed to be\nfar away from each other and the whole model factorizes. Thus, the (identical) posterior moments\ncan be calculated dimension-wise. (See Figure 3, regimes 1, 4 and 7.)\nFor very long length scales (? \u2192 \u221e), the prior becomes degenerate as all datapoints are deemed\nto be close to each other and takes the form of a cigar along the hyper-diagonal. (See Figure 3,\nregimes 3, 6 and 9.) A 1d example of functions drawn from GP priors with different lengthscales ?\nis shown in Figure 2 on the left. The lengthscale has to be suited to the data; if chosen too small, we\nwill overfit, if chosen too high underfitting will occur.\n2.1.2 LATENT FUNCTION SCALE\nThe sigmoid likelihood function sig(yifi) measures the agreement of the signs of the latent function\nand the label in a smooth way, that is, values close to one if the signs of yiand fiare the same and |fi|\nis large, and values close to zero if the signs are different and |fi| is large. The latent function scale\n\u03c3fof the data can be moved into the likelihood\u02dc sig\u03c3f(t)=sig(\u03c32\nthe likelihood and finally the smoothness of the agreement by interpolation between the two limiting\ncases \u201cignorant\u201d and \u201chard cut\u201d\nft), thus \u03c3fmodels the steepness of\nlim\n\u03c3f\u21920sig(t)\nlim\n\u2261\n\u2261\n1\n2\nstep(t) :=?\n\u201cignorant\",\n\u03c3f\u2192\u221esig(t)\n0, t < 0;\n1\n2, t = 0;1, 0 <t\n\u201chard cut\".\nIn the case of very small latent scales (\u03c3f\u2192 0), the likelihood is flat causing the posterior to\nequal the prior. The marginal likelihood is again Z = 2\u2212n. (See Figure 3, regimes 7, 8 and 9.)\nIn the case of large latent scales (\u03c3f? 1), the likelihood approaches the step function. (See\nFigure 3, regimes 1, 2 and 3.) A further increase of the latent scale does not change the model\nanymore. The model is effectively the same for all \u03c3fabove a threshold.\n1. Furthermore, we require g(0) = 1 and limt\u2192\u221eg(t) = 0.\n2039"},{"page":6,"text":"NICKISCH AND RASMUSSEN\n02468 10\n\u22124\n\u22122\n0\n2\n4\na) Prior lengthscales\n02468 10\n\u22124\n\u22122\n0\n2\n4\nb) f~Prior\n0246810\n0\n0.2\n0.4\n0.6\n0.8\n1\nc) sig(f), f~Prior\n02468 10\n\u22124\n\u22122\n0\n2\n4\nd) f~Posterior, n=7\n0246810\n0\n0.2\n0.4\n0.6\n0.8\n1\ne) sig(f), n=7\n0246810\n\u22124\n\u22122\n0\n2\n4\nf) f~Posterior, n=20\n02468 10\n0\n0.2\n0.4\n0.6\n0.8\n1\ng) sig(f), n=20\nFigure 2: Pictorial illustration of binary Gaussian process classification in 1d: Plot a) shows 3 sam-\nple functions drawn from GPs with different lengthscales ?. Then, three pairs of plots\nshow distributions over functions f : R \u2192 R and sig(f) : R \u2192 [0,1] occurring in GP clas-\nsification. b+c) the prior, d+e) a posterior with n = 7 observations and f+g) a posterior\nwithn=20observationsalongwiththenobservationswithbinarylabels. Thethickblack\nline is the mean, the gray background is the \u00b1 standard deviation and the thin lines are\nsample functions. With more and more data points observed, the uncertainty is gradually\nshrunk. At the decision boundary the uncertainty is smallest.\n2.2 Gaussian Approximations\nUnfortunately, the posterior over the latent values (Equation 2) is not Gaussian due to the non-\nGaussian likelihood (Equation 1). Therefore, the latent distribution (Equation 3), the predictive\ndistribution (Equation 4) and the marginal likelihood Z cannot be written as analytical expressions.\nTo obtain exact answers, one can resort to sampling algorithms (MCMC). However, if sig is con-\ncave in the logarithmic domain, the posterior can be shown to be unimodal motivating Gaussian\napproximations to the posterior. Five different Gaussian approximations corresponding to methods\nexplained later onwards in the paper are depicted in Figure 4.\nA quadratic approximation to the log likelihood \u03c6(fi) := lnP(yi|fi) at\u02dcfi\n\u03c6(\u02dcfi)+\u03c6?(\u02dcfi)(fi\u2212\u02dcfi)+1\n\u03c6(fi)\n\u2248\n2\u03c6??(\u02dcfi)(fi\u2212\u02dcfi)2= \u22121\n2wif2\ni+bifi+constfi\nmotivates the following approximate posterior Q(f|y,X,\u03b8)\nlnP(f|y,X,\u03b8)\n(2)\n=\n\u22121\n\u22121\n\u22121\nlnN (f|m,V) =: lnQ(f|y,X,\u03b8),\n2f?K\u22121f+\n2f?K\u22121f\u22121\n2(f\u2212m)??K\u22121+W?(f\u2212m)+constf\nn\n\u2211\ni=1\nlnP(yi|fi)+constf\nquad. approx.\n\u2248\nm:=(K\u22121+W)\u22121b\n=\n2f?Wf+b?f+constf\n=\n(5)\n2040"},{"page":7,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nPrior\nl2 small\nPrior\nl2 medium\nPrior\nl2 large\nLik.\n\u03c3f\n2 large\nLik.\n\u03c3f\n2 medium\nLik.\n\u03c3f\n2 small\n123\n456\n789\nFigure 3: Gaussian Process Classification: Prior, Likelihood and exact Posterior: Nine num-\nbered quadrants show posterior obtained by multiplication of different priors and like-\nlihoods. The leftmost column illustrates the likelihood function for three different\nsteepness parameters \u03c3f and the upper row depicts the prior for three different length\nscales ?. Here, we use \u03c3f as a parameter of the likelihood. Alternatively, rows cor-\nrespond to \u201cdegree of Gaussianity\u201d and columns stand for \u201cdegree of isotropy\u201c. The\naxes show the latent function values f1= f(x1) and f2= f(x2). A simple toy exam-\nple employing the cumulative Gaussian likelihood and a squared exponential covariance\nk(x,x?) = \u03c32\ntion scales ln\u03c3f= {\u22121.5,0,1.5} is used. Two data points x1=\u221a2, x2= \u2212\u221a2 with\ncorresponding labels y1= 1, y2= \u22121 form the data set.\nfexp(\u2212?x\u2212x??2\/2?2) with length scales ln? = {0,1,2.5} and latent func-\nwhere V\u22121= K\u22121+W and W denotes the precision of the effective likelihood (see Equation 7). It\nturns out that the methods discussed in the following sections correspond to particular choices of m\nand V.\nLet us assume, we have found such a Gaussian approximation to the posterior with mean m\nand (co)variance V. Consequently, the latent distribution for a test point becomes a tractable one-\ndimensional Gaussian P(f\u2217|x\u2217,y,X,\u03b8) = N (f\u2217|\u00b5\u2217,\u03c32\nand Williams, 2006, p. 44 and 56):\n\u2217) with the following moments (Rasmussen\n\u00b5\u2217\n\u03c32\n=\n=\nk?\nk\u2217\u2217\u2212k?\n\u2217K\u22121m = k?\n\u2217\n\u2217\u03b1,\u03b1 = K\u22121m,\nk\u2217\u2217\u2212k?\n\u2217\n?K\u22121\u2212K\u22121VK\u22121?k\u2217\n=\n\u2217\n?K+W\u22121?\u22121k\u2217.\n(6)\nSince Gaussians are closed under multiplication, one can\u2014given the Gaussian prior P(f|X,\u03b8)\nand the Gaussian approximation to the posterior Q(f|y,X,\u03b8)\u2014deduce the Gaussian factor Q(y|f)\nsuch that Q(f|y,X,\u03b8) \u221d Q(y|f)P(f|X,\u03b8). Consequently, this Gaussian factor can be thought of as\nan effective likelihood. Five different effective likelihoods, corresponding to methods discussed sub-\n2041"},{"page":8,"text":"NICKISCH AND RASMUSSEN\nbest Gaussian posterior, KL=0.118\n\u221250510\n\u221210\n\u22125\n0\n5\nLA posterior, KL=0.557\n\u221250510\n\u221210\n\u22125\n0\n5\nEP posterior, KL=0.118\n\u221250510\n\u221210\n\u22125\n0\n5\nVB posterior, KL=3.546\n\u221250510\n\u221210\n\u22125\n0\n5\nKL posterior, KL=0.161\n\u221250510\n\u221210\n\u22125\n0\n5\nFigure 4: Five Gaussian Approximations to the Posterior (exact Posterior and mode in gray): Dif-\nferent Gaussian approximations to the exact posterior using the regime 2 setting of Figure\n3 are shown. The exact posterior is represented in gray by a cross at the mode and a sin-\ngle equiprobability contour line. From left to right: The best Gaussian approximation\n(intractable) matches the moments of the true posterior, the Laplace approximation does\na Taylor expansion around the mode, the EP approximation iteratively matches marginal\nmoments, the variational method maximizes a lower bound on the marginal likelihood\nand the KL method minimizes the Kullback-Leibler to the exact posterior. The axes show\nthe latent function values f1= f(x1) and f2= f(x2).\nsequently in the paper, are depicted in Figure 5. By \u201cdividing\u201d the approximate Gaussian posterior\n(see Appendix B.2) by the true Gaussian prior we find the contribution of the effective likelihood\nQ(y|f):\nQ(y|f) \u221dN (f|m,V)\nN (f|0,K)\nWe see (also from Equation 5) that W models the precision of the effective likelihood. In general, W\nis a full matrix containing n2parameters.2However, all algorithms maintaining a Gaussian posterior\napproximationworkwithadiagonal Wtoenforcetheeffectivelikelihoodtofactorizeoverexamples\n(as the true likelihood does, see Figure 1) in order to reduce the number of parameters. We are not\naware of work quantifying the error made by this assumption.\n\u221d N\n?\nf|(KW)\u22121m+m,W\u22121?\n.\n(7)\n2.3 Log Marginal Likelihood\nPrior knowledge over the latent function f is encoded in the choice of a covariance function k con-\ntaining hyperparameters \u03b8. In principle, one can do inference jointly over f and \u03b8 e.g., by sampling\ntechniques. Another approach to model selection is maximum likelihood type II also known as\nthe evidence framework (MacKay, 1992), where the hyperparameters \u03b8 are chosen to maximize\nthe marginal likelihood or evidence P(y|X,\u03b8). In other words, one maximizes the agreement be-\ntween observed data and the model. Therefore, one has a strong motivation to estimate the marginal\nlikelihood.\nGeometrically, the marginal likelihood measures the volume of the prior times the likelihood.\nHigh volume implies a strong consensus between our initial belief and our observations. In GP clas-\nsification, each data point xigives rise to a dimension fiin latent space. The likelihood implements\na mechanism, for smoothly restricting the posterior along the axis of fito the side corresponding\n?\n2. Numerical moment matching with K =\n7\n6\n6\n7\n?\n, y1= y2= 1 and sigprobitleads to W =\n?\n0.142\n\u22120.017\n\u22120.017\n0.142\n?\n.\n2042"},{"page":9,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nbest Gaussian likelihood\n\u221250510\n\u221210\n\u22125\n0\n5\nLA likelihood\n\u221250510\n\u221210\n\u22125\n0\n5\nEP likelihood\n\u221250510\n\u221210\n\u22125\n0\n5\nVB likelihood\n\u221250510\n\u221210\n\u22125\n0\n5\nKL likelihood\n\u221250510\n\u221210\n\u22125\n0\n5\nFigure 5: Five Effective Likelihoods (exact Prior\/Likelihood in gray): A Gaussian approximation\nto the posterior induces a Gaussian effective likelihood (Equation 7). Different effective\nlikelihoods are shown; order and setting are the same as described in Figure 4. The axes\nshow the latent function values f1= f(x1) and f2= f(x2). The effective likelihood re-\nplaces the non-Gaussian likelihood (indicated by three gray lines). A good replacement\nbehaves like the exact likelihood in regions of high prior density (indicated by gray el-\nlipses). EP and KL yield a good coverage of that region. However LA and VB yield too\nconcentrated replacements.\nto the sign of yi. Thus, the latent space Rnis softly cut down to the orthant given by the values in\ny. The log marginal likelihood measures, what fraction of the prior lies in that orthant. Finally, the\nvalue Z = 2\u2212ncorresponds to the case, where half of the prior lies on either side along each axis in\nlatent space. Consequently, successful inference is characterized by Z > 2\u2212n.\nSome posterior approximations (Sections 3 and 4) provide an approximation to the marginal\nlikelihood, other methods provide a lower bound (Sections 5 and 6). Any Gaussian approximation\nQ(f|\u03b8) = N (f|m,V) to the posterior P(f|y,X,\u03b8) gives rise to a lower bound ZBto the marginal\nlikelihood Z by application of Jensen\u2019s inequality. This bound has been used in the context of\nsparse approximations (Seeger, 2003).\nlnZ = lnP(y|X,\u03b8)=\nln\nZ\nQ(f|\u03b8)lnP(y|f)P(f|X,\u03b8)\nP(y|f)P(f|X,\u03b8)df = ln\nZ\ndf =: lnZB.\nQ(f|\u03b8)P(y|f)P(f|X,\u03b8)\nQ(f|\u03b8)\ndf\nJensen\n\u2265\nZ\nQ(f|\u03b8)\n(8)\nSome algebra (Appendix B.3) leads to the following expression for lnZB:\nn\n\u2211\ni=1\n?\nModel selection means maximization of lnZB. Term 1) is a sum of one-dimensional Gaussian\nintegrals of sigmoid functions in the logarithmic domain with adjustable offset and steepness. The\nintegrals can be numerically computed in an efficient way using Gauss-Hermite quadrature (Press\net al., 1993, \u00a74.5). As the sigmoid in the log domain takes only negative values, the first term will\nbe negative. That means, maximization of the first term is done by shifting the log-sigmoid such\nthat the high-density region of the Gaussian is multiplied by small values. Term 2) is the equivalent\nZ\nN (f|,0,1)lnsig?yi\n?\u221aViif +mi\n??df\n???\n1) data fit\n+1\n2[n\u2212m?K\u22121m\n????\n2) data fit\n+ln??VK\u22121??\u2212tr?VK\u22121?\n????\n3) regularizer\n].\n(9)\n2043"},{"page":10,"text":"NICKISCH AND RASMUSSEN\nof the data-fit term in GP regression (Rasmussen and Williams, 2006, Ch. 5.4.1). Thus, the first\nand the second term encourage fitting the data by favouring small variances Viiand large means mi\nhaving the same sign as yi. The third term can be rewritten as \u2212ln|I+KW|\u2212tr?(I+KW)\u22121?and\neigenvalues of KW small, thereby favouring a smaller class of functions\u2014this can be seen as an\ninstance of Occam\u2019s razor.\nFurthermore, the bound\nyields \u2212\u2211n\ni=1ln(1+\u03bbi)+\n1\n1+\u03bbiwith \u03bbi\u2265 0 being the eigenvalues of KW. Thus, term 3) keeps the\nlnZB\n=\nZ\nQ(f|\u03b8)lnP(f|y,X,\u03b8)P(y|X)\nQ(f|\u03b8)\ndf = lnZ\u2212KL(Q(f|\u03b8) ? P(f|y,X,\u03b8))\n(10)\ncan be decomposed into the exact marginal likelihood minus the Kullback-Leibler (KL) diver-\ngence between the exact posterior and the approximate posterior. Thus by maximizing the lower\nbound lnZBon lnZ, we effectively minimize the KL-divergence between P(f|y,X,\u03b8) and Q(f|\u03b8)=\nN (f|m,V). The bound is tight if and only if Q(f|\u03b8) = P(f|y,X,\u03b8).\n3. Laplace Approximation (LA)\nA second order Taylor expansion around the posterior mode m leads to a natural way of constructing\na Gaussian approximation to the log-posterior \u03a8(f) = lnP(f|y,X,\u03b8) (Williams and Barber, 1998;\nRasmussen and Williams, 2006, Ch. 3). The mode m is taken as the mean of the approximate\nGaussian. Linear terms of \u03a8 vanish because the gradient at the mode is zero. The quadratic term of\n\u03a8 is given by the negative Hessian W, which - due to the likelihood\u2019s factorial structure - turns out\nto be diagonal. The mode m is found by Newton\u2019s method.\n3.1 Posterior\nP(f|y,X,\u03b8)\n\u2248 N (f|m,V) =N\n=\nargmax\nf\u2208Rn\n\u2212\u22022lnP(y|f)\n\u2202f\u2202f?\n?\nf|m,?K\u22121+W?\u22121?\n?\n,\nm\nP(y|f)P(f|X,\u03b8),\n????f=m\nW\n=\n= \u2212\n\u22022lnP(yi|fi)\n\u2202f2\ni\n????\nfi=mi\n?\nii\n.\n3.2 Log Marginal Likelihood\nThe unnormalized posterior P(y|f)P(f|X,\u03b8) has its maximum h = exp(\u03a8(m)) at its mode m,\nwherethegradientvanishes. ATaylorexpansionof\u03a8isthengivenby\u03a8(f)\u2248h\u22121\nW)(f\u2212m). Consequently, the log marginal likelihood can be approximated by plugging in the ap-\nproximation of \u03a8(f).\n2(f\u2212m)?(K\u22121+\nlnZ = lnP(y|X,\u03b8)=\nln\nZ\nP(y|f)P(f|X,\u03b8)df = ln\nZ\nlnP(y|m)\u22121\nZ\nexp(\u03a8(f))df\n\u2248\nlnh+lnexp\n?\n\u22121\n2(f\u2212m)??K\u22121+W?(f\u2212m)\n2m?K\u22121m+1\n?\ndf\n=\n2ln|I+KW|.\n2044"},{"page":11,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\n4. Expectation Propagation (EP)\nEP (Minka, 2001b) is an iterative method to find approximations based on approximate marginal\nmoments, which can be applied to Gaussian processes. See (Rasmussen and Williams, 2006, Ch. 3)\nfor details. The individual likelihood terms are replaced by site functions ti(fi) being unnormalized\nGaussians\nP(yi|fi) \u2248 ti\nsuch that the approximate marginal moments of Q(fi) :=RN (f|0,K)\u220fn\nagree with the marginals of\nbased on the exact likelihood term P(yj|fj). That means, there are 3n quantities \u00b5i, \u03c32\nto be iteratively optimized. Convergence of EP is not generally guaranteed, but there always exists\na fixed-point for the EP updates in GP classification (Minka, 2001a). If the EP iterations converge,\nthe solution obtained is a saddle point of a special energy function (Minka, 2001a). However, an\nEP update does not necessarily imply a decrease in energy. For our case of log-concave likelihood\nfunctions, we always observed convergence, but we are not aware of a formal proof.\n?fi,\u00b5i,\u03c32\ni,Zi\n?:= ZiN?fi|\u00b5i,\u03c32\n?\ni\n?\nj=1ZjN\ndf\u00aciof the approximation\n?\nfj|\u00b5j,\u03c32\nj\n?\ndf\u00aci\nRN (f|0,K)P(yi|fi)\u220fj?=iZjN\nfj|\u00b5j,\u03c32\nj\n?\niand Zi\n4.1 Posterior\nBased on these local approximations, the approximate posterior can be written as:\nP(f|y,X,\u03b8)\n\u2248 N (f|m,V) =N\n=\ni\n=\nVW\u00b5 =\n?\nf|m,?K\u22121+W?\u22121?\nI\u2212K?K+W\u22121?\u22121?\n,\nW\n?\u03c3\u22122\n?\nii,\nm\n?\nKW\u00b5, \u00b5 = (\u00b51,...,\u00b5n)?.\n4.2 Log Marginal Likelihood\n>From the likelihood approximations, one can directly obtain an expression for the approximate log\nmarginal likelihood\nlnZ = lnP(y|X,\u03b8)=\nln\nZ\nZ\nP(y|f)P(f|X,\u03b8)df\nn\n\u220f\ni=1\nlnZi\u22121\nZi\n\u221a2\u03c0\u22121\n\u2248\nln\nt?fi,\u00b5i,\u03c32\n2\u00b5??K+W\u22121?\u22121\u00b5\u22121\n2m??K\u22121+K\u22121W\u22121K\u22121?m\u22121\ni,Zi\n?P(f|X,\u03b8)df\n=\nn\n\u2211\ni=1\nn\n\u2211\ni=1\n2ln??K+W\u22121??\u2212n\n2ln2\u03c0\n=\nln\n2ln??K+W\u22121??=: lnZEP.\nThe lower bound provided by Jensen\u2019s inequality ZB(Equation 9) is known to be below the approx-\nimation ZEPobtained by EP (Opper and Winther, 2005, page 2183). From ZEP\u2265 ZBand Z \u2265 ZBit\nis not clear, which value one should use. In principle, ZEPcould be a bad approximation. However,\nour experimental findings and extensive Monte Carlo simulations suggest that ZEPis very accurate.\n2045"},{"page":12,"text":"NICKISCH AND RASMUSSEN\n4.3 Thouless, Anderson & Palmer method (TAP)\nBased on ideas rooted in Statistical Physics, one can approach the problem from a slightly different\nangle(OpperandWinther,2000). IndividualGaussianapproximationsN (fi|\u00b5\u00aci,\u03c32\nto predictive distributions P?fi|xi,y\\i,X\\i,\u03b8?for data points xithat have been previously removed\nparameters of interest.\n\u00aci)areonlymade\nfrom the training set. Based on \u00b5\u00aciand \u03c32\n\u00acione can derive explicit expressions for (\u03b1,W\n1\n2), our\n\u03b1i\n\u2248\nR\n\u2202\n\u2202fiP(yi|fi)N (fi|\u00b5\u00aci,\u03c32\nRP(yi|fi)N (fi|\u00b5\u00aci,\u03c32\n\u03c32\n\u00aci\n\u03b1i[K\u03b1]i\n\u00aci)dfi\n\u00aci)dfi\n,\n?W\u22121?\nii\n\u2248\n?\n1\n\u22121\n?\n.\n(11)\nIn turn, the 2n parameters (\u00b5\u00aci,\u03c32\n\u00aci) can be expressed as a function of \u03b1, K and W\n??K+W\u22121?\u22121?\n=[K\u03b1]i\u2212\u03c32\n1\n2.\n\u03c32\n\u00aci\n=\n1\/\nii\u2212?W\u22121?\nii,\n\u00b5\u00aci\n\u00aci\u03b1i.\n(12)\nAs a result, a system (Equations 11\/12) of nonlinear equations in \u00b5\u00aciand \u03c32\nby iteration. Each step involves a matrix inversion of cubic complexity. A faster \u201cna\u00efve\u201d variant\nupdating only n parameters has also been proposed (Opper and Winther, 2000) but it does not lead\nto the same fixed point. As in the FV algorithm (Section 7), a formal complex transformation leads\nto a simplified version by fixing \u03c32\nFinally, for prediction, the predictive posterior P(f\u2217|x\u2217,y,X,\u03b8) is approximated by a Gaussian\nN (f\u2217|\u00b5\u2217,\u03c32\nA fixed-point of the TAP mean-field equations is also a fixed-point of the EP algorithm (Minka,\n2001a). This theoretical result was confirmed in our numerical simulations. However, the EP algo-\nrithm is more practical and typically much faster. For this reason, we are not going to treat the TAP\nmethod as an independent algorithm in this paper.\n\u00acihas to be solved\n\u00aci= Kii, called (TAPnaive) in the sequel.\n\u2217) at a test point x\u2217based on the parameters (\u03b1,W\n1\n2) and according to equation (6).\n5. KL-Divergence Minimization (KL)\nIn principle, we simply want to minimize a dissimilarity measure between the approximate posterior\nQ(f|\u03b8) = N (f|m,V) and the exact posterior P(f|y,X,\u03b8). One quantity to minimize is the KL-\ndivergence\nKL(P(f|y,X,\u03b8) ? Q(f|\u03b8))=\nZ\nP(f|y,X,\u03b8)lnP(f|y,X,\u03b8)\nQ(f|\u03b8)\ndf.\nUnfortunately, this expression is intractable. If instead, we measure the reverse KL-divergence, we\nregain tractability\nKL(Q(f|\u03b8) ? P(f|y,X,\u03b8))=\nZ\nN (f|m,V)lnN (f|m,V)\nP(f|y,X,\u03b8)df =: KL(m,V).\n2046"},{"page":13,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nA similar approach has been followed for regression with Laplace or Cauchy noise (Opper and\nArchambeau, 2008). Finally, we minimize the following objective (see Appendix B.3) with respect\nto the variables m and V. Constant terms have been dropped from the expression:\nKL(m,V)\nc= \u2212\nZ\nN (f)\n?\nn\n\u2211\ni=1\nlnsig(\u221aviiyif +miyi)\n?\ndf \u22121\n2ln|V|+1\n2m?K\u22121m+1\n2tr?K\u22121V?.\nWe refer to the first term of KL(m,V) as a(m,V) to keep the expressions short. We calculate first\nderivatives and equate them with zero to obtain necessary conditions that have to be fulfilled at a\nlocal optimum (m\u2217,V\u2217)\n\u2202KL\n\u2202m\n\u2202KL\n\u2202V\n=\n\u2202a\n\u2202m\u2212K\u22121m = 0 \u21d2 K\u22121m =\u2202a\n\u2202a\n\u2202V+1\n\u2202m= \u03b1,\n?\n=\n2V\u22121\u22121\n2K\u22121= 0 \u21d2 V =\nK\u22121\u22122\u2202a\n\u2202V\n?\u22121\n=?K\u22121\u22122\u039b?\u22121\nwhich defines \u039b. If the approximate posterior is parametrized by (m,V), there are in principle in\nthe order of n2parameters. But if the necessary conditions for a local minimum are fulfilled (i.e., the\nderivatives \u2202KL\/\u2202m and \u2202KL\/\u2202V vanish), the problem can be re-parametrized in terms of (\u03b1,\u039b).\nSince \u039b = \u2202a\/\u2202V is a diagonal matrix (see Equation 17), the optimum is characterized 2n free\nparameters. This fact was already pointed out by Manfred Opper (personal communication) and\nMatthias Seeger (Seeger, 1999, Ch. 5.21, Eq. 5.3). Thus, a minimization scheme based on Newton\niterations on the joint vector \u03be :=[\u03b1?,\u039bii]?takesO(8\u00b7n3) operations. Details about the derivatives\n\u2202KL\/\u2202\u03be and \u22022KL\/\u2202\u03be\u2202\u03be?are provided in Appendix A.2.\n5.1 Posterior\nBased on these local approximations, the approximate posterior can be written as:\nP(f|y,X,\u03b8)\n\u2248 N (f|m,V) =N\n=\n\u22122\u039b,\n=\nK\u03b1.\n?\nf|m,?K\u22121+W?\u22121?\n,\nW\nm\n5.2 Log Marginal Likelihood\nSince the method inherently maximizes a lower bound on the marginal likelihood, this bound (Equa-\ntion 9) is used as approximation to the marginal likelihood.\n6. Variational Bounds (VB)\nThe following variational bounding method (Gibbs and MacKay, 2000) is a special case of the KL\nmethod. Instead of optimizing a bound on the joint (Eq. 8), they impose the bounding condition on\neach likelihood term individually. Here, we treat parametrization based on quadratic lower bounds\non the individual likelihoods in the logarithmic domain. We first derive all calculations based on\n2047"},{"page":14,"text":"NICKISCH AND RASMUSSEN\ngeneral likelihoods. Individual likelihood bounds\nP(yi|fi)\n\u21d2 P(y|f)\n\u2265\n\u2265\nexp?aif2\nexp\ni+biyifi+ci\nf?Af+(b?y)?f+c?\n?, \u2200fi\u2208 R\u2200i\n?\n??\n=: Q(y|f,A,b,c), \u2200f \u2208 R\nare defined in terms of coefficients ai,biand ci, where ? denotes the element-wise product of two\nvectors. This lower bound on the likelihood induces a lower bound on the marginal likelihood.\nZ\nCarrying out the Gaussian integral\nZ\nleads to (see Appendix B.4)\n+1\nZ =\nP(f|X)P(y|f)df\n\u2265\nZ\nP(f|X)Q(y|f,A,b,c)df = ZB.\nZB\n=\nN (f|0,K)exp\n?\nf?Af+(b?y)?f+c?\n??\ndf\nlnZB\n=\nc?\n?\n2(b?y)??K\u22121\u22122A?\u22121(b?y)\u22121\n2ln|I\u22122AK|\n(13)\nwhich can now be maximized with respect to the coefficients ai,biand ci. In order to get an efficient\nalgorithm, one has to calculate the first and second derivatives \u2202lnZB\/\u2202\u03c2, \u22022lnZB\/\u2202\u03c2\u2202\u03c2?(as done\nin Appendix A.1). Hyperparameters can be optimized using the gradient \u2202lnZB\/\u2202\u03b8.\n6.1 Logit Bound\nOptimizing the logistic likelihood function (Gibbs and MacKay, 2000), we obtain the necessary\nconditions\nA\u03c2\n:=\n\u2212\u039b\u03c2,\n1\n2\n\u03c22\nb\u03c2\n:=\n?,\nc\u03c2,i\n:=\ni\u03bb(\u03c2i)\u22121\n2\u03c2i+lnsiglogit(\u03c2i)\nwhere we define \u03bb(\u03c2i) =?2siglogit(\u03c2i)\u22121?\/(4\u03c2i) and \u039b\u03c2= [\u03bb(\u03c2i)]ii. This shows, that we only have\nis symmetric and tight at f = \u00b1\u03c2.\nto optimize with respect to n parameters \u03c2. We apply Newton\u2019s method for this purpose. The bound\n6.2 Probit Bound\nForreasonsofcompleteness, wederivesimilarexpressions(AppendixB.5)forthecumulativeGaus-\nsian likelihood sigprobit(fi) with necessary conditions\n\u22121\n2\nN (\u03c2i)\nsigprobit(\u03c2i),\n?\u03c2i\nwhich again depend only on a single vector of parameters we optimize using Newton\u2019s method. The\nbound is tight for f = \u03c2.\na\u03c2\n:=\n?,\n(14)\nb\u03c2,i\n:=\n\u03c2i+\nc\u03c2,i\n:=\n2\u2212bi\n?\n\u03c2i+ln?sigprobit(\u03c2i)?\n2048"},{"page":15,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\n6.3 Posterior\nBased on these local approximations, the approximate posterior can be written as\nP(f|y,X,\u03b8)\n\u2248 N (f|m,V) =N\n=\n\u22122A\u03c2,\n=\n?\nf|m,?K\u22121+W?\u22121?\n?\u22121(y?b\u03c2),\n,\nW\nmV(y?b\u03c2) =?K\u22121\u22122A\u03c2\nwhere we have expressed the posterior parameters directly as a function of the coefficients. Finally,\nwe deal with an approximate posterior Q(f|\u03b8) = N (f|m\u03c2,V\u03c2) only depending on a vector \u03c2 of\nn variational parameters and a mapping \u03c2 ?\u2192 (m\u03c2,V\u03c2). In the KL method, every combination of\nvalues m and W is allowed, in the VB method, m\u03c2and V\u03c2cannot be chosen independently, since\nthe have to be compatible with the bounding requirements. Therefore, the variational posterior is\nmore constrained than the general Gaussian posterior and thus easier to optimize.\n6.4 Log Marginal Likelihood\nIt turns out, that the approximation to the marginal likelihood (Equation 13) is often quite poor and\nthe more general Jensen bound approach (Equation 9) is much tighter. In practice, one would have\nto evaluate both of them and keep the maximum value.\n7. Factorial Variational Method (FV)\nInstead of approximating the posterior P(f|y,X,\u03b8) by the closest Gaussian distribution, one can use\nthe closest factorial distribution Q(f|y,X,\u03b8)=\u220fiQ(fi), also called ensemble learning (Csat\u00f3 et al.,\n2000). Another kind of factorial approximation Q(f) = Q(f+)Q(f\u2212)\u2014a posterior factorizing over\nclasses\u2014is used in multi-class classification (Girolami and Rogers, 2006).\n7.1 Posterior\nAsaresultoffree-formminimizationoftheKullback-LeiblerdivergenceKL(Q(f|y,X,\u03b8) ? P(f|y,X,\u03b8))\nby equating its functional derivative \u03b4KL\/\u03b4Q(fi) with the zero function (Appendix B.6), one finds\nthe best approximation to be of the following form:\n??\u00b5i,\u03c32\n\u03c32\ni\n=\nZ\nIn fact, the best product distribution consists of a factorial Gaussian times the original likelihood.\nThe Gaussian has the same moments as the Leave-One-Out prediction (Sundararajan and Keerthi,\n2001). Since the posterior is factorial, the effective likelihood of the factorial approximation has an\nodd shape. It effectively has to annihilate the correlations in the prior, and these correlations are\nusually what allows learning to happen in the first place. However, the best fitting factorial is still\nable to ensure that the latent means have the right signs. Even though all correlations are neglected,\nQ(fi)\n\u221d N?fi\n?K\u22121?\u22121\n=\ni\n?P(yi|fi),\n\u00b5i\n=\nmi\u2212\u03c32\ni\n?K\u22121m?\nii,\nfiQ(fi)dfi.\ni= [K\u03b1]i\u2212\u03c32\ni\u03b1i,\nmi\n(15)\n2049"},{"page":16,"text":"NICKISCH AND RASMUSSEN\nit is still possible that the model picks up the most important structure, since the expectations are\ncoupled. Of course, at test time, it is essential that correlations are taken into account again using\nEquation 6, as it would otherwise be impossible to inject any knowledge into the predictive dis-\ntribution. For predictions we use the Gaussian N (f|m,Dg(v)) instead of Q(f). This is a further\napproximation, but it allows to stay inside the Gaussian framework.\nParameters \u00b5iand miare found by the following algorithm. Starting from m = 0, iterate the\nfollowing until convergence; (1) compute \u00b5i, (2) update miby taking a step in the direction towards\nmias given by Equation 15. Stepsizes are adapted.\n7.2 Log Marginal Likelihood\nSurprisingly, one can obtain a lower bound on the marginal likelihood (Csat\u00f3 et al., 2000):\nn\n\u2211\ni=1\nlnZ\n\u2265\nlnsig\n?yimi\n\u03c3i\n?\n\u22121\n2\u03b1??\nK\u2212Dg(?\u03c32\n1,...,\u03c32\nn\n??)\n?\n\u03b1\u22121\n2ln|K|+\nn\n\u2211\ni=1\nln\u03c3i.\n8. Label Regression Method (LR)\nClassification has also been treated using label regression or least squares classification (Rifkin and\nKlautau, 2004). In its simplest form, this method simply ignores the discreteness of the class labels\nat the cost of not being able to provide proper probabilistic predictions. However, we treat LR\nas a heuristic way of choosing \u03b1 and W, which allows us to think of it as yet another Gaussian\napproximation to the posterior allowing for valid predictions of class probabilities.\n8.1 Posterior\nAfter inference, according to Equation 6, the moments of the (Gaussian approximation to the) pos-\nterior GP can be written as \u00b5\u2217= k?\nW\u22121= \u03c32\nand\nwe obtain GP regression from data points xi\u2208 X to real labels yi\u2208 R with noise of variance \u03c32\nas a special case. In regression, the posterior moments are given by \u00b5\u2217= k?\n\u03c32\n\u2217\ny can be absorbed by the hyperparameters. There is an additional parameter \u03c3n, describing the width\nof the effective likelihood. In experiments, we selected \u03c3n\u2208 [0.5,2] to maximise the log marginal\nlikelihood.\n\u2217\u03b1 and \u03c32\n\u03b1 =?K+W\u22121?\u22121?K+W\u22121?\u03b1 =?K+W\u22121?\u22121y,\n\u2217= k\u2217\u2217\u2212k?\n\u2217\n?K+W\u22121?\u22121k\u2217. Fixing\nnI\nn\n\u2217\n?K+\u03c32\nnI?\u22121y and\n\u2217= k\u2217\u2217\u2212k?\n?K+\u03c32\nnI?\u22121k\u2217(Rasmussen and Williams, 2006). The arbitrary scale of the discrete\n8.2 Log Marginal Likelihood\nThere are two ways of obtaining an estimate of the log marginal likelihood. One can simply ignore\nthe binary nature and use the regression marginal likelihood lnZregas proxy for lnZ\u2014an approach\nwe only mention but not use in the experiments\nlnZreg\n=\n\u22121\n2\u03b1??K+\u03c32\nnI?\u03b1\u22121\n2ln??K+\u03c32\nnI??\u2212n\n2ln2\u03c0.\nAlternatively, the Jensen bound (8) yields a lower bound lnZ \u2265 lnZB\u2014which seems more in line\nwith the classification scenario than lnZreg.\n2050"},{"page":17,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\n9. Relations Between the Methods\nAll considered approximations can be separated into local and global methods. Local methods\nexploit properties (such as derivatives) of the posterior at a special location only. Global methods\nminimize the KL-divergence KL(Q||P) =RQ(f)lnQ(f)\/P(f)df between the posterior P(f) and a\nalgorithm.\ntractable family of distributions Q(f). Often this methodology is also referred to as a variational\nassumptionrelationconditionsapprox. posterior Q(f)\nname\nQ(f) =N (f|m,V)\n\u2192\nm\nW\n=\n=\nargmaxfP(f)\n\u2212\u22022lnP(y|f)\n\u2202f\u2202f?\nN (f|m,(K\u22121+W)\u22121)\nLA\nQ(f) = \u220fiqi(fi)\n\u2192\n?\n\u03b4KL\n\u03b4qi(fi)\u2261 0\nqi(fi)=?fd\n\u2202KL\n\u2202V,m= 0\n\u220fiN (fi|\u00b5i,\u03c32\nN?f|m,(K\u22121+W)\u22121?\nN?f|m,(K\u22121+W)\u22121?\nN?f|m\u03c2\u2217,(K\u22121+W\u03c2\u2217)\u22121?\nN (f|m,(K\u22121+\u03c3\u22122\ni)P(yi|fi)\nFV\n?fd\ni\n?\ni\n?\nQ(fi)\nEP\n?\n\u2192\n?\n\u2192\nQ(f) =N (f|m,V)\nKL\nP(yi|fi) \u2265N (fi|\u00b5\u03c2i,\u03c32\n\u03c2i)\n\u2202KL\n\u2202\u03c2\u2217= 0\nVB\nP(yi|fi) :=N (fi|yi,\u03c32\nn)\n\u2192\nm = (I+\u03c32\nnK\u22121)\u22121y\nnI)\u22121)\nLR\nThe only local method considered is the LA approximation matching curvature at the posterior\nmode. Common tractable distributions for global methods include factorial and Gaussian distri-\nbutions. They have their direct correspondent in the FV method and the KL method. Individual\nlikelihood bounds make the VB method a more constrained and easier-to-optimize version of the\nKL method. Interestingly, EP can be seen in some sense as a hybrid version of FV and KL, com-\nbining the advantages of both methods. Within the Expectation Consistence framework (Opper and\nWinther,2005), EPcanbethoughtofasanalgorithmthatimplicitlyworkswithtwodistributions\u2014a\nfactorial and a Gaussian\u2014having the same marginal moments?fd\nIn the divergence measure and message passing framework (Minka, 2005), EP is cast as a mes-\nsage passing algorithm template: Iterative minimization of local divergences to a tractable family\nof distributions yields a small global divergence. From that viewpoint, FV and KL are considered\nas special cases with divergence measure KL(Q||P) combined with factorial and Gaussian distribu-\ntions.\ni\n?. By means of iterative updates,\none keeps these expectations consistent and produces a posterior approximation.\nThere is also a link between local and global methods, namely from the KL to the LA method.\nThe necessary conditions for the LA method do hold on average for the KL method (Opper and\nArchambeau, 2008).\nFinally, LR neither qualifies as local nor global\u2014it is just a heuristic way of setting m and W.\n2051"},{"page":18,"text":"NICKISCH AND RASMUSSEN\n10. Markov Chain Monte Carlo (MCMC)\nThe only way of getting a handle on the ground truth for the moments Z, m and V is by applying\nsampling techniques. In the limit of long runs, one is guaranteed to get the right answer. But in\npractice, these methods can be very slow, compared to analytic approximations discussed previ-\nously. MCMC runs are rather supposed to provide a gold standard for the comparison of the other\nmethods.\nIt turns out to be most challenging to obtain reliable marginal likelihood estimates as it is equiv-\nalent to solving the free energy problem in physics. We employ Annealed Importance Sampling\n(AIS) and thermodynamic integration to yield the desired marginal likelihoods. Instead of starting\nannealing from the prior distribution, we propose to directly start from an approximate posterior in\norder to speed up the sampling process.\nAccurate estimates of the first and second moments can be obtained by sampling directly from\nthe (unnormalized) posterior using Hybrid Monte Carlo methods (Neal, 1993).\n10.1 Thermodynamic Integration\nThe goal is to calculate the marginal likelihood Z =RP(y|f)P(f|X)df. AIS (Neal, 1993, 2001)\ndenotes an inverse temperature schedule with the properties \u03c4(0) = 0, \u03c4(T) = 1 and \u03c4(t +1) \u2265 \u03c4(t)\nleading to Z0=RP(f|X)df = 1 and ZT= Z.\nZt\/Zt\u22121can be approximated by importance sampling with samples fsfrom the \u201cintermediate pos-\nterior\u201d P(f|y,X,t \u22121) := P(y|f)\u03c4(t\u22121)P(f|X)\/Zt\u22121at time t.\nZt\nZt\u22121\n=\nP(y|f)\u2206\u03c4(t)P(f|y,X,t \u22121)df\n1\nS\n\u2211\ns=1\nThis works fine for small temperature changes \u2206\u03c4(t) := \u03c4(t)\u2212\u03c4(t \u22121). In the limit, we smoothly\ninterpolate between P(y|f)0P(f|X) and P(y|f)1P(f|X), that is, we start by sampling from the prior\nand finally approach the posterior. Note that sampling is algorithmically possible even though the\ndistribution is only known up to a constant factor.\nworks with intermediate quantities Zt:=RP(y|f)\u03c4(t)P(f|X)df. Here, \u03c4 : N \u2283 [0,T] \u2192 [0,1] \u2282 R\nOn the other hand, we have Z = ZT\/Z0= \u220fT\nt=1Zt\/Zt\u22121\u2014an expanded fraction. Each factor\n=\nRP(y|f)\u03c4(t)P(f|X)df\nZt\u22121\nZ\nS\nP(y|fs)\u2206\u03c4(t),\n=\nZ\nP(y|f)\u03c4(t)\nP(y|f)\u03c4(t\u22121)\nP(y|f)\u03c4(t\u22121)P(f|X)\nZt\u22121\ndf\n\u2248\nfs\u223c P(f|y,X,t \u22121).\n10.2 Amelioration Using an Approximation to the Posterior\nIn practice, the posterior can be quite different from the prior. That means that individual fractions\nZt\/Zt\u22121may be difficult to estimate. One can make these fractions more similar by increasing the\nnumber of steps T or by \u201cstarting\u201d from a distribution close to the posterior rather than from the\nprior. Let Q(f) = N (f|m,V) \u2248 P(f|y,X,T) = P(y|f)P(f|X)\/ZTdenote an approximation to the\nposterior. Setting N (f|m,V) = Q(y|f)P(f|X), one can calculate the effective likelihood Q(y|f) by\ndivision (see Appendix B.2).\nFor the integration we use Zt=RP(y|f)\u03c4(t)Q(y|f)1\u2212\u03c4(t)P(f|X)df where Z0=RQ(y|f)P(f|X)df\ncan be computed analytically. Again, each factor\nZt\nZt\u22121of the expanded fraction can be approximated\n2052"},{"page":19,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nby importance sampling from the modified intermediate posterior:\nP(f|y,X,t \u22121)=\nP(y|f)\u03c4(t\u22121)Q(y|f)1\u2212\u03c4(t\u22121)P(f|X)\/Zt\u22121\n?P(y|f)\n=\nQ(y|f)\n?\u03c4(t\u22121)\nQ(y|f)P(f|X)\/Zt\u22121,\nZt\nZt\u22121\n=\nRP(y|f)\u03c4(t)Q(y|f)1\u2212\u03c4(t)P(f|X)df\nZt\u22121\nZ\nZ ?P(y|f)\n1\nS\n\u2211\ns=1\nQ(y|fs)\n=\nP(y|f)\u03c4(t)Q(y|f)1\u2212\u03c4(t)\nP(y|f)\u03c4(t\u22121)Q(y|f)1\u2212\u03c4(t\u22121)\n?\u2206\u03c4(t)\nS\n?P(y|fs)\nP(y|f)\u03c4(t\u22121)Q(y|f)1\u2212\u03c4(t\u22121)P(f|X)\nZt\u22121\ndf\n=\nQ(y|f)\nP(f|y,X,t \u22121)df\n?\u2206\u03c4(t)\n\u2248\n,\nfs\u223c P(f|y,X,t \u22121).\nThechoiceofQ(f)tobeagoodapproximationtothetrueposteriormakesthefractionP(y|f)\/Q(y|f)\nas constant as possible, which in turn reduces the error due to the finite step size in thermodynamical\nintegration.\n10.3 Algorithm\nIf only one sample ftis used per temperature \u03c4(t), the value of the entire fraction is obtained as\nln\nZt\nZt\u22121\n= \u2206\u03c4(t)[lnP(y|ft)\u2212lnQ(y|ft)]\nwhich gives rise to the full estimate\nlnZ \u2248\nT\n\u2211\nt=1\nln\nZt\nZt\u22121\n= lnZQ+\nT\n\u2211\nt=1\n\u2206\u03c4(t)\n?\nlnP(y|ft)+1\n2(ft\u2212 \u02dc m)?W(ft\u2212 \u02dc m)\n?\nfor a single run r. The finite temperature change bias can be removed by combining results Zrfrom\nR different runs by their arithmetic mean1\nR\u2211rZr(Neal, 2001)\nlnZ = ln\nZ\nP(y|f)P(f|X)df \u2248 ln\n?\n1\nR\nR\n\u2211\nr=1\nZr\n?\n.\nFinally, the only primitive needed to obtain MCMC estimates of Z, m and V is an efficient\nsampler for the \u201cintermediate\u201d posterior P(f|y,X,t \u22121). We use Hybrid Monte Carlo sampling\n(Neal, 1993).\n10.4 Results\nIf the posterior is very close to the prior (as in regimes 7-9 of Figure 3), it does not make a dif-\nference, which we start from. However, if the posterior can be well approximated by a Gaussian\n2053"},{"page":20,"text":"NICKISCH AND RASMUSSEN\n(regimes 4-6), but is sufficiently different from the prior, then the method decreases variance and\nconsequently improves runtimes of AIS. Different approximation methods lead also to differences\nin the improvement. Namely, the Laplace approximation performs worse than the approximation\nfound by Expectation Propagation because Laplace\u2019s method approximates around the mode which\ncan be far away from the mean.\nFor our evaluations of the approximations to the marginal likelihood, however we started the\nalgorithmfromtheprior. Otherwise, onemightbeworriedofbiasingtheMCMCsimulationtowards\nthe initial distribution in cases where the chain fails to mix properly.\n11. Implementation\nImplementationsofallmethodsdiscussedareprovidedat http:\/\/www.kyb.mpg.de\/~hn\/approxXX.\ntar.gz. The code is designed as an extension to the Gaussian Processes for Machine Learning\n(GPML) (Rasmussen and Williams, 2006) Matlab Code.3Approximate inference for Gaussian\nprocesses is done by the binaryGP.m function, which takes as arguments the covariance func-\ntion, the likelihood function and the approximation method. The existing GPML package provides\napproxLA.m for Laplace\u2019s method and approxEP.m for Expectation Propagation. These implemen-\ntations are generic to the likelihood function. We provide cumGauss.m and logistic.m that were\ndesigned to avoid numerical problems. In the extension, approxKL.m, approxVB.m, approxFV.m\nand approxTAP.m are included, among others not discussed here, for example sparse and online\nmethods outside the scope of the current investigation. The implementations are straight-forward,\nalthough special care has been taken to avoid numerical problems e.g., situations where K is close\nto singular. More concretely, we use the well-conditioned matrix4B = W\nits Cholesky decomposition to calculate V =?K\u22121+W?\u22121or k?\nEspecially LA and EP show a high level of robustness along the full spectrum of possible hyper-\nparameters. KL uses Gauss-Hermite quadrature; we did not notice problems stemming therefrom.\nThe FV and TAP methods work very reliably, although, we had to add a small (10\u22126) ridge for FV\nto regularize K. As a general statement, we did not observe any numerical problems for a wide\nrange of hyperparameters reaching from reasonable values to very extreme scales.\nIn addition to the code for the algorithms, we provide also a tarball containing all necessary\nscripts to reproduce the figures of the paper. We offer two versions: The first version contains only\nthe code for running the experiments and drawing the figures.5The second version additionally\nincludes the results of the experiments.6\n1\n2KW\n1\n2+I = LL?and\n\u2217\n?K+W\u22121?\u22121k\u2217. The posterior\nmean is represented in terms of \u03b1 to avoid multiplications with K\u22121and facilitate predictions.\n12. Experiments\nThe purpose of the experiments is to illustrate the strengths and weaknesses of the different approxi-\nmation methods. First of all, the quality of the approximation itself in terms of posterior moments Z,\n3. The package is available at http:\/\/www.gaussianprocess.org\/gpml\/code.\n4. All eigenvalues \u03bb of B satisfy 1 \u2264 \u03bb \u2264 1+n\n5. The code base (\u223c 9Mb) can be obtained from http:\/\/www.kyb.mpg.de\/~hn\/supplement_code.tar.gz.\n6. The complete code base (\u223c 400Mb) including all simulation results and scripts to generate figures is stored at\nhttp:\/\/www.kyb.mpg.de\/~hn\/supplement_all.tar.gz.\n4maxijKij, thus B\u22121and |B| can be safely computed.\n2054"},{"page":21,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nm and V is studied. At a second level, building on the \u201clow-level\u201d features, we compare predictive\nperformance in terms of the predictive probability p\u2217given by (Equations 4 and 6):\np\u2217:= P(y\u2217= 1|x\u2217,y,X,\u03b8)\n\u2248\nZ\nsig(f\u2217)N?f\u2217|\u00b5\u2217,\u03c32\n\u2217\n?df\u2217.\n(16)\nOn a third level, we assess higher order properties such as the information score, describing how\nmuch information the model managed to extract about the target labels, and the error rate\u2014a binary\nmeasure of whether a test input is assigned the right class. Uncertainty predictions provided by the\nmodel are not captured by the error rate.\nAccurate marginal likelihood estimates Z are a key to hyperparameter learning. In that respect,\nZ can be seen as a high-level feature and as the \u201czeroth\u201d posterior moment at the same time.\nA summary of the whole section is provided in Table 1.\n12.1 Data Sets\nOne main idea of the paper is to study the general behavior of approximate GP classification. Our\nresults for the different approximation methods are not specific to a particular data set but apply to a\nwide range of application domains. This is reflected by the choice of our reference data sets, widely\nused in the machine learning literature. Due to limited space, we don\u2019t include the full experiments\non all data sets in this paper. However, we have verified that the same qualitative conclusions hold\nfor all the data sets considered. The full results are available via the web.7\nData set\nBreast\nCrabs\nIonosphere\nPima\nSonar\nUSPS 3 vs. 5\nntrain\n300\n100\n200\n350\n108\n767\nntest\n383\n100\n151\n418\n100\n773\nd\n9\n6\n34\n8\n60\n256\nBrief description of problem domain\nBreast cancer8\nSex of Leptograpsus crabs9\nClassification of radar returns from the ionosphere10\nDiabetes in Pima Indians11\nSonar signals bounced by a metal or rock cylinder12\nBinary sub-problem of the USPS handwritten digit data set13\n12.2 Results\nIn the following, we report our experimental results covering posterior moments and predictive per-\nformance. Findings for all 5 methods are provided to make the methods as comparable as possible.\n7. See links in Footnotes 5 and 6.\n8. Data set at http:\/\/mlearn.ics.uci.edu\/databases\/breast-cancer-wisconsin\/.\n9. Data set at http:\/\/www.stats.ox.ac.uk\/pub\/PRNN\/.\n10. Data set at http:\/\/mlearn.ics.uci.edu\/databases\/ionosphere\/.\n11. Data set at http:\/\/mlearn.ics.uci.edu\/databases\/pima-indians-diabetes\/.\n12. Datasetat\nftp:\/\/ftp.ics.uci.edu\/pub\/machine-learning-databases\/undocumented\/\nconnectionist-bench\/sonar\/.\n13. Data set at http:\/\/www.gaussianprocess.org\/gpml\/data\/.\n2055"},{"page":22,"text":"NICKISCH AND RASMUSSEN\nTraining marginals\n\u22122000 200\n\u2212200\n0\n200\n\u00b5 for LA\n\u22122000200\n\u2212200\n0\n200\n\u00b5 for EP\n\u22122000 200\n\u2212200\n0\n200\n\u00b5 for VB\n\u22122000200\n\u2212200\n0\n200\n\u00b5 for KL\n\u22122000 200\n\u2212200\n0\n200\n\u00b5 for FV\n0 20 40\n0\n20\n40\n\u03c3 for LA\n020 40\n0\n20\n40\n\u03c3 for EP\n0 2040\n0\n20\n40\n\u03c3 for VB\n02040\n0\n20\n40\n\u03c3 for KL\n02040\n0\n20\n40\n\u03c3 for FV\n0 0.51\n0\n0.5\n1\np for LA\n00.51\n0\n0.5\n1\np for EP\n0 0.51\n0\n0.5\n1\np for VB\n00.5 1\n0\n0.5\n1\np for KL\n0 0.51\n0\n0.5\n1\np for FV\nTest marginals\n\u22122000200\n\u2212200\n0\n200\n\u00b5 for LA\n\u22122000200\n\u2212200\n0\n200\n\u00b5 for EP\n\u22122000200\n\u2212200\n0\n200\n\u00b5 for VB\n\u22122000 200\n\u2212200\n0\n200\n\u00b5 for KL\n\u22122000200\n\u2212200\n0\n200\n\u00b5 for FV\n020 40\n0\n20\n40\n\u03c3 for LA\n020 40\n0\n20\n40\n\u03c3 for EP\n02040\n0\n20\n40\n\u03c3 for VB\n02040\n0\n20\n40\n\u03c3 for KL\n02040\n0\n20\n40\n\u03c3 for FV\n0 0.51\n0\n0.5\n1\np for LA\n00.5 1\n0\n0.5\n1\np for EP\n00.51\n0\n0.5\n1\np for VB\n00.51\n0\n0.5\n1\np for KL\n00.51\n0\n0.5\n1\np for FV\nFigure 6: Marginals of USPS 3 vs. 5 for a highly non-Gaussian posterior: Each row consists of\nfive plots showing MCMC ground truth on the x-axis and LA, EP, VB, KL and FV on\nthe y-axis. Based on the logistic likelihood function and the squared exponential covari-\nance function with parameters ln? = 2.25 and ln\u03c3f= 4.25 we plot the marginal means,\nstandard deviations and resulting predictive probabilities in rows 1-3. We are working\nin regime 2 of Figure 3 that means the posterior is highly non-Gaussian. The upper part\nshows marginals of training points and the lower part shows test point marginals.\n2056"},{"page":23,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nLA EP*VB\nlogit|probit\nlower bound\non indiv.\nlikelihoods\nKLFVMCMC\nidea\nquadratic\nexpansion\naround the\nmode\nNewton steps\nmarginal\nmoment\nmatching\nKL minim.,\naverage w.r.t.\nwrong Q(f)\nbest\nfree-form\nfactorial\nsampling,\nthermo-\ndynamic\nintegration\nHybrid MC,\nAIS\nO(n3)\nvery slow\n>500\nalgorithm\niterative\nmatching\nO(n3)\nfast\n10\nNewton stepsNewton stepsfixed-point\niteration\nO(n3)\nvery fast\n4\ncomplexity\nspeed\nrunning\ntime\nlikelihood\nproperties\nevidence Z\nmean m\ncovariance\nV\ninformation\nI\nPRO\nO(n3)\nvery fast\n1\nO(n3)\nfast\n8\nO(8n3)\nslow\n150\n1st-3rd log.\nderivative\n\u2013\n\u2013 \u2013\n\u2013\nN -integralslower boundsimple\nevaluation\n\u2013\n+\n\u2013\nN -integrals1st log\nderivative\n=\n=\n=\n\u2248\n\u2248\n\u2248\n\u2013 \u2013\u2013 \u2013 \u2013\n\u2013\n\u2013 \u2013\n++| \u2013 \u2013\n\u2013 \u2013\n\u2013\n\u2248 \u2248| \u2013\n\u2248\n\u2013=\nspeedpractical\naccuracy\nspeed\nprincipled\nmethod\noverconfidence\nspeedtheoretical\naccuracy\nvery slow\nCON\nmean?=mode,\nlow info I\nstrong over-\nconfidence\nfactorizing\napproxima-\ntion\nTable 1: Feature summary of the considered algorithms: For each of the six algorithms under con-\nsideration, the major properties are listed in the above table. The basic idea of the method\nalong with its computational algorithm and complexity is summarized, the requirements to\nthe likelihood functions are given, the accuracy of evidence and moment estimates as well\nas information is outlined and some striking advantages and drawbacks are compared. Six\nrelations characterize accuracy: \u2013 \u2013 \u2013 extreme underestimation, \u2013 \u2013 heavy underestimation,\n\u2013 underestimation, = ground truth, \u2248 good approximation, + overestimation and ++ heavy\noverestimation. Running times were calculated by running each algorithm for 9 different\nhyperparameter regimes and both likelihoods on all data sets. An average running time\nper data set was calculated for each method and scaled to yield 1 for LA. In the table, the\naverage of these numbers are shown. We are well aware of the fact, that these numbers\nalso depend on our Matlab implementations and choices of convergence thresholds.\n12.2.1 MEAN m AND (CO)VARIANCE V\nThe posterior process, or equivalently the posterior distribution over the latent values f, is deter-\nmined by its location parameter m and its width parameter V. In that respect, these two low-level\nquantities are the basis for all further calculations. In general, one can say that the methods show\n2057"},{"page":24,"text":"NICKISCH AND RASMUSSEN\n050100150\n0\n0.005\n0.01\n0.015\n0.02\n0.025\n0.03\n0.035\nmarginal # 353\n \n \nbest N(\u00b5,\u03c32)\nMC\nLA\nEP\nKL\nVB\nFigure 7: Marginals USPS 3 vs. 5 for digit #353 \u2261\npoint from Figure 6 is shown. Ground truth in terms of true marginal and best Gaus-\nsian marginal (matching the moments of the true marginal) are plotted in gray, Gaussian\napproximations are visualized as lines. For multivariate Gaussians N (m,V), the i-th\nmarginal is given by N ([m]i,[V]ii). Thus, the mode miof marginal i coincides with the i-\nth coordinate of the mode of the joint [m]i. This relation does not hold for general skewed\ndistribution. Therefore, the marginal given by the Laplace approximation is not centered\nat the mode of the true marginal.\n: Posterior marginals for one special training\nsignificant differences in the case of highly non-Gaussian posteriors (regimes 1-5 of Figure 3). Even\nin the two-dimensional toy example of Figures 4 and 5, significant differences are apparent. The\nmeans are inaccurate for LA and VB; whereas the variances are somewhat underestimated by LA\nand KL and severely so by VB. Marginal means m and variances dg(V) for USPS 3 vs. 5 are\nshown in Figure 6; an exemplary marginal is pictured in Figure 7 for all approximate methods and\nthe MCMC estimate. Along the same lines, a close-to-Gaussian posterior is illustrated in Figure 8.\nWe chose the hyperparameters for the non Gaussian case of Figure 6 to maximize the EP marginal\nlikelihood (see Figure 9), whereas the hyperparameters of Figure 8 were selected to yield a posterior\nthat is almost Gaussian but still has reasonable predictive performance.\nTheLAmethodhastheprincipledweaknessofexpandingaroundthemode. Inhigh-dimensional\nspaces, the mode can be very far away from the mean (Kuss and Rasmussen, 2005). The absolute\nvalue of the mean is strongly underestimated. Furthermore, the posterior is highly curved at its\nmode which leads to an underestimated variance, too. These effects can be seen in the first column\nof Figures 6 and 7, although in the close-to-Gaussian regime LA works well, Figure 8. For large\nlatent function scales \u03c32\nproaches the origin and the curvature at the mode becomes larger. Thus the approximate posterior\nas found by LA becomes a zero-mean Gaussian which is much too narrow.\nThe EP method almost perfectly agrees with the MCMC estimates, second column of Figure\n6. That means, iterative matching of approximate marginal moments leads to accurate marginal\nmoments of the posterior.\nThe KL method minimizes the KL-divergence KL(Q(f) ? P(f)) =RQ(f)lnQ(f)\nP(f) is very small, Q(f) has to be very small as well. In the limit that means P(f) = 0 \u21d2 Q(f) = 0.\nf, in the limit \u03c32\nf\u2192 \u221e, the likelihood becomes a step function, the mode ap-\nP(f)df with the av-\nerage taken to the approximate distribution Q(f). The method is zero-forcing i.e., in regions where\n2058"},{"page":25,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nTraining \u2248 Test marginals\n\u00b5 for EP\n\u22124 \u22122024\n\u22124\n\u22122\n0\n2\n4\n\u00b5 for LA\n\u22124 \u22122024\n\u22124\n\u22122\n0\n2\n4\n\u22124 \u22122024\n\u22124\n\u22122\n0\n2\n4\n\u00b5 for VB\n\u22124 \u22122024\n\u22124\n\u22122\n0\n2\n4\n\u00b5 for KL\n\u22124 \u22122024\n\u22124\n\u22122\n0\n2\n4\n\u00b5 for FV\n00.2 0.4 0.6\n0\n0.2\n0.4\n0.6\n\u03c3 for LA\n00.2 0.4 0.6\n0\n0.2\n0.4\n0.6\n\u03c3 for EP\n00.2 0.4 0.6\n0\n0.2\n0.4\n0.6\n\u03c3 for VB\n00.2 0.4 0.6\n0\n0.2\n0.4\n0.6\n\u03c3 for KL\n00.2 0.4 0.6\n0\n0.2\n0.4\n0.6\n\u03c3 for FV\n0 0.51\n0\n0.5\n1\np for LA\n00.51\n0\n0.5\n1\np for EP\n00.51\n0\n0.5\n1\np for VB\n00.51\n0\n0.5\n1\np for KL\n00.51\n0\n0.5\n1\np for FV\nFigure 8: Marginals of USPS 3 vs. 5 for a close-to-Gaussian posterior: Using the squared ex-\nponential covariance and the logistic likelihood function with parameters ln? = 3 and\nln\u03c3f= 0.5, we plot the marginal means, standard deviations and resulting predictive\nprobabilities in rows 1-3. Only the quantities for the trainings set are shown, because the\ntest set results are very similar. We are working in regime 8 of Figure 3 that means the\nposterior is of rather Gaussian shape. Each row consists of five plots showing MCMC\nground truth on the x-axis and LA, EP, VB, KL and FV on the y-axis.\nThus, the support of Q(f) is smaller than the support of P(f) and hence the variance is underesti-\nmated. Typically, the posterior has a long tail away from zero as seen in Figure 3 regimes 1-5. The\nzero forcing property shifts the mean of the approximation away from the origin, which results in a\nslightly overestimated mean, fourth column of Figure 6.\nFinally, the VB method can be seen as a more constrained version of the KL method with\ndeteriorated approximation properties. The variance underestimation and mean overestimation is\nmagnified, third column of Figure 6. Due to the required lower bounding property of each individual\nlikelihood term, the approximate posterior has to obey severe restrictions. Especially, the lower\nbound to the cumulative Gaussian cannot adjust its width since the asymptotic behavior does not\ndepend on the variational parameter (Equation 14).\nThe FV method has a special r\u00f4le because it does not lead to a Gaussian approximation to\nthe posterior but to the closest (in terms of KL-divergence) factorial distribution. If the prior is\nquite isotropic (regimes 1,4 and 7 of Figure 3), the factorial approximation provides a reasonable\napproximation. If the latent function values are correlated, the approximation fails. Because of\nthe zero forcing property, mentioned in the discussion of the KL method, both the means and the\nvariances are underestimated. Since a factorial distribution cannot capture correlations, the effect\ncan be severe. It is worth mentioning that there is no difference whether the posterior is close to a\n2059"},{"page":26,"text":"NICKISCH AND RASMUSSEN\nGaussian or not. In that respect, the FV method complements the LA method, which has difficulties\nin regimes 1, 2 and 4 of Figure 3.\n12.2.2 PREDICTIVE PROBABILITY p\u2217AND INFORMATION SCORE I\nLow-level features like posterior moments are not a goal per se, they are only needed for the purpose\nof calculating predictive probabilities. Figures 4 and 6 show predictive probabilities in the last row.\nIn principle, a bad approximation in terms of posterior moments can still provide reasonable\npredictions. Consider the predictive probability from Equation 16 using a cumulative Gaussian\nlikelihood\nZ\nIt is easy to see that the predictive probability p\u2217is constant if \u00b5\u2217\/?1+\u03c32\u2217is constant. That\nwhile keeping the sign of \u00b5\u2217fixed, does not affect the probabilistic prediction. In the limit of large\n\u00b5\u2217and large \u03c3\u2217, rescaling does not change the prediction.\nSummarizing all predictive probabilities piwe consider the scaled information score I. As a\nbaseline model we use the best model ignoring the inputs xi. This model simply returns predictions\nmatching the class frequencies of the training set\np\u2217\n=\nsigprobit(f\u2217)N (f\u2217|\u00b5\u2217,\u03c32\n\u2217)df\u2217= sigprobit(\u00b5\u2217\/\n?\n1+\u03c32\u2217).\nmeans, moving mean \u00b5\u2217and standard deviation \u03c3\u2217along the hyperbolic curve \u00b52\n\u2217\/C2\u2212\u03c32\n\u2217= 1,\nB\n=\n\u2212\n\u2211\ny={+1,\u22121}\nny\ntest\nn+1\ntest+n\u22121\ntest\nlog2\nny\ntrain\nn+1\ntrain+n\u22121\ntrain\n\u2264 1[bit].\nWe take the difference between the baseline B (entropy) and the average negative log predictive\nprobabilities log2P(y\u2217|x\u2217,y,X) to obtain the information score\nI\n=\nB+\n1\n2ntest\nntest\n\u2211\ni=1\n(1+yi)log2(pi)+(1\u2212yi)log2(1\u2212 pi),\nwhich is 1[bit] for perfect (and confident) prediction and 0[bits] for random guessing (for equiprob-\nable classes). Figures 9(c), 10(middle) and 11(c) contain information scores for 5 different approx-\nimation methods on two different data sets as a function of the hyperparameters of the covariance\nfunction. According to the EP and KL plots (most prominently in Figure 11(c)), there are two\nstrategies for a model to achieve good predictive performance:\n\u2022 Find a good length scale ? (e.g., ln? \u2248 2) and choose a latent function scale \u03c3fabove some\nthreshold (e.g., ln\u03c3f> 3).\n\u2022 Start from a good set of hyperparameters (e.g., ln? \u2248 2, ln\u03c3f\u2248 2) and compensate a harder\ncutting likelihood (\u03c32\nf\u2191) by making the data points more similar to each other (?2\u2191).\nThe LA method heavily underestimates the marginal means in the non-Gaussian regime (regimes\n1-5 of Figure 3). As a consequence, the predictive probabilities are strongly under-confident in the\nnon-Gaussian regime, first column of Figure 6. The information score\u2019s value is too small in the\nnon-Gaussian regime, Figures 9(c) and 11(c).\n2060"},{"page":27,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\n\u2212130\n\u2212130\n\u2212115\n\u2212115\n\u2212105\n\u2212105\n\u2212100\n\u2212200\n\u2212200\n\u2212160\n\u2212160\nlog Evidence for LA\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u2212130\n\u2212130\n\u2212115\n\u2212115\n\u2212105\n\u2212105\n\u2212100\n\u2212100\n\u221295\n\u221292\n\u2212200\n\u2212200\n\u2212160\n\u2212160\nlog Evidence for EP\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u2212130\n\u2212130\n\u2212115\n\u2212115\n\u2212160\n\u2212105\n\u2212105\n\u2212100\n\u2212200\n\u2212200\n\u2212160\nlog Evidence for KL\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u2212200\n\u2212200\n\u2212160\n\u2212160\nlog Evidence for VB\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n(a) Evidence\n\u2212130\n\u2212130\n\u2212115\n\u2212115\n\u2212200\n\u2212200\n\u2212200\n\u2212160\n\u2212160\n\u2212160\nlog Evidence for LA\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u2212130\n\u2212130\n\u2212160\n\u2212115\n\u2212115\n\u2212105\n\u2212105\n\u2212100\n\u2212200\n\u2212200\n\u2212160\nlog Evidence for EP\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u2212130\n\u2212130\n\u2212115\n\u2212115\n\u2212105\n\u2212105\n\u2212100\n\u2212200\n\u2212200\n\u2212160\n\u2212160\nlog Evidence for KL\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u2212130\n\u2212130\n\u2212200\n\u2212200\n\u2212200\n\u2212160\n\u2212160\nlog Evidence for VB\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n(b) Lower bound on evidence\n0.7\n0.7\n0.7\n0.8\n0.8\n0.84\n0.25\n0.25\n0.5\n0.5\n0.5\nInformation [bits] for LA\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n0.7\n0.7\n0.8\n0.8\n0.84\n0.84\n0.86\n0.86\n0.88\n0.25\n0.5\n0.5\nInformation [bits] for EP\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n0.7\n0.7\n0.8\n0.8\n0.84\n0.84\n0.86\n0.86\n0.88\n0.89\n0.25\n0.5\n1\n0.5\nInformation [bits] for KL\nln(l)\nln(\u03c3f)\n2345\n0\n1\n2\n3\n4\n5\n0.7\n0.7\n0.7\n0.8\n0.8\n0.8\n0.84\n0.84\n0.84\n0.86\n0.86\n0.88\n0.88\n0.89\n0.25\n0.5\n0.5\nInformation [bits] for VB\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n(c) Information in bits\n16\n18\n18\n18\n20\n20\n20\n25\n25\n25\n30\n30\n30\n30\n35\n35\n35\n40\n45\n40\n45\n50\n50\nNo test errors for LA\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n18\n18\n18\n18\n20\n20\n25\n30\n25\n30\n35\n40\n3\n45\n50\nNo test errors for EP\nln(l)\nln(\u03c3f)\n1245\n0\n1\n2\n3\n4\n5\n18\n18\n18\n18\n18\n20\n25\n25\n30\n30\n35\n40\n45\n50\nNo test errors for KL\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n16\n18\n20\n20\n25\n25\n30\n30\n30\n30\n35\n35\n40\n45\n50\n4\nNo test errors for VB\nln(l)\nln(\u03c3f)\n1235\n0\n1\n2\n3\n4\n5\n(d) Number of errors\nFigure 9: Evidence and classification performance for LA, EP, KL & VB on USPS 3 vs. 5: The\nlength scale ? and the latent scale \u03c3fdetermine the working regime (1-9) of the Gaussian\nProcess as drafted in Figure 3. We use the logistic likelihood and the squared exponential\ncovariance function to classify handwritten digits. The four panels illustrate the model\nperformance in terms of evidence, information and classification errors over the space\nof hyperparameters (?,\u03c3f). For better visibility we choose a logarithmic scale of the\naxes. Panel (a) shows the inherent evidence approximation of the four methods and panel\n(b) contains the Jensen lower bound (Equation 9) on the evidence used in KL method.\nBoth panels share the same contour levels for all four methods. Note that for the VB\nmethod, the general lower bound is a better evidence estimate than the bound provided\nby the method itself. Panel (c) and (d) show the information score and the number of\nmisclassifications. One can read-off the divergence between posterior and approximation\nby recalling KL(Q||P) = lnZ \u2212lnZBfrom Equation 10 and assuming lnZEP\u2248 lnZ. In\nthe figure this corresponds to subtracting Subplots (b, LA-VB) from Subplots (a, EP).\nObviously, the divergence vanishes for close-to-Gaussian posteriors (regimes 3,5-6,7-9).\n2061"},{"page":28,"text":"NICKISCH AND RASMUSSEN\n\u2212250\n\u2212750\n\u2212500\n\u2212400\n\u2212300\nlog Evidence for FV\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n0.7\n0.7\n0.8\n0.84\n0.8\n0.86\n0.88\n0.25\n0.5\n2\nInformation [bits] for FV\nln(l)\nln(\u03c3f)\n1345\n0\n1\n2\n3\n4\n5\n16\n16\n18\n18\n18\n20\n25\n25\n30\n35\n4\n30\n30\n35\n30\n35\n40\n3\n40\n45\n50\n45\n50\nNo test errors for FV\nln(l)\nln(\u03c3f)\n125\n0\n1\n2\n3\n4\n5\nFigure 10: Evidence and classification performance for FV on USPS 3 vs. 5: The plots are a sup-\nplement to Figure 9 in that they make the factorial variational method comparable, even\nthough we use the cumulative Gaussian likelihood. The levels of the contour lines for\nthe information score and the number of misclassifications are the same as in Figure 9.\nFor the marginal likelihood other contours are shown, since it has significantly different\nvalues.\nSince the EP algorithm yields marginal moments very close to the MCMC estimates (second\ncolumn of Figure 6), its predictive probabilities and information score is consequently also very\naccurate, Figures 9(c) and 11(c). The plots corresponding to EP can be seen as the quasi gold\nstandard (Kuss and Rasmussen, 2005, Figures 4 and 5).\nThe KL method slightly underestimates the variance and slightly overestimates the mean which\nleads to slightly overconfident predictions, fourth column of Figure 6. Overconfidence, in general,\nleads to a degradation of the information score, however in this example, the information score is\nvery close to the EP values and at the peak it is even slightly (0.01[bits]) higher, Figures 9(c) and\n11(c).\nThe VB method, again, has the same problems as the KL method only amplified. The predic-\ntions are overconfident, third column of Figure 6. Consequently, the information measured score\nin the non-Gaussian regime is too small. The logistic likelihood function (Figure 9(c)) yields much\nbetter results than the cumulative Gaussian likelihood function (Figure 11(c)).\nFinally, as the FV method is accurate if the prior is isotropic, predictive probabilities and in-\nformation scores are very high in regimes 1, 4 and 7 of Figure 3. For correlated priors, the FV\nmethod achieves only low information scores, Figure 10(middle). The method seems to benefit\nfrom the \u201chyperbolic scaling invariance\u201d of the predictive probabilities mentioned earlier in that\nsection because both the mean and the variance are strongly underestimated.\n12.2.3 NUMBER OF ERRORS E\nIf one is only interested in the actual class and not in the associated confidence level, one can simply\nmeasure the number of misclassifications. Results for 5 approximation methods and 2 data sets are\nshown in Figures 9(d), 10(right) and 11(d).\nInterestingly, all four Gaussian approximation have very similar error rates. The reason is\nmainly due to the fact that all methods manage to compute the right sign of the marginal mean.\nOnly the FV method with cumulative Gaussian likelihood seems a bit problematic, even though the\n2062"},{"page":29,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\n\u221265\n\u221265\n\u221265\n\u221270\n\u221260\n\u221260\n\u221260\n\u221280\n\u221275\n\u221275\n\u221270\n\u221270\nlog Evidence for LA\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u221265\n\u221265\n\u221260\n\u221260\n\u221260\n\u221255\n\u221255\n\u221275\n\u221270\n\u221270\nlog Evidence for EP\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u221265\n\u221265\n\u221265\n\u221260\n\u221260\n\u221260\n\u221255\n\u221280\n\u221275\n\u221275\n\u221270\n\u221270\n\u221270\nlog Evidence for KL\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u221280\n\u221275\n\u221280\n\u221275\n\u221270\n\u221270\n\u221275\nlog Evidence for VB\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n(a) Evidence\n\u221265\n\u221265\n\u221265\n\u221260\n\u221260\n\u221260\n\u221280\n\u221280\n\u221275\n\u221275\n\u221275\n\u221270\n\u221270\n\u221270\nlog Evidence for LA\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u221265\n\u221265\n\u221265\n\u221270\n\u221260\n\u221260\n\u221260\n\u221280\n\u221275\n\u221270\n\u221280\n\u221275\n\u221270\n\u221275\nlog Evidence for EP\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u221265\n\u221265\n\u221265\n\u221260\n\u221260\n\u221260\n\u221255\n\u221280\n\u221275\n\u221275\n\u221270\n\u221270\n\u221270\nlog Evidence for KL\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n\u221265\n\u221265\n\u221265\n\u221270\n\u221280\n\u221270\n\u221280\n\u221270\n\u221275\n\u221275\n\u221275\nlog Evidence for VB\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n(b) Lower bound on evidence\n0.3\n0.3\n0.3\n0.05\n0.05\n0.1\n0.1\n0.2\n0.2\n0.2\nInformation [bits] for LA\nln(l)\nln(\u03c3f)\n12345\n0\n1\n2\n3\n4\n5\n0.3\n0.3\n0.3\n0.4\n0.4\n0.05\n4\n0.1\n4\n0.1\n0.2\n0.2\n0.2\nInformation [bits] for EP\nln(l)\nln(\u03c3f)\n1235\n0\n1\n2\n3\n5\n0.3\n0.3\n0.2\n0.3\n0.4\n0.4\n0.5\n0.05\n0.1\n1\n0.1\n0.1\n0.2\n0.2\nInformation [bits] for KL\nln(l)\nln(\u03c3f)\n12345\n0\n2\n3\n4\n5\n0.3\n0.3\n0.3\n0.4\n0.05\n0.05\n0.05\n0.1\n0.1\n0.1\n0.2\n1\n0.2\n0.2\nInformation [bits] for VB\nln(l)\nln(\u03c3f)\n2345\n0\n1\n2\n3\n4\n5\n(c) Information in bits\n13\n15\n15\n17\n17\n20\n1\n20\n25\n25\n25\n25\n25\n30\n35\nNo test errors for LA\nln(l)\nln(\u03c3f)\n2345\n0\n1\n2\n3\n4\n5\n15\n17\n17\n20\n25\n20\n20\n25\n25\n30\n2\n30\n35\nNo test errors for EP\nln(l)\nln(\u03c3f)\n1345\n0\n1\n2\n3\n4\n5\n15\n17\n20\n25\n20\n25\n30\n25\n30\n2\n30\n35\nNo test errors for KL\nln(l)\nln(\u03c3f)\n1345\n0\n1\n2\n3\n4\n5\n13\n15\n15\n17\n17\n20\n1\n20\n25\n25\n25\n25\n25\n30\n2\n35\nNo test errors for VB\nln(l)\nln(\u03c3f)\n345\n0\n1\n2\n3\n4\n5\n(d) Number of errors\nFigure 11: Evidence and classification performance for LA, EP, KL & VB on Sonar: We show the\nsame quantities as in Figure 9, only for the Sonar Mines versus Rocks data set and using\nthe cumulative Gaussian likelihood function.\ndifference is only very small. Small error rates do not imply high information scores, it is rather the\nother way round. In Figure 9(d) at ln? = 2 and ln\u03c3f= 4 only 16 errors are made by the LA method\nwhile the information score (Figure 9(c)) is only of 0.25[bits].\nEven the FV method yields very accurate classes, having only small error rates.\n2063"},{"page":30,"text":"NICKISCH AND RASMUSSEN\n12.2.4 MARGINAL LIKELIHOOD Z\nAgreement of model and data is typically measured by the marginal likelihood Z. Hyperparameters\ncan conveniently be optimized using Z not least because the gradient\u2202lnZ\nefficiently computed for all methods. Formally, the marginal likelihood is the volume of the product\nof prior and likelihood. In classification, the likelihood is a product of sigmoid functions (Figure\n3), so that only the orthant {f|f?y \u2265 0 \u2208 Rn} contains values P(f|y) \u22651\nare bounded by lnZ \u2264 0 where lnZ = 0 corresponds to a perfect model. As pointed out in Section\n2.1.1, the marginal likelihood for a model ignoring the data and having equiprobable targets has the\nvalue lnZ = \u2212nln2, which serves as a baseline.\nEvidences provided by LA, EP and VB for two data sets are shown in Figures 9(a), 10(left) and\n11(a). As the Jensen bound can be applied to any Gaussian approximation of the posterior, we also\nreport it in Figures 9(b) and 11(b).\nThe LA method strongly underestimates the evidence in the non-Gaussian regime, because it is\nforced to center its approximation at the mode, Figures 9(a) and 11(a). Nevertheless, there is a good\nagreement between the value of the marginal likelihood and the corresponding information score.\nThe Jensen lower bound is not tight for the LA approximation, Figures 9(b) and 11(b).\nThe EP method yields the highest values among all other methods. As described in Section\n2.1.2, for high latent function scales \u03c32\nbehavior is only to be seen for the EP method, Figures 9(a) and 11(a). Again, the Jensen bound\nis not tight for the EP method, Figures 9(b) and 11(b). The difference between EP and MCMC\nmarginal likelihood estimate is vanishingly small (Kuss and Rasmussen, 2005, Figures 4 and 5).\nThe KL method directly uses the Jensen bound (Equation 8) which can only be tight for Gaus-\nsian posterior distributions. If the posterior is very skew, the bound inherently underestimates the\nmarginal likelihood. Therefore, Figures 9(a) and 9(b) and Figures 11(a) and 11(b) show the same\nvalues. The disagreement between information score and marginal likelihood makes hyperparame-\nter selection based on the KL method problematic.\nThe VB method\u2019s lower bound on the evidence turns out to be very loose, Figures 9(a) and\n11(a). Theoretically, it cannot be better than the more general Jensen bound due to the additional\nconstraints imposed by the individual bound on each likelihood factor, Figures 9(b) and 11(b). In\npractice, one uses the Jensen bound for hyperparameter selection. Again, the maximum of the\nbound to the evidence is not very helpful for finding regions of high information score.\nFinally, the FV method only yields a poor approximation to the marginal likelihood due to the\nfactorial approximation, Figure 10. The more isotropic the model becomes (small ?), the tighter\nis the bound. For strongly correlated priors (large ?) the evidence drops even below the baseline\nlnZ =\u2212nln2. Thus, the bound is not adequate to do hyperparameter selection as its maximum does\nnot lie in regions with high information score.\n\u2202\u03b8can be analytically and\n2. In principle, evidences\nf, the model becomes effectively independent of \u03c32\nf. This\n12.2.5 CHOICE OF LIKELIHOOD\nIn the experiments, we worked with two different likelihood functions, namely the logistic and\nthe cumulative Gaussian likelihood. The two functions differ in their slope at the origin and their\nasymptotic behavior. We did not find empirical evidence supporting the use of either likelihood.\nTheoretically, the cumulative Gaussian likelihood should be less robust against outliers due to the\nquadratic asymptotics. Practically, the different slopes result in a shift of the latent function length\nscale in the order of ln1\n4\u2212ln\n1\n\u221a2\u03c0\u2248 0.46 on a log scale in that the logistic likelihood prefers a\n2064"},{"page":31,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nbigger latent scale. Only for the VB method, differences were significant because the logistic bound\nis more concise. Numerically, however the cumulative Gaussian is preferable.\n12.3 Results Across Data Sets\nWe conclude with a quantitative summary of experiments conducted on 6 data sets (breast, crabs,\nionosphere, diabetes, sonar, USPS 3 vs. 5), two different likelihoods (cumulative Gaussian, logistic)\nand 8 covariance functions (linear, polynomial of degree 1-3, Mat\u00e9rn \u03bd \u2208 {3\ntial and neural network) resulting in 96 trials. All 7 approximate classification methods were trained\non a 16\u00d716 grid of hyperparameters to compare their behavior under a wide range of conditions.\nWe calculated the maximum (over the hyperparameter grid) amount of information, every algorithm\nmanaged to extract from the data in each of the 96 trials. The table shows the number of trials, where\nthe respective algorithm had a maximum information score that was above the mean\/median (over\nthe 7 methods).\n2,5\n2}, squared exponen-\nTest \\ Method\n# trials, information below mean\n# trials, information below median\nLA\n31\n54\nEP\n0\n0\nKL\n0\n0\nVB\n6\n15\nFV\n34\n48\nLR\n92\n96\nTAPnaive\n31\n51\n13. Conclusions\nIn the present paper we provide a comprehensive overview of methods for approximate Gaussian\nprocess classification. We present an exhaustive analysis of the considered algorithms using the-\noretical arguments. We deliver thorough empirical evidence supporting our insights revealing the\nstrengthsandweaknessesofthealgorithms. Finally, wemakeaunifiedandmodularimplementation\nof all methods available to the research community.\nWe are able to conclude that the Expectation Propagation algorithm is, in terms of accuracy,\nalways the method of choice, except when you cannot afford the slightly longer running time com-\npared to the Laplace approximation.\nOur comparisons include the Laplace approximation and the Expectation Propagation algorithm\n(Kuss and Rasmussen, 2005). We extend the latter to the logistic likelihood. We apply Kullback-\nLeibler divergence minimization to Gaussian process classification and derive an efficient Newton\nalgorithm. Although the principles behind this method have been known for some time, we are\nunaware that this method has been previously implemented for GPs in practise. The existing varia-\ntional method (Gibbs and MacKay, 2000) is extended by a lower bound on the cumulative Gaussian\nlikelihood and we provide an implementation based on Newton\u2019s method. Furthermore, we give a\ndetailed analysis of the Factorial Variational method (Csat\u00f3 et al., 2000).\nAll methods are considered in a common framework, approximation quality is assessed, predic-\ntive performance is measured and model selection is benchmarked.\nIn practice, an approximation method has to satisfy a wide range of requirements. If runtime\nis the major concern or one is interested in error rate only, the Laplace approximation or label\nregression should be considered. Only Expectation Propagation and\u2014although a lot slower\u2014the\nKL-method deliver accurate marginals as well as reliable class probabilities and allow for faithful\nmodel selection.\nIf an application demands a non-standard likelihood function, this also affects the choice of\nthe algorithm: The Laplace approximation requires derivatives, Expectation Propagation and the\n2065"},{"page":32,"text":"NICKISCH AND RASMUSSEN\nFactorial Variational method need integrability with respect to Gaussian measures. However, the\nKL-method simply needs to evaluate the likelihood and known lower bounds naturally lead to the\nVB algorithm.\nFinally, if the classification problem contains a lot of label noise (\u03c3fis small), the exact pos-\nterior distribution is effectively close to Gaussian. In that case, the choice of the approximation\nmethod is not crucial since in the Gaussian regime, they will give the same answer. For weakly\ncoupled training data, the Factorial Variational method can lead to quite reasonable approximations.\nAs a future goal remains an in-depth understanding of the properties of sparse and online ap-\nproximations to the posterior and a coverage of a broader range of covariance functions. Also, the\napproximation techniques discussed can be applied to other non-Gaussian inference problems be-\nsides the narrow applications to binary GP classification discussed here, and there is hope that some\nof the insights presented may be useful more generally.\nAcknowledgments\nThanks to Manfred Opper for pointing us initially to the practical possibility of the KL method and\nthe three anonymous reviewers.\nAppendix A. Derivatives\nIn the following, we provide the expressions for the derivatives needed to implement the VB and\nthe KL method.\nA.1 Derivatives for VB\nSome notational remarks. Partial derivatives w.r.t. one single parameter such as\nmatrices or vectors, respectively. Lowercase letters {a,b,c}\u03c2indicate vectors, upper case letters\n{A,B,C}\u03c2stand for the corresponding diagonal matrices with the vector as diagonal. The dot\nnotation applies to both lower and uppercase letters and denote derivatives w.r.t. the variational\nparameter vector \u03c2\n\u2202A\u03c2\n\u2202\u03c2ior\u2202b\u03c2\n\u2202\u03c2istay\n\u02d9 a\u03c2\n:=\n?\u2202a\u03c2i\n?\u22022a\u03c2i\nDg(\u02d9 a\u03c2).\n\u2202\u03c2i\n?\ni\n=\u2202a\u03c2\n\u2202\u03c2, vector,\n=\u22022a\u03c2\n\u2202\u03c22, vector,\n\u00a8 a\u03c2\n:=\n\u2202\u03c22\ni\n?\ni\n\u02d9A\u03c2\n:=\nThe operators Dg : Rn\u2192 Rn\u00d7nand dg : Rn\u00d7n\u2192 Rnmanipulate matrix diagonals. The result of\nDg(x) is a diagonal matrix X containing x as diagonal, whereas dg(X) returns the diagonal of X as\na vector. Hence, we have Dg(dg(x)) = x, but in general dg(Dg(X)) = X does only hold true for\ndiagonal matrices.\n2066"},{"page":33,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nA.1.1 SOME SHORTCUTS USED LATER ONWARDS\n\u02dcK\u03c2\n:=\n?K\u22121\u22122A\u03c2\nDg(y)b\u03c2= y?b\u03c2,\n\u02dcK\u03c2\u02dcb\u03c2=?K\u22121\u22122A\u03c2\n\u02dcK\u03c2\n\u2202\u03c2j\n\u02dcK\u03c2K\u22121\u2202K\n\u2202\u03b8iK\u22121\u02dcK\u03c2(y?b\u03c2),\n\u2202l\u03c2\n\u2202\u03c2?=\u02dcK\u03c2\n\u02d9b\u03c2?y?l\u03c2+dg\n\u02d9b\u03c2?y?l\u03c2+l\u03c2?l\u03c2? \u02d9 a\u03c2,\ny?l\u03c2?\u2202\u02d9b\u03c2\n\u2202\u03c2j\n\u2202r\u03c2\n\u2202\u03c2?= Dg?y?\u02d9b\u03c2+2l\u03c2? \u02d9 a\u03c2\n?\u22121 condK small\n=\nK\u2212K\n?\nK\u22121\n2A\u22121\n\u03c2\n?\u22121\nK,\n\u02dcb\u03c2\nl\u03c2\n\u2202l\u03c2\n\u2202\u03c2j\n\u2202l\u03c2\n\u2202\u03b8i\n\u02d9L\u03c2\n:=\n:=\n?\u22121(y?b\u03c2),\n\u2202\u03c2j\n=\n?\n2\u2202A\u03c2\nl\u03c2+y?\u2202b\u03c2\n?\n,\n=\n:=\n?2Dg(l\u03c2)\u02d9A\u03c2+Dg(y)\u02d9B\u03c2\nl\u03c2l?\n?,\nr\u03c2\n:=\n?\n\u03c2\u02d9A\u03c2\n?\n\u2202\u03c2j+2l\u03c2? \u02d9 a\u03c2?\u2202l\u03c2\n?\u02d9L\u03c2+Dg?l\u03c2??y?\u00a8b\u03c2+l\u03c2? \u00a8 a\u03c2\n=\n\u2202r\u03c2\n\u2202\u03c2j\n=+\u02d9b\u03c2?y?\u2202l\u03c2\n\u2202\u03c2j+l\u03c2?l\u03c2?\u2202\u02d9 a\u03c2\n\u2202\u03c2j,\n\u02d9R\u03c2\n:=\n??\n=\nDg?y?\u02d9b\u03c2+2l\u03c2? \u02d9 a\u03c2\n?\u02dcK\u03c2Dg?y?\u02d9b\u03c2+2l\u03c2? \u02d9 a\u03c2\n?+Dg?l\u03c2??y?\u00a8b\u03c2+l\u03c2? \u00a8 a\u03c2\n??.\nA.1.2 FIRST DERIVATIVES W.R.T. VARIATIONAL PARAMETERS \u03c2iYIELDING THE GRADIENT\nlnZB\n=\nc?\n\u03c2\n\u2202ci\n\u2202\u03c2i+\u02dcb?\n\u2202ci\n\u2202\u03c2i+l?\n?\u2202ci\n?\u2202ci\n?\u2202ci\n\u02d9 c\u03c2+l\u03c2??\u02d9b\u03c2?y+l\u03c2? \u02d9 a\u03c2\n?\n+1\n2\n\u02dcb?\n\u03c2\u02dcK\u03c2\u02dcb\u03c2\u22121\n?\ny?\u2202b\u03c2\n\u2202\u03c2i\n+\u02d9b\u03c2?y??\u02dcK\u03c2\u02dcb\u03c2\n+\u02d9b\u03c2?y?l\u03c2+dg\n+r\u03c2+dg?\u02dcK\u03c2\u02d9A\u03c2\n2ln|I\u22122A\u03c2K|,\ny?\u2202b\u03c2\n\u2202\u03c2i\n+\u2202A\u03c2\n\u2202\u03c2i\n\u2202lnZB\n\u2202\u03c2i\n=\n\u03c2\u02dcK\u03c2\n?\n+\u2202A\u03c2\n\u2202\u03c2i\n\u02dcK\u03c2\u02dcb\u03c2\n?\n\u02dcK\u03c2\u2202A\u03c2\n+tr\n?\n(I\u22122A\u03c2K)\u2212?K\u2202A\u03c2\n?\n\u03c2\u02dcK\u03c2\u02d9A\u03c2\n\u2202\u03c2i\n?\nl\u03c2,\u02dcK\u03c2\n=\n\u03c2\nl\u03c2\n?\n+tr\n?\n\u02dcK\u03c2\u02dcb\u03c2\u02dcb?\n\u2202\u03c2i\n,\n\u2202lnZB\n\u2202\u03c2\n=\n\u2202\u03c2i\n?\n?\n?\ni\n?+dg\n?\n?\n??\n?\n+dg?\u02dcK\u03c2\u02d9A\u03c2\n?\nl\u03c2=\n\u2202\u03c2i\ni\nl\u03c2l?\n\u03c2\u02d9A\u03c2\n?\n+dg?\u02dcK\u03c2\u02d9A\u03c2\nr\u03c2=\n\u2202\u03c2i\ni\n=\n?+dg?\u02dcK\u03c2\n2067\n?? \u02d9 a\u03c2."},{"page":34,"text":"NICKISCH AND RASMUSSEN\nA.1.3 SECOND DERIVATIVES W.R.T. VARIATIONAL PARAMETERS \u03c2iYIELDING THE HESSIAN\n?\n\u22022lnZB\n\u2202\u03c2\u2202\u03c2?\n\u2202\u03c22\ni\nii\n=\n\u22022lnZB\n\u2202\u03c2j\u2202\u03c2i\n=\n\u22022ci\n\u2202\u03c2j\u2202\u03c2i+\u2202r\u03c2,i\n?\u22022ci\n\u00a8C\u03c2+\u02d9R\u03c2+2?\u02dcK\u03c2\u02d9A\u03c2\n\u2202\u03c2j\n\u2202\u03c2?+2?\u02dcK\u03c2\u02d9A\u03c2\n+tr2\u02dcK\u03c2\u2202A\u03c2\n\u2202\u03c2j\n???\u02dcK\u03c2\u02d9A\u03c2\n???\u02dcK\u03c2\u02d9A\u03c2\n\u02dcK\u03c2\u2202A\u03c2\n\u2202\u03c2i\n+\u02dcK\u03c2\n\u22022A\u03c2\n\u2202\u03c2j\u2202\u03c2i\n?\n,\n=\n?\n+\u2202r\u03c2\n??+Dg?dg(\u02dcK\u03c2)? \u00a8 a\u03c2\n??+Dg?dg(\u02dcK\u03c2)? \u00a8 a\u03c2\n?\n?.\nA.1.4 MIXED DERIVATIVES W.R.T. HYPER- \u03b8iAND VARIATIONAL PARAMETERS \u03c2i\n\u22022lnZB\n\u2202\u03b8i\u2202\u03c2\n=\n\u02d9 a\u03c2?\n\u2202\n\u2202\u03b8i\n?\n?l\u03c2?l\u03c2+dg?\u02dcK\u03c2\n2l\u03c2?\u2202l\u03c2\n??+\u02d9b\u03c2?y?\u2202l\u03c2\n\u02dcK\u03c2K\u22121\u2202K\n\u2202\u03b8iK\u22121\u02dcK\u03c2\n\u2202\u03b8i\n=\n\u02d9 a\u03c2?\n\u2202\u03b8i+dg\n???\n+\u02d9b\u03c2?y?\u2202l\u03c2\n\u2202\u03b8i.\nA.1.5 FIRST DERIVATIVES W.R.T. HYPERPARAMETERS \u03b8i:\nFor a gradient optimization with respect to \u03b8, we need the gradient of the objective \u2202lnZB\/\u2202\u03b8.\nNa\u00efvely, the gradient is given by:\n\u2202lnZB\n\u2202\u03b8i\n=\n1\n2\n1\n2l?\n\u02dcb?\n\u03c2\u02dcK\u03c2K\u22121\u2202K\n\u2202\u03b8iK\u22121\u02dcK\u03c2\u02dcb\u03c2+tr\n?\n(I\u22122A\u03c2K)\u2212?A\u03c2\u2202K\n(I\u22122A\u03c2K)\u2212?A\u03c2\u2202K\n\u2202\u03b8i\n?\nl\u03c2=\n\u03c2K\u22121\u2202K\n\u2202\u03b8iK\u22121l\u03c2+tr\n?\n\u2202\u03b8i\n?\n.\nHowever, the optimal variational parameter \u03c2\u2217depends implicitly on the actual choice of \u03b8 and one\nhas to account for that in the derivative by adding an extra \u201cimplicit\u201d term\n????\u03c2=\u03c2\u2217\ntheorem for continuous and differentiable functions F:\n\u2202lnZB(\u03b8,\u03c2)\n\u2202\u03b8i\n=\n\u2202lnZB(\u03b8,\u03c2\u2217)\n\u2202\u03b8i\n+\nn\n\u2211\nj=1\n\u2202lnZB(\u03b8,\u03c2\u2217)\n\u2202\u03c2\u2217\nj\n\u2202\u03c2\u2217\n\u2202\u03b8i.\nj\nThe question of how to find an expression for\u2202\u03c2\u2217\n\u2202\u03b8can be solved by means of the implicit function\nF : Rp\u00d7Rn\u2192 Rn,\nSetting F(x,y) \u2261\u2202lnZB\nF(x,y) = 0\n\u21d2\n\u2202y\n\u2202x(x) = \u2212\n?\u2202F\n\u2202y(x,y(x))\n?\u22121\u2202F\n\u2202x(x,y(x))ifF(x,y(x)) = 0.\n\u2202\u03c2(\u03b8,\u03c2) leads to\n\u2202\u03c2\u2217\n\u2202\u03b8?\n\u03b8\n=\n\u2212\n?\u22022lnZB(\u03b8,\u03c2\u2217\n\u03b8)\n\u2202\u03c2\u2202\u03c2?\n?\u22121\u22022lnZB(\u03b8,\u03c2\u2217\n\u03b8)\n\u2202\u03b8?\u2202\u03c2\nand in turn combines to\n\u2202lnZB\n\u2202\u03b8i\n????\u03c2=\u03c2\u2217\n=\n\u2202lnZB\n\u2202\u03b8i\n\u2212\n?\u2202lnZB\n\u2202\u03c2\n???\u22022lnZB\n\u2202\u03c2\u2202\u03c2?\n?\u22121\u22022lnZB\n\u2202\u03b8i\u2202\u03c2\nwhere all terms are known.\n2068"},{"page":35,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nA.2 Derivatives for KL\nThe lower bound lnZBto the log marginal likelihood lnZ is given by Equation 9 as\nlnZ\n\u2265\n= lnZB(m,V) = a(y,m,V)+1\n2ln??VK\u22121??+n\nRN (fi|mi,vii)lnsig(yifi)dfi. As a first step, we calcu-\n2\u22121\n2m?K\u22121m\u22121\n2tr?VK\u22121?\nwhere we used the shortcut a(y,m,V)=\u2211n\nlate the first derivatives of lnZBwith respect to the posterior moments m and V to derive necessary\nconditions for the optimum by equating them with zero:\ni=1\n\u2202lnZB\n\u2202V\n=\u2202a(y,m,V)\n\u2202V\n\u2202lnZB\n\u2202m\n+1\n2V\u22121\u22121\n2K\u22121 != 0\n\u21d2\nV =\n?\nK\u22121\u22122Dgdg\u2202a\nm = K\u2202a\n\u2202m.\n\u2202V\n?\u22121\n,\n=\u2202a(y,m,V)\n\u2202m\n\u2212K\u22121m\n!= 0\n\u21d2\nThese two expressions are plugged in the original expression for lnZBusing A = (I\u22122K\u039b)\u22121and\n\u039b = Dgdg\u2202a\n\u2202Vto yield:\nlnZB(\u03b1,\u039b)=\na?y,K\u03b1,(K\u22121\u22122\u039b)\u22121?+1\n2ln|A|\u22121\n2trA+n\n2\u22121\n2\u03b1?K\u03b1.\nOur algorithm uses the parameters \u03b1, \u039b, so we calculate first and second derivatives to implement\nNewton\u2019s method.\nA.2.1 FIRST DERIVATIVES W.R.T. PARAMETERS \u03b1, \u039b YIELDING THE GRADIENT\n\u2202lnZB\n\u2202\u03bb\n=\u2202a\n\u2202\u03bb+dg(V)\u2212dg(VA?)\nand\n\u2202lnZB\n\u2202\u03b1\n=\u2202a\n\u2202\u03b1\u2212K\u03b1.\nOnly the terms containing derivatives of a need further attention, namely\n\u2202a\n\u2202\u03b1= K\u2202a\n\u2202m\nand\nd(dgV)=\ndg\n?\nd?K\u22121\u22122\u039b?\u22121?\n2(V?V)d\u03bb \u21d2\u2202dgV\n2(V?V)\u2202a(y,m,V)\n= 2dg[Vd\u039bV] = 2dg\n?\n\u2211\nk\nvkv?\nkd\u03bbk\n?\n= 2\u2211\nk\n(vk?vk)d\u03bbk\n=\n\u2202\u03bb?= 2V?V,\n\u2202a\n\u2202\u03bb\n=\n\u2202dgV\n.\nAs a last step, the derivatives w.r.t. m and the diagonal part of V yield\n2069"},{"page":36,"text":"NICKISCH AND RASMUSSEN\n\u2202a\n\u2202mi\n=\nZ\u2202N (f|mi,vii)\n1\n\u221avii\n\u2202mi\nf \u00b7N (f)lnsig(\u221aviiyif +miyi)df,\nlnsig(yif)df =\nZ\nf \u2212mi\nvii\nN (f|mi,vii)lnsig(yif)df\n=\nZ\n\u2202a\n\u2202vii\n=\nZ\u2202N (f|mi,vii)\n1\n2vii\n\u2202vii\nlnsig(yif)df =\nZ\n\uf8eb\n\uf8ed(f \u2212mi)2\nv\n3\n2\nii\n\u2212\n1\n\u221avii\n\uf8f6\n\uf8f8N (f|mi,vii)lnsig(yif)df\n=\nZ?f2\u22121?\u00b7N (f)lnsig(\u221aviiyif +miyi)df.\nA.2.2 SECOND DERIVATIVES W.R.T. PARAMETERS \u03b1, \u039b YIELDING THE HESSIAN\nAgain, we proceed in two steps, calculating derivatives w.r.t. \u03b1 and \u039b and by the chain rule compute\nthose w.r.t. m and V.\n\u22022lnZB\n\u2202\u03b1\u2202\u03b1?\n=\n\u22022a\n\u2202\u03b1\u2202\u03b1?+K =\n\u2202\n\u2202\u03b1\n\u2202m?\n\u22022a\n\u2202m\u2202m?K+K,\n\u22022a\n\u2202\u03bb\u2202\u03b1?=\n\u2202\n\u2202\u03b1\n?\n\u2202a\n\u2202m?\n\u2202m\n\u2202\u03b1?\n\u2202\n\u2202m\n?\n?\n+K =\n\u2202\n\u2202\u03b1\n?\n\u2202a\n\u2202m?K\n?\n+K\n=\n?\n\u2202a\n?\nK+K =\u2202m?\n\u2202\u03b1\n\u2202a\n\u2202m?\n?\nK+K\n=\nK\n\u22022lnZB\n\u2202\u03bb\u2202\u03b1?\n=\n\u2202\n\u2202\u03bb\n\u22022a\n?\n\u2202a\n\u2202m?\n?\nK =\u2202(dgV)?\n\u2202\u03bb\n\u2202\n\u2202dgV\n?\n\u2202a\n\u2202m?\n?\nK\n=\n2V?V\n\u22022a\n\u2202\u03bb\u2202\u03bb?+R,\n2\u2202\n\u2202\u03bb\n\u2202dgV\u2202m?K,\n\u22022lnZB\n\u2202\u03bb\u2202\u03bb?\n=\nR := 2V?(V\u2212AV?\u2212VA?)\n?\n?\n\u2202(dgV)?\n\u22022a\n\u2202dgV\u2202(dgV)?V?V+4\n=\n?\n\u2202a\n\u2202(dgV)?V?V\n\u22022a\n\u2202\u03bb\u2202(dgV)?V?V+2\n+R\n=\n2\n\u2202a\n\u2202V?V\n\u2202\u03bbi\n?\n\u2202(dgV)?\n\u2202a\n\u2202(dgV)?\n?\ni\n+R\n=\n2\u2202(dgV)?\n\u2202\u03bb\n\u2202a\n?\nV?\u2202V\n\u2202\u03bbi\n??\n???\ni\n+R\n=\n4V?V\n\u22022a\n\u2202dgV\u2202(dgV)?V?V+8\n?\n?\nV?\n?\nviv?\ni\ni\n+R.\nIn the following, we abbreviate N (f|mi,vii) by Ni.\n2070"},{"page":37,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\n\u22022a\n\u2202m2\ni\n=\nZ\u22022Ni\n1\nvii\nZ\n1\n3\n2\nii\nZ\u22022Ni\n1\n4v2\nii\n\u2202m2\nZ?f2\u22121?\u00b7N (f)lnsig(\u221aviiyif +miyi)df,\n\u22022Ni\n\u2202vii\u2202milnsig(yif)df =\nZ?f3\u22123f?\u00b7N (f)lnsig(\u221aviiyif +miyi)df,\nlnsig(yif)df =\ni\nlnsig(yif)df =\nZ(f \u2212mi)2\u2212cii\nv2\nii\nNilnsig(yif)df\n=\n\u22022a\n\u2202cii\u2202mi\n=\nZ(f \u2212mi)3\u22123(f \u2212mi)vii\n2v3\nii\nNilnsig(yif)df\n=\n2v\n\u22022a\n\u2202v2\nii\n=\n\u2202v2\nZ?f4\u22126f2+3?\u00b7N (f)lnsig(\u221aviiyif +miyi)df.\nA.2.3 FIRST DERIVATIVES W.R.T. HYPERPARAMETERS \u03b8i:\nii\nZ(f \u2212mi)4\u22126vii(f \u2212mi)2+3v2\nii\n4v4\nii\nNilnsig(yif)df\n=\nThe direct gradient is given by the following equation where we have marked the dependency of the\ncovariance K on \u03b8iby subscripts\n\u2202lnZB(\u03b1,\u039b)\n\u2202\u03b8i\n=\n\u03b1?\u2202K\u03b8\n?\n\u2202\u03b8i\nA?\u039b\u2202K\u03b8\n\u2202a(y,m,V)\n\u2202m\n+dg\n?\n?\nA\u2202K\u03b8\n\u2202\u03b8i\nA?\n?\n??\u2202a(y,m,V)\n\u22121\n\u2202dgV\n+tr\n\u2202\u03b8i\n?\n\u2212tr\nA\u2202K\u03b8\n\u2202\u03b8i\n\u039bA\n2\u03b1?\u2202K\u03b8\n\u2202\u03b8i\n\u03b1.\nAgain we have would have to add an implicit term to the gradient, but in our implementation, we\nforbore from doing so.\nAppendix B. Auxiliary Calculations\nIn the following, we enumerate some calculations we removed from the main text in order to im-\nprove on readability.\nB.1 Limits of the Covariance Matrix and Corresponding Marginal Likelihood\nWe investigate the behavior of the covariance matrix K for extreme lengthscales ?. The matrix is\ngiven by [K]ij= \u03c32\nwith g(0) = 1 and limt\u2192\u221eg(t) = 0. >From this definition we have [K]ii= \u03c32\n|xi\u2212xj|\/? > 0 for i ?= j. From\nfg(|xi\u2212xj|\/?) where g : R \u2192 R is monotonously decreasing and continuous\nf. We define \u2206ij:=\nlim\n?\u21920[K]ij\nlim\ni?=j\n=\nlim\n?\u21920\u03c32\nlim\nfg(|xi\u2212xj|\/?) = \u03c32\nflim\n\u2206ij\u2192\u221eg(\u2206ij) = 0,\nflim\n?\u2192\u221e[K]ij\ni?=j\n=\n?\u2192\u221e\u03c32\nfg(|xi\u2212xj|\/?) = \u03c32\n\u2206ij\u21920g(\u2206ij) = 1\nwe conclude\n2071"},{"page":38,"text":"NICKISCH AND RASMUSSEN\nlim\n?\u21920K\nlim\n=\n\u03c32\nfI,\n?\u2192\u221eK\n=\n\u03c32\nf\n????.\nThe sigmoids are normalized sig(\u2212fi)+sig(fi) = 1 and the Gaussian is symmetric N (fi) =\nN (\u2212fi). Consequently, we have\nZ\nsig(yifi)N (fi|0,\u03c32\nf)dfi\n=\nZ\nZ0\nZ\u221e\nZ\u221e\nZ\u221e\nsig(fi)N (fi|0,\u03c32\nf)dfi\n=\n\u2212\u221esig(fi)N (fi|0,\u03c32\nsig(\u2212fi)N (\u2212fi|0,\u03c32\n[sig(\u2212fi)+sig(fi)]N (fi|0,\u03c32\n1\u00b7N (fi|0,\u03c32\nf)dfi+\nZ\u221e\n0\nsig(fi)N (fi|0,\u03c32\nZ\u221e\nf)dfi\nf)dfi\n=\n0\nf)dfi+\n0\nsig(fi)N (fi|0,\u03c32\nf)dfi\n=\n0\n=\n0\nf)dfi=1\n2.\nThe marginal likelihood is given by\nZ\n=\nZ\nZ\nP(y|f)P(f|X,\u03b8)df\nn\n\u220f\ni=1\n=\nsig(yifi)|2\u03c0K|\u22121\n2exp(\u22121\n2f?K\u22121f)df.\nB.1.1 LENGTHSCALE TO ZERO\nFor K = \u03c32\nfI the prior factorizes and we get\nZ?\u21920\n=\nn\n\u220f\ni=1\nZ\nsig(yifi)\n1\n?\n2\u03c0\u03c32\nf\nexp(\u2212f2\ni\n2\u03c32\nf\n)dfi\n(17)\n=\nn\n\u220f\ni=1\n1\n2= 2\u2212n.\n2072"},{"page":39,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nB.1.2 LENGTHSCALE TO INFINITY\nTo get K \u2192 \u03c32\nposition of K is written as K = \u2211n\nf\n????we write K = \u03c32\nf1+\u03b52I with 1 =\ni=1uiu?\n????and let \u03b5 \u2192 0. The eigenvalue decom-\n1\n\u221an\ni\u03bbiwith u1=\n?, \u03bb1= \u03c32\nf+\u03b52and all other \u03bbi= \u03b52\nZ 1\n\u03b5\nK=U\u039bU?\n=\nZ\nZ\nZ\nZ\nZ\n2\u2212n+1\nn\n\u220f\ni=1\nn\n\u220f\ni=1\nn\n\u220f\ni=1\nsig(yifi)|2\u03c0\u039b|\u22121\n?\nsig\nyi\n?\nn\n?\u03c3f\nZ\n2\u2212n+1\nsig\n2exp(\u22121\n?\n?\n\uf8f6\n2f?U\u039b\u22121U?f)df\nt=\u039b\u22121\n2U?f\n=\nsig\nyi\n?\n?\nf+\u03b52\n\u03bbi\u00b7t?ui\n|2\u03c0\u039b|\u22121\n2exp(\u22121\n2t?t)\n???\u039b\n1\n2\n???dt\n=\n?\n\u03bbi\u00b7t?ui\nN (ti)dt\n=\nsig\n\uf8eb\n\uf8ed\n\u03c32\n\u00b7t?\n?\n\uf8f8N (t1)\nn\n\u220f\ni=2\nn\n\u220f\ni=2\n?\n?\nsig\n?\n\u03b5\u00b7t?ui\n??\nN (ti)dt,\nZ?\u2192\u221e= lim\n\u03b5\u21920Z\n=\nsig\n\u221an\u00b7t?\n??\nN (t1)\n?1\n2\nN (ti)dt\n(17)\n=\nsig\n?\u03c3f\n?\u03c3f\n\u221an\u00b7t?\n??\nN (r)dr\nN (t)dt\nr=t?\n=\n?\nZ\n\u221an\u00b7r\n?\n(17)\n=\n2\u2212n.\nB.1.3 LATENT SCALE TO ZERO\nWe define \u03c32\nf\u02dcK = K and \u03c3f\u02dcf = f and derive\nZ\u03c3f\n=\nZ\nZ\nZ\nZ\nZ\nn\n\u220f\ni=1\nn\n\u220f\ni=1\nn\n\u220f\ni=1\nn\n\u220f\ni=1\nn\n\u220f\ni=1\nsig(yifi)|2\u03c0K|\u22121\n2exp(\u22121\n2f?K\u22121f)df\n=\nsig?yi\u03c3f\u02dcfi\nsig?yi\u03c3f\u02dcfi\n?sig?yi\u03c3f\u02dcfi\n?1\n?|2\u03c0K|\u22121\n???2\u03c0\u03c32\n??N?\u02dcf|0,\u02dcK?d\u02dcf,\nN?\u02dcf|0,\u02dcK?d\u02dcf = 2\u2212n.\n2exp(\u2212\u03c32\nf\n2\n\u02dcf?K\u22121\u02dcf)\u03c3n\nfd\u02dcf\n=\nf\u02dcK??\u22121\n2exp(\u2212\u03c32\nf\n2\n\u02dcf?\u03c3\u22122\nf\n\u02dcK\u22121\u02dcf)\u03c3n\nfd\u02dcf\n=\nZ\u03c3f\u21920= lim\n\u03c3f\u21920Z\n=\n2\n?\nNote that the functions, we are using are all well-behaved, such that the limits do exist.\n2073"},{"page":40,"text":"NICKISCH AND RASMUSSEN\nB.2 Posterior Divided by Prior = Effective Likelihood\nQ(y|f)=\nN (f|m,V)\nP(f|X)\nN?f| \u02dc m,W\u22121?\n(2\u03c0)\u2212n\n=\nN\n?\nf|m,?K\u22121+W?\u22121?\nN (f|0,K)\n\u02dc m = (KW)\u22121m+m\n?\n2exp\n\u22121\n\u22121\n\u22121\n2(f\u2212 \u02dc m)?W(f\u2212 \u02dc m)\n2\u02dc m??K+W\u22121?\u22121\u02dc m\u22121\n=\nN ( \u02dc m|0,K+W\u22121),\n2??W\u22121??\u22121\n=\n2exp\n\u22121\n?\n2(f\u2212 \u02dc m)?W(f\u2212 \u02dc m)\n2\u02dc m?(K+W\u22121)\u22121\u02dc m\n?\n(2\u03c0)\u2212n\n2|K+W\u22121|\u22121\nexp\n?\n=\n?\n1\nZQexp\n\u22121\n|KW+I|\n?\n2(f\u2212 \u02dc m)?W(f\u2212 \u02dc m)\n2\u02dc m?(K+W\u22121)\u22121\u02dc m\n?\nexp\n?\n?\n=:\n?\n\u22121\n?\n,\nlnZQ\n=\n2ln|KW+I|\nB.3 Kullback-Leibler Divergence for KL method\nWe wish to calculate the divergence between the approximate posterior, a Gaussian, and the true\nposterior\nKL(Q(f|\u03b8) ? P(f|y,X,\u03b8))=\nZ\nZ\nlnZ+\nZ\nZ\nN (f|m,V)lnN (f|m,V)\nP(f|y,X,\u03b8)df\nZ\u00b7N (f|m,V)\nN (f|m,V)\u220fn\nN (f|m,V)lnN (f|m,V)df\nn\n\u220f\ni=1\nN (f|m,V)lnN (f|0,K)df.\n(2)\n=\nN (f|m,V)ln\nZ\nN (f|m,V)ln\ni=1P(yi|fi)df\n=\n\u2212\nP(yi|fi)df\n\u2212\nThere are three Gaussian integrals to evaluate; the entropy of the approximate posterior and two\nother expectations\nKL(Q(f|\u03b8) ? P(f|y,X,\u03b8))=\nlnZ\u22121\nZ\n+n\n2ln|V|\u2212n\n?\n2ln2\u03c0+1\n2\u2212n\nlnsig(\u221aviiyif +miyi)\n2ln2\u03c0\n\u2212\nN (f)\nn\n\u2211\ni=1\n2ln|K|+1\n?\n2tr?K\u22121V?.\ndf\n(17)\n2m?K\u22121m+1\n2074"},{"page":41,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nSumming up and dropping the constant (w.r.t. m and V) terms, we arrive at\nKL(m,V)\nc= \u2212\nZ\nN (f)\n?\nn\n\u2211\ni=1\nlnsig(\u221aviiyif +miyi)\n?\ndf \u22121\n2ln|V|+1\n2m?K\u22121m+1\n2tr?K\u22121V?.\nB.4 Gaussian Integral for VB Lower Bound\nZB\n=\nZ\nP(f|X)Q(y|f,A,b,c)df =\nexp?c?\nexp?c?\nexp?c?\nc?\nZ\nN (f|0,K)exp\n?\nf?Af+(b?y)?f+c?\n?\n??\ndf\n=\n? ?\n?(2\u03c0)n|K|\n?(2\u03c0)n|K|\n?|I\u22122AK|exp\nZ\n?\nexp\n?\n\u22121\n2f??K\u22121\u22122A?f+(b?y)?f\n(2\u03c0)n\n|K\u22121\u22122A|exp\n?1\n2(b?y)??K\u22121\u22122A?\u22121(b?y)\u22121\ndf\n=\n? ?\n?1\n2(b?y)??K\u22121\u22122A?\u22121(b?y)\n2(b?y)??K\u22121\u22122A?\u22121(b?y)\n?\n=\n? ?\n?\n,\nlnZB\n=\n?\n+1\n2ln|I\u22122AK|.\nB.5 Lower Bound for the Cumulative Gaussian Likelihood\nA lower bound\nsigprobit(yifi)\n\u2265\nQ(yi|fi,\u03c2i) = aif2\ni+bifi+ci\nfor the cumulative Gaussian likelihood function is derived by matching the function at one point \u03c2\nQ(yi= +1|fi,\u03c2i)=\nsigprobit(\u03c2i), \u2200i\nand by matching the first derivative\n\u2202\n\u2202filnQ(yi= +1|fi,\u03c2i)\n????\u03c2i\n=\n\u2202lnsigprobit(yifi)\n\u2202fi\n=\nN (\u03c2i)\nsigprobit(\u03c2i), \u2200i\nat this point for a tight approximation. Solving for these constraints leads to the coefficients\nasymptotic behavior \u21d2 ai\n=\n\u22121\n2,\nfirst derivative \u21d2 bi\n=\n\u03c2i+\n?\u03c2i\nN (\u03c2i)\nsigprobit(\u03c2i),\n?\npoint matching \u21d2 ci\n=\n2\u2212bi\n\u03c2i+logsigprobit(\u03c2i).\n2075"},{"page":42,"text":"NICKISCH AND RASMUSSEN\nB.6 Free Form Optimization for FV\nWe make a factorial approximation P(f|y,X) \u2248 Q(f) := \u220fiQ(fi) to the posterior by minimizing\nKL[Q(f)||P(f)]=\nZ\nn\n\u220f\ni=1\nZ\nQ(fi)ln\nZ\u00b7\u220fn\ni=1Q(fi)\ni=1P(yi|fi)df\nZ\nN (f|m,V)\u220fn\nQ(fi)\nP(yi|fi)dfi+1\n= \u2211\ni\nQ(fi)ln\n2\nn\n\u220f\ni=1\nQ(fi)f?K\u22121fdf+constf.\nFree-form optimization proceeds by equating the functional derivative with zero\n\u03b4KL\n\u03b4Q(fi)\n=\nlnQ(fi)+1\u2212lnP(yi|fi)+1\n2\n\u03b4\n\u03b4Q(fi)\nZ\nn\n\u220f\ni=1\nQ(fi)f?K\u22121fdf.\n(18)\nWe abbreviate the integral in the last term with \u03be and rewrite it in terms of simple one-dimensional\nintegrals ml=RflQ(fl)dfland vl=Rf2\n\u03be\n=\n\u220f\ni\nj,k\nZ\ni?=l\nj?=l\n\uf8ee\n\uf8f0\n=\nj?=l\n=\ninduction over l\n= \u2211\nlj<l\nlQ(fl)dfl\u2212m2\nl\nZ\nQi\u2211\nfj\n?K\u22121?\nQl\njkfkdf\n=\n\u220f\nQi\n?Z\n?\nf2\nl\n?K\u22121?\nll+2fl\u2211\nfj\n?K\u22121?\njl+ \u2211\nj?=l,k?=l\nfj\n?K\u22121?\njkfk\n?\ndfl\n?\ndf\u00acl\n=\nZ\n\u220f\ni?=l\nQi\n\uf8ef\uf8ef\uf8ef\n?K\u22121?\nll\nZ\n?\nf2\nlQldfl\n??\nmj\n?\nvl+m2\nl\n+2(\u2211\nj?=l\nfj\n?K\u22121?\nZ\njl)\nZ\n?\nflQldfl\n???\nml\n+ \u2211\nj?=l,k?=l\nfj\n?K\u22121?\njkfk\n\uf8f9\n\uf8fbdf\u00acl\n\uf8fa\uf8fa\uf8fa\n?K\u22121?\n?K\u22121?\nll(vl+m2\nl)+2\u2211\n?K\u22121?\n?K\u22121?\n\u03b4Rfp\njlml+\n\u220f\ni?=l\nQi \u2211\nj?=l,k?=l\nfj\n?K\u22121?\njkfkdf\u00acl\nll(vl+m2\nl)+2\u2211\nmj\njlml.\nPlugging this into Equation 18 and using\nlQ(fl)dfl\n\u03b4Q(fl)\n= fp\nl, we find\n\u03b4KL\n\u03b4Q(fi)\n=\nlnQ(fi)+1\u2212lnP(yi|fi)+1\n?\n?\n2fi\n?K\u22121?\n?K\u22121?\n?\niifi+ fi\u2211\n?\nl\n?K\u22121?\nilml\n!\u2261 0\n\u21d2 Q(fi)\n\u221d\nexp\n\u22121\n?????mi\u2212\n2fi\n?K\u22121?\n?K\u22121m?\niifi\u2212 fi\u2211\nl?=i\nilml\nP(yi|fi)\n\u21d2 Q(fi)\n\u221d N\nfi\ni\n[K\u22121]ii\n,?K\u22121?\u22121\nii\nP(yi|fi)\nas the functional form of the best possible factorial approximation, namely a product of the true\nlikelihood times a Gaussian with the same precision as the prior marginal.\n2076"},{"page":43,"text":"APPROXIMATE GAUSSIAN PROCESS CLASSIFICATION\nReferences\nYasemin Altun, Thomas Hofmann, and Alex Smola. Gaussian process classification for segmenting\nand annotating sequences. In International Conference on Machine Learning, 2004.\nWei Chu, Zoubin Ghahramani, Francesco Falciani, and David L. Wild. Biomarker discovery in\nmicroarray gene expression data with gaussian processes. Bioinformatics, 21:3385\u20133393, 2005.\nLehel Csat\u00f3, Ernest Fokou\u00e9, Manfred Opper, and Bernhard Schottky. Efficient Approaches to Gaus-\nsian Process Classification. In Neural Information Processing Systems 12, pages 251\u2013257. MIT\nPress, 2000.\nMark N. Gibbs and David J. C. MacKay. Variational Gaussian Process Classifiers. IEEE Transac-\ntions on Neural Networks, 11(6):1458\u20131464, 2000.\nMark Girolami and Simon Rogers. Variational Bayesian Multinomial Probit Regression with Gaus-\nsian Process Priors. Neural Computation, 18:1790\u20131817, 2006.\nAshish Kapoor and Rosalind W. Picard. Multimodal affect recognition in learning environments.\nIn ACM international conference on Multimedia, 2005.\nAshish Kapoor, Kristen Grauman, Raquel Urtasun, and Trevor Darrell. Active learning with gaus-\nsian processes for object categorization. In ICCV, 2007.\nMalte Kuss and Carl Edward Rasmussen. Assessing Approximate Inference for Binary Gaussian\nProcess Classification. Journal of Machine Learning Research, 6:1679 \u2013 1704, 10 2005.\nDavid J. C. MacKay. Bayesian Interpolation. Neural Computation, 4(3):415\u2013447, 1992.\nThomas P. Minka. Expectation Propagation for Approximate Bayesian Inference. In UAI, pages\n362\u2013369. Morgan Kaufmann, 2001a.\nThomas P. Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD thesis, De-\npartment of Electrical Engineering and Computer Science, MIT, 2001b.\nTom Minka. Divergence Measures and Message Passing. Technical report, Microsoft Research,\n2005.\nRadford M. Neal. Annealed Importance Sampling. Statistics and Computing, 11:125\u2013139, 2001.\nRadford M. Neal. Probabilistic Inference Using Markov Chain Monte Carlo Methods. Technical\nReportCRG-TR-93-1, DepartmentofComputerScience, UniversityofToronto, September1993.\nManfred Opper and C\u00e9dric Archambeau. The Variational Gaussian Approximation Revisited. Neu-\nral Computation, accepted, 2008.\nManfred Opper and Ole Winther. Gaussian Processes for Classification: Mean Field Algorithms.\nNeural Computation, 12(11):2655\u20132684, 2000.\nManfred Opper and Ole Winther. Expectation Consistent Approximate Inference. Journal of Ma-\nchine Learning Research, 6:2177\u20132204, 2005.\n2077"},{"page":44,"text":"NICKISCH AND RASMUSSEN\nWilliam H. Press, Saul A. Teukolsky, William T. Vetterling, and Brian P. Flannery. Numerical\nRecipes in C. Cambridge University Press, 2nd edition, February 1993.\nCarlEdwardRasmussenandChristopherK.I.Williams. GaussianProcessesforMachineLearning.\nThe MIT Press, Cambridge, MA, 2006.\nRyan Rifkin and Aldebaro Klautau. In defense of one-vs-all classification. JMLR, 5:101\u2013141, 2004.\nAnton Schwaighofer, Volker Tresp, Peter Mayer, Alexander K. Scheel, and Gerhard M\u00fcller. The\nRA scanner: Prediction of rheumatoid joint inflammation based on laser imaging. In NIPS, 2002.\nMatthias Seeger. Bayesian Gaussian Process Models: PAC-Bayesian Generalisation Error Bounds\nand Sparse Approximations. PhD thesis, University of Edinburgh, 2003.\nMatthias Seeger. Bayesian Methods for Support Vector Machines and Gaussian Processes. Master\u2019s\nthesis, Universit\u00e4t Karlsruhe, 1999.\nS. Sundararajan and S. S. Keerthi. Predictive Approaches for Choosing Hyperparameters in Gaus-\nsian Processes. Neural Computation, 13:1103\u20131118, 2001.\nChristopher K. I. Williams and David Barber. Bayesian Classification with Gaussian Processes.\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 12(20):1342\u20131351, 1998.\nMingjun Zhong, Fabien Lotte, Mark Girolami, and Anatole L\u00e9cuyer. Classifying eeg for brain\ncomputer interfaces using gaussian processes. Pattern Recognition Letters, 29:354\u2013359, 2008.\n2078"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Hannes_Nickisch\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000.pdf","widgetId":"rgw27_56ab9ff619504"},"id":"rgw27_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=41781800&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw28_56ab9ff619504"},"id":"rgw28_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=41781800&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":41781800,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0c96051a5e4f9e85d3000000","name":"Hannes Nickisch","date":null,"nameLink":"profile\/Hannes_Nickisch","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Hannes_Nickisch\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Hannes_Nickisch\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"c4ca54c67a96262b6d179e9853409a9a","showFileSizeNote":false,"fileSize":"2.38 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0c96051a5e4f9e85d3000000","name":"Hannes Nickisch","date":null,"nameLink":"profile\/Hannes_Nickisch","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Hannes_Nickisch\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Hannes_Nickisch\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"c4ca54c67a96262b6d179e9853409a9a","showFileSizeNote":false,"fileSize":"2.38 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=WwtFh9EJNi8orfOz3gILRYWQVBptenkTCuQfXeWs-xZzp2WGeMXto7pEutt-z4BLxYZj_Q_vnaFOwnkgTjELRQ.Es2CzDM32Qo_W33qaz2Nkr3tOhVhKLp9NJlv77z1Xt1VGa9KgSFi-M_SWJ38CHavqkpgwrauThdxoF-b-zyB5Q","clickOnPill":"publication.PublicationFigures.html?_sg=ODtRouCHOnTO8KeZZ79ScIjWigEURo4MA9tTfwzVzarEiVUUVXwYIPXUESZFIfspa1R1efMPNBtMF4TlnsGxqg.mUrxKfd-i1PuQFzY4IJM02UOm87KacjlvBUHkrDEA-mLZe1ha6TfOIefqbDA-aThJDqx-ykPhqmJWhhGLbtQYg"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FHannes_Nickisch%2Fpublication%2F41781800_Approximations_for_Binary_Gaussian_Process_Classification%2Flinks%2F0c96051a5e4f9e85d3000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=TLnQuV3FdNXzap_EQfDPjbB14Zmft6K7MaKEFRGMTyfEZXhbtb1UricZXPCg1w5RvnREJEbxd436lUPy6DJk_g","urlHash":"89b601ec4e7b1b03157f1cfed7c55f9f","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=0c2ARLuywe1-iRR0Srj2Xw7dCYdnCQg1p89e52rVJC6nIzXoFZGH2jDiqZuLMO-VpW4Jtrk8PZdFXdPMaP4ma2ncC_yKZs05N8RY8WIMNes.A_pNoq2T0PM1Cn5fFhS-UTyhA9Z5ZJzYIO6e3j6Vzu1Pz9szEJd-lq0OKoelQCBVX0lsD8m1N3S-ULLv-Hd0pg.v7BBbd5z7Jm3OtfIuk2zRTK1TbFhyhVIaZUa26TeroCEUkXispsFDf2wQ7h7ZFXzP_5bwTI32qH_4PeHVl9c1w","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0c96051a5e4f9e85d3000000","trackedDownloads":{"0c96051a5e4f9e85d3000000":{"v":false,"d":false}},"assetId":"AS:99642234638345@1400767970703","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":41781800,"commentCursorPromo":null,"widgetId":"rgw30_56ab9ff619504"},"id":"rgw30_56ab9ff619504","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FHannes_Nickisch%2Fpublication%2F41781800_Approximations_for_Binary_Gaussian_Process_Classification%2Flinks%2F0c96051a5e4f9e85d3000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A99642234638345%401400767970703&publicationUid=41781800&linkId=0c96051a5e4f9e85d3000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Approximations for Binary Gaussian Process Classification","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=5FCreOweiFCNeoq0trkt7EZdNVk5kZZTdLNhzYpNVZCaoSxWZbAQ_QEuI-lqtrI_BQaAepqb80QKoZx4WX5pSHnV40QLDv9SGgxGcQ9sWp8.WchnbLhIyzSJ50f1bN3TrkND85LhSYR0201TBJnebDht7cdmbkjQan2ViF76bQM6ZZNWEZ458KWFVQRaTQbvtQ.UnOco45L9hVe2IwNShGClZxLpPVSGzj_719RINxP91KYTyDZ4ShTMYiHLV4lBHkVyQ9yaqXMlaYPfm45phUGzg","publicationUid":41781800,"trackedDownloads":{"0c96051a5e4f9e85d3000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw32_56ab9ff619504"},"id":"rgw32_56ab9ff619504","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw33_56ab9ff619504"},"id":"rgw33_56ab9ff619504","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw34_56ab9ff619504"},"id":"rgw34_56ab9ff619504","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw35_56ab9ff619504"},"id":"rgw35_56ab9ff619504","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw36_56ab9ff619504"},"id":"rgw36_56ab9ff619504","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw31_56ab9ff619504"},"id":"rgw31_56ab9ff619504","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw29_56ab9ff619504"},"id":"rgw29_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9ff619504"},"id":"rgw2_56ab9ff619504","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":41781800},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=41781800&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9ff619504"},"id":"rgw1_56ab9ff619504","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"zs+qzA\/4i6AaF7N\/X3mnlr1Z4nhV5yqyOZbgRWDVxQHvM1W9j2epsJJSS5pZRb0pegg5x7PnZEO5jk0PqfH1cwIsbJWj33Z27uoYJQpCkR4WLIfWI2RQsDyVhYQU\/hScsNOJ7ua8ukPRklosK6EsnYH2pQ\/2ObFxvR22h3Yp4ajgSvCXzCB\/MegEZk+PXfPEo9ZNN00xBW5R+Nehr4C6cniQ4qmRKKoCEHepoQSk4KjnffTaMaueM3qfUg0KK9YrpPqpu3FrSWGCL3DgYGb1RH2Yz\/JSvEVq2gXWsC0f0F8=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Approximations for Binary Gaussian Process Classification\" \/>\n<meta property=\"og:description\" content=\"We provide a comprehensive overview of many recent algorithms for approximate inference in Gaussian process models for probabilistic binary classification. The relationships between several...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\" \/>\n<meta property=\"rg:id\" content=\"PB:41781800\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Approximations for Binary Gaussian Process Classification\" \/>\n<meta name=\"citation_author\" content=\"Hannes Nickisch\" \/>\n<meta name=\"citation_author\" content=\"Carl Edward Rasmussen\" \/>\n<meta name=\"citation_publication_date\" content=\"2008\/10\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Journal of Machine Learning Research\" \/>\n<meta name=\"citation_issn\" content=\"1533-7928\" \/>\n<meta name=\"citation_volume\" content=\"9\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Hannes_Nickisch\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\/links\/0c96051a5e4f9e85d3000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-4c772283-4f02-481f-b4d3-df18d3d6a7f0","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":484,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw37_56ab9ff619504"},"id":"rgw37_56ab9ff619504","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-4c772283-4f02-481f-b4d3-df18d3d6a7f0", "3641a1850150da5c0ace952409898c198c38d979");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-4c772283-4f02-481f-b4d3-df18d3d6a7f0", "3641a1850150da5c0ace952409898c198c38d979");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw38_56ab9ff619504"},"id":"rgw38_56ab9ff619504","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/41781800_Approximations_for_Binary_Gaussian_Process_Classification","requestToken":"fkPz19ElCCZX9y3LVhKzopFTKem7apDY8Drvxoe2L6WjmTEOwF8\/cNlBGYCn0wQxHIFeT2FI2K0Nu9kE6YmAnfi8w+2C+SZJAa2EKax2djpvzY91NCmgwVpfiCcND2o6\/Jv8XL7nMnAYZ06AupJNJ5uH2GpFw8xEzv\/fdNrC8H\/IjFHltm5yLcvP1cCCvmp85aT9C9HWMESJNGb3DVxKQATcseqXW1ZjVBE6ZEFlxE\/hpecFvYpb3wIwcQWQZgGqMdJq9cFhpRfFusChCOjf4wXg+CWIZbNxC353htGqSTA=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=E1DTcL2ARtLD20Hq6Qb5IyRxuwpaj5dWQn8KvwrMreYZM_O02LZsZ1KPM8O1tebL","encodedUrlAfterLogin":"cHVibGljYXRpb24vNDE3ODE4MDBfQXBwcm94aW1hdGlvbnNfZm9yX0JpbmFyeV9HYXVzc2lhbl9Qcm9jZXNzX0NsYXNzaWZpY2F0aW9u","signupCallToAction":"Join for free","widgetId":"rgw40_56ab9ff619504"},"id":"rgw40_56ab9ff619504","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw39_56ab9ff619504"},"id":"rgw39_56ab9ff619504","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw41_56ab9ff619504"},"id":"rgw41_56ab9ff619504","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
