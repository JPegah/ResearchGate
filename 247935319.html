<!DOCTYPE html> <html lang="en" class="" id="rgw27_56ab9f7f4f463"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="kz450nb9S7D82We45t+pZ/tQ3vEwxcncX3eDynuihcBBPFcKF4/VYpAg3b4XosR57giLkOuhcHbTacGwYND61DE2ZMlIlC9P9MEYeMI254/2kOKKb2Fch/5VA4woTkyzOEMq5PxbVJpbT56BEd8MhVBSM8lr9K1KKzv7NSYpdL36Gqln1SSiG98vcxiuWHQcX4ZaCTZE0Rcntu/mnpeniXaYUx3S/Ic9x3TjJrDdoA2hDh3QeOVRUwP8MOF3WNwW6huUU2GAg8tqE9EiapNXtQkGKscZ8cEmGLgJoSXDcEw="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-cf054aa0-2dff-4f43-be8d-df4d89eaeccc",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/247935319_Bayesian_Nonparametric_Models" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Bayesian Nonparametric Models" />
<meta property="og:description" content="" />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/247935319_Bayesian_Nonparametric_Models/links/029313640cf270384305b058/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/247935319_Bayesian_Nonparametric_Models" />
<meta property="rg:id" content="PB:247935319" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1007/978-0-387-30164-8_66" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Bayesian Nonparametric Models" />
<meta name="citation_author" content="Peter Orbanz" />
<meta name="citation_author" content="Yee Whye Teh" />
<meta name="citation_doi" content="10.1007/978-0-387-30164-8_66" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/247935319_Bayesian_Nonparametric_Models" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/247935319_Bayesian_Nonparametric_Models" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Bayesian Nonparametric Models</title>
<meta name="description" content="Bayesian Nonparametric Models on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab9f7f4f463" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab9f7f4f463" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56ab9f7f4f463">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1007%2F978-0-387-30164-8_66&rft.atitle=Bayesian%20Nonparametric%20Models&rft.au=Peter%20Orbanz%2CYee%20Whye%20Teh&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Bayesian Nonparametric Models</h1> <meta itemprop="headline" content="Bayesian Nonparametric Models">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/247935319_Bayesian_Nonparametric_Models/links/029313640cf270384305b058/smallpreview.png">  <div id="rgw9_56ab9f7f4f463" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw10_56ab9f7f4f463"> <a href="researcher/69976956_Peter_Orbanz" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Peter Orbanz" alt="Peter Orbanz" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Peter Orbanz</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab9f7f4f463">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69976956_Peter_Orbanz"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Peter Orbanz" alt="Peter Orbanz" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69976956_Peter_Orbanz" class="display-name">Peter Orbanz</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab9f7f4f463"> <a href="researcher/9164246_Yee_Whye_Teh" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Yee Whye Teh" alt="Yee Whye Teh" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Yee Whye Teh</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab9f7f4f463">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/9164246_Yee_Whye_Teh"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Yee Whye Teh" alt="Yee Whye Teh" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/9164246_Yee_Whye_Teh" class="display-name">Yee Whye Teh</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">              DOI:&nbsp;10.1007/978-0-387-30164-8_66           </div>       <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56ab9f7f4f463">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw25_56ab9f7f4f463"  itemprop="articleBody">  <p>Page 1</p> <p>Bayesian Nonparametric Models<br />Peter Orbanz, Cambridge University<br />Yee Whye Teh, University College London<br />Related keywords: Bayesian Methods, Prior Probabilities, Dirichlet Process,<br />Gaussian Processes.<br />Definition<br />A Bayesian nonparametric model is a Bayesian model on an infinite-dimensional<br />parameter space. The parameter space is typically chosen as the set of all possi-<br />ble solutions for a given learning problem. For example, in a regression problem<br />the parameter space can be the set of continuous functions, and in a density esti-<br />mation problem the space can consist of all densities. A Bayesian nonparametric<br />model uses only a finite subset of the available parameter dimensions to explain<br />a finite sample of observations, with the set of dimensions chosen depending on<br />the sample, such that the effective complexity of the model (as measured by the<br />number of dimensions used) adapts to the data. Classical adaptive problems,<br />such as nonparametric estimation and model selection, can thus be formulated<br />as Bayesian inference problems. Popular examples of Bayesian nonparametric<br />models include Gaussian process regression, in which the correlation structure<br />is refined with growing sample size, and Dirichlet process mixture models for<br />clustering, which adapt the number of clusters to the complexity of the data.<br />Bayesian nonparametric models have recently been applied to a variety of ma-<br />chine learning problems, including regression, classification, clustering, latent<br />variable modeling, sequential modeling, image segmentation, source separation<br />and grammar induction.<br />Motivation and Background<br />Most of machine learning is concerned with learning an appropriate set of pa-<br />rameters within a model class from training data. The meta level problems<br />of determining appropriate model classes are referred to as model selection or<br />model adaptation. These constitute important concerns for machine learning<br />practitioners, chiefly for avoidance of over-fitting and under-fitting, but also for<br />discovery of the causes and structures underlying data. Examples of model se-<br />lection and adaptation include: selecting the number of clusters in a clustering<br />problem, the number of hidden states in a hidden Markov model, the number<br />1</p>  <p>Page 2</p> <p>of latent variables in a latent variable model, or the complexity of features used<br />in nonlinear regression.<br />Nonparametric models constitute an approach to model selection and adap-<br />tation, where the sizes of models are allowed to grow with data size. This is<br />as opposed to parametric models which uses a fixed number of parameters. For<br />example, a parametric approach to density estimation would be to fit a Gaus-<br />sian or a mixture of a fixed number of Gaussians by maximum likelihood. A<br />nonparametric approach would be a Parzen window estimator, which centers a<br />Gaussian at each observation (and hence uses one mean parameter per observa-<br />tion). Another example is the support vector machine with a Gaussian kernel.<br />The representer theorem shows that the decision function is a linear combina-<br />tion of Gaussian radial basis functions centered at every input vector, and thus<br />has a complexity that grows with more observations. Nonparametric methods<br />have long been popular in classical (non-Bayesian) statistics [1]. They often per-<br />form impressively in applications and, though theoretical results for such models<br />are typically harder to prove than for parametric models, appealing theoretical<br />properties have been established for a wide range of models.<br />Bayesian nonparametric methods provide a Bayesian framework for model<br />selection and adaptation using nonparametric models. A Bayesian formulation<br />of nonparametric problems is nontrivial, since a Bayesian model defines prior<br />and posterior distributions on a single fixed parameter space, but the dimen-<br />sion of the parameter space in a nonparametric approach should change with<br />sample size. The Bayesian nonparametric solution to this problem is to use<br />an infinite-dimensional parameter space, and to invoke only a finite subset of<br />the available parameters on any given finite data set. This subset generally<br />grows with the data set. In the context of Bayesian nonparametric models,<br />“infinite-dimensional” can therefore be interpreted as “of finite but unbounded<br />dimension”. More precisely, a Bayesian nonparametric model is a model that<br />(1) constitutes a Bayesian model on an infinite-dimensional parameter space<br />and (2) can be evaluated on a finite sample in a manner that uses only a finite<br />subset of the available parameters to explain the sample.<br />We make the above description more concrete in the next section when we<br />describe a number of standard machine learning problems and the correspond-<br />ing Bayesian nonparametric solutions. As we will see, the parameter space in<br />(1) typically consists of functions or of measures, while (2) is usually achieved<br />by marginalizing out surplus dimensions over the prior. Random functions and<br />measures, and more generally probability distributions on infinite-dimensional<br />random objects, are called stochastic processes; examples we will encounter<br />include Gaussian processes, Dirichlet processes and beta processes. Bayesian<br />nonparametric models are often named after the stochastic processes they con-<br />tain. The examples are then followed by theoretical considerations, including<br />formal constructions and representations of the stochastic processes used in<br />Bayesian nonparametric models, exchangeability, and issues of consistency and<br />convergence rate. We conclude this article with future directions and a reading<br />list.<br />2</p>  <p>Page 3</p> <p>Examples<br />Clustering with mixture models. Bayesian nonparametric generalizations<br />of finite mixture models provide an approach for estimating both the number<br />of components in a mixture model and the parameters of the individual mix-<br />ture components simultaneously from data. Finite mixture models define a<br />density function over data items x of the form p(x) =?K<br />nent k. The density can be written in a non-standard manner as an integral:<br />p(x) =?p(x|θ)G(θ)dθ, where G =?K<br />tribution (atom) centered at θ. Bayesian nonparametric mixtures use mixing<br />distributions consisting of a countably infinite number of atoms instead:<br />k=1πkp(x|θk), where<br />πk is the mixing proportion and θk are parameters associated with compo-<br />k=1πkδθkis a discrete mixing distribution<br />encapsulating all the parameters of the mixture model and δθ is a Dirac dis-<br />G =<br />∞<br />?<br />k=1<br />πkδθk.<br />(1)<br />This gives rise to mixture models with an infinite number of components. When<br />applied to a finite training set, only a finite (but varying) number of components<br />will be used to model the data, since each data item is associated with exactly<br />one component but each component can be associated with multiple data items.<br />Inference in the model then automatically recovers both the number of compo-<br />nents to use and the parameters of the components. Being Bayesian, we need<br />a prior over the mixing distribution G, and the most common prior to use is a<br />Dirichlet process (DP). The resulting mixture model is called a DP mixture.<br />Formally, a Dirichlet process DP(α,H) parametrized by a concentration<br />paramter α &gt; 0 and a base distribution H is a prior over distributions (probabil-<br />ity measures) G such that, for any finite partition A1,...,Amof the parameter<br />space, the induced random vector (G(A1),...,G(Am)) is Dirichlet distributed<br />with parameters (αH(A1),...,αH(Am)) (see the theory section for a discussion<br />of subtleties involved in this definition). It can be shown that draws from a DP<br />will be discrete distributions as given in (1). The DP also induces a distribution<br />over partitions of integers called the Chinese restaurant process (CRP), which<br />directly describes the prior over how data items are clustered under the DP<br />mixture. For more details on the DP and the CRP, see DP entry [?].<br />Nonlinear regression. The aim of regression is to infer a continuous function<br />from a training set consisting of input-output pairs {(ti,xi)}n<br />approaches parametrize the function using a finite number of parameters and<br />attempt to infer these parameters from data. The prototypical Bayesian non-<br />parametric approach to this problem is to define a prior distribution over con-<br />tinuous functions directly by means of a Gaussian process (GP). As explained in<br />GP entry [?], a GP is a distribution on an infinite collection of random variables<br />Xt, such that the joint distribution of each finite subset Xt1,...,Xtmis a mul-<br />tivariate Gaussian. A value xttaken by the variable Xtcan be regarded as the<br />value of a continuous function f at t, that is, f(t) = xt. Given the training set,<br />the Gaussian process posterior is again a distribution on functions, conditional<br />i=1. Parametric<br />3</p>  <p>Page 4</p> <p>on these functions taking values f(t1) = x1,...,f(tn) = xn.<br />Latent feature models. Latent feature models represent a set of objects in<br />terms of a set of latent features, each of which represents an independent degree<br />of variation exhibited by the data. Such a representation of data is sometimes<br />referred to as a distributed representation. In analogy to nonparametric mix-<br />ture models with an unknown number of clusters, a Bayesian nonparametric<br />approach to latent feature modeling allows for an unknown number of latent<br />features. The stochastic processes involved here are known as the Indian buffet<br />process (IBP) and the beta process (BP). Draws from BPs are random discrete<br />measures, where each of an infinite number of atoms has a mass in (0,1) but<br />the masses of atoms need not sum to 1. Each atom corresponds to a feature,<br />with the mass corresponding to the probability that the feature is present for<br />an object. We can visualize the occurrences of features among objects using a<br />binary matrix, where the (i,k) entry is 1 if object i has feature k and 0 oth-<br />erwise. The distribution over binary matrices induced by the BP is called the<br />IBP.<br />Hidden Markov models. Hidden Markov models (HMMs) are popular mod-<br />els for sequential or temporal data, where each time step is associate with a<br />state, with state transitions dependent on the previous state. An infinite HMM<br />is a Bayesian nonparametric approach to HMMs, where the number of states is<br />unbounded and allowed to grow with the sequence length. It is defined using<br />one DP prior for the transition probabilities going out from each state. To en-<br />sure that the set of states reachable from each outgoing state is the same, the<br />base distributions of the DPs are shared and given a DP prior recursively. The<br />construction is called a hierarchical Dirichlet process (HDP); see below.<br />Density estimation. A nonparametric Bayesian approach to density estima-<br />tion requires a prior on densities or distributions. However, the DP is not useful<br />in this context, since it generates discrete distributions. A useful density estima-<br />tor should smooth the empirical density (such as a Parzen window estimator),<br />which requires a prior that can generate smooth distributions. Priors applicable<br />in density estimation problems include DP mixture models and P´ olya trees.<br />DP mixture models: Since the mixing distribution in the DP mixture is<br />random, the induced density p(x) is random thus the DP mixture can be used<br />as a prior over densities. Despite the fact that these are now primarily used in<br />machine learning as clustering models, they were in fact originally proposed for<br />density estimation.<br />P´ olya Trees are priors on probability distributions that can generate both<br />discrete and piecewise continuous distributions, depending on the choice of pa-<br />rameters. P´ olya trees are defined by a recursive infinitely deep binary subdi-<br />vision of the domain of the generated random measure. Each subdivision is<br />associated with a beta random variable which describes the relative amount of<br />mass on each side of the subdivision. The DP is a special case of a P´ olya tree<br />corresponding to a particular parametrization. For other parametrizations the<br />resulting random distribution can be smooth so is suitable for density estima-<br />tion.<br />Power-law Phenomena. Many naturally occurring phenomena exhibit power-<br />4</p>  <p>Page 5</p> <p>law behavior. Examples include natural languages, images and social and ge-<br />netic networks. An interesting generalization of the DP, called the Pitman-Yor<br />process, PYP(α,d,H), has recently been successfully used as models of such<br />power-law data. The Pitman-Yor process augments the DP by a third parame-<br />ter d ∈ [0,1). When d = 0 the PYP is a DP(α,H), while when α = 0 it is a so<br />called normalized stable process.<br />Sequential modeling. HMMs model sequential data using latent variables<br />representing the underlying state of the system, and assuming that each state<br />only depends on the previous state (the so called Markov property). In some<br />applications, for example language modeling and text compression, we are inter-<br />ested in directly modeling sequences without using latent variables, and without<br />making any Markov assumptions, i.e. modeling each observation conditional on<br />all previous observations in the sequence. Since the set of potential sequences<br />of previous observations is unbounded, this calls for nonparametric models. A<br />hierarchical Pitman-Yor process can be used to construct a Bayesian nonpara-<br />metric solution whereby the conditional probabilities are coupled hierarchically.<br />Dependent and hierarchical models. Most of the Bayesian nonparametric<br />models described above are applied in settings where observations are homo-<br />geneous or exchangeable. In many real world settings observations are often<br />not homogeneous, in fact they are often structured in interesting ways. For<br />example, the data generating process might change over time thus observations<br />at different times are not exchangeable, or observations might come in distinct<br />groups with those in the same group being more similar than across groups.<br />Significant recent efforts in Bayesian nonparametrics research have been<br />placed in developing extensions that can handle these non-homogeneous set-<br />tings. Dependent Dirichlet processes are stochastic processes, typically over a<br />spatial or temporal domain, which define a Dirichlet process (or a related ran-<br />dom measure) at each point with neighboring DPs being more dependent. These<br />are used for spatial modeling, nonparametric regression, as well as for modeling<br />temporal changes. Alternatively, hierarchical Bayesian nonparametric models<br />like the hierarchical DP aim to couple multiple Bayesian nonparametric mod-<br />els within a hierarchical Bayesian framework. The idea is to allow sharing of<br />statistical strength across multiple groups of observations. Among other appli-<br />cations, these have been used in the infinite HMM, topic modeling, language<br />modeling, word segmentation, image segmentation and grammar induction. For<br />an overview of various dependent Bayesian nonparametric models and their ap-<br />plications in biostatistics please consult [2]. [3] is an overview of hierarchical<br />Bayesian nonparametric models as well as a variety of applications in machine<br />learning.<br />Theory<br />As we saw in the preceding examples, Bayesian nonparametric models often<br />make use of priors over functions and measures. Because these spaces typically<br />have an uncountable number of dimensions, extra care has to be taken to define<br />5</p>  <p>Page 6</p> <p>the priors properly and to study the asymptotic properties of estimation in the<br />resulting models. In this section we give an overview of the basic concepts<br />involved in the theory of Bayesian nonparametric models. We start with a<br />discussion of the importance of exchangeability in Bayesian parametric and<br />nonparametric statistics. This is followed by representations of the priors and<br />issues of convergence.<br />Exchangeability<br />The underlying assumption of all Bayesian methods is that the parameter spec-<br />ifying the observation model is a random variable. This assumption is subject<br />to much criticism, and at the heart of the Bayesian versus non-Bayesian debate<br />that has long divided the statistics community. However, there is a very general<br />type of observations for which the existence of such a random variable can be<br />derived mathematically: For so-called exchangeable observations, the Bayesian<br />assumption that a randomly distributed parameter exists is not a modeling<br />assumption, but a mathematical consequence of the data’s properties.<br />Formally, a sequence of variables X1,X2,...,Xnover the same probability<br />space (X,Ω) is exchangeable if their joint distribution is invariant to permuting<br />the variables. That is, if P is the joint distribution and σ any permutation of<br />{1,...,n}, then<br />P(X1=x1,X2=x2...Xn=xn) = P(X1=xσ(1),X2=xσ(2)...Xn=xσ(n)) (2)<br />An infinite sequence X1,X2,... is infinitely exchangeable if X1,...,Xn is ex-<br />changeable for every n ≥ 1. In this paper we will mean infinite exchangeability<br />whenever we write exchangeability. Exchangeability reflects the assumption<br />that the variables do not depend on their indices although they may be depen-<br />dent among themselves. This is typically a reasonable assumption in machine<br />learning and statistical applications, even if the variables are not themselves iid<br />(independently and identically distributed). Exchangeability is a much weaker<br />assumption than iid; iid variables are automatically exchangeable.<br />If θ parametrizes the underlying distribution, and one assumes a prior dis-<br />tribution over θ, then the resulting marginal distribution over X1,X2,... with<br />θ marginalized out will still be exchangeable. A fundamental result credited<br />to de Finetti [4] states that the converse is also true. That is, if X1,X2,... is<br />(infinitely) exchangeable, then there is a random θ such that:<br />?<br />for every n ≥ 1. In other words, the seemingly innocuous assumption of ex-<br />changeability automatically implies the existence of a hierarchical Bayesian<br />model with θ being the random latent parameter. This the crux of the fun-<br />damental importance of exchangeability to Bayesian statistics.<br />In de Finetti’s Theorem it is important to stress that θ can be infinite dimen-<br />sional (it is typically a random measure), thus the hierarchical Bayesian model<br />P(X1,...,Xn) =<br />P(θ)<br />n<br />?<br />i=1<br />P(Xi|θ)dθ<br />(3)<br />6</p>  <p>Page 7</p> <p>(3) is typically a nonparametric one. For example, the Blackwell-MacQueen<br />urn scheme (related to the CRP) is exchangeable thus implicitly defines a ran-<br />dom measure, namely the DP (see the DP entry [?] for more details). In this<br />sense, we will see below that de Finetti’s Theorem is an alternative route to Kol-<br />mogorov’s Extension Theorem, which implicitly defines the stochastic processes<br />underlying Bayesian nonparametric models.<br />Model Representations<br />In finite dimensions, a probability model is usually defined by a density function<br />or probability mass function. In infinite-dimensional spaces, this approach is<br />not generally feasible, for reasons explained below. To define or work with<br />a Bayesian nonparametric model, we have to choose alternative mathematical<br />representations.<br />Weak Distributions. A weak distribution is a representation for the dis-<br />tribution of a stochastic process, that is, for a probability distribution on an<br />infinite-dimensional sample space. If we assume that the dimensions of the<br />space are indexed by t ∈ T, the stochastic process can be regarded as the joint<br />distribution P of an infinite set of random variables {Xt}t∈T. For any finite<br />subset S ⊂ T of dimensions, the joint distribution PS of the corresponding<br />subset {Xt}t∈Sof random variables is a finite-dimensional marginal of P. The<br />weak distribution of a stochastic process is the set of all its finite-dimensional<br />marginals, that is, the set {PS: S ⊂ T,|S| &lt; ∞}. For example, the customary<br />definition of the Gaussian process as an infinite collection of random variables,<br />each finite subset of which has a joint Gaussian distribution, is an example of<br />a weak distribution representation. In contrast to the explicit representations<br />to be described below, this representation is generally not generative, because<br />it represents the distribution rather than a random draw, but is more widely<br />applicable.<br />Apparently, just defining a weak distribution in this manner need not im-<br />ply that it is a valid representation of a stochastic process. A given collection<br />of finite-dimensional distributions represents a stochastic process only (1) if a<br />process with these distributions as its marginals actually exists, and (2) if it is<br />uniquely defined by the marginals. The mathematical result which guarantees<br />that weak distribution representations are valid is the Kolmogorov Extension<br />Theorem (also known as the Daniell-Kolmogorov theorem or the Kolmogorov<br />Consistency Theorem). Suppose that a collection {PS : S ⊂ T,|S| &lt; ∞} of<br />distributions is given. If all distributions in the collection are marginals of each<br />other, that is, if PS1is a marginal of PS2whenever S1⊂ S2, the set of distribu-<br />tions is called a projective family. The Kolmogorov Extension Theorem states<br />that, if the set T is countable, and if the distributions PS form a projective<br />family, then there exists a uniquely defined stochastic process with the collec-<br />tion {PS} as its marginal distributions. In other words, any projective family<br />for a countable set T of dimensions is the weak distribution of a stochastic pro-<br />cess. Conversely, any stochastic process can be represented in this manner, by<br />computing its set of finite-dimensional marginals.<br />7</p>  <p>Page 8</p> <p>The weak distribution representation assumes that all individual random<br />variable Xtof the stochastic process take values in the same sample space Ω.<br />The stochastic process P defined by the weak distribution is then a probability<br />distribution on the sample space ΩT, which can be interpreted as the set of all<br />function f : T → Ω. For example, to construct a GP we might choose T = Q<br />and Ω = R to obtain real-valued functions on the countable space of rational<br />numbers. Since Q is dense in R, the function f can then be extended to all of<br />R by continuity. To define the DP as a distribution over probability measures<br />on R, we note that a probability measure is a set function that maps “random<br />events”, i.e. elements of the Borel σ-algebra B(R) of R, into probabilities in<br />[0,1]. We could therefore choose a weak distribution consisting of Dirichlet<br />distributions, and set T = B(R) and Ω = [0,1]. However, this approach raises a<br />new problem because the set B(R) is not countable. As in the GP, we can first<br />define the DP on a countable “base” for B(R) then extend to all random events<br />by continuity of measures. More precise descriptions are unfortunately beyond<br />the scope of this entry.<br />Explicit Representations. Explicit representations directly describe a ran-<br />dom draw from a stochastic process, rather than describing its distribution. A<br />prominent example of an explicit representation is the so-called stick-breaking<br />representation of the Dirichlet process.The discrete random measure G in (1)<br />is completely determined by the two infinite sequences {πk}k∈Nand {θk}k∈N.<br />The stick-breaking representation of the DP generates these two sequences by<br />drawing θk∼ H iid and vk∼ Beta(1,α) for k = 1,2,.... The coefficients πkare<br />then computed as πk= vk<br />shown to be distributed according to a DP(α,G0). Similar representations can<br />be derived for the Pitman-Yor process and the beta process as well. Explicit<br />representations, if they exist for a given model, are typically of great practical<br />importance for the derivation of algorithms.<br />Implicit Representations.<br />A third representation of infinite dimensional<br />models is based on de Finetti’s Theorem. Any exchangeable sequence X1,...,Xn<br />uniquely defines a stochastic process θ, called the de Finetti measure, making<br />the Xi’s iid. If the Xi’s are sufficient to define the rest of the model and their<br />conditional distributions are easily specified, then it is sufficient to work directly<br />with the Xi’s and have the underlying stochastic process implicitly defined. Ex-<br />amples include the Chinese restaurant process (an exchangeable distribution<br />over partitions) with the DP as the de Finetti measure, and the Indian buffet<br />process (an exchangeable distribution over binary matrices) with the BP being<br />the corresponding de Finetti measure. These implicit representations are useful<br />in practice as they can lead to simple and efficient inference algorithms.<br />Finite Representations. A fourth representation of Bayesian nonparametric<br />models is as the infinite limit of finite (parametric) Bayesian models. For exam-<br />ple, DP mixtures can be derived as the infinite limit of finite mixture models<br />with particular Dirichlet priors on mixing proportions, GPs can be derived as<br />the infinite limit of particular Bayesian regression models with Gaussian priors,<br />while BPs can be derived as from the limit of an infinite number of indepen-<br />?k−1<br />j=1(1 − vk). The measure G so obtained can be<br />8</p>  <p>Page 9</p> <p>dent beta variables. These representations are sometimes more intuitive for<br />practitioners familiar with parametric models. However not all Bayesian non-<br />parametric models can be expressed in this fashion, and they do not necessarily<br />make clear the mathematical subtleties involved.<br />Consistency and Convergence Rates<br />Recent work in mathematical statistics examines the convergence properties of<br />Bayesian nonparametric models, and in particular the questions of consistency<br />and convergence rates.<br />Intuitively, a consistent estimation procedure is a method that will result<br />in a correct estimate if it has access to an infinite amount of data, that is,<br />a procedure that will be accurate unless it is hampered by insufficient sample<br />size. In Bayesian statistics, an estimate is a distribution over possible parameter<br />values (the posterior), and hence consistency in Bayesian models is defined by<br />demanding that the posterior converges to a delta peak at the true parameter.<br />A classic theorem by J. L. Doob shows that for any Bayesian model, the set<br />of all possible true parameter values for which the model will be consistent has<br />probability one under the prior. In other words, if the true parameter (and<br />hence the data distribution) is chosen at random from the prior, we will always<br />end up with a consistent model, and a Bayesian who is certain about the prior<br />need not worry about inconsistency. This result does not hold anymore if the<br />true parameter value is not in the domain of the prior. In nonparametric mod-<br />els, which have to spread out their probability mass over an infinite-dimensional<br />space, this can result in seemingly unreasonable behavior of the model. For ex-<br />ample, the Dirichlet process may be used as a prior in density estimation. Its<br />support consists of the discrete distributions, which means that if the data is<br />drawn from any smooth distribution, the true model is not in the support of the<br />prior (so Doob’s theorem does not apply). As shown by Diaconis and Freedman<br />[5], the posterior will not necessarily concentrate in a region close to the true<br />model. Intuitively, the implication is that we cannot generally assume in non-<br />parametric models that the effect of the prior will eventually become negligible<br />if only we see enough data. However, this does not mean that nonparametric<br />Bayesian models are generally inconsistent: A large and growing literature in<br />mathematical statistics shows that consistency can be guaranteed by proper<br />choice of an adequate nonparametric model [6].<br />Recent results, notably by van der Vaart and Ghosal, apply modern meth-<br />ods of mathematical statistics to study the convergence properties of Bayesian<br />nonparametric models (see [6] for further references). Consistency has been es-<br />tablished for a number of models, including Gaussian processes and Dirichlet<br />process mixtures. A particularly interesting aspect of this line of work are re-<br />sults on convergence rates, which specify how rapidly the posterior concentrates<br />with growing sample size, depending on the complexity of the model and on how<br />much probability mass the prior places around the true solution. To make such<br />results quantitative requires a measure for the complexity of a Bayesian non-<br />parametric model. This is done by means of complexity measures developed in<br />9</p>  <p>Page 10</p> <p>empirical process theory and statistical learning theory, such as metric entropies,<br />covering numbers and bracketing, some of which are well-known in theoretical<br />machine learning. Examples of such results include the consistency of Dirichlet<br />process mixture models for density estimation if both the target density and the<br />parametric mixture components are smooth, and a range of consistency results<br />for regression and density estimation with Gaussian processes. For all of these<br />results, convergence rates can be specified as well (references are given in [6]).<br />A large class of infinite-dimensional models which do behave well even if the<br />true parameter is not in the domain of the prior is identified in [7]. In this case,<br />the posterior will concentrate in the region of the prior support which is closest<br />to the true parameter in a Kullback-Leibler sense.<br />Inference<br />There are two aspects to inference in Bayesian nonparametric models: the ana-<br />lytic tractability of posteriors for the stochastic processes embedded in Bayesian<br />nonparametric models, and practical inference algorithms for the overall mod-<br />els. Bayesian nonparametric models typically include stochastic processes such<br />as the Gaussian process and the Dirichlet process. These processes have an<br />infinite number of dimensions thus na¨ ıve algorithmic approaches to computing<br />posteriors is generally infeasible. Fortunately, these processes typically have<br />analytically tractable posteriors, so all but finitely many of the dimensions can<br />be analytically integrated out efficiently. The remaining dimensions, along with<br />the parametric parts of the models, can then be handled by the usual infer-<br />ence techniques employed in parametric Bayesian modeling, including Markov<br />chain Monte Carlo, sequential Monte Carlo, variational inference, and message-<br />passing algorithms like expectation propagation. The precise choice of approx-<br />imations to use will depend on the specific models under consideration, with<br />speed/accuracy trade-offs between different techniques generally following those<br />for parametric models. In the following, we will given two examples to illus-<br />trate the above points, and discuss a few theoretical issues associated with the<br />analytic tractability of stochastic processes.<br />Examples<br />In Gaussian process regression, we model the relationship between an input x<br />and an output y using a function f, so that y ∼ f(x)+? where ? is iid Gaussian<br />noise. Given a GP prior over f and a finite training data set {(xi,yi)}n<br />wish to compute the posterior over f. Here we can use the weak representation<br />of f and note that {f(xi)}n<br />mean and covariance given by the mean and covariance functions of the GP.<br />Inference for {f(xi)}n<br />of equivalently as marginalizing out the whole function except its values on the<br />training inputs. Note that although we only have the posterior over {f(xi)}n<br />this is sufficient to reconstruct the function evaluated at any other point x0(say<br />i=1we<br />i=1is simply a finite-dimensional Gaussian with<br />i=1is then straightforward. The approach can be thought<br />i=1,<br />10</p>  <p>Page 11</p> <p>the test input), since f(x0) is Gaussian and independent of the training data<br />{(xi,yi)}n<br />can be computed exactly. In GP classification or other regression settings with<br />nonlinear likelihood functions, the typical approach is to use sparse methods<br />based on variational approximations or expectation propagation; see GP entry<br />[?] for details.<br />Our second example involves Dirichlet process mixture models. Recall that<br />the DP induces a clustering structure on the data items. If our training set<br />consists of n data items, since each item can only belong to one cluster, there<br />are at most n clusters represented in the training set. Even though the DP<br />mixture itself has an infinite number of potential clusters, all but finitely many<br />of these are not associated with data, thus the associated variables need not<br />be explicitly represented at all. This can be understood either as marginalizing<br />out these variables, or as an implicit representation which can be made explicit<br />whenever required by sampling from the prior. This idea is applicable for DP<br />mixtures using both the Chinese restaurant process and the stick-breaking rep-<br />resentations. In the CRP representation, each data item xiis associated with a<br />cluster index zi, and each cluster k with a parameter θ∗<br />be marginalized out if H is conjugate to F), and these are the only latent vari-<br />ables that need be represented in memory. In the stick-breaking representation,<br />clusters are ordered by decreasing prior expected size, with cluster k associated<br />with a parameter θ∗<br />a cluster index zi, and only the clusters up to K = max(z1,...,zn) need be<br />represented. All clusters with index &gt; K need not be represented since their<br />posterior conditioning on {(xi,zi)}n<br />i=1given {f(xi)}n<br />i=1. In GP regression the posterior over {f(xi)}n<br />i=1<br />k(these parameters can<br />kand a size πk. Each data item is again associated with<br />i=1is just the prior.<br />On Bayes Equations and Conjugacy<br />It is worth noting that the posterior of a Bayesian model is, in abstract terms,<br />defined as the conditional distribution of the parameter given the data and<br />the hyperparameters, and this definition does not require the existence of a<br />Bayes equation. If a Bayes equation exists for the model, the posterior can<br />equivalently be defined as the left-hand side of the Bayes equation. However for<br />some stochastic processes, notably the DP on an uncountable space such as R,<br />it is not possible to define a Bayes equation even though the posterior is still<br />a well-defined mathematical object. Technically speaking, existence of a Bayes<br />equation requires the family of all possible posteriors to be dominated by the<br />prior, but this is not the case for the DP. That posteriors of these stochastic<br />processes can be evaluated at all is solely due to the fact that they admit an<br />analytic representation.<br />The particular form of tractability exhibited by many stochastic processes in<br />the literature is that of a conjugate posterior, that is, the posterior belongs to the<br />same model family as the prior, and the posterior parameters can be computed<br />as a function of the prior hyperparameters and the observed data. For example,<br />the posterior of a DP(α,G0) under observations θ1,...,θnis again a Dirichlet<br />process, DP(α + n,<br />1<br />α+n(αG0+?δθi)). Similarly the posterior of a GP under<br />11</p>  <p>Page 12</p> <p>observations of f(x1),...,f(xn) is still a GP. It is this conjugacy that allows<br />practical inference in the examples above. A Bayesian nonparametric model<br />is conjugate if and only if the elements of its weak distribution, i.e. its finite-<br />dimensional marginals, have a conjugate structure as well [8]. In particular, this<br />characterizes a class of conjugate Bayesian nonparametric models whose weak<br />distributions consist of exponential family models. Note however that lack of<br />conjugacy do not imply intractable posteriors. An example is given by the<br />Pitman-Yor process, where the posterior is given by a sum of a finite number of<br />atoms and a Pitman-Yor process independent from the atoms.<br />Future Directions<br />Since MCMC sampling algorithms for Dirichlet process mixtures became avail-<br />able in the 1990s and made latent variable models with nonparametric Bayesian<br />components applicable to practical problems, the development of Bayesian non-<br />parametrics has experienced explosive growth [9, 10]. Arguably, though, the<br />results available so far have only scratched the surface. The repertoire of avail-<br />able models is still mostly limited to using the Gaussian process, the Dirichlet<br />process, the beta process, and generalizations derived from those. In princi-<br />ple, Bayesian nonparametric models may be defined on any infinite-dimensional<br />mathematical object of possible interest to machine learning and statistics. Pos-<br />sible examples are kernels, infinite graphs, special classes of functions (e.g. piece-<br />wise continuous or Sobolev functions), and permutations.<br />Aside from the obvious modeling questions, two major future directions are<br />to make Bayesian nonparametric methods available to a larger audience of re-<br />searchers and practitioners through the development of software packages, and<br />to understand and quantify the theoretical properties of available methods.<br />General-Purpose Software Package<br />There is currently significant growth in the application of Bayesian nonparamet-<br />ric models across a variety of application domains both in machine learning and<br />in statistics. However significant hurdles still exist, especially the expense and<br />expertise needed to develop computer programs for inference in these complex<br />models. One future direction is thus the development of software packages that<br />can compile efficient inference algorithms automatically given model specifica-<br />tions, thus allowing a much wider range of modeler to make use of these models.<br />Current developments include the R DPpackage1, the hierarchical Bayesian com-<br />piler2, adaptor grammars3, the MIT-Church project4, as well as efforts to add<br />Bayesian nonparametric models to the repertoire of current Bayesian modeling<br />1http://cran.r-project.org/web/packages/DPpackage<br />2http://www.cs.utah.edu/ hal/HBC<br />3http://www.cog.brown.edu/ mj/Software.htm<br />4http://projects.csail.mit.edu/church/wiki/Church<br />12</p>  <p>Page 13</p> <p>environments like OpenBugs5and infer.NET6.<br />Statistical Properties of Models<br />Recent work in mathematical statistics provides some insight into the quan-<br />titative behavior of Bayesian nonparametric models (cf theory section). The<br />elegant, methodical approach underlying these results, which quantifies model<br />complexity by means of empirical process theory and then derives convergence<br />rates as a function of the complexity, should be applicable to a wide range of<br />models. So far, however, only results for Gaussian processes and Dirichlet pro-<br />cess mixtures have been proven, and it will be of great interest to establish<br />properties for other priors. Some models developed in machine learning, such<br />as the infinite HMM, may pose new challenges to theoretical methodology, since<br />their study will probably have to draw on both the theory of algorithms and<br />mathematical statistics. Once a wider range of results is available, they may in<br />turn serve to guide the development of new models, if it is possible to establish<br />how different methods of model construction affect the statistical properties of<br />the constructed model.<br />Cross Reference<br />Dirichlet Processes, Gaussian Processes, Bayesian Methods, Prior Probabilities.<br />Recommended Reading<br />In addition to the references embedded in the text above, we recommend the<br />books [11, 12] and the review articles [13, 14] on Bayesian nonparametrics.<br />References for DPs and DP mixture models can be found in the DP entry [?],<br />for GPs in the GP entry [?], while for most of the other examples can be found<br />in the chapter [3] of the book [11].<br />[1] L. Wasserman. All of Nonparametric Statistics. Springer, 2006.<br />[2] D. B. Dunson.<br />N. Hjort, C. Holmes, P. M¨ uller, and S. Walker, editors, Bayesian Non-<br />parametrics. Cambridge University Press, 2010.<br />Nonparametric Bayes applications to biostatistics. In<br />[3] Y. W. Teh and M. I. Jordan. Hierarchical Bayesian nonparametric models<br />with applications. In N. Hjort, C. Holmes, P. M¨ uller, and S. Walker, editors,<br />Bayesian Nonparametrics. Cambridge University Press, 2010.<br />[4] B. de Finetti. Funzione caratteristica di un fenomeno aleatorio. Atti della<br />R. Academia Nazionale dei Lincei, Serie 6. Memorie, Classe di Scienze<br />Fisiche, Mathematice e Naturale, 4, 1931.<br />5http://mathstat.helsinki.fi/openbugs<br />6http://research.microsoft.com/en-us/um/cambridge/projects/infernet<br />13</p>  <p>Page 14</p> <p>[5] P. Diaconis and D. Freedman. On the consistency of Bayes estimates (with<br />discussion). Annals of Statistics, 14(1):1–67, 1986.<br />[6] S. Ghosal. The Dirichlet process, related priors and posterior asymptotics.<br />In N. Hjort, C. Holmes, P. M¨ uller, and S. Walker, editors, Bayesian Non-<br />parametrics. Cambridge University Press, 2010.<br />[7] B. J. K. Kleijn and A. W. van der Vaart. Misspecification in infinite-<br />dimensional Bayesian statistics. Annals of Statistics, 34:837–877, 2006.<br />[8] P. Orbanz. Construction of nonparametric Bayesian models from para-<br />metric Bayes equations. In Advances in Neural Information Processing<br />Systems, 2010.<br />[9] M. D. Escobar and M. West. Bayesian density estimation and inference<br />using mixtures. Journal of the American Statistical Association, 90:577–<br />588, 1995.<br />[10] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture<br />models. Journal of Computational and Graphical Statistics, 9:249–265,<br />2000.<br />[11] N. Hjort, C. Holmes, P. M¨ uller, and S. Walker, editors. Bayesian Nonpara-<br />metrics. Number 28 in Cambridge Series in Statistical and Probabilistic<br />Mathematics. Cambridge University Press, 2010.<br />[12] J. K. Ghosh and R. V. Ramamoorthi. Bayesian Nonparametrics. Springer,<br />2002.<br />[13] S. G. Walker, P. Damien, P. W. Laud, and A. F. M. Smith. Bayesian<br />nonparametric inference for random distributions and related functions.<br />Journal of the Royal Statistical Society, 61(3):485–527, 1999.<br />[14] P. M¨ uller and F. A. Quintana. Nonparametric Bayesian data analysis.<br />Statistical Science, 19(1):95–110, 2004.<br />14</p>   </div> <div id="rgw18_56ab9f7f4f463" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab9f7f4f463">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab9f7f4f463"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/OrbTeh2010a.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Bayesian Nonparametric Models">Bayesian Nonparametric Models</a> </div>  <div class="details">   Available from <a href="http://www.gatsby.ucl.ac.uk/~ywteh/research/npbayes/OrbTeh2010a.pdf" target="_blank" rel="nofollow">gatsby.ucl.ac.uk</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw22_56ab9f7f4f463" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab9f7f4f463">  </ul> </div> </div>   <div id="rgw14_56ab9f7f4f463" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab9f7f4f463"> <div> <h5> <a href="publication/279811099_Planning_under_uncertainty_with_Bayesian_nonparametric_models" class="color-inherit ga-similar-publication-title"><span class="publication-title">Planning under uncertainty with Bayesian nonparametric models</span></a>  </h5>  <div class="authors"> <a href="researcher/2077514143_Robert_H_Robert_Henry_Klein" class="authors ga-similar-publication-author">Robert H. (Robert Henry) Klein</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab9f7f4f463"> <div> <h5> <a href="publication/257814700_Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian Nonparametric Models for Multiway Data Analysis</span></a>  </h5>  <div class="authors"> <a href="researcher/2034533595_Zenglin_Xu" class="authors ga-similar-publication-author">Zenglin Xu</a>, <a href="researcher/70220978_Feng_Yan" class="authors ga-similar-publication-author">Feng Yan</a>, <a href="researcher/2034487446_Yaun_Qi" class="authors ga-similar-publication-author">Yaun Qi</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab9f7f4f463"> <div> <h5> <a href="publication/286647489_Bayesian_nonparametric_models_for_combining_heterogeneous_reliability_data" class="color-inherit ga-similar-publication-title"><span class="publication-title">Bayesian nonparametric models for combining heterogeneous reliability data</span></a>  </h5>  <div class="authors"> <a href="researcher/2088630734_RL_Warr" class="authors ga-similar-publication-author">R.L. Warr</a>, <a href="researcher/2088549572_DH_Collins" class="authors ga-similar-publication-author">D.H. Collins</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw28_56ab9f7f4f463" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw29_56ab9f7f4f463">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw30_56ab9f7f4f463" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=-R_K2bDSmetCVXiRAzWmewLDqMzV8KYv2eOdnHrMVQ2xokwpBq7is-ARwSJKhxd2" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="zxxr7VzXryIFOSS5hyfeIG73P8OPQ3OwUULqh2WKD+Dwjkd2nOPvLbRxldaE2sGmNGnULoXQeEPq4ZtP5xkpoI8qSmVR6LXX5U8bvPSF1azUfhxhzon12zfFlVYjRIqTY5GHIFQnCVagSiUvVSkvs4wjqr5rmGNVohNY0XyworioOVoa9fMgADb/eGpZx49Gl7OVMvduuXNFxrUXkZmpNyGuVAvdIb60TzvhoAXc4aMwGC8dLTjdBk04GApIkYmPXP9cASBlKlu0Hf5sRdh0lwOantadG5xq4Jg6R0XuK0A="/> <input type="hidden" name="urlAfterLogin" value="publication/247935319_Bayesian_Nonparametric_Models"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjQ3OTM1MzE5X0JheWVzaWFuX05vbnBhcmFtZXRyaWNfTW9kZWxz"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjQ3OTM1MzE5X0JheWVzaWFuX05vbnBhcmFtZXRyaWNfTW9kZWxz"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjQ3OTM1MzE5X0JheWVzaWFuX05vbnBhcmFtZXRyaWNfTW9kZWxz"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw31_56ab9f7f4f463"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 707;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/69976956_Peter_Orbanz","fullname":"Peter Orbanz","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"17.65","widgetId":"rgw5_56ab9f7f4f463"},"id":"rgw5_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=69976956","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":20,"widgetId":"rgw6_56ab9f7f4f463"},"id":"rgw6_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=69976956","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56ab9f7f4f463"},"id":"rgw4_56ab9f7f4f463","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=69976956","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab9f7f4f463"},"id":"rgw3_56ab9f7f4f463","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=247935319","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":247935319,"title":"Bayesian Nonparametric Models","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"doi":"10.1007\/978-0-387-30164-8_66","journalInfos":{"journal":"","publicationDate":"","publicationDateRobot":false,"article":""}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1007\/978-0-387-30164-8_66"},{"key":"rft.atitle","value":"Bayesian Nonparametric Models"},{"key":"rft.au","value":"Peter Orbanz,Yee Whye Teh"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw8_56ab9f7f4f463"},"id":"rgw8_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=247935319","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":247935319,"peopleItems":[{"data":{"authorUrl":"researcher\/69976956_Peter_Orbanz","authorNameOnPublication":"Peter Orbanz","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Peter Orbanz","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69976956_Peter_Orbanz","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab9f7f4f463"},"id":"rgw11_56ab9f7f4f463","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69976956&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab9f7f4f463"},"id":"rgw10_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69976956&authorNameOnPublication=Peter%20Orbanz","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/9164246_Yee_Whye_Teh","authorNameOnPublication":"Yee Whye Teh","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Yee Whye Teh","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/9164246_Yee_Whye_Teh","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab9f7f4f463"},"id":"rgw13_56ab9f7f4f463","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=9164246&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab9f7f4f463"},"id":"rgw12_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=9164246&authorNameOnPublication=Yee%20Whye%20Teh","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw9_56ab9f7f4f463"},"id":"rgw9_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=247935319&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":null,"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/247935319_Bayesian_Nonparametric_Models\/links\/029313640cf270384305b058\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56ab9f7f4f463"},"id":"rgw7_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=247935319&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2077514143,"url":"researcher\/2077514143_Robert_H_Robert_Henry_Klein","fullname":"Robert H. (Robert Henry) Klein","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2014","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/279811099_Planning_under_uncertainty_with_Bayesian_nonparametric_models","usePlainButton":true,"publicationUid":279811099,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/279811099_Planning_under_uncertainty_with_Bayesian_nonparametric_models","title":"Planning under uncertainty with Bayesian nonparametric models","displayTitleAsLink":true,"authors":[{"id":2077514143,"url":"researcher\/2077514143_Robert_H_Robert_Henry_Klein","fullname":"Robert H. (Robert Henry) Klein","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/279811099_Planning_under_uncertainty_with_Bayesian_nonparametric_models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/279811099_Planning_under_uncertainty_with_Bayesian_nonparametric_models\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab9f7f4f463"},"id":"rgw15_56ab9f7f4f463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=279811099","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2034533595,"url":"researcher\/2034533595_Zenglin_Xu","fullname":"Zenglin Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70220978,"url":"researcher\/70220978_Feng_Yan","fullname":"Feng Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2034487446,"url":"researcher\/2034487446_Yaun_Qi","fullname":"Yaun Qi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2013","journal":"IEEE Transactions on Software Engineering","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/257814700_Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis","usePlainButton":true,"publicationUid":257814700,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"5.78","url":"publication\/257814700_Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis","title":"Bayesian Nonparametric Models for Multiway Data Analysis","displayTitleAsLink":true,"authors":[{"id":2034533595,"url":"researcher\/2034533595_Zenglin_Xu","fullname":"Zenglin Xu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70220978,"url":"researcher\/70220978_Feng_Yan","fullname":"Feng Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2034487446,"url":"researcher\/2034487446_Yaun_Qi","fullname":"Yaun Qi","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Software Engineering 10\/2013; 37(2). DOI:10.1109\/TPAMI.2013.201"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/257814700_Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/257814700_Bayesian_Nonparametric_Models_for_Multiway_Data_Analysis\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab9f7f4f463"},"id":"rgw16_56ab9f7f4f463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=257814700","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2088630734,"url":"researcher\/2088630734_RL_Warr","fullname":"R.L. Warr","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088549572,"url":"researcher\/2088549572_DH_Collins","fullname":"D.H. Collins","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Apr 2014","journal":"Proceedings of the Institution of Mechanical Engineers Part O Journal of Risk and Reliability","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/286647489_Bayesian_nonparametric_models_for_combining_heterogeneous_reliability_data","usePlainButton":true,"publicationUid":286647489,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.86","url":"publication\/286647489_Bayesian_nonparametric_models_for_combining_heterogeneous_reliability_data","title":"Bayesian nonparametric models for combining heterogeneous reliability data","displayTitleAsLink":true,"authors":[{"id":2088630734,"url":"researcher\/2088630734_RL_Warr","fullname":"R.L. Warr","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088549572,"url":"researcher\/2088549572_DH_Collins","fullname":"D.H. Collins","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Proceedings of the Institution of Mechanical Engineers Part O Journal of Risk and Reliability 04\/2014; 228(2):166-175. DOI:10.1177\/1748006X13503319"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/286647489_Bayesian_nonparametric_models_for_combining_heterogeneous_reliability_data","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/286647489_Bayesian_nonparametric_models_for_combining_heterogeneous_reliability_data\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab9f7f4f463"},"id":"rgw17_56ab9f7f4f463","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=286647489","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab9f7f4f463"},"id":"rgw14_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=247935319&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":247935319,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":247935319,"publicationType":"article","linkId":"029313640cf270384305b058","fileName":"Bayesian Nonparametric Models","fileUrl":"http:\/\/www.gatsby.ucl.ac.uk\/~ywteh\/research\/npbayes\/OrbTeh2010a.pdf","name":"gatsby.ucl.ac.uk","nameUrl":"http:\/\/www.gatsby.ucl.ac.uk\/~ywteh\/research\/npbayes\/OrbTeh2010a.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw20_56ab9f7f4f463"},"id":"rgw20_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=247935319&linkId=029313640cf270384305b058&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab9f7f4f463"},"id":"rgw19_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=247935319&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw21_56ab9f7f4f463"},"id":"rgw21_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=247935319","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab9f7f4f463"},"id":"rgw18_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=247935319&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":247935319,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56ab9f7f4f463"},"id":"rgw23_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=247935319&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw24_56ab9f7f4f463"},"id":"rgw24_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=247935319","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab9f7f4f463"},"id":"rgw22_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=247935319&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Bayesian Nonparametric Models\nPeter Orbanz, Cambridge University\nYee Whye Teh, University College London\nRelated keywords: Bayesian Methods, Prior Probabilities, Dirichlet Process,\nGaussian Processes.\nDefinition\nA Bayesian nonparametric model is a Bayesian model on an infinite-dimensional\nparameter space. The parameter space is typically chosen as the set of all possi-\nble solutions for a given learning problem. For example, in a regression problem\nthe parameter space can be the set of continuous functions, and in a density esti-\nmation problem the space can consist of all densities. A Bayesian nonparametric\nmodel uses only a finite subset of the available parameter dimensions to explain\na finite sample of observations, with the set of dimensions chosen depending on\nthe sample, such that the effective complexity of the model (as measured by the\nnumber of dimensions used) adapts to the data. Classical adaptive problems,\nsuch as nonparametric estimation and model selection, can thus be formulated\nas Bayesian inference problems. Popular examples of Bayesian nonparametric\nmodels include Gaussian process regression, in which the correlation structure\nis refined with growing sample size, and Dirichlet process mixture models for\nclustering, which adapt the number of clusters to the complexity of the data.\nBayesian nonparametric models have recently been applied to a variety of ma-\nchine learning problems, including regression, classification, clustering, latent\nvariable modeling, sequential modeling, image segmentation, source separation\nand grammar induction.\nMotivation and Background\nMost of machine learning is concerned with learning an appropriate set of pa-\nrameters within a model class from training data. The meta level problems\nof determining appropriate model classes are referred to as model selection or\nmodel adaptation. These constitute important concerns for machine learning\npractitioners, chiefly for avoidance of over-fitting and under-fitting, but also for\ndiscovery of the causes and structures underlying data. Examples of model se-\nlection and adaptation include: selecting the number of clusters in a clustering\nproblem, the number of hidden states in a hidden Markov model, the number\n1"},{"page":2,"text":"of latent variables in a latent variable model, or the complexity of features used\nin nonlinear regression.\nNonparametric models constitute an approach to model selection and adap-\ntation, where the sizes of models are allowed to grow with data size. This is\nas opposed to parametric models which uses a fixed number of parameters. For\nexample, a parametric approach to density estimation would be to fit a Gaus-\nsian or a mixture of a fixed number of Gaussians by maximum likelihood. A\nnonparametric approach would be a Parzen window estimator, which centers a\nGaussian at each observation (and hence uses one mean parameter per observa-\ntion). Another example is the support vector machine with a Gaussian kernel.\nThe representer theorem shows that the decision function is a linear combina-\ntion of Gaussian radial basis functions centered at every input vector, and thus\nhas a complexity that grows with more observations. Nonparametric methods\nhave long been popular in classical (non-Bayesian) statistics [1]. They often per-\nform impressively in applications and, though theoretical results for such models\nare typically harder to prove than for parametric models, appealing theoretical\nproperties have been established for a wide range of models.\nBayesian nonparametric methods provide a Bayesian framework for model\nselection and adaptation using nonparametric models. A Bayesian formulation\nof nonparametric problems is nontrivial, since a Bayesian model defines prior\nand posterior distributions on a single fixed parameter space, but the dimen-\nsion of the parameter space in a nonparametric approach should change with\nsample size. The Bayesian nonparametric solution to this problem is to use\nan infinite-dimensional parameter space, and to invoke only a finite subset of\nthe available parameters on any given finite data set. This subset generally\ngrows with the data set. In the context of Bayesian nonparametric models,\n\u201cinfinite-dimensional\u201d can therefore be interpreted as \u201cof finite but unbounded\ndimension\u201d. More precisely, a Bayesian nonparametric model is a model that\n(1) constitutes a Bayesian model on an infinite-dimensional parameter space\nand (2) can be evaluated on a finite sample in a manner that uses only a finite\nsubset of the available parameters to explain the sample.\nWe make the above description more concrete in the next section when we\ndescribe a number of standard machine learning problems and the correspond-\ning Bayesian nonparametric solutions. As we will see, the parameter space in\n(1) typically consists of functions or of measures, while (2) is usually achieved\nby marginalizing out surplus dimensions over the prior. Random functions and\nmeasures, and more generally probability distributions on infinite-dimensional\nrandom objects, are called stochastic processes; examples we will encounter\ninclude Gaussian processes, Dirichlet processes and beta processes. Bayesian\nnonparametric models are often named after the stochastic processes they con-\ntain. The examples are then followed by theoretical considerations, including\nformal constructions and representations of the stochastic processes used in\nBayesian nonparametric models, exchangeability, and issues of consistency and\nconvergence rate. We conclude this article with future directions and a reading\nlist.\n2"},{"page":3,"text":"Examples\nClustering with mixture models. Bayesian nonparametric generalizations\nof finite mixture models provide an approach for estimating both the number\nof components in a mixture model and the parameters of the individual mix-\nture components simultaneously from data. Finite mixture models define a\ndensity function over data items x of the form p(x) =?K\nnent k. The density can be written in a non-standard manner as an integral:\np(x) =?p(x|\u03b8)G(\u03b8)d\u03b8, where G =?K\ntribution (atom) centered at \u03b8. Bayesian nonparametric mixtures use mixing\ndistributions consisting of a countably infinite number of atoms instead:\nk=1\u03c0kp(x|\u03b8k), where\n\u03c0k is the mixing proportion and \u03b8k are parameters associated with compo-\nk=1\u03c0k\u03b4\u03b8kis a discrete mixing distribution\nencapsulating all the parameters of the mixture model and \u03b4\u03b8 is a Dirac dis-\nG =\n\u221e\n?\nk=1\n\u03c0k\u03b4\u03b8k.\n(1)\nThis gives rise to mixture models with an infinite number of components. When\napplied to a finite training set, only a finite (but varying) number of components\nwill be used to model the data, since each data item is associated with exactly\none component but each component can be associated with multiple data items.\nInference in the model then automatically recovers both the number of compo-\nnents to use and the parameters of the components. Being Bayesian, we need\na prior over the mixing distribution G, and the most common prior to use is a\nDirichlet process (DP). The resulting mixture model is called a DP mixture.\nFormally, a Dirichlet process DP(\u03b1,H) parametrized by a concentration\nparamter \u03b1 > 0 and a base distribution H is a prior over distributions (probabil-\nity measures) G such that, for any finite partition A1,...,Amof the parameter\nspace, the induced random vector (G(A1),...,G(Am)) is Dirichlet distributed\nwith parameters (\u03b1H(A1),...,\u03b1H(Am)) (see the theory section for a discussion\nof subtleties involved in this definition). It can be shown that draws from a DP\nwill be discrete distributions as given in (1). The DP also induces a distribution\nover partitions of integers called the Chinese restaurant process (CRP), which\ndirectly describes the prior over how data items are clustered under the DP\nmixture. For more details on the DP and the CRP, see DP entry [?].\nNonlinear regression. The aim of regression is to infer a continuous function\nfrom a training set consisting of input-output pairs {(ti,xi)}n\napproaches parametrize the function using a finite number of parameters and\nattempt to infer these parameters from data. The prototypical Bayesian non-\nparametric approach to this problem is to define a prior distribution over con-\ntinuous functions directly by means of a Gaussian process (GP). As explained in\nGP entry [?], a GP is a distribution on an infinite collection of random variables\nXt, such that the joint distribution of each finite subset Xt1,...,Xtmis a mul-\ntivariate Gaussian. A value xttaken by the variable Xtcan be regarded as the\nvalue of a continuous function f at t, that is, f(t) = xt. Given the training set,\nthe Gaussian process posterior is again a distribution on functions, conditional\ni=1. Parametric\n3"},{"page":4,"text":"on these functions taking values f(t1) = x1,...,f(tn) = xn.\nLatent feature models. Latent feature models represent a set of objects in\nterms of a set of latent features, each of which represents an independent degree\nof variation exhibited by the data. Such a representation of data is sometimes\nreferred to as a distributed representation. In analogy to nonparametric mix-\nture models with an unknown number of clusters, a Bayesian nonparametric\napproach to latent feature modeling allows for an unknown number of latent\nfeatures. The stochastic processes involved here are known as the Indian buffet\nprocess (IBP) and the beta process (BP). Draws from BPs are random discrete\nmeasures, where each of an infinite number of atoms has a mass in (0,1) but\nthe masses of atoms need not sum to 1. Each atom corresponds to a feature,\nwith the mass corresponding to the probability that the feature is present for\nan object. We can visualize the occurrences of features among objects using a\nbinary matrix, where the (i,k) entry is 1 if object i has feature k and 0 oth-\nerwise. The distribution over binary matrices induced by the BP is called the\nIBP.\nHidden Markov models. Hidden Markov models (HMMs) are popular mod-\nels for sequential or temporal data, where each time step is associate with a\nstate, with state transitions dependent on the previous state. An infinite HMM\nis a Bayesian nonparametric approach to HMMs, where the number of states is\nunbounded and allowed to grow with the sequence length. It is defined using\none DP prior for the transition probabilities going out from each state. To en-\nsure that the set of states reachable from each outgoing state is the same, the\nbase distributions of the DPs are shared and given a DP prior recursively. The\nconstruction is called a hierarchical Dirichlet process (HDP); see below.\nDensity estimation. A nonparametric Bayesian approach to density estima-\ntion requires a prior on densities or distributions. However, the DP is not useful\nin this context, since it generates discrete distributions. A useful density estima-\ntor should smooth the empirical density (such as a Parzen window estimator),\nwhich requires a prior that can generate smooth distributions. Priors applicable\nin density estimation problems include DP mixture models and P\u00b4 olya trees.\nDP mixture models: Since the mixing distribution in the DP mixture is\nrandom, the induced density p(x) is random thus the DP mixture can be used\nas a prior over densities. Despite the fact that these are now primarily used in\nmachine learning as clustering models, they were in fact originally proposed for\ndensity estimation.\nP\u00b4 olya Trees are priors on probability distributions that can generate both\ndiscrete and piecewise continuous distributions, depending on the choice of pa-\nrameters. P\u00b4 olya trees are defined by a recursive infinitely deep binary subdi-\nvision of the domain of the generated random measure. Each subdivision is\nassociated with a beta random variable which describes the relative amount of\nmass on each side of the subdivision. The DP is a special case of a P\u00b4 olya tree\ncorresponding to a particular parametrization. For other parametrizations the\nresulting random distribution can be smooth so is suitable for density estima-\ntion.\nPower-law Phenomena. Many naturally occurring phenomena exhibit power-\n4"},{"page":5,"text":"law behavior. Examples include natural languages, images and social and ge-\nnetic networks. An interesting generalization of the DP, called the Pitman-Yor\nprocess, PYP(\u03b1,d,H), has recently been successfully used as models of such\npower-law data. The Pitman-Yor process augments the DP by a third parame-\nter d \u2208 [0,1). When d = 0 the PYP is a DP(\u03b1,H), while when \u03b1 = 0 it is a so\ncalled normalized stable process.\nSequential modeling. HMMs model sequential data using latent variables\nrepresenting the underlying state of the system, and assuming that each state\nonly depends on the previous state (the so called Markov property). In some\napplications, for example language modeling and text compression, we are inter-\nested in directly modeling sequences without using latent variables, and without\nmaking any Markov assumptions, i.e. modeling each observation conditional on\nall previous observations in the sequence. Since the set of potential sequences\nof previous observations is unbounded, this calls for nonparametric models. A\nhierarchical Pitman-Yor process can be used to construct a Bayesian nonpara-\nmetric solution whereby the conditional probabilities are coupled hierarchically.\nDependent and hierarchical models. Most of the Bayesian nonparametric\nmodels described above are applied in settings where observations are homo-\ngeneous or exchangeable. In many real world settings observations are often\nnot homogeneous, in fact they are often structured in interesting ways. For\nexample, the data generating process might change over time thus observations\nat different times are not exchangeable, or observations might come in distinct\ngroups with those in the same group being more similar than across groups.\nSignificant recent efforts in Bayesian nonparametrics research have been\nplaced in developing extensions that can handle these non-homogeneous set-\ntings. Dependent Dirichlet processes are stochastic processes, typically over a\nspatial or temporal domain, which define a Dirichlet process (or a related ran-\ndom measure) at each point with neighboring DPs being more dependent. These\nare used for spatial modeling, nonparametric regression, as well as for modeling\ntemporal changes. Alternatively, hierarchical Bayesian nonparametric models\nlike the hierarchical DP aim to couple multiple Bayesian nonparametric mod-\nels within a hierarchical Bayesian framework. The idea is to allow sharing of\nstatistical strength across multiple groups of observations. Among other appli-\ncations, these have been used in the infinite HMM, topic modeling, language\nmodeling, word segmentation, image segmentation and grammar induction. For\nan overview of various dependent Bayesian nonparametric models and their ap-\nplications in biostatistics please consult [2]. [3] is an overview of hierarchical\nBayesian nonparametric models as well as a variety of applications in machine\nlearning.\nTheory\nAs we saw in the preceding examples, Bayesian nonparametric models often\nmake use of priors over functions and measures. Because these spaces typically\nhave an uncountable number of dimensions, extra care has to be taken to define\n5"},{"page":6,"text":"the priors properly and to study the asymptotic properties of estimation in the\nresulting models. In this section we give an overview of the basic concepts\ninvolved in the theory of Bayesian nonparametric models. We start with a\ndiscussion of the importance of exchangeability in Bayesian parametric and\nnonparametric statistics. This is followed by representations of the priors and\nissues of convergence.\nExchangeability\nThe underlying assumption of all Bayesian methods is that the parameter spec-\nifying the observation model is a random variable. This assumption is subject\nto much criticism, and at the heart of the Bayesian versus non-Bayesian debate\nthat has long divided the statistics community. However, there is a very general\ntype of observations for which the existence of such a random variable can be\nderived mathematically: For so-called exchangeable observations, the Bayesian\nassumption that a randomly distributed parameter exists is not a modeling\nassumption, but a mathematical consequence of the data\u2019s properties.\nFormally, a sequence of variables X1,X2,...,Xnover the same probability\nspace (X,\u03a9) is exchangeable if their joint distribution is invariant to permuting\nthe variables. That is, if P is the joint distribution and \u03c3 any permutation of\n{1,...,n}, then\nP(X1=x1,X2=x2...Xn=xn) = P(X1=x\u03c3(1),X2=x\u03c3(2)...Xn=x\u03c3(n)) (2)\nAn infinite sequence X1,X2,... is infinitely exchangeable if X1,...,Xn is ex-\nchangeable for every n \u2265 1. In this paper we will mean infinite exchangeability\nwhenever we write exchangeability. Exchangeability reflects the assumption\nthat the variables do not depend on their indices although they may be depen-\ndent among themselves. This is typically a reasonable assumption in machine\nlearning and statistical applications, even if the variables are not themselves iid\n(independently and identically distributed). Exchangeability is a much weaker\nassumption than iid; iid variables are automatically exchangeable.\nIf \u03b8 parametrizes the underlying distribution, and one assumes a prior dis-\ntribution over \u03b8, then the resulting marginal distribution over X1,X2,... with\n\u03b8 marginalized out will still be exchangeable. A fundamental result credited\nto de Finetti [4] states that the converse is also true. That is, if X1,X2,... is\n(infinitely) exchangeable, then there is a random \u03b8 such that:\n?\nfor every n \u2265 1. In other words, the seemingly innocuous assumption of ex-\nchangeability automatically implies the existence of a hierarchical Bayesian\nmodel with \u03b8 being the random latent parameter. This the crux of the fun-\ndamental importance of exchangeability to Bayesian statistics.\nIn de Finetti\u2019s Theorem it is important to stress that \u03b8 can be infinite dimen-\nsional (it is typically a random measure), thus the hierarchical Bayesian model\nP(X1,...,Xn) =\nP(\u03b8)\nn\n?\ni=1\nP(Xi|\u03b8)d\u03b8\n(3)\n6"},{"page":7,"text":"(3) is typically a nonparametric one. For example, the Blackwell-MacQueen\nurn scheme (related to the CRP) is exchangeable thus implicitly defines a ran-\ndom measure, namely the DP (see the DP entry [?] for more details). In this\nsense, we will see below that de Finetti\u2019s Theorem is an alternative route to Kol-\nmogorov\u2019s Extension Theorem, which implicitly defines the stochastic processes\nunderlying Bayesian nonparametric models.\nModel Representations\nIn finite dimensions, a probability model is usually defined by a density function\nor probability mass function. In infinite-dimensional spaces, this approach is\nnot generally feasible, for reasons explained below. To define or work with\na Bayesian nonparametric model, we have to choose alternative mathematical\nrepresentations.\nWeak Distributions. A weak distribution is a representation for the dis-\ntribution of a stochastic process, that is, for a probability distribution on an\ninfinite-dimensional sample space. If we assume that the dimensions of the\nspace are indexed by t \u2208 T, the stochastic process can be regarded as the joint\ndistribution P of an infinite set of random variables {Xt}t\u2208T. For any finite\nsubset S \u2282 T of dimensions, the joint distribution PS of the corresponding\nsubset {Xt}t\u2208Sof random variables is a finite-dimensional marginal of P. The\nweak distribution of a stochastic process is the set of all its finite-dimensional\nmarginals, that is, the set {PS: S \u2282 T,|S| < \u221e}. For example, the customary\ndefinition of the Gaussian process as an infinite collection of random variables,\neach finite subset of which has a joint Gaussian distribution, is an example of\na weak distribution representation. In contrast to the explicit representations\nto be described below, this representation is generally not generative, because\nit represents the distribution rather than a random draw, but is more widely\napplicable.\nApparently, just defining a weak distribution in this manner need not im-\nply that it is a valid representation of a stochastic process. A given collection\nof finite-dimensional distributions represents a stochastic process only (1) if a\nprocess with these distributions as its marginals actually exists, and (2) if it is\nuniquely defined by the marginals. The mathematical result which guarantees\nthat weak distribution representations are valid is the Kolmogorov Extension\nTheorem (also known as the Daniell-Kolmogorov theorem or the Kolmogorov\nConsistency Theorem). Suppose that a collection {PS : S \u2282 T,|S| < \u221e} of\ndistributions is given. If all distributions in the collection are marginals of each\nother, that is, if PS1is a marginal of PS2whenever S1\u2282 S2, the set of distribu-\ntions is called a projective family. The Kolmogorov Extension Theorem states\nthat, if the set T is countable, and if the distributions PS form a projective\nfamily, then there exists a uniquely defined stochastic process with the collec-\ntion {PS} as its marginal distributions. In other words, any projective family\nfor a countable set T of dimensions is the weak distribution of a stochastic pro-\ncess. Conversely, any stochastic process can be represented in this manner, by\ncomputing its set of finite-dimensional marginals.\n7"},{"page":8,"text":"The weak distribution representation assumes that all individual random\nvariable Xtof the stochastic process take values in the same sample space \u03a9.\nThe stochastic process P defined by the weak distribution is then a probability\ndistribution on the sample space \u03a9T, which can be interpreted as the set of all\nfunction f : T \u2192 \u03a9. For example, to construct a GP we might choose T = Q\nand \u03a9 = R to obtain real-valued functions on the countable space of rational\nnumbers. Since Q is dense in R, the function f can then be extended to all of\nR by continuity. To define the DP as a distribution over probability measures\non R, we note that a probability measure is a set function that maps \u201crandom\nevents\u201d, i.e. elements of the Borel \u03c3-algebra B(R) of R, into probabilities in\n[0,1]. We could therefore choose a weak distribution consisting of Dirichlet\ndistributions, and set T = B(R) and \u03a9 = [0,1]. However, this approach raises a\nnew problem because the set B(R) is not countable. As in the GP, we can first\ndefine the DP on a countable \u201cbase\u201d for B(R) then extend to all random events\nby continuity of measures. More precise descriptions are unfortunately beyond\nthe scope of this entry.\nExplicit Representations. Explicit representations directly describe a ran-\ndom draw from a stochastic process, rather than describing its distribution. A\nprominent example of an explicit representation is the so-called stick-breaking\nrepresentation of the Dirichlet process.The discrete random measure G in (1)\nis completely determined by the two infinite sequences {\u03c0k}k\u2208Nand {\u03b8k}k\u2208N.\nThe stick-breaking representation of the DP generates these two sequences by\ndrawing \u03b8k\u223c H iid and vk\u223c Beta(1,\u03b1) for k = 1,2,.... The coefficients \u03c0kare\nthen computed as \u03c0k= vk\nshown to be distributed according to a DP(\u03b1,G0). Similar representations can\nbe derived for the Pitman-Yor process and the beta process as well. Explicit\nrepresentations, if they exist for a given model, are typically of great practical\nimportance for the derivation of algorithms.\nImplicit Representations.\nA third representation of infinite dimensional\nmodels is based on de Finetti\u2019s Theorem. Any exchangeable sequence X1,...,Xn\nuniquely defines a stochastic process \u03b8, called the de Finetti measure, making\nthe Xi\u2019s iid. If the Xi\u2019s are sufficient to define the rest of the model and their\nconditional distributions are easily specified, then it is sufficient to work directly\nwith the Xi\u2019s and have the underlying stochastic process implicitly defined. Ex-\namples include the Chinese restaurant process (an exchangeable distribution\nover partitions) with the DP as the de Finetti measure, and the Indian buffet\nprocess (an exchangeable distribution over binary matrices) with the BP being\nthe corresponding de Finetti measure. These implicit representations are useful\nin practice as they can lead to simple and efficient inference algorithms.\nFinite Representations. A fourth representation of Bayesian nonparametric\nmodels is as the infinite limit of finite (parametric) Bayesian models. For exam-\nple, DP mixtures can be derived as the infinite limit of finite mixture models\nwith particular Dirichlet priors on mixing proportions, GPs can be derived as\nthe infinite limit of particular Bayesian regression models with Gaussian priors,\nwhile BPs can be derived as from the limit of an infinite number of indepen-\n?k\u22121\nj=1(1 \u2212 vk). The measure G so obtained can be\n8"},{"page":9,"text":"dent beta variables. These representations are sometimes more intuitive for\npractitioners familiar with parametric models. However not all Bayesian non-\nparametric models can be expressed in this fashion, and they do not necessarily\nmake clear the mathematical subtleties involved.\nConsistency and Convergence Rates\nRecent work in mathematical statistics examines the convergence properties of\nBayesian nonparametric models, and in particular the questions of consistency\nand convergence rates.\nIntuitively, a consistent estimation procedure is a method that will result\nin a correct estimate if it has access to an infinite amount of data, that is,\na procedure that will be accurate unless it is hampered by insufficient sample\nsize. In Bayesian statistics, an estimate is a distribution over possible parameter\nvalues (the posterior), and hence consistency in Bayesian models is defined by\ndemanding that the posterior converges to a delta peak at the true parameter.\nA classic theorem by J. L. Doob shows that for any Bayesian model, the set\nof all possible true parameter values for which the model will be consistent has\nprobability one under the prior. In other words, if the true parameter (and\nhence the data distribution) is chosen at random from the prior, we will always\nend up with a consistent model, and a Bayesian who is certain about the prior\nneed not worry about inconsistency. This result does not hold anymore if the\ntrue parameter value is not in the domain of the prior. In nonparametric mod-\nels, which have to spread out their probability mass over an infinite-dimensional\nspace, this can result in seemingly unreasonable behavior of the model. For ex-\nample, the Dirichlet process may be used as a prior in density estimation. Its\nsupport consists of the discrete distributions, which means that if the data is\ndrawn from any smooth distribution, the true model is not in the support of the\nprior (so Doob\u2019s theorem does not apply). As shown by Diaconis and Freedman\n[5], the posterior will not necessarily concentrate in a region close to the true\nmodel. Intuitively, the implication is that we cannot generally assume in non-\nparametric models that the effect of the prior will eventually become negligible\nif only we see enough data. However, this does not mean that nonparametric\nBayesian models are generally inconsistent: A large and growing literature in\nmathematical statistics shows that consistency can be guaranteed by proper\nchoice of an adequate nonparametric model [6].\nRecent results, notably by van der Vaart and Ghosal, apply modern meth-\nods of mathematical statistics to study the convergence properties of Bayesian\nnonparametric models (see [6] for further references). Consistency has been es-\ntablished for a number of models, including Gaussian processes and Dirichlet\nprocess mixtures. A particularly interesting aspect of this line of work are re-\nsults on convergence rates, which specify how rapidly the posterior concentrates\nwith growing sample size, depending on the complexity of the model and on how\nmuch probability mass the prior places around the true solution. To make such\nresults quantitative requires a measure for the complexity of a Bayesian non-\nparametric model. This is done by means of complexity measures developed in\n9"},{"page":10,"text":"empirical process theory and statistical learning theory, such as metric entropies,\ncovering numbers and bracketing, some of which are well-known in theoretical\nmachine learning. Examples of such results include the consistency of Dirichlet\nprocess mixture models for density estimation if both the target density and the\nparametric mixture components are smooth, and a range of consistency results\nfor regression and density estimation with Gaussian processes. For all of these\nresults, convergence rates can be specified as well (references are given in [6]).\nA large class of infinite-dimensional models which do behave well even if the\ntrue parameter is not in the domain of the prior is identified in [7]. In this case,\nthe posterior will concentrate in the region of the prior support which is closest\nto the true parameter in a Kullback-Leibler sense.\nInference\nThere are two aspects to inference in Bayesian nonparametric models: the ana-\nlytic tractability of posteriors for the stochastic processes embedded in Bayesian\nnonparametric models, and practical inference algorithms for the overall mod-\nels. Bayesian nonparametric models typically include stochastic processes such\nas the Gaussian process and the Dirichlet process. These processes have an\ninfinite number of dimensions thus na\u00a8 \u0131ve algorithmic approaches to computing\nposteriors is generally infeasible. Fortunately, these processes typically have\nanalytically tractable posteriors, so all but finitely many of the dimensions can\nbe analytically integrated out efficiently. The remaining dimensions, along with\nthe parametric parts of the models, can then be handled by the usual infer-\nence techniques employed in parametric Bayesian modeling, including Markov\nchain Monte Carlo, sequential Monte Carlo, variational inference, and message-\npassing algorithms like expectation propagation. The precise choice of approx-\nimations to use will depend on the specific models under consideration, with\nspeed\/accuracy trade-offs between different techniques generally following those\nfor parametric models. In the following, we will given two examples to illus-\ntrate the above points, and discuss a few theoretical issues associated with the\nanalytic tractability of stochastic processes.\nExamples\nIn Gaussian process regression, we model the relationship between an input x\nand an output y using a function f, so that y \u223c f(x)+? where ? is iid Gaussian\nnoise. Given a GP prior over f and a finite training data set {(xi,yi)}n\nwish to compute the posterior over f. Here we can use the weak representation\nof f and note that {f(xi)}n\nmean and covariance given by the mean and covariance functions of the GP.\nInference for {f(xi)}n\nof equivalently as marginalizing out the whole function except its values on the\ntraining inputs. Note that although we only have the posterior over {f(xi)}n\nthis is sufficient to reconstruct the function evaluated at any other point x0(say\ni=1we\ni=1is simply a finite-dimensional Gaussian with\ni=1is then straightforward. The approach can be thought\ni=1,\n10"},{"page":11,"text":"the test input), since f(x0) is Gaussian and independent of the training data\n{(xi,yi)}n\ncan be computed exactly. In GP classification or other regression settings with\nnonlinear likelihood functions, the typical approach is to use sparse methods\nbased on variational approximations or expectation propagation; see GP entry\n[?] for details.\nOur second example involves Dirichlet process mixture models. Recall that\nthe DP induces a clustering structure on the data items. If our training set\nconsists of n data items, since each item can only belong to one cluster, there\nare at most n clusters represented in the training set. Even though the DP\nmixture itself has an infinite number of potential clusters, all but finitely many\nof these are not associated with data, thus the associated variables need not\nbe explicitly represented at all. This can be understood either as marginalizing\nout these variables, or as an implicit representation which can be made explicit\nwhenever required by sampling from the prior. This idea is applicable for DP\nmixtures using both the Chinese restaurant process and the stick-breaking rep-\nresentations. In the CRP representation, each data item xiis associated with a\ncluster index zi, and each cluster k with a parameter \u03b8\u2217\nbe marginalized out if H is conjugate to F), and these are the only latent vari-\nables that need be represented in memory. In the stick-breaking representation,\nclusters are ordered by decreasing prior expected size, with cluster k associated\nwith a parameter \u03b8\u2217\na cluster index zi, and only the clusters up to K = max(z1,...,zn) need be\nrepresented. All clusters with index > K need not be represented since their\nposterior conditioning on {(xi,zi)}n\ni=1given {f(xi)}n\ni=1. In GP regression the posterior over {f(xi)}n\ni=1\nk(these parameters can\nkand a size \u03c0k. Each data item is again associated with\ni=1is just the prior.\nOn Bayes Equations and Conjugacy\nIt is worth noting that the posterior of a Bayesian model is, in abstract terms,\ndefined as the conditional distribution of the parameter given the data and\nthe hyperparameters, and this definition does not require the existence of a\nBayes equation. If a Bayes equation exists for the model, the posterior can\nequivalently be defined as the left-hand side of the Bayes equation. However for\nsome stochastic processes, notably the DP on an uncountable space such as R,\nit is not possible to define a Bayes equation even though the posterior is still\na well-defined mathematical object. Technically speaking, existence of a Bayes\nequation requires the family of all possible posteriors to be dominated by the\nprior, but this is not the case for the DP. That posteriors of these stochastic\nprocesses can be evaluated at all is solely due to the fact that they admit an\nanalytic representation.\nThe particular form of tractability exhibited by many stochastic processes in\nthe literature is that of a conjugate posterior, that is, the posterior belongs to the\nsame model family as the prior, and the posterior parameters can be computed\nas a function of the prior hyperparameters and the observed data. For example,\nthe posterior of a DP(\u03b1,G0) under observations \u03b81,...,\u03b8nis again a Dirichlet\nprocess, DP(\u03b1 + n,\n1\n\u03b1+n(\u03b1G0+?\u03b4\u03b8i)). Similarly the posterior of a GP under\n11"},{"page":12,"text":"observations of f(x1),...,f(xn) is still a GP. It is this conjugacy that allows\npractical inference in the examples above. A Bayesian nonparametric model\nis conjugate if and only if the elements of its weak distribution, i.e. its finite-\ndimensional marginals, have a conjugate structure as well [8]. In particular, this\ncharacterizes a class of conjugate Bayesian nonparametric models whose weak\ndistributions consist of exponential family models. Note however that lack of\nconjugacy do not imply intractable posteriors. An example is given by the\nPitman-Yor process, where the posterior is given by a sum of a finite number of\natoms and a Pitman-Yor process independent from the atoms.\nFuture Directions\nSince MCMC sampling algorithms for Dirichlet process mixtures became avail-\nable in the 1990s and made latent variable models with nonparametric Bayesian\ncomponents applicable to practical problems, the development of Bayesian non-\nparametrics has experienced explosive growth [9, 10]. Arguably, though, the\nresults available so far have only scratched the surface. The repertoire of avail-\nable models is still mostly limited to using the Gaussian process, the Dirichlet\nprocess, the beta process, and generalizations derived from those. In princi-\nple, Bayesian nonparametric models may be defined on any infinite-dimensional\nmathematical object of possible interest to machine learning and statistics. Pos-\nsible examples are kernels, infinite graphs, special classes of functions (e.g. piece-\nwise continuous or Sobolev functions), and permutations.\nAside from the obvious modeling questions, two major future directions are\nto make Bayesian nonparametric methods available to a larger audience of re-\nsearchers and practitioners through the development of software packages, and\nto understand and quantify the theoretical properties of available methods.\nGeneral-Purpose Software Package\nThere is currently significant growth in the application of Bayesian nonparamet-\nric models across a variety of application domains both in machine learning and\nin statistics. However significant hurdles still exist, especially the expense and\nexpertise needed to develop computer programs for inference in these complex\nmodels. One future direction is thus the development of software packages that\ncan compile efficient inference algorithms automatically given model specifica-\ntions, thus allowing a much wider range of modeler to make use of these models.\nCurrent developments include the R DPpackage1, the hierarchical Bayesian com-\npiler2, adaptor grammars3, the MIT-Church project4, as well as efforts to add\nBayesian nonparametric models to the repertoire of current Bayesian modeling\n1http:\/\/cran.r-project.org\/web\/packages\/DPpackage\n2http:\/\/www.cs.utah.edu\/ hal\/HBC\n3http:\/\/www.cog.brown.edu\/ mj\/Software.htm\n4http:\/\/projects.csail.mit.edu\/church\/wiki\/Church\n12"},{"page":13,"text":"environments like OpenBugs5and infer.NET6.\nStatistical Properties of Models\nRecent work in mathematical statistics provides some insight into the quan-\ntitative behavior of Bayesian nonparametric models (cf theory section). The\nelegant, methodical approach underlying these results, which quantifies model\ncomplexity by means of empirical process theory and then derives convergence\nrates as a function of the complexity, should be applicable to a wide range of\nmodels. So far, however, only results for Gaussian processes and Dirichlet pro-\ncess mixtures have been proven, and it will be of great interest to establish\nproperties for other priors. Some models developed in machine learning, such\nas the infinite HMM, may pose new challenges to theoretical methodology, since\ntheir study will probably have to draw on both the theory of algorithms and\nmathematical statistics. Once a wider range of results is available, they may in\nturn serve to guide the development of new models, if it is possible to establish\nhow different methods of model construction affect the statistical properties of\nthe constructed model.\nCross Reference\nDirichlet Processes, Gaussian Processes, Bayesian Methods, Prior Probabilities.\nRecommended Reading\nIn addition to the references embedded in the text above, we recommend the\nbooks [11, 12] and the review articles [13, 14] on Bayesian nonparametrics.\nReferences for DPs and DP mixture models can be found in the DP entry [?],\nfor GPs in the GP entry [?], while for most of the other examples can be found\nin the chapter [3] of the book [11].\n[1] L. Wasserman. All of Nonparametric Statistics. Springer, 2006.\n[2] D. B. Dunson.\nN. Hjort, C. Holmes, P. M\u00a8 uller, and S. Walker, editors, Bayesian Non-\nparametrics. Cambridge University Press, 2010.\nNonparametric Bayes applications to biostatistics. In\n[3] Y. W. Teh and M. I. Jordan. Hierarchical Bayesian nonparametric models\nwith applications. In N. Hjort, C. Holmes, P. M\u00a8 uller, and S. Walker, editors,\nBayesian Nonparametrics. Cambridge University Press, 2010.\n[4] B. de Finetti. Funzione caratteristica di un fenomeno aleatorio. Atti della\nR. Academia Nazionale dei Lincei, Serie 6. Memorie, Classe di Scienze\nFisiche, Mathematice e Naturale, 4, 1931.\n5http:\/\/mathstat.helsinki.fi\/openbugs\n6http:\/\/research.microsoft.com\/en-us\/um\/cambridge\/projects\/infernet\n13"},{"page":14,"text":"[5] P. Diaconis and D. Freedman. On the consistency of Bayes estimates (with\ndiscussion). Annals of Statistics, 14(1):1\u201367, 1986.\n[6] S. Ghosal. The Dirichlet process, related priors and posterior asymptotics.\nIn N. Hjort, C. Holmes, P. M\u00a8 uller, and S. Walker, editors, Bayesian Non-\nparametrics. Cambridge University Press, 2010.\n[7] B. J. K. Kleijn and A. W. van der Vaart. Misspecification in infinite-\ndimensional Bayesian statistics. Annals of Statistics, 34:837\u2013877, 2006.\n[8] P. Orbanz. Construction of nonparametric Bayesian models from para-\nmetric Bayes equations. In Advances in Neural Information Processing\nSystems, 2010.\n[9] M. D. Escobar and M. West. Bayesian density estimation and inference\nusing mixtures. Journal of the American Statistical Association, 90:577\u2013\n588, 1995.\n[10] R. M. Neal. Markov chain sampling methods for Dirichlet process mixture\nmodels. Journal of Computational and Graphical Statistics, 9:249\u2013265,\n2000.\n[11] N. Hjort, C. Holmes, P. M\u00a8 uller, and S. Walker, editors. Bayesian Nonpara-\nmetrics. Number 28 in Cambridge Series in Statistical and Probabilistic\nMathematics. Cambridge University Press, 2010.\n[12] J. K. Ghosh and R. V. Ramamoorthi. Bayesian Nonparametrics. Springer,\n2002.\n[13] S. G. Walker, P. Damien, P. W. Laud, and A. F. M. Smith. Bayesian\nnonparametric inference for random distributions and related functions.\nJournal of the Royal Statistical Society, 61(3):485\u2013527, 1999.\n[14] P. M\u00a8 uller and F. A. Quintana. Nonparametric Bayesian data analysis.\nStatistical Science, 19(1):95\u2013110, 2004.\n14"}],"widgetId":"rgw25_56ab9f7f4f463"},"id":"rgw25_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=247935319&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56ab9f7f4f463"},"id":"rgw26_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=247935319&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":247935319,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/247935319_Bayesian_Nonparametric_Models","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab9f7f4f463"},"id":"rgw2_56ab9f7f4f463","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":247935319},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=247935319&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab9f7f4f463"},"id":"rgw1_56ab9f7f4f463","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"kz450nb9S7D82We45t+pZ\/tQ3vEwxcncX3eDynuihcBBPFcKF4\/VYpAg3b4XosR57giLkOuhcHbTacGwYND61DE2ZMlIlC9P9MEYeMI254\/2kOKKb2Fch\/5VA4woTkyzOEMq5PxbVJpbT56BEd8MhVBSM8lr9K1KKzv7NSYpdL36Gqln1SSiG98vcxiuWHQcX4ZaCTZE0Rcntu\/mnpeniXaYUx3S\/Ic9x3TjJrDdoA2hDh3QeOVRUwP8MOF3WNwW6huUU2GAg8tqE9EiapNXtQkGKscZ8cEmGLgJoSXDcEw=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/247935319_Bayesian_Nonparametric_Models\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Bayesian Nonparametric Models\" \/>\n<meta property=\"og:description\" content=\"\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/247935319_Bayesian_Nonparametric_Models\/links\/029313640cf270384305b058\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/247935319_Bayesian_Nonparametric_Models\" \/>\n<meta property=\"rg:id\" content=\"PB:247935319\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1007\/978-0-387-30164-8_66\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Bayesian Nonparametric Models\" \/>\n<meta name=\"citation_author\" content=\"Peter Orbanz\" \/>\n<meta name=\"citation_author\" content=\"Yee Whye Teh\" \/>\n<meta name=\"citation_doi\" content=\"10.1007\/978-0-387-30164-8_66\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/247935319_Bayesian_Nonparametric_Models\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/247935319_Bayesian_Nonparametric_Models\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-cf054aa0-2dff-4f43-be8d-df4d89eaeccc","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":689,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw27_56ab9f7f4f463"},"id":"rgw27_56ab9f7f4f463","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-cf054aa0-2dff-4f43-be8d-df4d89eaeccc", "068b31c077deead93271e9ac7414f5ef640cb7cc");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-cf054aa0-2dff-4f43-be8d-df4d89eaeccc", "068b31c077deead93271e9ac7414f5ef640cb7cc");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw28_56ab9f7f4f463"},"id":"rgw28_56ab9f7f4f463","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/247935319_Bayesian_Nonparametric_Models","requestToken":"zxxr7VzXryIFOSS5hyfeIG73P8OPQ3OwUULqh2WKD+Dwjkd2nOPvLbRxldaE2sGmNGnULoXQeEPq4ZtP5xkpoI8qSmVR6LXX5U8bvPSF1azUfhxhzon12zfFlVYjRIqTY5GHIFQnCVagSiUvVSkvs4wjqr5rmGNVohNY0XyworioOVoa9fMgADb\/eGpZx49Gl7OVMvduuXNFxrUXkZmpNyGuVAvdIb60TzvhoAXc4aMwGC8dLTjdBk04GApIkYmPXP9cASBlKlu0Hf5sRdh0lwOantadG5xq4Jg6R0XuK0A=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=-R_K2bDSmetCVXiRAzWmewLDqMzV8KYv2eOdnHrMVQ2xokwpBq7is-ARwSJKhxd2","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjQ3OTM1MzE5X0JheWVzaWFuX05vbnBhcmFtZXRyaWNfTW9kZWxz","signupCallToAction":"Join for free","widgetId":"rgw30_56ab9f7f4f463"},"id":"rgw30_56ab9f7f4f463","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw29_56ab9f7f4f463"},"id":"rgw29_56ab9f7f4f463","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw31_56ab9f7f4f463"},"id":"rgw31_56ab9f7f4f463","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
