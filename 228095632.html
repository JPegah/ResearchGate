<!DOCTYPE html> <html lang="en" class="" id="rgw49_56ab1c94bcd25"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="VpYgsz6tIaCUugV3JPPGD2pmFqgryTh64fhefVsgl6HoCrxhTXBSBBJASip1O7DIs9XwkZHIkUiTiqZW//ThS5wFfml+/MM1DRG4aJGlDsHq3hvYzTTjxe7JnZNMgzaMQJniY2BtxZwHBYaXRdtha7Bj7EA8f+L8di+RwGsieSB/ahjbM5irFm2pg3kdRXnompJ5hUGUerAFlJJ6/B7wfwtLK0cyH6sbF5ixCtbHmApoPSSSx4QDvSWzir5tebCLXEsINucf05IAyI7fjXjYSN9LEs4eF3yCzTRS0s3Xcew="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-c059dd7a-cbcb-4eab-a3b0-2dba9d8722a7",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Variational Bayesian Inference with Stochastic Search" />
<meta property="og:description" content="Mean-field variational inference is a method for approximate Bayesian
posterior inference. It approximates a full posterior distribution with a
factorized set of distributions by maximizing a..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search/links/53fe1e670cf21edafd14cb6c/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search" />
<meta property="rg:id" content="PB:228095632" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Variational Bayesian Inference with Stochastic Search" />
<meta name="citation_author" content="John Paisley" />
<meta name="citation_author" content="David Blei" />
<meta name="citation_author" content="Michael Jordan" />
<meta name="citation_publication_date" content="2012/06/27" />
<meta name="citation_volume" content="2" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Michael_Jordan13/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search/links/53fe1e670cf21edafd14cb6c.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Variational Bayesian Inference with Stochastic Search (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Variational Bayesian Inference with Stochastic Search on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1c94bcd25" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1c94bcd25" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1c94bcd25">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Variational%20Bayesian%20Inference%20with%20Stochastic%20Search&rft.title=Proceedings%20of%20the%2029th%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202012&rft.jtitle=Proceedings%20of%20the%2029th%20International%20Conference%20on%20Machine%20Learning%2C%20ICML%202012&rft.volume=2&rft.date=2012&rft.au=John%20Paisley%2CDavid%20Blei%2CMichael%20Jordan&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Variational Bayesian Inference with Stochastic Search</h1> <meta itemprop="headline" content="Variational Bayesian Inference with Stochastic Search">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search/links/53fe1e670cf21edafd14cb6c/smallpreview.png">  <div id="rgw7_56ab1c94bcd25" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1c94bcd25"> <a href="researcher/70672430_John_Paisley" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="John Paisley" alt="John Paisley" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">John Paisley</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56ab1c94bcd25">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/70672430_John_Paisley"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="John Paisley" alt="John Paisley" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/70672430_John_Paisley" class="display-name">John Paisley</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1c94bcd25"> <a href="researcher/2064238818_David_Blei" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="David Blei" alt="David Blei" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David Blei</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab1c94bcd25">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2064238818_David_Blei"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David Blei" alt="David Blei" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2064238818_David_Blei" class="display-name">David Blei</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1c94bcd25" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Michael_Jordan13" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A278648436346888%401443446372379_m" title="Michael Jordan" alt="Michael Jordan" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michael Jordan</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw13_56ab1c94bcd25" data-account-key="Michael_Jordan13">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Michael_Jordan13"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A278648436346888%401443446372379_l" title="Michael Jordan" alt="Michael Jordan" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Michael_Jordan13" class="display-name">Michael Jordan</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_California_Berkeley" title="University of California, Berkeley">University of California, Berkeley</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     Proceedings of the 29th International Conference on Machine Learning, ICML 2012   <meta itemprop="datePublished" content="2012-06">  06/2012;  2.             <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1206.6430" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab1c94bcd25" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Mean-field variational inference is a method for approximate Bayesian<br />
posterior inference. It approximates a full posterior distribution with a<br />
factorized set of distributions by maximizing a lower bound on the marginal<br />
likelihood. This requires the ability to integrate a sum of terms in the log<br />
joint likelihood using this factorized distribution. Often not all integrals<br />
are in closed form, which is typically handled by using a lower bound. We<br />
present an alternative algorithm based on stochastic optimization that allows<br />
for direct optimization of the variational lower bound. This method uses<br />
control variates to reduce the variance of the stochastic search gradient, in<br />
which existing lower bounds can play an important role. We demonstrate the<br />
approach on two non-conjugate models: logistic regression and an approximation<br />
to the HDP.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw15_56ab1c94bcd25" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab1c94bcd25">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw41_56ab1c94bcd25">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Michael_Jordan13/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search/links/53fe1e670cf21edafd14cb6c.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Michael_Jordan13">Michael Jordan</a>, <span class="js-publication-date"> Aug 27, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw43_56ab1c94bcd25" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw44_56ab1c94bcd25" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw45_56ab1c94bcd25" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw46_56ab1c94bcd25" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw47_56ab1c94bcd25" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw48_56ab1c94bcd25" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw42_56ab1c94bcd25" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Jordan13%2Fpublication%2F228095632_Variational_Bayesian_Inference_with_Stochastic_Search%2Flinks%2F53fe1e670cf21edafd14cb6c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56ab1c94bcd25"  itemprop="articleBody">  <p>Page 1</p> <p>Variational Bayesian Inference with Stochastic Search<br />John Paisley1<br />David M. Blei3<br />Michael I. Jordan1,2<br />1Department of EECS,2Department of Statistics, UC Berkeley<br />3Department of Computer Science, Princeton University<br />jpaisley@berkeley.edu<br />blei@cs.princeton.edu<br />jordan@eecs.berkeley.edu<br />Abstract<br />Mean-field variational inference is a method<br />for approximate Bayesian posterior inference.<br />It approximates a full posterior distribution<br />with a factorized set of distributions by max-<br />imizing a lower bound on the marginal likeli-<br />hood. This requires the ability to integrate a<br />sum of terms in the log joint likelihood using<br />this factorized distribution. Often not all in-<br />tegrals are in closed form, which is typically<br />handled by using a lower bound. We present<br />an alternative algorithm based on stochastic<br />optimization that allows for direct optimiza-<br />tion of the variational lower bound.<br />method uses control variates to reduce the<br />variance of the stochastic search gradient, in<br />which existing lower bounds can play an im-<br />portant role. We demonstrate the approach<br />on two non-conjugate models: logistic regres-<br />sion and an approximation to the HDP.<br />This<br />1. Introduction<br />Mean-field variational Bayesian (MFVB) inference is<br />an optimization-based approach to approximating the<br />full posterior of the latent variables of a Bayesian<br />model (Jordan et al., 1999). It has been applied to<br />many problem domains, for example mixture model-<br />ing (Blei &amp; Jordan, 2006), sequential modeling (Beal,<br />2003) and factor analysis (Paisley &amp; Carin, 2009). In<br />addition, recent development of the theory has ex-<br />tended the method to online inference and stochastic<br />optimization settings, making variational Bayes a vi-<br />able approach for Bayesian learning with massive data<br />sets (Hoffman et al., 2010; Wang et al., 2011).<br />Appearing in Proceedings of the 29thInternational Confer-<br />ence on Machine Learning, Edinburgh, Scotland, UK, 2012.<br />Copyright 2012 by the author(s)/owner(s).<br />Variational Bayes approximates the full posterior by<br />attempting to minimize the Kullback-Leibler diver-<br />gence between the true posterior and a predefined fac-<br />torized distribution on the same variables. Minimiz-<br />ing this divergence is equivalent to maximizing the fa-<br />miliar variational objective function. To review, let<br />Θ = {θi} represent the set of latent variables (ran-<br />dom effects and parameters) in the model and X rep-<br />resent the data. The joint likelihood of X and Θ is<br />P(X,Θ|Υ), with Υ the set of hyperparameters. Varia-<br />tional inference approximates the posterior P(Θ|X,Υ)<br />with a Q distribution that takes a set of variational<br />parameters Ψ = {ψi}. This distribution is factorized,<br />Q(Θ|Ψ) =?<br />iqi(θi|ψi), and the values of Ψ are opti-<br />mized to maximize the objective function,<br />L(X,Ψ) = EQ[lnP(X,Θ|Υ)] + H[Q(Θ|Ψ)].(1)<br />The solution is only locally optimal when L is not<br />convex, which is usually the case. Most variational<br />inference algorithms optimize L by coordinate ascent,<br />which repeatedly cycles through and optimizes with<br />respect to each variational parameter ψi. Often the<br />locally optimal value of ψihas a closed-form solution,<br />for example in conjugate exponential models.<br />The log of the joint likelihood results in a sum of terms;<br />a major issue that often arises in MFVB is that not all<br />expectations in this sum are in closed form. A typical<br />solution in this case is to replace the problematic func-<br />tion with another function of the same variables (plus<br />auxiliary variables) that is a point-wise lower bound.<br />This new function is selected such that the expectation<br />is tractable. While inference can now proceed, a draw-<br />back of introducing bounds is that the true variational<br />objective function is no longer being optimized, which<br />may lead to a significantly worse posterior approxi-<br />mation. Therefore, much attention has been paid to<br />developing tight bounds of commonly occurring func-<br />tions (e.g., Jaakkola &amp; Jordan (2000), Marlin et al.<br />(2011), Leisink &amp; Kappen (2001)).</p>  <p>Page 2</p> <p>Variational Bayesian Inference with Stochastic Search<br />We present a method for directly optimizing Eq. (1)<br />for models in which not all expectations are tractable;<br />we show how a stochastic approximation of ∇ψiL<br />can allow for optimization of L when the expecta-<br />tion Eqi[lnP(X,Θ|Υ)] is not in closed form. The ap-<br />proximation is unbiased, and so by using the proposed<br />stochastic method we are directly optimizing L.<br />Our stochastic approximation is based on Monte Carlo<br />integration, for which the number of samples heavily<br />depends on the variance of this approximation. We in-<br />troduce a control variate (Ross, 2006) to significantly<br />reduce the variance of this stochastic approximation.<br />A control variate is a tractable function g that is highly<br />correlated with the intractable function f. The func-<br />tion g replaces f in Eq. (1), and the gradient is then<br />stochastically corrected for bias.<br />Existing lower bounds have properties that make them<br />ideal as control variates, and thus can improve the<br />speed of the algorithm. However, a major advantage<br />of the control variate methodology is that it does not<br />require the tractable function g to bound f, but only to<br />correlate well with it (i.e., to approximate it well mod-<br />ulo a scaling). This opens the door to many more func-<br />tions that may give better approximations than a lower<br />bound. One of these possible functions is the second-<br />order Taylor expansion, which often gives a very good<br />approximation, while also allowing for closed-form ex-<br />pectations. We show the potential performance gain<br />using this function as a control variate, which we de-<br />note the control variate delta method for MFVB.<br />Related work.<br />(2011) has also addressed the problem of intractable<br />expectations in MFVB inference in the context of de-<br />veloping a more general variational message passing al-<br />gorithm. Our solution arises from a different perspec-<br />tive and results in a new algorithm based on stochastic<br />optimization. Graves (2011) considers a similar prob-<br />lem for neural networks, but a lack of control variates<br />limits the algorithm to significantly simpler variational<br />approximations.Stochastic search algorithms have<br />also been developed for models of Evolution Strate-<br />gies (see, e.g., Yi et al. (2009)).<br />Recent work by Knowles &amp; Minka<br />2. Mean-field variational inference<br />Mean-field variational Bayesian (MFVB) inference ap-<br />proximates the full posterior of the latent variables of<br />a Bayesian model with a factorized distribution. As<br />motivated in the introduction, let Θ = {θi} be these<br />variables, X the data and Υ all hyperparameters of<br />the prior distributions on Θ. We define the factor-<br />ized distribution on Θ to be Q(Θ|Ψ) =?<br />iqi(θi|ψi),<br />where ψi are the parameters of the qi distributions.<br />The variational objective function arises by bounding<br />the marginal likelihood using the Q distribution,<br />lnP(X|Υ)= ln<br />?<br />?<br />Θ<br />Q(Θ|Ψ)lnP(X,Θ|Υ)<br />P(X,Θ|Υ)dΘ (2)<br />≥<br />Θ<br />Q(Θ|Ψ)<br />dΘ.<br />Maximizing this lower bound (denoted L) with re-<br />spect to Ψ is equivalent to minimizing the Kullback-<br />Leibler divergence between Q(Θ) and P(Θ|X,Υ),<br />which makes up the difference in Eq. (2).<br />To facilitate our discussion, we write the functions ap-<br />pearing in the log joint likelihood as lnP(X,Θ|Υ) =<br />?<br />appearing in function j. We note that the index j does<br />not correspond to variables or distributions, but to the<br />terms of the log joint likelihood. Using this notation,<br />the variational lower bound in Eq. (1) becomes<br />jfj(XAj,ΘBj), where Aj indexes the data appear-<br />ing in function j and Bj indexes the latent variables<br />L =?<br />For each function fj, those θi / ∈ ΘBjwill have their<br />corresponding qi removed from the expectation. For<br />those θi∈ ΘBj, the expectation of fjresults in a new<br />function of variational parameters ψi∈ ΨBj. Ideally,<br />all expectations will be in closed form, allowing for the<br />optimization of Ψ to proceed.<br />jEQ[fj(XAj,ΘBj)] +?<br />iH[qi(θi|ψi)]. (3)<br />In the case where an expectation in Eq. (3) is not<br />tractable, a nicer functional lower bound can replace<br />the problematic function. That is, let Eqi[fj(θi)] be<br />intractable.1A common approach to dealing with this<br />issue is to introduce a function g(θi,ξ) that replaces fj<br />and is a point-wise lower bound: fj(θi) ≥ g(θi,ξ) for<br />all θi. The function g usually takes auxiliary variables<br />ξ, which determines how tightly g approximates fjand<br />is tuned along with the other parameters during infer-<br />ence. The expectation Eqi[g(θi,ξ)] has a closed-form<br />solution, and gives a lower bound on the variational<br />objective that can be optimized.<br />To illustrate, consider the case where fjis convex in θi.<br />Then a bound g could be a first-order Taylor expansion<br />of fj about the point ξ, which has a closed-form ex-<br />pectation. Significantly tighter tractable bounds have<br />also been developed for various frequently occurring<br />functions (e.g., Marlin et al. (2011), Knowles &amp; Minka<br />(2011)). In general, the looser the bound the further<br />one is from optimizing the variational objective, and<br />learning of ψican suffer as a result.<br />1We have simplified the notation for clarity.</p>  <p>Page 3</p> <p>Variational Bayesian Inference with Stochastic Search<br />3. Stochastic search variational Bayes<br />We next present a method based on stochastic search<br />for directly optimizing the variational objective func-<br />tion L in cases where some expectations cannot be<br />computed in the log joint likelihood.<br />uses a stochastic approximation of the gradient with<br />respect to the variational parameters of the associated<br />q distribution. To further simplify notation, we drop<br />all indices; f is the intractable function of θ (plus other<br />variational parameters), and θ has a variational distri-<br />bution q taking parameters ψ.<br />This method<br />We separate the lower bound L into two functions, Ef<br />and h, where h(X,Ψ) contains everything in L except<br />for Ef. Notably, h contains all other functions of ψ<br />resulting from expectations calculated with respect to<br />q. In coordinate ascent variational inference, the first<br />step in optimizing q with respect to its parameters ψ<br />is to take the gradient of the variational objective,<br />∇ψL = ∇ψEq[f(θ)] + ∇ψh(X,Ψ).<br />This gradient contains a tractable term resulting from<br />∇ψh, and an intractable term ∇ψEqf. Our goal is to<br />make a stochastic approximation of this gradient. To<br />this end, assuming the necessary regularity conditions,<br />we rewrite this function as<br />?<br />=<br />θ<br />?<br />We use the identity ∇ψq(θ|ψ) = q(θ|ψ)∇ψlnq(θ|ψ).<br />It follows that ∇ψEq[f(θ)] = Eq[f(θ)∇ψlnq(θ|ψ)]. We<br />can stochastically approximate this expectation using<br />Monte Carlo integration,<br />(4)<br />∇ψEq[f(θ)] = ∇ψ<br />θ<br />f(θ)q(θ|ψ)dθ (5)<br />?<br />f(θ)∇ψq(θ|ψ)dθ<br />=<br />θ<br />f(θ)q(θ|ψ)∇ψlnq(θ|ψ)dθ.<br />∇ψEq[f(θ)] ≈<br />1<br />S<br />S<br />?<br />s=1<br />f(θ(s))∇ψlnq(θ(s)|ψ), (6)<br />where θ(s)iid<br />fore replace ∇ψEq[f(θ)] with the unbiased stochastic<br />approximation of this gradient in Eq. (6). Denote this<br />approximation as ζ. At iteration t, we update the vari-<br />ational parameter ψ by taking a gradient step,<br />∼ q(θ|ψ) for s = 1,...,S. We can there-<br />ψ(t+1)= ψ(t)+ ρt∇ψh(X,Ψ(t)) + ρtζt.<br />By decreasing the step size ρtsuch that?∞<br />lution of L is guaranteed. For example, ρt= (w+t)−η<br />with η ∈ (0.5,1] and w ≥ 0 satisfies this requirement.<br />(7)<br />t=1ρt= ∞<br />and?∞<br />t=1ρ2<br />t&lt; ∞, convergence to a local optimal so-<br />4. Searching with control variates<br />A practical issue with the stochastic approximation<br />proposed in Sec. 3 is that the variance of the gradient<br />approximation may be very large. Given S samples<br />of a random vector X, the covariance of its unbiased<br />sample mean¯ X is known to be Cov(¯ X) = Cov(X)/S.<br />When the diagonal values of Cov(X) are large, many<br />samples will be required to bring this variance below<br />a desired level for approximating the expectation. As<br />our experiments will show in Sec. 6, the value of S can<br />be very large in practice and lead to a slow algorithm.<br />We therefore seek a variance reduction method to re-<br />duce the number of samples needed to construct the<br />stochastic search direction.<br />We introduce a control variate (Ross, 2006) to reduce<br />the variance of the stochastic gradient constructed in<br />Eq. (6). A control variate is a random variable that<br />is highly correlated with an intractable variable, but<br />for which the expectation is tractable. In this case<br />the random variable is f(θ), for which we introduce<br />a control variate g(θ). Control variates are ideal for<br />MFVB because they can leverage the existing bounds,<br />though they also admit a larger class of functions. We<br />next review this variance reduction technique for Ef,<br />and discuss the modifications needed to account for<br />the stochastic vector f(θ)∇ψlnq(θ|ψ).<br />4.1. A control variate for f(θ)<br />Generally speaking, variance reduction works by mod-<br />ifying a function of a random variable such that its ex-<br />pectation remains the same, but its variance decreases.<br />Toward this end, we introduce a control variate g(θ),<br />which approximates f(θ) well in the highly probable<br />regions as defined by q(θ), but also has a closed-form<br />expectation under q. Using g and a scalar a ∈ R, we<br />first form the new functionˆf,<br />ˆf(θ) = f(θ) − a(g(θ) − Eq[g(θ)]).<br />This function has the same expectation as f and there-<br />fore can replace it in L in Eq. (3).<br />The next step is to set the value of a to minimize the<br />variance ofˆf. A simple calculation shows that<br />(8)<br />Var(ˆf) = Var(f) − 2aCov(f,g) + a2Var(g).<br />Taking the derivative with respect to a and setting to<br />zero gives the optimal value,<br />(9)<br />a =Cov(f,g)<br />Var(g)<br />.(10)<br />As is usual, this covariance and variance is unknown<br />in the functions we encounter. We can approximate</p>  <p>Page 4</p> <p>Variational Bayesian Inference with Stochastic Search<br />Algorithm 1 Variational Bayes with stochastic search<br />Goal To calculate ∇ψL = ∇ψEq[f(θ)] + ∇ψh(X,Ψ).<br />Approximate ∇ψL using stochastic search.<br />input Variance reduction parameter ?.<br />1: Introduce the function g(θ) as a control variate<br />that highly correlates with f(θ).<br />2: Sample an initial (small) collection θ(s)∼ q(θ|ψ).<br />3: Sum the sample variances and covariances<br />β =?K<br />4: Set ˆ a = α/β and S = (γ − α2/β)/?K.<br />5: Sample θ(s)∼ q(θ|ψ) i.i.d. for s = 1,...,?S?.<br />6: Construct the stochastic search vector<br />ζ =1<br />S<br />7: Step in the direction of the stochastic gradient<br />ψ?= ψ + ρζ + ρ∇ψ(h(X,Ψ) + ˆ aEq[g(θ)]).<br />k=1Var(g∂ lnq<br />α =?K<br />∂ψk), γ =?K<br />k=1Var(f∂ lnq<br />∂ψk),<br />k=1Cov(f∂ lnq<br />∂ψk,g∂ lnq<br />∂ψk).<br />?S<br />s=1{f(θ(s)) − ˆ ag(θ(s))}∇ψlnq(θ(s)|ψ).<br />a with ˆ a, found by plugging the sample variance and<br />covariance into Eq. (10) using samples from the algo-<br />rithm.<br />The potential reduction in variance is seen by plugging<br />Eq. (10) into Eq. (9) and taking the ratio of the two<br />variances,<br />Var(ˆf)/Var(f) = 1 − Corr(f,g)2.<br />Therefore, the greater the correlation between f and g,<br />the greater the variance reduction. Tight lower bounds<br />of f by construction have this high correlation, but we<br />note that tight upper bounds work as well, as do well-<br />approximating functions that do not bound f.<br />(11)<br />Using the control variate g, we now write the stochastic<br />approximation to the gradient as<br />∇ψEq[ˆf(θ)] ≈ ˆ a∇ψEq[g(θ)]<br />+1<br />S<br />s=1<br />where θ(s)iid<br />∼ q(θ|ψ) for s = 1,...,S.<br />Writing the stochastic approximation this way allows<br />for a more intuitive understanding of the algorithm.<br />By separating the tractable and stochastic parts as<br />done in Eq. (12), we first replace the intractable func-<br />tion f with a tractable approximation g. (This resem-<br />bles the standard method when g lower bounds f.)<br />The gradient of Eg is then corrected by a stochastic<br />vector. The variance of the correction is smaller than<br />that of the original stochastic approximation in Sec. 3,<br />since the function f(θ) is close to ˆ ag(θ). The gradient<br />of Eg can be thought of as an initial guess, followed<br />by a stochastic correction which ensures that we are<br />optimizing the variational objective function.<br />(12)<br />S<br />?<br />{f(θ(s)) − ˆ ag(θ(s))}∇ψlnq(θ(s)|ψ),<br />4.2. The stochastic search case<br />We have introduced a control variate for f(θ), but in<br />fact we would like to minimize the variance of the vec-<br />tor f(θ)∇ψlnq(θ|ψ) in Eq. (6). In this case, the con-<br />trol variate becomes g(θ)∇ψlnq(θ|ψ) and we have the<br />following modification.<br />Let ψk be the kth dimension of ψ.<br />dimension the discussion in Sec. 4.1 carries through,<br />but for f∂ lnq<br />∂ψk<br />∂ψk<br />variance of each dimension again follows Eq. (9), and<br />we seek an a to minimize the sum of these equations.<br />This results in the optimal value<br />a =?<br />which we approximate using samples. We summarize<br />stochastic search variational Bayes in Algorithm 1.<br />Then for each<br />and g∂ lnq<br />instead of f and g. The<br />kCov(f∂ lnq<br />∂ψk,g∂ lnq<br />∂ψk)/?<br />kVar(g∂ lnq<br />∂ψk),<br />5. Stochastic search VB for two models<br />We next illustrate stochastic search variational infer-<br />ence on logistic regression and a finite approximation<br />to the hierarchical Dirichlet process (Teh et al., 2007).<br />For logistic regression, we will consider two control<br />variates, one of which is a lower bound and the other<br />of which is not a bound. For the finite HDP, we will<br />consider a piecewise control variate, one part being an<br />upper bound on the original function.<br />5.1. Logistic regression<br />Binary logistic regression takes in d-dimensional data<br />vectors xnand predicts the class yn∈ {−1,1} to which<br />each belongs. The parameter is θ ∈ Rdand the predic-<br />tion law is Pr(yn|xn,θ) = σ(ynxT<br />sigmoid function, σ(b) = (1+e−b)−1. Bayesian logistic<br />regression places a prior distribution on the coefficient<br />vector, θ ∼ Normal(0,cI). For inference we define a<br />Gaussian variational q distribution<br />nθ) where σ(·) is the<br />q(θ) = Normal(µ,Σ).(13)<br />The variational lower bound for this model is<br />L =?N<br />The expectations of fn(yn,xn;θ) := lnσ(ynxT<br />intractable. One approach to avoiding this issue is to<br />forgo variational inference and use Laplace’s method<br />to approximate q. This method sets µ to the MAP so-<br />lution, and Σ−1to the negative Hessian of the log joint<br />likelihood evaluated at µ. Another is to lower bound<br />fnwith the bound in, e.g., Jaakkola &amp; Jordan (2000),<br />which allows for closed-form variational inference. We<br />consider this bound as a control variate.<br />n=1Eq[lnσ(ynxT<br />nθ)]+Eq[lnp(θ)−lnq(θ)]. (14)<br />nθ) are</p>  <p>Page 5</p> <p>Variational Bayesian Inference with Stochastic Search<br />−10−8−6 −4−202<br />−0.5<br />0<br />0.5<br />1<br />|| | | | | || ||| | |||| || | | || || ||| | | ||| | | || ||||||||||||||| | || || | |||||| | || | || | || || | | || ||||| |||||||| ||| | |||||<br />−202468<br />−1<br />−0.5<br />0<br />0.5<br />1<br />|| | ||||||||||||||||||||||| | ||||||| | ||||||||||||| ||||||||| | ||| | |||||| |||||| | |||||||||||||||||||||||||<br />Jaakkola bound<br />Taylor expansion<br />�<br />Figure 1. Approximation error between lnσ(θ) and the two<br />control variates considered. The mean and variance of q<br />used in these examples are (left) µ = 3, σ2= 3 and (right)<br />µ = −5, σ2= 1. We show 100 samples from these q distri-<br />butions, at which points the functions would be evaluated<br />for the stochastic gradient (for a = 1). The Taylor expan-<br />sion is closer to the true function at the region of interest<br />as defined by q. The benefit of this is that fewer samples<br />will be necessary to approximate the gradient.<br />A lower bound control variate.<br />for fn developed by Jaakkola &amp; Jordan (2000) is a<br />useful control variate for variational logistic regression.<br />For each pair (xn,yn), this bound takes an auxiliary<br />parameter ξn&gt; 0 and has the form<br />The lower bound<br />gn(yn,xn;θ,ξn) = lnσ(ξn) +1<br />2(ynxT<br />nθ − ξn)<br />nθ)2− ξ2<br />− λ(ξn)((xT<br />n). (15)<br />We have λ(ξn) = (2σ(ξn) − 1)/(4ξn). We select this<br />bound for illustrative purposes, but any lower bound<br />will work in principle. For a multivariate Gaussian q<br />distribution, having a quadratic term in g is essential<br />for stochastically learning a full covariance matrix. In<br />general, tighter bounds will require fewer samples, but<br />for some functions finding tight bounds may require<br />much effort. We next consider a general purpose con-<br />trol variate that can help in this case.<br />Control variate delta method.<br />the second-order Taylor expansion of f as a control<br />variate. The second-order Taylor expansion often ac-<br />curately approximates a function of interest, and when<br />used alone is known as the delta method. In addition<br />to accuracy, the quadratic approximation of the delta<br />method results in a function for which the expectation<br />with respect to q is very likely to be analytic.<br />We also consider<br />The delta method arguably should not be used for<br />mean-field variational inference because the second-<br />order Taylor expansion is not a lower bound. On the<br />other hand, the first-order Taylor expansion often is a<br />lower bound. Therefore, though their bounds are typ-<br />ically loose, first-order approximations are commonly<br />employed for MFVB. An advantage of the proposed<br />stochastic search algorithm is that second-order meth-<br />ods can now be used as a control variate to (i) more<br />accurately approximate the function of interest, and<br />(ii) significantly reduce the variance of the stochastic<br />gradient. We call this approach of using Taylor expan-<br />sion control variates the control variate delta method.<br />We consider a second-order Taylor expansion at ˆ µ,<br />the current mean of q, for approximating lnσ(ynxT<br />Letting σn:= σ(ynxT<br />nθ).<br />nˆ µ), this control variate is<br />gn(yn,xn;θ, ˆ µ) = lnσn+ yn(1 − σn)(θ − ˆ µ)Txn<br />−1<br />(16)<br />2σn(1 − σn)(θ − ˆ µ)TxnxT<br />n(θ − ˆ µ).<br />As with the Jaakkola &amp; Jordan (2000) bound, this<br />control variate contains a quadratic term that helps in<br />learning the covariance matrix of q.<br />We compare these control variates in Figure 1. In these<br />plots we show the difference fn− gnfor two specific q<br />distributions, and with x = 1. We also show 100 sam-<br />ples from q, which indicates the regions where these<br />functions would be evaluated (for a = 1). The plots<br />show that the second-order Taylor expansion approxi-<br />mates fnsignificantly better where it matters; we sup-<br />port this conclusion with the experiments in Sec. 6.<br />5.2. Hierarchical Dirichlet processes<br />We also investigate a stochastic search VB algorithm<br />for an approximation to the hierarchical Dirichlet pro-<br />cess (Teh et al., 2007).We focus on the two-level<br />generative structure using finite dimensional Dirichlet<br />priors as an approximation to the infinite dimensional<br />process—in the limit the HDP is recovered. In this<br />finite process, a top-level Dirichlet-distributed proba-<br />bility vector θ parameterizes the Dirichlet distribution<br />for d = 1,...,D second-level probability vectors,<br />(πd1,...,πdK)<br />(θ1,...,θK) ∼ Dirichlet(α<br />In topic models, these πd vectors are often used as<br />distributions on word distributions. In this section,<br />we focus solely on the generic hierarchical structure in<br />Eq. (17). We define the approximate posterior of θ as<br />iid<br />∼ Dirichlet(βθ1,...,βθK),<br />K,...,α<br />K). (17)<br />q(θ) = Dirichlet(c1,...,cK). (18)<br />The part of the lower bound associated with θ is<br />Lθ =?<br />The expectation Eq[lnΓ(βθk)] is intractable for each<br />k. We use a stochastic approximation, and introduce<br />two control variates for this function, depending on<br />the current expected value of βθk.<br />kβEq[θk]?<br />dEq[lnπdk] −?<br />kDEq[lnΓ(βθk)]<br />+Eq[lnp(θ) − lnq(θ)]. (19)</p>  <p>Page 6</p> <p>Variational Bayesian Inference with Stochastic Search<br />01234567<br />−8<br />−6<br />−4<br />−2<br />0<br />2<br />0 0.51 1.52 2.5<br />−1.5<br />��<br />−1<br />−0.5<br />0<br />0.5<br />-ln�����<br />-ln�����-ln��<br />Figure 2. (left) The intractable function in the HDP.<br />(right) the difference after introducing a control variate and<br />setting a = 1. Since −lnΓ(βθ)−lnβθ = −lnΓ(βθ+1), the<br />right figure is the left figure shifted by one unit to the left<br />and truncated at zero. The very large variance near zero<br />(where most values of βθ will lie) has been significantly re-<br />duced. For larger values of βθ, we use a first-order Taylor<br />approximation at βEqθ of the nearly linear function.<br />As βθk approaches zero, the function fk(βθk) =<br />−lnΓ(βθk) diverges to −∞. By construction of the<br />Dirichlet prior, many values of θkwill be very small.<br />(In the infinite limit, there are an infinite number of<br />such values smaller than any δ &gt; 0.) The variance in<br />this region is massive—when computer precision be-<br />comes an issue it can be infinite (see Figure 2).<br />We propose the control variate gk(βθk) = lnβθk, with<br />Eq[lnθk] = φ(ck)−φ(?<br />with fk, but if a = 1, limβθk→0fk−agk= 0, as shown<br />in Figure 2. This results from the equality<br />jcj) where φ(·) is the digamma<br />function. This control variate not only correlates well<br />−lnΓ(βθk) − lnβθk= −lnΓ(βθk+ 1).<br />For all other values of a, this equality does not hold,<br />and the difference fk− agkdiverges as βθk→ 0. For<br />this model, we can thus give the optimal value of a in<br />advance, and we set a = 1.<br />(20)<br />From Figure 2, we also see that the approximation gets<br />worse when βθkgets large, which can occur for a few<br />highly probable dimensions when β is large. Since fk<br />is approximately linear in this regime, we use a first-<br />order Taylor expansion of fk about the mean¯θk =<br />Eq[θk] as a control variate. This gives the following<br />two control variates,<br />gk = lnβθk,<br />gk = −lnΓ(β¯θk) − β(θk−¯θk)φ(β¯θk),<br />Since fk is concave, this second control variate is an<br />upper bound on Lθwithout the stochastic correction.<br />We discuss the boundaries κ1and κ2in Sec. 6.<br />0 &lt; β¯θk&lt; κ1, (21)<br />β¯θk&gt; κ2.<br />Thus far, we’ve focused mainly on reducing the vari-<br />ance induced by fk, but in Sec. 4.2 we noted that<br />∇lnq introduces variance to the Monte Carlo integral<br />as well. This suggests that we should look at other<br />parts of the integral for potential variance reduction.<br />We briefly show how this can be done for the HDP.<br />The lower bound in Eq. (19) contains a sum of K in-<br />tractable integrals over the probability simplex ∆K.<br />We perform separate stochastic approximations of<br />each gradient. Using the fact that each gamma func-<br />tion is over a single dimension of the simplex, for a<br />function of θkthe variables θi?=kwill integrate out. In<br />this case, marginalizing a Dirichlet distribution to a<br />single dimension yields a beta distribution. That is,<br />?<br />θ∈∆K<br />lnΓ(βθk)q(θ|c)dθ =<br />?1<br />0<br />lnΓ(βθk)q?<br />k(θk|c)dθk,<br />where q?<br />k(θk|c) = Beta(θk|ck,?<br />We can choose which of these integrals to stochas-<br />tically approximate for gradient ascent.<br />the stochastic gradient using q?<br />cantly less variance than for q since θ(s)<br />zero; the vector ∇clnq?<br />ln(1−θ(s)<br />lnθ(s)<br />i<br />− Eq[lnθi] for i = 1,...,K when using ∇clnq.<br />i?=kci).<br />However,<br />kresults in signifi-<br />will be near<br />khas K − 1 entries containing<br />k)−Eq[ln(1−θk)], while these values will be<br />k<br />6. Experiments<br />We perform experiments using stochastic search VB<br />for binary classification with logistic regression and for<br />topic modeling with the approximate HDP. We next<br />give the details of the experiments we perform and the<br />data sets and algorithms used for comparison.<br />Data and set-up.<br />five data sets from the UCI repository: Iris, Pima,<br />SPECTF, Voting and WDBC. These data sets range<br />from 150 to 768 labeled examples living in 5 to 45 di-<br />mensions, including a dimension of all ones to account<br />for offset. We perform experiments with stochastic<br />search variational inference using the two control vari-<br />ates discussed in Sec. 5.1. We compare with two ad-<br />ditional methods for posterior approximation: varia-<br />tional inference with the Jaakkola &amp; Jordan (2000)<br />bound and Laplace’s method.<br />mance on the true variational objective function in Eq.<br />(14) using each posterior approximation.<br />For logistic regression, we use<br />We evaluate perfor-<br />For the HDP topic model, we use 8,000 documents<br />with 3,012 vocabulary size from The New York Times.<br />We compare with (i) a point estimate of the top-level<br />probability vector using a delta q distribution, and (ii)<br />fixing the top-level distribution to the uniform vector,<br />which is equivalent to LDA (Blei et al., 2003). We<br />perform experiments for different corpus sizes, differ-<br />ent values of β, and we set K = 200.</p>  <p>Page 7</p> <p>Variational Bayesian Inference with Stochastic Search<br />0 50100150200<br />0<br />0.05<br />0.1<br />0.15<br />0 50<br />Iteration number<br />100 150200<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />1.2<br /># samples<br />variance reduction<br />SPECTF<br />VOTE WDBC<br />Taylor CV<br />J&amp;J Bound CV<br />a^<br />0 50100 150200<br />0<br />5<br />10<br />15<br />x10-3<br />0 50100 150200<br />0.7<br />0.8<br />0.9<br />1<br />1.1<br />0 50100150 200<br />0<br />0.05<br />0.1<br />0.15<br />0.2<br />0 50100150 200<br />0.2<br />0.4<br />0.6<br />0.8<br />1<br />1.2<br />0 50 100150 200<br />102<br />104<br />106<br />108<br />0 50100150200<br />102<br />104<br />106<br />108<br />0 50 100150200<br />102<br />104<br />106<br />108<br />Figure 3. Experimental results for variational logistic regression. We compare the variance reduction obtained by the two<br />control variates under consideration. (top row) The number of samples per iteration setting ? = 0.1 in Algorithm 1. The<br />yellow and black lines represent the estimated number that would be required without variance reduction according to<br />each control variate. As expected, these curves overlap. (middle row) The variance reduction factor of Eq. (11). The<br />selected control variates significantly reduce the variance. The second-order Taylor control variate is significantly better<br />than the lower bound. (bottom row) The optimal scaling factor estimated from samples.<br />Table 1. Optimizing the variational objective function.<br />The stochastic search methods (indicated by CV) signif-<br />icantly outperform the other methods toward this end. All<br />values were calculated for the true lower bound in Eq. (14)<br />using their respective posterior approximations.<br />model|data<br />Taylor CV-7.9-3974<br />J&amp;J CV-7.9 -3974<br />Laplace-11.9-3985<br />J&amp;J bnd -11.5-3976<br />iris pima spectf<br />-165<br />-165<br />-170<br />-173<br />vote<br />-67.8<br />-67.6<br />-70.5<br />-74.6<br />wdbc<br />-74.6<br />-74.8<br />-80.0<br />-86.2<br />Table 2. Running time of each algorithm on each data set.<br />We use the approximated number of samples required with-<br />out a control variate to estimate the last value. The times<br />are given in milliseconds (ms), seconds (s), minutes (m),<br />hours (hr) and years (yr).<br />model|data<br />Taylor CV0.33s1.7m<br />J&amp;J CV0.42s 18m<br />Laplace 21ms 29ms<br />J&amp;J bnd64ms 88ms<br />SS no CV2.4s&gt;2yr<br />irispimaspectf<br />20s<br />1.2m<br />94ms<br />0.13s<br />6.6hr<br />vote<br />17s<br />1.2m<br />20ms<br />97ms<br />9hr<br />wdbc<br />11s<br />2.3m<br />0.10s<br />0.15s<br />1.4hr<br />Logistic regression results.<br />the variational lower bound for each model on each<br />data set. Since all algorithms return an approxima-<br />tion of the posterior distribution on the vector θ, this<br />comparison is meaningful and gives a measure of how<br />close each posterior is to the true posterior. We see<br />a considerable improvement for the stochastic algo-<br />rithms (denoted by their control variate). Since both<br />stochastic algorithms optimize the same objective, the<br />performance should be the same.<br />In Table 1 we show<br />We show performance details of the stochastic search<br />VB algorithm in Figure 3 and Table 2. In Figure 3,<br />we show the number of samples, the variance reduction<br />factor and the scaling ˆ a as a function of iteration. We<br />see that the control variates provide a major reduction<br />in variance. Also, the Taylor expansion control vari-<br />ate (i.e., control variate delta method) requires signif-<br />icantly fewer samples than the bound control variate,<br />which benefits the running time (see Table 2). While<br />the non-sampling methods are faster, control variates<br />make stochastic search VB a viable inference method<br />when compared to the base algorithm of Sec. 3.</p>  <p>Page 8</p> <p>Variational Bayesian Inference with Stochastic Search<br />Table 3. The fraction of times that algorithm ?row? was<br />ranked ?column? for the 32 different parameter/data size<br />pairs using the variational lower bound.<br />model | rank<br />HDP-stochastic<br />HDP-point<br />LDA<br />1st<br />0.66<br />0.34<br />0<br />2nd<br />0.31<br />0.66<br />0.03<br />3rd<br />0.03<br />0<br />0.97<br />�<br />�<br />c ln q(�|c)<br />c ln qk&#39; (�|c)<br />number of docs (x1000)<br /># samples<br />�����<br />12345678<br />101<br />102<br />103<br />104<br />105<br />12345678<br />102<br />103<br />104<br />105<br />�����<br />Figure 4. Average number of samples per iteration for the<br />two equivalent gradient approximations, ∇clnq vs ∇clnq?<br />where q is the Dirichlet and q?<br />kthe beta distribution. Sam-<br />pling is further reduced (see text for discussion).<br />k,<br />Hierarchical Dirichlet process results.<br />topic models to The New York Times using different<br />numbers of documents (D = 1000 to 8000) and concen-<br />tration parameter values β ∈ {1,5,10,15}. As switch<br />points for the two control variates, we set κ1= 1 and<br />κ2 = 2. We summarize our results in Table 3. In<br />general, fitting a variational posterior on the top-level<br />Dirichlet vector yielded a better posterior approxima-<br />tion than a point estimate and a θ fixed as uniform.<br />However, this improvement was not as dramatic as for<br />logistic regression.<br />We fit<br />In Figure 4, we show the number of samples required<br />from the Dirichlet q distribution to approximate the<br />stochastic integral. We compare the two methods dis-<br />cussed in Sec. 5.2 for reducing the variance of the<br />stochastic vector ∇clnq by instead using ∇clnq?<br />see a significant reduction in the number of samples.<br />Experiments without control variates were not possi-<br />ble due to computer precision issues and the massive<br />variance of lnΓ(βθ) near zero.<br />k. We<br />7. Conclusion<br />We have presented stochastic search variational Bayes,<br />a method for optimizing intractable variational ob-<br />jective functions such as those arising from non-<br />conjugacy. The algorithm relies on a stochastic ap-<br />proximation of the gradient; we showed how control<br />variates can significantly reduce the variance of this<br />Monte Carlo integral. Since existing lower bounds can<br />be recast as control variates, our approach is relevant<br />to many existing MFVB algorithms. However, a lack<br />of restrictions on control variates allows for other types<br />of function approximations when a good bound is not<br />readily available. We introduced the control variate<br />delta method toward this end.<br />Acknowledgements<br />ONR grant number N00014-11-1-0688 under the MURI<br />program. D.B. is supported by ONR N00014-11-1-0651,<br />NSF CAREER 0745520, AFOSR FA9550-09-1-0668, the<br />Alfred P. Sloan foundation, and a grant from Google.<br />J.P. and M.J. are supported by<br />References<br />Beal, M.J.<br />Bayesian Inference. PhD thesis, Gatsby Computational<br />Neuroscience Unit, University College London, 2003.<br />Variational Algorithms for Approximate<br />Blei, D. and Jordan, M. Variational inference for Dirichlet<br />process mixtures. Bayesian Analysis, 1:121–144, 2006.<br />Blei, D., Ng, A., and Jordan, M. Latent Dirichlet alloca-<br />tion. Journal of Machine Learning Research, 3:993–1022,<br />2003.<br />Graves, A. Practical variational inference for neural net-<br />works. In Neural Information Processing Systems, 2011.<br />Hoffman, M., Blei, D., and Bach, F. Online learning for<br />latent Dirichlet allocation. In Neural Information Pro-<br />cessing Systems, 2010.<br />Jaakkola, T. and Jordan, M.I. Bayesian parameter estima-<br />tion via variational methods. Statistics and Computing,<br />10:25–37, 2000.<br />Jordan, M.I., Ghahramani, Z., Jaakkola, T., and Saul,<br />L.K. An introduction to variational methods for graph-<br />ical models. Machine Learning, 37:183–233, 1999.<br />Knowles, D.A. and Minka, T.P. Non-conjugate variational<br />message passing for multinomial and binary regression.<br />In Neural Information Processing Systems, 2011.<br />Leisink, M.A.R. and Kappen, H.J. A tighter bound for<br />graphical models.Neural Computation, 13(9):2149–<br />2171, 2001.<br />Marlin, B., Khan, E., and Murphy, K. Piecewise bounds<br />for estimating Bernoulli-logistic latent Gaussian models.<br />In International Conference on Machine Learning, 2011.<br />Paisley, J. and Carin, L. Nonparametric factor analysis<br />with beta process priors. In International Conference<br />on Machine Learning, 2009.<br />Ross, S.M. Simulation. Academic Press, San Diego, 4th<br />edition, 2006.<br />Teh, Y., Jordan, M., Beal, M., and Blei, D. Hierarchical<br />Dirichlet processes. Journal of the American Statistical<br />Association, 101(476):1566–1581, 2007.<br />Wang, C., Paisley, J., and Blei, D. Online variational infer-<br />ence for the hierarchical Dirichlet process. In Artificial<br />Intelligence and Statistics, 2011.<br />Yi, S., Wierstra, D., Schaul, T., and Schmidhuber, J.<br />Stochastic search using the natural gradient. In Inter-<br />national Conference on Machine Learning, 2009.</p>  <a href="https://www.researchgate.net/profile/Michael_Jordan13/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search/links/53fe1e670cf21edafd14cb6c.pdf">Download full-text</a> </div> <div id="rgw20_56ab1c94bcd25" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab1c94bcd25">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab1c94bcd25"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Michael_Jordan13/publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search/links/53fe1e670cf21edafd14cb6c.pdf" class="publication-viewer" title="53fe1e670cf21edafd14cb6c.pdf">53fe1e670cf21edafd14cb6c.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Michael_Jordan13">Michael Jordan</a> &middot; Aug 27, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab1c94bcd25"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://arxiv.org/pdf/1206.6430" target="_blank" rel="nofollow" class="publication-viewer" title="Variational Bayesian Inference with Stochastic Search">Variational Bayesian Inference with Stochastic Sea...</a> </div>  <div class="details">   Available from <a href="http://arxiv.org/pdf/1206.6430" target="_blank" rel="nofollow">arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw30_56ab1c94bcd25" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (21) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw31_56ab1c94bcd25" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab1c94bcd25" >  <div class="indent-left">  <div id="rgw33_56ab1c94bcd25" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/284579582_Black_box_variational_inference_for_state_space_models">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Evan_Archer" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Evan William Archer </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab1c94bcd25">  <li class="citation-context-item"> "Typically at least some terms of eq. 3 cannot be integrated in closed form. While it is often possible to estimate the gradient by sampling directly from q(z|x), in general the approximate gradient exhibits high variance (Paisley et al., 2012). One approach to addressing this difficulty, independently proposed by Kingma &amp; Welling (2013), Rezende et al. (2014) and Titsias &amp; Lázaro-Gredilla (2014), is to compute the integral using the &quot; reparameterization trick &quot; : choose an easy-to-sample random variable with distribution p() and parameterize z through a function g of observations x and parameters φ, " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/284579582_Black_box_variational_inference_for_state_space_models"> <span class="publication-title js-publication-title">Black box variational inference for state space models</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/75272153_Evan_Archer" class="authors js-author-name ga-publications-authors">Evan Archer</a> &middot;     <a href="researcher/16191178_Il_Memming_Park" class="authors js-author-name ga-publications-authors">Il Memming Park</a> &middot;     <a href="researcher/2085695295_Lars_Buesing" class="authors js-author-name ga-publications-authors">Lars Buesing</a> &middot;     <a href="researcher/2085800824_John_Cunningham" class="authors js-author-name ga-publications-authors">John Cunningham</a> &middot;     <a href="researcher/39697633_Liam_Paninski" class="authors js-author-name ga-publications-authors">Liam Paninski</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Latent variable time-series models are among the most heavily used tools from
machine learning and applied statistics. These models have the advantage of
learning latent structure both from noisy observations and from the temporal
ordering in the data, where it is assumed that meaningful correlation structure
exists across time. A few highly-structured models, such as the linear
dynamical system with linear-Gaussian observations, have closed-form inference
procedures (e.g. the Kalman Filter), but this case is an exception to the
general rule that exact posterior inference in more complex generative models
is intractable. Consequently, much work in time-series modeling focuses on
approximate inference procedures for one particular class of models. Here, we
extend recent developments in stochastic variational inference to develop a
`black-box&#39; approximate inference technique for latent variable models with
latent dynamical structure. We propose a structured Gaussian variational
approximate posterior that carries the same intuition as the standard Kalman
filter-smoother but, importantly, permits us to use the same inference approach
to approximate the posterior of much more general, nonlinear latent variable
generative models. We show that our approach recovers accurate estimates in the
case of basic models with closed-form posteriors, and more interestingly
performs well in comparison to variational approaches that were designed in a
bespoke fashion for specific non-conjugate models. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Nov 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Evan_Archer/publication/284579582_Black_box_variational_inference_for_state_space_models/links/565cca6b08ae1ef92981f7a5.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab1c94bcd25" >  <div class="indent-left">  <div id="rgw36_56ab1c94bcd25" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Yutian_Chen3" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Yutian Chen </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab1c94bcd25">  <li class="citation-context-item"> "Instead we use recent developments in sampling-based variational inference (Blei et al., 2012) to avoid integrating the latent variables with the Softmax analytically. Our approach takes advantage of this tool to obtain simple yet powerful model and inference. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data"> <span class="publication-title js-publication-title">Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2069013556_Yarin_Gal" class="authors js-author-name ga-publications-authors">Yarin Gal</a> &middot;     <a href="researcher/69632150_Yutian_Chen" class="authors js-author-name ga-publications-authors">Yutian Chen</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Multivariate categorical data occur in many applications of machine learning.
One of the main difficulties with these vectors of categorical variables is
sparsity. The number of possible observations grows exponentially with vector
length, but dataset diversity might be poor in comparison. Recent models have
gained significant improvement in supervised tasks with this data. These models
embed observations in a continuous space to capture similarities between them.
Building on these ideas we propose a Bayesian model for the unsupervised task
of distribution estimation of multivariate categorical data. We model vectors
of categorical variables as generated from a non-linear transformation of a
continuous latent space. Non-linearity captures multi-modality in the
distribution. The continuous representation addresses sparsity. Our model ties
together many existing models, linking the linear categorical latent Gaussian
model, the Gaussian process latent variable model, and Gaussian process
classification. We derive inference for our model based on recent developments
in sampling based variational inference. We show empirically that the model
outperforms its linear and discrete counterparts in imputation tasks of sparse
data. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Mar 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Yutian_Chen3/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data/links/565d728808aefe619b25b735.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw38_56ab1c94bcd25" >  <div class="indent-left">  <div id="rgw39_56ab1c94bcd25" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1503.01494" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw40_56ab1c94bcd25">  <li class="citation-context-item"> "where each x (s) is an independent draw from q v (x). While this estimate is unbiased, it has been observed to severely suffer from high variance so that in practice it is necessary to consider variance reduction techniques such as those based on control variates [7] [8] [6]. Despite this limitation the above framework is very general as it can deal with any variational distribution over both discrete and continuous latent variables. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference"> <span class="publication-title js-publication-title">Local Expectation Gradients for Doubly Stochastic Variational Inference</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2077568820_Michalis_K_Titsias" class="authors js-author-name ga-publications-authors">Michalis K. Titsias</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We introduce local expectation gradients which is a general purpose
stochastic variational inference algorithm for constructing stochastic
gradients through sampling from the variational distribution. This algorithm
divides the problem of estimating the stochastic gradients over multiple
variational parameters into smaller sub-tasks so that each sub-task exploits
intelligently the information coming from the most relevant part of the
variational distribution. This is achieved by performing an exact expectation
over the single random variable that mostly correlates with the variational
parameter of interest resulting in a Rao-Blackwellized estimate that has low
variance and can work efficiently for both continuous and discrete random
variables. Furthermore, the proposed algorithm has interesting similarities
with Gibbs sampling but at the same time, unlike Gibbs sampling, it can be
trivially parallelized. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Mar 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw25_56ab1c94bcd25" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab1c94bcd25">  </ul> </div> </div>   <div id="rgw16_56ab1c94bcd25" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1c94bcd25"> <div> <h5> <a href="publication/282556505_State-of-Health_Monitoring_and_Prediction_of_Lithium-Ion_Battery_Using_Probabilistic_Indication_and_State-Space_Model" class="color-inherit ga-similar-publication-title"><span class="publication-title">State-of-Health Monitoring and Prediction of Lithium-Ion Battery Using Probabilistic Indication and State-Space Model</span></a>  </h5>  <div class="authors"> <a href="researcher/2082233819_Jianbo_Yu" class="authors ga-similar-publication-author">Jianbo Yu</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1c94bcd25"> <div> <h5> <a href="publication/283975517_Spatial_pattern_in_prevalence_of_paratuberculosis_infection_diagnosed_with_misclassification_in_Danish_dairy_herds_in_2009_2013" class="color-inherit ga-similar-publication-title"><span class="publication-title">Spatial pattern in prevalence of paratuberculosis infection diagnosed with misclassification in Danish dairy herds in 2009 &amp; 2013</span></a>  </h5>  <div class="authors"> <a href="researcher/2062812221_Kristine_Bihrmann" class="authors ga-similar-publication-author">Kristine Bihrmann</a>, <a href="researcher/14523646_Soren_Saxmose_Nielsen" class="authors ga-similar-publication-author">Søren Saxmose Nielsen</a>, <a href="researcher/16389346_Annette_Kjaer_Ersboll" class="authors ga-similar-publication-author">Annette Kjær Ersbøll</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1c94bcd25"> <div> <h5> <a href="publication/282433207_GMM_and_optimal_principal_components-based_Bayesian_method_for_multimode_fault_diagnosis" class="color-inherit ga-similar-publication-title"><span class="publication-title">GMM and optimal principal components-based Bayesian method for multimode fault diagnosis</span></a>  </h5>  <div class="authors"> <a href="researcher/2031341121_Qingchao_Jiang" class="authors ga-similar-publication-author">Qingchao Jiang</a>, <a href="researcher/10702092_Biao_Huang" class="authors ga-similar-publication-author">Biao Huang</a>, <a href="researcher/82638685_Xuefeng_Yan" class="authors ga-similar-publication-author">Xuefeng Yan</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw50_56ab1c94bcd25" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw51_56ab1c94bcd25">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw52_56ab1c94bcd25" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=05W45Kqu6oa0XMnQdji2yaoV1fORA1K7WxQCCFpUts0nLF561CfsUbvkFVA6yBGs" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="In8WasW+06UKlPbo82Iu7ekK+CP5jrzi5JAZrBY3sreE+JiumgdjF7NtozUbQ9Z4A68dJk/DDCxb9EgFilKy5pM8co9YJVamsY5bdAGMXAc3e5PKohyV9vWo1UwLfNGjlezEu8/235AJ2NlZPDmwzRcYNb9WzHZ8mBtmsqrtywbLbx4qaamq+If7NiU8uQuGc+r19SeFuosI2tcipwO5huhFHBAeMePX1bOaPZmW6vKX7gknIBU6ujWX/RjTV1pp8oIhWLpSfCLBA67wMj4Wp6+lXyZPnp5anBApE3ZHtoM="/> <input type="hidden" name="urlAfterLogin" value="publication/228095632_Variational_Bayesian_Inference_with_Stochastic_Search"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NjMyX1ZhcmlhdGlvbmFsX0JheWVzaWFuX0luZmVyZW5jZV93aXRoX1N0b2NoYXN0aWNfU2VhcmNo"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NjMyX1ZhcmlhdGlvbmFsX0JheWVzaWFuX0luZmVyZW5jZV93aXRoX1N0b2NoYXN0aWNfU2VhcmNo"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjI4MDk1NjMyX1ZhcmlhdGlvbmFsX0JheWVzaWFuX0luZmVyZW5jZV93aXRoX1N0b2NoYXN0aWNfU2VhcmNo"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw53_56ab1c94bcd25"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 950;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Michael Jordan","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Michael_Jordan13","institution":"University of California, Berkeley","institutionUrl":false,"widgetId":"rgw4_56ab1c94bcd25"},"id":"rgw4_56ab1c94bcd25","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=5451072","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1c94bcd25"},"id":"rgw3_56ab1c94bcd25","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=228095632","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":228095632,"title":"Variational Bayesian Inference with Stochastic Search","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"Proceedings of the 29th International Conference on Machine Learning, ICML 2012","publicationDate":"06\/2012;","publicationDateRobot":"2012-06","article":"2."}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1206.6430","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Variational Bayesian Inference with Stochastic Search"},{"key":"rft.title","value":"Proceedings of the 29th International Conference on Machine Learning, ICML 2012"},{"key":"rft.jtitle","value":"Proceedings of the 29th International Conference on Machine Learning, ICML 2012"},{"key":"rft.volume","value":"2"},{"key":"rft.date","value":"2012"},{"key":"rft.au","value":"John Paisley,David Blei,Michael Jordan"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1c94bcd25"},"id":"rgw6_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=228095632","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":228095632,"peopleItems":[{"data":{"authorUrl":"researcher\/70672430_John_Paisley","authorNameOnPublication":"John Paisley","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"John Paisley","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/70672430_John_Paisley","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56ab1c94bcd25"},"id":"rgw9_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=70672430&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1c94bcd25"},"id":"rgw8_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=70672430&authorNameOnPublication=John%20Paisley","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2064238818_David_Blei","authorNameOnPublication":"David Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David Blei","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2064238818_David_Blei","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab1c94bcd25"},"id":"rgw11_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2064238818&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1c94bcd25"},"id":"rgw10_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2064238818&authorNameOnPublication=David%20Blei","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Michael Jordan","accountUrl":"profile\/Michael_Jordan13","accountKey":"Michael_Jordan13","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michael Jordan","profile":{"professionalInstitution":{"professionalInstitutionName":"University of California, Berkeley","professionalInstitutionUrl":"institution\/University_of_California_Berkeley"}},"professionalInstitutionName":"University of California, Berkeley","professionalInstitutionUrl":"institution\/University_of_California_Berkeley","url":"profile\/Michael_Jordan13","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278648436346888%401443446372379_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Michael_Jordan13","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw13_56ab1c94bcd25"},"id":"rgw13_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=5451072&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of California, Berkeley","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":228095632,"widgetId":"rgw12_56ab1c94bcd25"},"id":"rgw12_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=5451072&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=228095632","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1c94bcd25"},"id":"rgw7_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=228095632&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":228095632,"abstract":"<noscript><\/noscript><div>Mean-field variational inference is a method for approximate Bayesian<br \/>\nposterior inference. It approximates a full posterior distribution with a<br \/>\nfactorized set of distributions by maximizing a lower bound on the marginal<br \/>\nlikelihood. This requires the ability to integrate a sum of terms in the log<br \/>\njoint likelihood using this factorized distribution. Often not all integrals<br \/>\nare in closed form, which is typically handled by using a lower bound. We<br \/>\npresent an alternative algorithm based on stochastic optimization that allows<br \/>\nfor direct optimization of the variational lower bound. This method uses<br \/>\ncontrol variates to reduce the variance of the stochastic search gradient, in<br \/>\nwhich existing lower bounds can play an important role. We demonstrate the<br \/>\napproach on two non-conjugate models: logistic regression and an approximation<br \/>\nto the HDP.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw14_56ab1c94bcd25"},"id":"rgw14_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=228095632","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw15_56ab1c94bcd25"},"id":"rgw15_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1c94bcd25"},"id":"rgw5_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=228095632&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2082233819,"url":"researcher\/2082233819_Jianbo_Yu","fullname":"Jianbo Yu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2015","journal":"IEEE Transactions on Instrumentation and Measurement","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282556505_State-of-Health_Monitoring_and_Prediction_of_Lithium-Ion_Battery_Using_Probabilistic_Indication_and_State-Space_Model","usePlainButton":true,"publicationUid":282556505,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.79","url":"publication\/282556505_State-of-Health_Monitoring_and_Prediction_of_Lithium-Ion_Battery_Using_Probabilistic_Indication_and_State-Space_Model","title":"State-of-Health Monitoring and Prediction of Lithium-Ion Battery Using Probabilistic Indication and State-Space Model","displayTitleAsLink":true,"authors":[{"id":2082233819,"url":"researcher\/2082233819_Jianbo_Yu","fullname":"Jianbo Yu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Instrumentation and Measurement 11\/2015; 64(11):1-1. DOI:10.1109\/TIM.2015.2444237"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282556505_State-of-Health_Monitoring_and_Prediction_of_Lithium-Ion_Battery_Using_Probabilistic_Indication_and_State-Space_Model","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282556505_State-of-Health_Monitoring_and_Prediction_of_Lithium-Ion_Battery_Using_Probabilistic_Indication_and_State-Space_Model\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1c94bcd25"},"id":"rgw17_56ab1c94bcd25","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282556505","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2062812221,"url":"researcher\/2062812221_Kristine_Bihrmann","fullname":"Kristine Bihrmann","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14523646,"url":"researcher\/14523646_Soren_Saxmose_Nielsen","fullname":"S\u00f8ren Saxmose Nielsen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":16389346,"url":"researcher\/16389346_Annette_Kjaer_Ersboll","fullname":"Annette Kj\u00e6r Ersb\u00f8ll","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":"Spatial and Spatio-temporal Epidemiology","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283975517_Spatial_pattern_in_prevalence_of_paratuberculosis_infection_diagnosed_with_misclassification_in_Danish_dairy_herds_in_2009_2013","usePlainButton":true,"publicationUid":283975517,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/283975517_Spatial_pattern_in_prevalence_of_paratuberculosis_infection_diagnosed_with_misclassification_in_Danish_dairy_herds_in_2009_2013","title":"Spatial pattern in prevalence of paratuberculosis infection diagnosed with misclassification in Danish dairy herds in 2009 & 2013","displayTitleAsLink":true,"authors":[{"id":2062812221,"url":"researcher\/2062812221_Kristine_Bihrmann","fullname":"Kristine Bihrmann","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14523646,"url":"researcher\/14523646_Soren_Saxmose_Nielsen","fullname":"S\u00f8ren Saxmose Nielsen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":16389346,"url":"researcher\/16389346_Annette_Kjaer_Ersboll","fullname":"Annette Kj\u00e6r Ersb\u00f8ll","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Spatial and Spatio-temporal Epidemiology 10\/2015; 16. DOI:10.1016\/j.sste.2015.10.001"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283975517_Spatial_pattern_in_prevalence_of_paratuberculosis_infection_diagnosed_with_misclassification_in_Danish_dairy_herds_in_2009_2013","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283975517_Spatial_pattern_in_prevalence_of_paratuberculosis_infection_diagnosed_with_misclassification_in_Danish_dairy_herds_in_2009_2013\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1c94bcd25"},"id":"rgw18_56ab1c94bcd25","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283975517","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2031341121,"url":"researcher\/2031341121_Qingchao_Jiang","fullname":"Qingchao Jiang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10702092,"url":"researcher\/10702092_Biao_Huang","fullname":"Biao Huang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82638685,"url":"researcher\/82638685_Xuefeng_Yan","fullname":"Xuefeng Yan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Sep 2015","journal":"Computers & Chemical Engineering","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282433207_GMM_and_optimal_principal_components-based_Bayesian_method_for_multimode_fault_diagnosis","usePlainButton":true,"publicationUid":282433207,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.78","url":"publication\/282433207_GMM_and_optimal_principal_components-based_Bayesian_method_for_multimode_fault_diagnosis","title":"GMM and optimal principal components-based Bayesian method for multimode fault diagnosis","displayTitleAsLink":true,"authors":[{"id":2031341121,"url":"researcher\/2031341121_Qingchao_Jiang","fullname":"Qingchao Jiang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10702092,"url":"researcher\/10702092_Biao_Huang","fullname":"Biao Huang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":82638685,"url":"researcher\/82638685_Xuefeng_Yan","fullname":"Xuefeng Yan","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Computers & Chemical Engineering 09\/2015; 84. DOI:10.1016\/j.compchemeng.2015.09.013"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282433207_GMM_and_optimal_principal_components-based_Bayesian_method_for_multimode_fault_diagnosis","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282433207_GMM_and_optimal_principal_components-based_Bayesian_method_for_multimode_fault_diagnosis\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1c94bcd25"},"id":"rgw19_56ab1c94bcd25","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282433207","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab1c94bcd25"},"id":"rgw16_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=228095632&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":228095632,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":228095632,"publicationType":"article","linkId":"53fe1e670cf21edafd14cb6c","fileName":"53fe1e670cf21edafd14cb6c.pdf","fileUrl":"profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf","name":"Michael Jordan","nameUrl":"profile\/Michael_Jordan13","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Aug 27, 2014","fileSize":"575.01 KB","widgetId":"rgw22_56ab1c94bcd25"},"id":"rgw22_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=228095632&linkId=53fe1e670cf21edafd14cb6c&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":228095632,"publicationType":"article","linkId":"02b045880cf27908e9dda48e","fileName":"Variational Bayesian Inference with Stochastic Search","fileUrl":"http:\/\/arxiv.org\/pdf\/1206.6430","name":"arxiv.org","nameUrl":"http:\/\/arxiv.org\/pdf\/1206.6430","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw23_56ab1c94bcd25"},"id":"rgw23_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=228095632&linkId=02b045880cf27908e9dda48e&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab1c94bcd25"},"id":"rgw21_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=228095632&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":24,"valueFormatted":"24","widgetId":"rgw24_56ab1c94bcd25"},"id":"rgw24_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=228095632","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab1c94bcd25"},"id":"rgw20_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=228095632&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":228095632,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab1c94bcd25"},"id":"rgw26_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=228095632&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":24,"valueFormatted":"24","widgetId":"rgw27_56ab1c94bcd25"},"id":"rgw27_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=228095632","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab1c94bcd25"},"id":"rgw25_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=228095632&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Variational Bayesian Inference with Stochastic Search\nJohn Paisley1\nDavid M. Blei3\nMichael I. Jordan1,2\n1Department of EECS,2Department of Statistics, UC Berkeley\n3Department of Computer Science, Princeton University\njpaisley@berkeley.edu\nblei@cs.princeton.edu\njordan@eecs.berkeley.edu\nAbstract\nMean-field variational inference is a method\nfor approximate Bayesian posterior inference.\nIt approximates a full posterior distribution\nwith a factorized set of distributions by max-\nimizing a lower bound on the marginal likeli-\nhood. This requires the ability to integrate a\nsum of terms in the log joint likelihood using\nthis factorized distribution. Often not all in-\ntegrals are in closed form, which is typically\nhandled by using a lower bound. We present\nan alternative algorithm based on stochastic\noptimization that allows for direct optimiza-\ntion of the variational lower bound.\nmethod uses control variates to reduce the\nvariance of the stochastic search gradient, in\nwhich existing lower bounds can play an im-\nportant role. We demonstrate the approach\non two non-conjugate models: logistic regres-\nsion and an approximation to the HDP.\nThis\n1. Introduction\nMean-field variational Bayesian (MFVB) inference is\nan optimization-based approach to approximating the\nfull posterior of the latent variables of a Bayesian\nmodel (Jordan et al., 1999). It has been applied to\nmany problem domains, for example mixture model-\ning (Blei & Jordan, 2006), sequential modeling (Beal,\n2003) and factor analysis (Paisley & Carin, 2009). In\naddition, recent development of the theory has ex-\ntended the method to online inference and stochastic\noptimization settings, making variational Bayes a vi-\nable approach for Bayesian learning with massive data\nsets (Hoffman et al., 2010; Wang et al., 2011).\nAppearing in Proceedings of the 29thInternational Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)\/owner(s).\nVariational Bayes approximates the full posterior by\nattempting to minimize the Kullback-Leibler diver-\ngence between the true posterior and a predefined fac-\ntorized distribution on the same variables. Minimiz-\ning this divergence is equivalent to maximizing the fa-\nmiliar variational objective function. To review, let\n\u0398 = {\u03b8i} represent the set of latent variables (ran-\ndom effects and parameters) in the model and X rep-\nresent the data. The joint likelihood of X and \u0398 is\nP(X,\u0398|\u03a5), with \u03a5 the set of hyperparameters. Varia-\ntional inference approximates the posterior P(\u0398|X,\u03a5)\nwith a Q distribution that takes a set of variational\nparameters \u03a8 = {\u03c8i}. This distribution is factorized,\nQ(\u0398|\u03a8) =?\niqi(\u03b8i|\u03c8i), and the values of \u03a8 are opti-\nmized to maximize the objective function,\nL(X,\u03a8) = EQ[lnP(X,\u0398|\u03a5)] + H[Q(\u0398|\u03a8)].(1)\nThe solution is only locally optimal when L is not\nconvex, which is usually the case. Most variational\ninference algorithms optimize L by coordinate ascent,\nwhich repeatedly cycles through and optimizes with\nrespect to each variational parameter \u03c8i. Often the\nlocally optimal value of \u03c8ihas a closed-form solution,\nfor example in conjugate exponential models.\nThe log of the joint likelihood results in a sum of terms;\na major issue that often arises in MFVB is that not all\nexpectations in this sum are in closed form. A typical\nsolution in this case is to replace the problematic func-\ntion with another function of the same variables (plus\nauxiliary variables) that is a point-wise lower bound.\nThis new function is selected such that the expectation\nis tractable. While inference can now proceed, a draw-\nback of introducing bounds is that the true variational\nobjective function is no longer being optimized, which\nmay lead to a significantly worse posterior approxi-\nmation. Therefore, much attention has been paid to\ndeveloping tight bounds of commonly occurring func-\ntions (e.g., Jaakkola & Jordan (2000), Marlin et al.\n(2011), Leisink & Kappen (2001))."},{"page":2,"text":"Variational Bayesian Inference with Stochastic Search\nWe present a method for directly optimizing Eq. (1)\nfor models in which not all expectations are tractable;\nwe show how a stochastic approximation of \u2207\u03c8iL\ncan allow for optimization of L when the expecta-\ntion Eqi[lnP(X,\u0398|\u03a5)] is not in closed form. The ap-\nproximation is unbiased, and so by using the proposed\nstochastic method we are directly optimizing L.\nOur stochastic approximation is based on Monte Carlo\nintegration, for which the number of samples heavily\ndepends on the variance of this approximation. We in-\ntroduce a control variate (Ross, 2006) to significantly\nreduce the variance of this stochastic approximation.\nA control variate is a tractable function g that is highly\ncorrelated with the intractable function f. The func-\ntion g replaces f in Eq. (1), and the gradient is then\nstochastically corrected for bias.\nExisting lower bounds have properties that make them\nideal as control variates, and thus can improve the\nspeed of the algorithm. However, a major advantage\nof the control variate methodology is that it does not\nrequire the tractable function g to bound f, but only to\ncorrelate well with it (i.e., to approximate it well mod-\nulo a scaling). This opens the door to many more func-\ntions that may give better approximations than a lower\nbound. One of these possible functions is the second-\norder Taylor expansion, which often gives a very good\napproximation, while also allowing for closed-form ex-\npectations. We show the potential performance gain\nusing this function as a control variate, which we de-\nnote the control variate delta method for MFVB.\nRelated work.\n(2011) has also addressed the problem of intractable\nexpectations in MFVB inference in the context of de-\nveloping a more general variational message passing al-\ngorithm. Our solution arises from a different perspec-\ntive and results in a new algorithm based on stochastic\noptimization. Graves (2011) considers a similar prob-\nlem for neural networks, but a lack of control variates\nlimits the algorithm to significantly simpler variational\napproximations.Stochastic search algorithms have\nalso been developed for models of Evolution Strate-\ngies (see, e.g., Yi et al. (2009)).\nRecent work by Knowles & Minka\n2. Mean-field variational inference\nMean-field variational Bayesian (MFVB) inference ap-\nproximates the full posterior of the latent variables of\na Bayesian model with a factorized distribution. As\nmotivated in the introduction, let \u0398 = {\u03b8i} be these\nvariables, X the data and \u03a5 all hyperparameters of\nthe prior distributions on \u0398. We define the factor-\nized distribution on \u0398 to be Q(\u0398|\u03a8) =?\niqi(\u03b8i|\u03c8i),\nwhere \u03c8i are the parameters of the qi distributions.\nThe variational objective function arises by bounding\nthe marginal likelihood using the Q distribution,\nlnP(X|\u03a5)= ln\n?\n?\n\u0398\nQ(\u0398|\u03a8)lnP(X,\u0398|\u03a5)\nP(X,\u0398|\u03a5)d\u0398 (2)\n\u2265\n\u0398\nQ(\u0398|\u03a8)\nd\u0398.\nMaximizing this lower bound (denoted L) with re-\nspect to \u03a8 is equivalent to minimizing the Kullback-\nLeibler divergence between Q(\u0398) and P(\u0398|X,\u03a5),\nwhich makes up the difference in Eq. (2).\nTo facilitate our discussion, we write the functions ap-\npearing in the log joint likelihood as lnP(X,\u0398|\u03a5) =\n?\nappearing in function j. We note that the index j does\nnot correspond to variables or distributions, but to the\nterms of the log joint likelihood. Using this notation,\nthe variational lower bound in Eq. (1) becomes\njfj(XAj,\u0398Bj), where Aj indexes the data appear-\ning in function j and Bj indexes the latent variables\nL =?\nFor each function fj, those \u03b8i \/ \u2208 \u0398Bjwill have their\ncorresponding qi removed from the expectation. For\nthose \u03b8i\u2208 \u0398Bj, the expectation of fjresults in a new\nfunction of variational parameters \u03c8i\u2208 \u03a8Bj. Ideally,\nall expectations will be in closed form, allowing for the\noptimization of \u03a8 to proceed.\njEQ[fj(XAj,\u0398Bj)] +?\niH[qi(\u03b8i|\u03c8i)]. (3)\nIn the case where an expectation in Eq. (3) is not\ntractable, a nicer functional lower bound can replace\nthe problematic function. That is, let Eqi[fj(\u03b8i)] be\nintractable.1A common approach to dealing with this\nissue is to introduce a function g(\u03b8i,\u03be) that replaces fj\nand is a point-wise lower bound: fj(\u03b8i) \u2265 g(\u03b8i,\u03be) for\nall \u03b8i. The function g usually takes auxiliary variables\n\u03be, which determines how tightly g approximates fjand\nis tuned along with the other parameters during infer-\nence. The expectation Eqi[g(\u03b8i,\u03be)] has a closed-form\nsolution, and gives a lower bound on the variational\nobjective that can be optimized.\nTo illustrate, consider the case where fjis convex in \u03b8i.\nThen a bound g could be a first-order Taylor expansion\nof fj about the point \u03be, which has a closed-form ex-\npectation. Significantly tighter tractable bounds have\nalso been developed for various frequently occurring\nfunctions (e.g., Marlin et al. (2011), Knowles & Minka\n(2011)). In general, the looser the bound the further\none is from optimizing the variational objective, and\nlearning of \u03c8ican suffer as a result.\n1We have simplified the notation for clarity."},{"page":3,"text":"Variational Bayesian Inference with Stochastic Search\n3. Stochastic search variational Bayes\nWe next present a method based on stochastic search\nfor directly optimizing the variational objective func-\ntion L in cases where some expectations cannot be\ncomputed in the log joint likelihood.\nuses a stochastic approximation of the gradient with\nrespect to the variational parameters of the associated\nq distribution. To further simplify notation, we drop\nall indices; f is the intractable function of \u03b8 (plus other\nvariational parameters), and \u03b8 has a variational distri-\nbution q taking parameters \u03c8.\nThis method\nWe separate the lower bound L into two functions, Ef\nand h, where h(X,\u03a8) contains everything in L except\nfor Ef. Notably, h contains all other functions of \u03c8\nresulting from expectations calculated with respect to\nq. In coordinate ascent variational inference, the first\nstep in optimizing q with respect to its parameters \u03c8\nis to take the gradient of the variational objective,\n\u2207\u03c8L = \u2207\u03c8Eq[f(\u03b8)] + \u2207\u03c8h(X,\u03a8).\nThis gradient contains a tractable term resulting from\n\u2207\u03c8h, and an intractable term \u2207\u03c8Eqf. Our goal is to\nmake a stochastic approximation of this gradient. To\nthis end, assuming the necessary regularity conditions,\nwe rewrite this function as\n?\n=\n\u03b8\n?\nWe use the identity \u2207\u03c8q(\u03b8|\u03c8) = q(\u03b8|\u03c8)\u2207\u03c8lnq(\u03b8|\u03c8).\nIt follows that \u2207\u03c8Eq[f(\u03b8)] = Eq[f(\u03b8)\u2207\u03c8lnq(\u03b8|\u03c8)]. We\ncan stochastically approximate this expectation using\nMonte Carlo integration,\n(4)\n\u2207\u03c8Eq[f(\u03b8)] = \u2207\u03c8\n\u03b8\nf(\u03b8)q(\u03b8|\u03c8)d\u03b8 (5)\n?\nf(\u03b8)\u2207\u03c8q(\u03b8|\u03c8)d\u03b8\n=\n\u03b8\nf(\u03b8)q(\u03b8|\u03c8)\u2207\u03c8lnq(\u03b8|\u03c8)d\u03b8.\n\u2207\u03c8Eq[f(\u03b8)] \u2248\n1\nS\nS\n?\ns=1\nf(\u03b8(s))\u2207\u03c8lnq(\u03b8(s)|\u03c8), (6)\nwhere \u03b8(s)iid\nfore replace \u2207\u03c8Eq[f(\u03b8)] with the unbiased stochastic\napproximation of this gradient in Eq. (6). Denote this\napproximation as \u03b6. At iteration t, we update the vari-\national parameter \u03c8 by taking a gradient step,\n\u223c q(\u03b8|\u03c8) for s = 1,...,S. We can there-\n\u03c8(t+1)= \u03c8(t)+ \u03c1t\u2207\u03c8h(X,\u03a8(t)) + \u03c1t\u03b6t.\nBy decreasing the step size \u03c1tsuch that?\u221e\nlution of L is guaranteed. For example, \u03c1t= (w+t)\u2212\u03b7\nwith \u03b7 \u2208 (0.5,1] and w \u2265 0 satisfies this requirement.\n(7)\nt=1\u03c1t= \u221e\nand?\u221e\nt=1\u03c12\nt< \u221e, convergence to a local optimal so-\n4. Searching with control variates\nA practical issue with the stochastic approximation\nproposed in Sec. 3 is that the variance of the gradient\napproximation may be very large. Given S samples\nof a random vector X, the covariance of its unbiased\nsample mean\u00af X is known to be Cov(\u00af X) = Cov(X)\/S.\nWhen the diagonal values of Cov(X) are large, many\nsamples will be required to bring this variance below\na desired level for approximating the expectation. As\nour experiments will show in Sec. 6, the value of S can\nbe very large in practice and lead to a slow algorithm.\nWe therefore seek a variance reduction method to re-\nduce the number of samples needed to construct the\nstochastic search direction.\nWe introduce a control variate (Ross, 2006) to reduce\nthe variance of the stochastic gradient constructed in\nEq. (6). A control variate is a random variable that\nis highly correlated with an intractable variable, but\nfor which the expectation is tractable. In this case\nthe random variable is f(\u03b8), for which we introduce\na control variate g(\u03b8). Control variates are ideal for\nMFVB because they can leverage the existing bounds,\nthough they also admit a larger class of functions. We\nnext review this variance reduction technique for Ef,\nand discuss the modifications needed to account for\nthe stochastic vector f(\u03b8)\u2207\u03c8lnq(\u03b8|\u03c8).\n4.1. A control variate for f(\u03b8)\nGenerally speaking, variance reduction works by mod-\nifying a function of a random variable such that its ex-\npectation remains the same, but its variance decreases.\nToward this end, we introduce a control variate g(\u03b8),\nwhich approximates f(\u03b8) well in the highly probable\nregions as defined by q(\u03b8), but also has a closed-form\nexpectation under q. Using g and a scalar a \u2208 R, we\nfirst form the new function\u02c6f,\n\u02c6f(\u03b8) = f(\u03b8) \u2212 a(g(\u03b8) \u2212 Eq[g(\u03b8)]).\nThis function has the same expectation as f and there-\nfore can replace it in L in Eq. (3).\nThe next step is to set the value of a to minimize the\nvariance of\u02c6f. A simple calculation shows that\n(8)\nVar(\u02c6f) = Var(f) \u2212 2aCov(f,g) + a2Var(g).\nTaking the derivative with respect to a and setting to\nzero gives the optimal value,\n(9)\na =Cov(f,g)\nVar(g)\n.(10)\nAs is usual, this covariance and variance is unknown\nin the functions we encounter. We can approximate"},{"page":4,"text":"Variational Bayesian Inference with Stochastic Search\nAlgorithm 1 Variational Bayes with stochastic search\nGoal To calculate \u2207\u03c8L = \u2207\u03c8Eq[f(\u03b8)] + \u2207\u03c8h(X,\u03a8).\nApproximate \u2207\u03c8L using stochastic search.\ninput Variance reduction parameter ?.\n1: Introduce the function g(\u03b8) as a control variate\nthat highly correlates with f(\u03b8).\n2: Sample an initial (small) collection \u03b8(s)\u223c q(\u03b8|\u03c8).\n3: Sum the sample variances and covariances\n\u03b2 =?K\n4: Set \u02c6 a = \u03b1\/\u03b2 and S = (\u03b3 \u2212 \u03b12\/\u03b2)\/?K.\n5: Sample \u03b8(s)\u223c q(\u03b8|\u03c8) i.i.d. for s = 1,...,?S?.\n6: Construct the stochastic search vector\n\u03b6 =1\nS\n7: Step in the direction of the stochastic gradient\n\u03c8?= \u03c8 + \u03c1\u03b6 + \u03c1\u2207\u03c8(h(X,\u03a8) + \u02c6 aEq[g(\u03b8)]).\nk=1Var(g\u2202 lnq\n\u03b1 =?K\n\u2202\u03c8k), \u03b3 =?K\nk=1Var(f\u2202 lnq\n\u2202\u03c8k),\nk=1Cov(f\u2202 lnq\n\u2202\u03c8k,g\u2202 lnq\n\u2202\u03c8k).\n?S\ns=1{f(\u03b8(s)) \u2212 \u02c6 ag(\u03b8(s))}\u2207\u03c8lnq(\u03b8(s)|\u03c8).\na with \u02c6 a, found by plugging the sample variance and\ncovariance into Eq. (10) using samples from the algo-\nrithm.\nThe potential reduction in variance is seen by plugging\nEq. (10) into Eq. (9) and taking the ratio of the two\nvariances,\nVar(\u02c6f)\/Var(f) = 1 \u2212 Corr(f,g)2.\nTherefore, the greater the correlation between f and g,\nthe greater the variance reduction. Tight lower bounds\nof f by construction have this high correlation, but we\nnote that tight upper bounds work as well, as do well-\napproximating functions that do not bound f.\n(11)\nUsing the control variate g, we now write the stochastic\napproximation to the gradient as\n\u2207\u03c8Eq[\u02c6f(\u03b8)] \u2248 \u02c6 a\u2207\u03c8Eq[g(\u03b8)]\n+1\nS\ns=1\nwhere \u03b8(s)iid\n\u223c q(\u03b8|\u03c8) for s = 1,...,S.\nWriting the stochastic approximation this way allows\nfor a more intuitive understanding of the algorithm.\nBy separating the tractable and stochastic parts as\ndone in Eq. (12), we first replace the intractable func-\ntion f with a tractable approximation g. (This resem-\nbles the standard method when g lower bounds f.)\nThe gradient of Eg is then corrected by a stochastic\nvector. The variance of the correction is smaller than\nthat of the original stochastic approximation in Sec. 3,\nsince the function f(\u03b8) is close to \u02c6 ag(\u03b8). The gradient\nof Eg can be thought of as an initial guess, followed\nby a stochastic correction which ensures that we are\noptimizing the variational objective function.\n(12)\nS\n?\n{f(\u03b8(s)) \u2212 \u02c6 ag(\u03b8(s))}\u2207\u03c8lnq(\u03b8(s)|\u03c8),\n4.2. The stochastic search case\nWe have introduced a control variate for f(\u03b8), but in\nfact we would like to minimize the variance of the vec-\ntor f(\u03b8)\u2207\u03c8lnq(\u03b8|\u03c8) in Eq. (6). In this case, the con-\ntrol variate becomes g(\u03b8)\u2207\u03c8lnq(\u03b8|\u03c8) and we have the\nfollowing modification.\nLet \u03c8k be the kth dimension of \u03c8.\ndimension the discussion in Sec. 4.1 carries through,\nbut for f\u2202 lnq\n\u2202\u03c8k\n\u2202\u03c8k\nvariance of each dimension again follows Eq. (9), and\nwe seek an a to minimize the sum of these equations.\nThis results in the optimal value\na =?\nwhich we approximate using samples. We summarize\nstochastic search variational Bayes in Algorithm 1.\nThen for each\nand g\u2202 lnq\ninstead of f and g. The\nkCov(f\u2202 lnq\n\u2202\u03c8k,g\u2202 lnq\n\u2202\u03c8k)\/?\nkVar(g\u2202 lnq\n\u2202\u03c8k),\n5. Stochastic search VB for two models\nWe next illustrate stochastic search variational infer-\nence on logistic regression and a finite approximation\nto the hierarchical Dirichlet process (Teh et al., 2007).\nFor logistic regression, we will consider two control\nvariates, one of which is a lower bound and the other\nof which is not a bound. For the finite HDP, we will\nconsider a piecewise control variate, one part being an\nupper bound on the original function.\n5.1. Logistic regression\nBinary logistic regression takes in d-dimensional data\nvectors xnand predicts the class yn\u2208 {\u22121,1} to which\neach belongs. The parameter is \u03b8 \u2208 Rdand the predic-\ntion law is Pr(yn|xn,\u03b8) = \u03c3(ynxT\nsigmoid function, \u03c3(b) = (1+e\u2212b)\u22121. Bayesian logistic\nregression places a prior distribution on the coefficient\nvector, \u03b8 \u223c Normal(0,cI). For inference we define a\nGaussian variational q distribution\nn\u03b8) where \u03c3(\u00b7) is the\nq(\u03b8) = Normal(\u00b5,\u03a3).(13)\nThe variational lower bound for this model is\nL =?N\nThe expectations of fn(yn,xn;\u03b8) := ln\u03c3(ynxT\nintractable. One approach to avoiding this issue is to\nforgo variational inference and use Laplace\u2019s method\nto approximate q. This method sets \u00b5 to the MAP so-\nlution, and \u03a3\u22121to the negative Hessian of the log joint\nlikelihood evaluated at \u00b5. Another is to lower bound\nfnwith the bound in, e.g., Jaakkola & Jordan (2000),\nwhich allows for closed-form variational inference. We\nconsider this bound as a control variate.\nn=1Eq[ln\u03c3(ynxT\nn\u03b8)]+Eq[lnp(\u03b8)\u2212lnq(\u03b8)]. (14)\nn\u03b8) are"},{"page":5,"text":"Variational Bayesian Inference with Stochastic Search\n\u221210\u22128\u22126 \u22124\u2212202\n\u22120.5\n0\n0.5\n1\n|| | | | | || ||| | |||| || | | || || ||| | | ||| | | || ||||||||||||||| | || || | |||||| | || | || | || || | | || ||||| |||||||| ||| | |||||\n\u2212202468\n\u22121\n\u22120.5\n0\n0.5\n1\n|| | ||||||||||||||||||||||| | ||||||| | ||||||||||||| ||||||||| | ||| | |||||| |||||| | |||||||||||||||||||||||||\nJaakkola bound\nTaylor expansion\n\ufffd\nFigure 1. Approximation error between ln\u03c3(\u03b8) and the two\ncontrol variates considered. The mean and variance of q\nused in these examples are (left) \u00b5 = 3, \u03c32= 3 and (right)\n\u00b5 = \u22125, \u03c32= 1. We show 100 samples from these q distri-\nbutions, at which points the functions would be evaluated\nfor the stochastic gradient (for a = 1). The Taylor expan-\nsion is closer to the true function at the region of interest\nas defined by q. The benefit of this is that fewer samples\nwill be necessary to approximate the gradient.\nA lower bound control variate.\nfor fn developed by Jaakkola & Jordan (2000) is a\nuseful control variate for variational logistic regression.\nFor each pair (xn,yn), this bound takes an auxiliary\nparameter \u03ben> 0 and has the form\nThe lower bound\ngn(yn,xn;\u03b8,\u03ben) = ln\u03c3(\u03ben) +1\n2(ynxT\nn\u03b8 \u2212 \u03ben)\nn\u03b8)2\u2212 \u03be2\n\u2212 \u03bb(\u03ben)((xT\nn). (15)\nWe have \u03bb(\u03ben) = (2\u03c3(\u03ben) \u2212 1)\/(4\u03ben). We select this\nbound for illustrative purposes, but any lower bound\nwill work in principle. For a multivariate Gaussian q\ndistribution, having a quadratic term in g is essential\nfor stochastically learning a full covariance matrix. In\ngeneral, tighter bounds will require fewer samples, but\nfor some functions finding tight bounds may require\nmuch effort. We next consider a general purpose con-\ntrol variate that can help in this case.\nControl variate delta method.\nthe second-order Taylor expansion of f as a control\nvariate. The second-order Taylor expansion often ac-\ncurately approximates a function of interest, and when\nused alone is known as the delta method. In addition\nto accuracy, the quadratic approximation of the delta\nmethod results in a function for which the expectation\nwith respect to q is very likely to be analytic.\nWe also consider\nThe delta method arguably should not be used for\nmean-field variational inference because the second-\norder Taylor expansion is not a lower bound. On the\nother hand, the first-order Taylor expansion often is a\nlower bound. Therefore, though their bounds are typ-\nically loose, first-order approximations are commonly\nemployed for MFVB. An advantage of the proposed\nstochastic search algorithm is that second-order meth-\nods can now be used as a control variate to (i) more\naccurately approximate the function of interest, and\n(ii) significantly reduce the variance of the stochastic\ngradient. We call this approach of using Taylor expan-\nsion control variates the control variate delta method.\nWe consider a second-order Taylor expansion at \u02c6 \u00b5,\nthe current mean of q, for approximating ln\u03c3(ynxT\nLetting \u03c3n:= \u03c3(ynxT\nn\u03b8).\nn\u02c6 \u00b5), this control variate is\ngn(yn,xn;\u03b8, \u02c6 \u00b5) = ln\u03c3n+ yn(1 \u2212 \u03c3n)(\u03b8 \u2212 \u02c6 \u00b5)Txn\n\u22121\n(16)\n2\u03c3n(1 \u2212 \u03c3n)(\u03b8 \u2212 \u02c6 \u00b5)TxnxT\nn(\u03b8 \u2212 \u02c6 \u00b5).\nAs with the Jaakkola & Jordan (2000) bound, this\ncontrol variate contains a quadratic term that helps in\nlearning the covariance matrix of q.\nWe compare these control variates in Figure 1. In these\nplots we show the difference fn\u2212 gnfor two specific q\ndistributions, and with x = 1. We also show 100 sam-\nples from q, which indicates the regions where these\nfunctions would be evaluated (for a = 1). The plots\nshow that the second-order Taylor expansion approxi-\nmates fnsignificantly better where it matters; we sup-\nport this conclusion with the experiments in Sec. 6.\n5.2. Hierarchical Dirichlet processes\nWe also investigate a stochastic search VB algorithm\nfor an approximation to the hierarchical Dirichlet pro-\ncess (Teh et al., 2007).We focus on the two-level\ngenerative structure using finite dimensional Dirichlet\npriors as an approximation to the infinite dimensional\nprocess\u2014in the limit the HDP is recovered. In this\nfinite process, a top-level Dirichlet-distributed proba-\nbility vector \u03b8 parameterizes the Dirichlet distribution\nfor d = 1,...,D second-level probability vectors,\n(\u03c0d1,...,\u03c0dK)\n(\u03b81,...,\u03b8K) \u223c Dirichlet(\u03b1\nIn topic models, these \u03c0d vectors are often used as\ndistributions on word distributions. In this section,\nwe focus solely on the generic hierarchical structure in\nEq. (17). We define the approximate posterior of \u03b8 as\niid\n\u223c Dirichlet(\u03b2\u03b81,...,\u03b2\u03b8K),\nK,...,\u03b1\nK). (17)\nq(\u03b8) = Dirichlet(c1,...,cK). (18)\nThe part of the lower bound associated with \u03b8 is\nL\u03b8 =?\nThe expectation Eq[ln\u0393(\u03b2\u03b8k)] is intractable for each\nk. We use a stochastic approximation, and introduce\ntwo control variates for this function, depending on\nthe current expected value of \u03b2\u03b8k.\nk\u03b2Eq[\u03b8k]?\ndEq[ln\u03c0dk] \u2212?\nkDEq[ln\u0393(\u03b2\u03b8k)]\n+Eq[lnp(\u03b8) \u2212 lnq(\u03b8)]. (19)"},{"page":6,"text":"Variational Bayesian Inference with Stochastic Search\n01234567\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n0 0.51 1.52 2.5\n\u22121.5\n\ufffd\ufffd\n\u22121\n\u22120.5\n0\n0.5\n-ln\ufffd\ufffd\ufffd\ufffd\ufffd\n-ln\ufffd\ufffd\ufffd\ufffd\ufffd-ln\ufffd\ufffd\nFigure 2. (left) The intractable function in the HDP.\n(right) the difference after introducing a control variate and\nsetting a = 1. Since \u2212ln\u0393(\u03b2\u03b8)\u2212ln\u03b2\u03b8 = \u2212ln\u0393(\u03b2\u03b8+1), the\nright figure is the left figure shifted by one unit to the left\nand truncated at zero. The very large variance near zero\n(where most values of \u03b2\u03b8 will lie) has been significantly re-\nduced. For larger values of \u03b2\u03b8, we use a first-order Taylor\napproximation at \u03b2Eq\u03b8 of the nearly linear function.\nAs \u03b2\u03b8k approaches zero, the function fk(\u03b2\u03b8k) =\n\u2212ln\u0393(\u03b2\u03b8k) diverges to \u2212\u221e. By construction of the\nDirichlet prior, many values of \u03b8kwill be very small.\n(In the infinite limit, there are an infinite number of\nsuch values smaller than any \u03b4 > 0.) The variance in\nthis region is massive\u2014when computer precision be-\ncomes an issue it can be infinite (see Figure 2).\nWe propose the control variate gk(\u03b2\u03b8k) = ln\u03b2\u03b8k, with\nEq[ln\u03b8k] = \u03c6(ck)\u2212\u03c6(?\nwith fk, but if a = 1, lim\u03b2\u03b8k\u21920fk\u2212agk= 0, as shown\nin Figure 2. This results from the equality\njcj) where \u03c6(\u00b7) is the digamma\nfunction. This control variate not only correlates well\n\u2212ln\u0393(\u03b2\u03b8k) \u2212 ln\u03b2\u03b8k= \u2212ln\u0393(\u03b2\u03b8k+ 1).\nFor all other values of a, this equality does not hold,\nand the difference fk\u2212 agkdiverges as \u03b2\u03b8k\u2192 0. For\nthis model, we can thus give the optimal value of a in\nadvance, and we set a = 1.\n(20)\nFrom Figure 2, we also see that the approximation gets\nworse when \u03b2\u03b8kgets large, which can occur for a few\nhighly probable dimensions when \u03b2 is large. Since fk\nis approximately linear in this regime, we use a first-\norder Taylor expansion of fk about the mean\u00af\u03b8k =\nEq[\u03b8k] as a control variate. This gives the following\ntwo control variates,\ngk = ln\u03b2\u03b8k,\ngk = \u2212ln\u0393(\u03b2\u00af\u03b8k) \u2212 \u03b2(\u03b8k\u2212\u00af\u03b8k)\u03c6(\u03b2\u00af\u03b8k),\nSince fk is concave, this second control variate is an\nupper bound on L\u03b8without the stochastic correction.\nWe discuss the boundaries \u03ba1and \u03ba2in Sec. 6.\n0 < \u03b2\u00af\u03b8k< \u03ba1, (21)\n\u03b2\u00af\u03b8k> \u03ba2.\nThus far, we\u2019ve focused mainly on reducing the vari-\nance induced by fk, but in Sec. 4.2 we noted that\n\u2207lnq introduces variance to the Monte Carlo integral\nas well. This suggests that we should look at other\nparts of the integral for potential variance reduction.\nWe briefly show how this can be done for the HDP.\nThe lower bound in Eq. (19) contains a sum of K in-\ntractable integrals over the probability simplex \u2206K.\nWe perform separate stochastic approximations of\neach gradient. Using the fact that each gamma func-\ntion is over a single dimension of the simplex, for a\nfunction of \u03b8kthe variables \u03b8i?=kwill integrate out. In\nthis case, marginalizing a Dirichlet distribution to a\nsingle dimension yields a beta distribution. That is,\n?\n\u03b8\u2208\u2206K\nln\u0393(\u03b2\u03b8k)q(\u03b8|c)d\u03b8 =\n?1\n0\nln\u0393(\u03b2\u03b8k)q?\nk(\u03b8k|c)d\u03b8k,\nwhere q?\nk(\u03b8k|c) = Beta(\u03b8k|ck,?\nWe can choose which of these integrals to stochas-\ntically approximate for gradient ascent.\nthe stochastic gradient using q?\ncantly less variance than for q since \u03b8(s)\nzero; the vector \u2207clnq?\nln(1\u2212\u03b8(s)\nln\u03b8(s)\ni\n\u2212 Eq[ln\u03b8i] for i = 1,...,K when using \u2207clnq.\ni?=kci).\nHowever,\nkresults in signifi-\nwill be near\nkhas K \u2212 1 entries containing\nk)\u2212Eq[ln(1\u2212\u03b8k)], while these values will be\nk\n6. Experiments\nWe perform experiments using stochastic search VB\nfor binary classification with logistic regression and for\ntopic modeling with the approximate HDP. We next\ngive the details of the experiments we perform and the\ndata sets and algorithms used for comparison.\nData and set-up.\nfive data sets from the UCI repository: Iris, Pima,\nSPECTF, Voting and WDBC. These data sets range\nfrom 150 to 768 labeled examples living in 5 to 45 di-\nmensions, including a dimension of all ones to account\nfor offset. We perform experiments with stochastic\nsearch variational inference using the two control vari-\nates discussed in Sec. 5.1. We compare with two ad-\nditional methods for posterior approximation: varia-\ntional inference with the Jaakkola & Jordan (2000)\nbound and Laplace\u2019s method.\nmance on the true variational objective function in Eq.\n(14) using each posterior approximation.\nFor logistic regression, we use\nWe evaluate perfor-\nFor the HDP topic model, we use 8,000 documents\nwith 3,012 vocabulary size from The New York Times.\nWe compare with (i) a point estimate of the top-level\nprobability vector using a delta q distribution, and (ii)\nfixing the top-level distribution to the uniform vector,\nwhich is equivalent to LDA (Blei et al., 2003). We\nperform experiments for different corpus sizes, differ-\nent values of \u03b2, and we set K = 200."},{"page":7,"text":"Variational Bayesian Inference with Stochastic Search\n0 50100150200\n0\n0.05\n0.1\n0.15\n0 50\nIteration number\n100 150200\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n# samples\nvariance reduction\nSPECTF\nVOTE WDBC\nTaylor CV\nJ&J Bound CV\na^\n0 50100 150200\n0\n5\n10\n15\nx10-3\n0 50100 150200\n0.7\n0.8\n0.9\n1\n1.1\n0 50100150 200\n0\n0.05\n0.1\n0.15\n0.2\n0 50100150 200\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n0 50 100150 200\n102\n104\n106\n108\n0 50100150200\n102\n104\n106\n108\n0 50 100150200\n102\n104\n106\n108\nFigure 3. Experimental results for variational logistic regression. We compare the variance reduction obtained by the two\ncontrol variates under consideration. (top row) The number of samples per iteration setting ? = 0.1 in Algorithm 1. The\nyellow and black lines represent the estimated number that would be required without variance reduction according to\neach control variate. As expected, these curves overlap. (middle row) The variance reduction factor of Eq. (11). The\nselected control variates significantly reduce the variance. The second-order Taylor control variate is significantly better\nthan the lower bound. (bottom row) The optimal scaling factor estimated from samples.\nTable 1. Optimizing the variational objective function.\nThe stochastic search methods (indicated by CV) signif-\nicantly outperform the other methods toward this end. All\nvalues were calculated for the true lower bound in Eq. (14)\nusing their respective posterior approximations.\nmodel|data\nTaylor CV-7.9-3974\nJ&J CV-7.9 -3974\nLaplace-11.9-3985\nJ&J bnd -11.5-3976\niris pima spectf\n-165\n-165\n-170\n-173\nvote\n-67.8\n-67.6\n-70.5\n-74.6\nwdbc\n-74.6\n-74.8\n-80.0\n-86.2\nTable 2. Running time of each algorithm on each data set.\nWe use the approximated number of samples required with-\nout a control variate to estimate the last value. The times\nare given in milliseconds (ms), seconds (s), minutes (m),\nhours (hr) and years (yr).\nmodel|data\nTaylor CV0.33s1.7m\nJ&J CV0.42s 18m\nLaplace 21ms 29ms\nJ&J bnd64ms 88ms\nSS no CV2.4s>2yr\nirispimaspectf\n20s\n1.2m\n94ms\n0.13s\n6.6hr\nvote\n17s\n1.2m\n20ms\n97ms\n9hr\nwdbc\n11s\n2.3m\n0.10s\n0.15s\n1.4hr\nLogistic regression results.\nthe variational lower bound for each model on each\ndata set. Since all algorithms return an approxima-\ntion of the posterior distribution on the vector \u03b8, this\ncomparison is meaningful and gives a measure of how\nclose each posterior is to the true posterior. We see\na considerable improvement for the stochastic algo-\nrithms (denoted by their control variate). Since both\nstochastic algorithms optimize the same objective, the\nperformance should be the same.\nIn Table 1 we show\nWe show performance details of the stochastic search\nVB algorithm in Figure 3 and Table 2. In Figure 3,\nwe show the number of samples, the variance reduction\nfactor and the scaling \u02c6 a as a function of iteration. We\nsee that the control variates provide a major reduction\nin variance. Also, the Taylor expansion control vari-\nate (i.e., control variate delta method) requires signif-\nicantly fewer samples than the bound control variate,\nwhich benefits the running time (see Table 2). While\nthe non-sampling methods are faster, control variates\nmake stochastic search VB a viable inference method\nwhen compared to the base algorithm of Sec. 3."},{"page":8,"text":"Variational Bayesian Inference with Stochastic Search\nTable 3. The fraction of times that algorithm ?row? was\nranked ?column? for the 32 different parameter\/data size\npairs using the variational lower bound.\nmodel | rank\nHDP-stochastic\nHDP-point\nLDA\n1st\n0.66\n0.34\n0\n2nd\n0.31\n0.66\n0.03\n3rd\n0.03\n0\n0.97\n\ufffd\n\ufffd\nc ln q(\ufffd|c)\nc ln qk' (\ufffd|c)\nnumber of docs (x1000)\n# samples\n\ufffd\ufffd\ufffd\ufffd\ufffd\n12345678\n101\n102\n103\n104\n105\n12345678\n102\n103\n104\n105\n\ufffd\ufffd\ufffd\ufffd\ufffd\nFigure 4. Average number of samples per iteration for the\ntwo equivalent gradient approximations, \u2207clnq vs \u2207clnq?\nwhere q is the Dirichlet and q?\nkthe beta distribution. Sam-\npling is further reduced (see text for discussion).\nk,\nHierarchical Dirichlet process results.\ntopic models to The New York Times using different\nnumbers of documents (D = 1000 to 8000) and concen-\ntration parameter values \u03b2 \u2208 {1,5,10,15}. As switch\npoints for the two control variates, we set \u03ba1= 1 and\n\u03ba2 = 2. We summarize our results in Table 3. In\ngeneral, fitting a variational posterior on the top-level\nDirichlet vector yielded a better posterior approxima-\ntion than a point estimate and a \u03b8 fixed as uniform.\nHowever, this improvement was not as dramatic as for\nlogistic regression.\nWe fit\nIn Figure 4, we show the number of samples required\nfrom the Dirichlet q distribution to approximate the\nstochastic integral. We compare the two methods dis-\ncussed in Sec. 5.2 for reducing the variance of the\nstochastic vector \u2207clnq by instead using \u2207clnq?\nsee a significant reduction in the number of samples.\nExperiments without control variates were not possi-\nble due to computer precision issues and the massive\nvariance of ln\u0393(\u03b2\u03b8) near zero.\nk. We\n7. Conclusion\nWe have presented stochastic search variational Bayes,\na method for optimizing intractable variational ob-\njective functions such as those arising from non-\nconjugacy. The algorithm relies on a stochastic ap-\nproximation of the gradient; we showed how control\nvariates can significantly reduce the variance of this\nMonte Carlo integral. Since existing lower bounds can\nbe recast as control variates, our approach is relevant\nto many existing MFVB algorithms. However, a lack\nof restrictions on control variates allows for other types\nof function approximations when a good bound is not\nreadily available. We introduced the control variate\ndelta method toward this end.\nAcknowledgements\nONR grant number N00014-11-1-0688 under the MURI\nprogram. D.B. is supported by ONR N00014-11-1-0651,\nNSF CAREER 0745520, AFOSR FA9550-09-1-0668, the\nAlfred P. Sloan foundation, and a grant from Google.\nJ.P. and M.J. are supported by\nReferences\nBeal, M.J.\nBayesian Inference. PhD thesis, Gatsby Computational\nNeuroscience Unit, University College London, 2003.\nVariational Algorithms for Approximate\nBlei, D. and Jordan, M. Variational inference for Dirichlet\nprocess mixtures. Bayesian Analysis, 1:121\u2013144, 2006.\nBlei, D., Ng, A., and Jordan, M. Latent Dirichlet alloca-\ntion. Journal of Machine Learning Research, 3:993\u20131022,\n2003.\nGraves, A. Practical variational inference for neural net-\nworks. In Neural Information Processing Systems, 2011.\nHoffman, M., Blei, D., and Bach, F. Online learning for\nlatent Dirichlet allocation. In Neural Information Pro-\ncessing Systems, 2010.\nJaakkola, T. and Jordan, M.I. Bayesian parameter estima-\ntion via variational methods. Statistics and Computing,\n10:25\u201337, 2000.\nJordan, M.I., Ghahramani, Z., Jaakkola, T., and Saul,\nL.K. An introduction to variational methods for graph-\nical models. Machine Learning, 37:183\u2013233, 1999.\nKnowles, D.A. and Minka, T.P. Non-conjugate variational\nmessage passing for multinomial and binary regression.\nIn Neural Information Processing Systems, 2011.\nLeisink, M.A.R. and Kappen, H.J. A tighter bound for\ngraphical models.Neural Computation, 13(9):2149\u2013\n2171, 2001.\nMarlin, B., Khan, E., and Murphy, K. Piecewise bounds\nfor estimating Bernoulli-logistic latent Gaussian models.\nIn International Conference on Machine Learning, 2011.\nPaisley, J. and Carin, L. Nonparametric factor analysis\nwith beta process priors. In International Conference\non Machine Learning, 2009.\nRoss, S.M. Simulation. Academic Press, San Diego, 4th\nedition, 2006.\nTeh, Y., Jordan, M., Beal, M., and Blei, D. Hierarchical\nDirichlet processes. Journal of the American Statistical\nAssociation, 101(476):1566\u20131581, 2007.\nWang, C., Paisley, J., and Blei, D. Online variational infer-\nence for the hierarchical Dirichlet process. In Artificial\nIntelligence and Statistics, 2011.\nYi, S., Wierstra, D., Schaul, T., and Schmidhuber, J.\nStochastic search using the natural gradient. In Inter-\nnational Conference on Machine Learning, 2009."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf","widgetId":"rgw28_56ab1c94bcd25"},"id":"rgw28_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=228095632&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab1c94bcd25"},"id":"rgw29_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=228095632&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":228095632,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":228095632,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":75272153,"url":"researcher\/75272153_Evan_Archer","fullname":"Evan Archer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":16191178,"url":"researcher\/16191178_Il_Memming_Park","fullname":"Il Memming Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085695295,"url":"researcher\/2085695295_Lars_Buesing","fullname":"Lars Buesing","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2085800824,"url":"researcher\/2085800824_John_Cunningham","fullname":"John Cunningham","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284579582_Black_box_variational_inference_for_state_space_models","usePlainButton":true,"publicationUid":284579582,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284579582_Black_box_variational_inference_for_state_space_models","title":"Black box variational inference for state space models","displayTitleAsLink":true,"authors":[{"id":75272153,"url":"researcher\/75272153_Evan_Archer","fullname":"Evan Archer","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":16191178,"url":"researcher\/16191178_Il_Memming_Park","fullname":"Il Memming Park","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085695295,"url":"researcher\/2085695295_Lars_Buesing","fullname":"Lars Buesing","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2085800824,"url":"researcher\/2085800824_John_Cunningham","fullname":"John Cunningham","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39697633,"url":"researcher\/39697633_Liam_Paninski","fullname":"Liam Paninski","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Latent variable time-series models are among the most heavily used tools from\nmachine learning and applied statistics. These models have the advantage of\nlearning latent structure both from noisy observations and from the temporal\nordering in the data, where it is assumed that meaningful correlation structure\nexists across time. A few highly-structured models, such as the linear\ndynamical system with linear-Gaussian observations, have closed-form inference\nprocedures (e.g. the Kalman Filter), but this case is an exception to the\ngeneral rule that exact posterior inference in more complex generative models\nis intractable. Consequently, much work in time-series modeling focuses on\napproximate inference procedures for one particular class of models. Here, we\nextend recent developments in stochastic variational inference to develop a\n`black-box' approximate inference technique for latent variable models with\nlatent dynamical structure. We propose a structured Gaussian variational\napproximate posterior that carries the same intuition as the standard Kalman\nfilter-smoother but, importantly, permits us to use the same inference approach\nto approximate the posterior of much more general, nonlinear latent variable\ngenerative models. We show that our approach recovers accurate estimates in the\ncase of basic models with closed-form posteriors, and more interestingly\nperforms well in comparison to variational approaches that were designed in a\nbespoke fashion for specific non-conjugate models.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284579582_Black_box_variational_inference_for_state_space_models","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Evan_Archer\/publication\/284579582_Black_box_variational_inference_for_state_space_models\/links\/565cca6b08ae1ef92981f7a5.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Evan_Archer","sourceName":"Evan William Archer","hasSourceUrl":true},"publicationUid":284579582,"publicationUrl":"publication\/284579582_Black_box_variational_inference_for_state_space_models","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/284579582_Black_box_variational_inference_for_state_space_models\/links\/565cca6b08ae1ef92981f7a5\/smallpreview.png","linkId":"565cca6b08ae1ef92981f7a5","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=284579582&reference=565cca6b08ae1ef92981f7a5&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1c94bcd25"},"id":"rgw33_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=284579582&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"565cca6b08ae1ef92981f7a5","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095632,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284579582_Black_box_variational_inference_for_state_space_models\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Typically at least some terms of eq. 3 cannot be integrated in closed form. While it is often possible to estimate the gradient by sampling directly from q(z|x), in general the approximate gradient exhibits high variance (Paisley et al., 2012). One approach to addressing this difficulty, independently proposed by Kingma & Welling (2013), Rezende et al. (2014) and Titsias & L\u00e1zaro-Gredilla (2014), is to compute the integral using the \" reparameterization trick \" : choose an easy-to-sample random variable with distribution p() and parameterize z through a function g of observations x and parameters \u03c6, "],"widgetId":"rgw34_56ab1c94bcd25"},"id":"rgw34_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab1c94bcd25"},"id":"rgw32_56ab1c94bcd25","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=284579582&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69632150,"url":"researcher\/69632150_Yutian_Chen","fullname":"Yutian Chen","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Mar 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","usePlainButton":true,"publicationUid":273388187,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","title":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data","displayTitleAsLink":true,"authors":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69632150,"url":"researcher\/69632150_Yutian_Chen","fullname":"Yutian Chen","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"Multivariate categorical data occur in many applications of machine learning.\nOne of the main difficulties with these vectors of categorical variables is\nsparsity. The number of possible observations grows exponentially with vector\nlength, but dataset diversity might be poor in comparison. Recent models have\ngained significant improvement in supervised tasks with this data. These models\nembed observations in a continuous space to capture similarities between them.\nBuilding on these ideas we propose a Bayesian model for the unsupervised task\nof distribution estimation of multivariate categorical data. We model vectors\nof categorical variables as generated from a non-linear transformation of a\ncontinuous latent space. Non-linearity captures multi-modality in the\ndistribution. The continuous representation addresses sparsity. Our model ties\ntogether many existing models, linking the linear categorical latent Gaussian\nmodel, the Gaussian process latent variable model, and Gaussian process\nclassification. We derive inference for our model based on recent developments\nin sampling based variational inference. We show empirically that the model\noutperforms its linear and discrete counterparts in imputation tasks of sparse\ndata.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Yutian_Chen3","sourceName":"Yutian Chen","hasSourceUrl":true},"publicationUid":273388187,"publicationUrl":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735\/smallpreview.png","linkId":"565d728808aefe619b25b735","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=273388187&reference=565d728808aefe619b25b735&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1c94bcd25"},"id":"rgw36_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=273388187&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"565d728808aefe619b25b735","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095632,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Instead we use recent developments in sampling-based variational inference (Blei et al., 2012) to avoid integrating the latent variables with the Softmax analytically. Our approach takes advantage of this tool to obtain simple yet powerful model and inference. "],"widgetId":"rgw37_56ab1c94bcd25"},"id":"rgw37_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab1c94bcd25"},"id":"rgw35_56ab1c94bcd25","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=273388187&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2077568820,"url":"researcher\/2077568820_Michalis_K_Titsias","fullname":"Michalis K. Titsias","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Mar 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference","usePlainButton":true,"publicationUid":273158161,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference","title":"Local Expectation Gradients for Doubly Stochastic Variational Inference","displayTitleAsLink":true,"authors":[{"id":2077568820,"url":"researcher\/2077568820_Michalis_K_Titsias","fullname":"Michalis K. Titsias","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We introduce local expectation gradients which is a general purpose\nstochastic variational inference algorithm for constructing stochastic\ngradients through sampling from the variational distribution. This algorithm\ndivides the problem of estimating the stochastic gradients over multiple\nvariational parameters into smaller sub-tasks so that each sub-task exploits\nintelligently the information coming from the most relevant part of the\nvariational distribution. This is achieved by performing an exact expectation\nover the single random variable that mostly correlates with the variational\nparameter of interest resulting in a Rao-Blackwellized estimate that has low\nvariance and can work efficiently for both continuous and discrete random\nvariables. Furthermore, the proposed algorithm has interesting similarities\nwith Gibbs sampling but at the same time, unlike Gibbs sampling, it can be\ntrivially parallelized.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1503.01494","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":273158161,"publicationUrl":"publication\/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference\/links\/55079dcb0cf27e990e07b9d1\/smallpreview.png","linkId":"55079dcb0cf27e990e07b9d1","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=273158161&reference=55079dcb0cf27e990e07b9d1&eventCode=&origin=publication_list","widgetId":"rgw39_56ab1c94bcd25"},"id":"rgw39_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=273158161&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":228095632,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/273158161_Local_Expectation_Gradients_for_Doubly_Stochastic_Variational_Inference\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["where each x (s) is an independent draw from q v (x). While this estimate is unbiased, it has been observed to severely suffer from high variance so that in practice it is necessary to consider variance reduction techniques such as those based on control variates [7] [8] [6]. Despite this limitation the above framework is very general as it can deal with any variational distribution over both discrete and continuous latent variables. "],"widgetId":"rgw40_56ab1c94bcd25"},"id":"rgw40_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw38_56ab1c94bcd25"},"id":"rgw38_56ab1c94bcd25","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=273158161&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":228095632,"publicationLink":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw31_56ab1c94bcd25"},"id":"rgw31_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=228095632&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=21","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":21,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw30_56ab1c94bcd25"},"id":"rgw30_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=228095632&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"53fe1e670cf21edafd14cb6c","name":"Michael Jordan","date":"Aug 27, 2014 ","nameLink":"profile\/Michael_Jordan13","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"d07bedf8e6842489f65abf5f3904ce25","showFileSizeNote":false,"fileSize":"575.01 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"53fe1e670cf21edafd14cb6c","name":"Michael Jordan","date":"Aug 27, 2014 ","nameLink":"profile\/Michael_Jordan13","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"d07bedf8e6842489f65abf5f3904ce25","showFileSizeNote":false,"fileSize":"575.01 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=j7YDvWm-OWTzxlhshT3ODhST5off3lHYx-CendWhqyVccYH57TUe-P3DXH_YyOh6GmQlQAshY9Z8mdk0qYmfoQ.XHiOTQIbZr4rbXeaRx7z4tCvCKisafIbEe-OYINU9bL8xqumA00wboHvmRyRlVtMbeYRbW8ibvuxByjY3lHSlg","clickOnPill":"publication.PublicationFigures.html?_sg=TEOEBcpxC4ZAG513xVsmMhIkexEOxBnp5ntLwMAc0PXtd2EvidmzAcXFgINFYQp0389FnQ7Nko_C5_TcSrdS3A.dnPmQ-73Ro8mWKZjjT3f0RGfZ_paS8Ca-g0XUvgw5OR8yPeC3r5wxLC66rhD1-CyiRSq0jtjdXc3K2GlEUn1EA"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Jordan13%2Fpublication%2F228095632_Variational_Bayesian_Inference_with_Stochastic_Search%2Flinks%2F53fe1e670cf21edafd14cb6c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=ZTGUgaDdfuJzvqkPyuG1Xf_DRccRV5VAzjYe-OyLauVxRq9Y7qqq2W_0vXZAg9EUEwKlNCZUH5h9KPLO4t9qxw","urlHash":"b9b3c60f303b1d94fc55f42163f3e38d","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=WSwD-s-JbIGGUBvHJKxQiHnDmOj6fox34VA3BaU54ZCpklY7lNpP7x3TBn-VDrSGof9FPPdQ2ZOpeP4ZDONoqiPdFydDJ_J-XWDxPJ1EnRc.OkxwsOY-G2ScPeS5yCUPuZMlZtnHcPxucS1d9TEcQZapcqJ76W1CvP5XmJMclw21mo2vNYuSMJqGX5vjLxqMWQ.V3KSnRhmmh2VEbAUiV_4wJALKUCzZ7V2j0VIp3UVN5fWOwcX2sOIX35T15hnRWJ-RN4tPlLohaOvTLzuISGxiA","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"53fe1e670cf21edafd14cb6c","trackedDownloads":{"53fe1e670cf21edafd14cb6c":{"v":false,"d":false}},"assetId":"AS:134852942766080@1409162855609","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":228095632,"commentCursorPromo":null,"widgetId":"rgw42_56ab1c94bcd25"},"id":"rgw42_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMichael_Jordan13%2Fpublication%2F228095632_Variational_Bayesian_Inference_with_Stochastic_Search%2Flinks%2F53fe1e670cf21edafd14cb6c.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A134852942766080%401409162855609&publicationUid=228095632&linkId=53fe1e670cf21edafd14cb6c&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Variational Bayesian Inference with Stochastic Search","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=HQ51DiJ-PQ94522pJDpMMifNi4KTAVw_tsC2Fx4Vl5t1FvygzUnIcDUTVtmgwC9Ygb4EHOGwcASQWkF4fgo3REesPDza_H88E1Gkh5GKR7o.dxL5RDDv58hUQlGS1jc2Jbrpl_KwSNY6FQ1NjUE_JPKDitN2MlepBPwkyNUy2pybZAn1Mb10al_Wou2jJIxANw.g-77XtDt6MHKaYO78JtNRlOrlBkitKBC_tjv7rWLSrq_Kqs4zgaonSPvH1zUly-01oi9YogHvGiD8GyKkVM9Ag","publicationUid":228095632,"trackedDownloads":{"53fe1e670cf21edafd14cb6c":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw44_56ab1c94bcd25"},"id":"rgw44_56ab1c94bcd25","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw45_56ab1c94bcd25"},"id":"rgw45_56ab1c94bcd25","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw46_56ab1c94bcd25"},"id":"rgw46_56ab1c94bcd25","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw47_56ab1c94bcd25"},"id":"rgw47_56ab1c94bcd25","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw48_56ab1c94bcd25"},"id":"rgw48_56ab1c94bcd25","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw43_56ab1c94bcd25"},"id":"rgw43_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw41_56ab1c94bcd25"},"id":"rgw41_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1c94bcd25"},"id":"rgw2_56ab1c94bcd25","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":228095632},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=228095632&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1c94bcd25"},"id":"rgw1_56ab1c94bcd25","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"VpYgsz6tIaCUugV3JPPGD2pmFqgryTh64fhefVsgl6HoCrxhTXBSBBJASip1O7DIs9XwkZHIkUiTiqZW\/\/ThS5wFfml+\/MM1DRG4aJGlDsHq3hvYzTTjxe7JnZNMgzaMQJniY2BtxZwHBYaXRdtha7Bj7EA8f+L8di+RwGsieSB\/ahjbM5irFm2pg3kdRXnompJ5hUGUerAFlJJ6\/B7wfwtLK0cyH6sbF5ixCtbHmApoPSSSx4QDvSWzir5tebCLXEsINucf05IAyI7fjXjYSN9LEs4eF3yCzTRS0s3Xcew=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Variational Bayesian Inference with Stochastic Search\" \/>\n<meta property=\"og:description\" content=\"Mean-field variational inference is a method for approximate Bayesian\nposterior inference. It approximates a full posterior distribution with a\nfactorized set of distributions by maximizing a...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\" \/>\n<meta property=\"rg:id\" content=\"PB:228095632\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Variational Bayesian Inference with Stochastic Search\" \/>\n<meta name=\"citation_author\" content=\"John Paisley\" \/>\n<meta name=\"citation_author\" content=\"David Blei\" \/>\n<meta name=\"citation_author\" content=\"Michael Jordan\" \/>\n<meta name=\"citation_publication_date\" content=\"2012\/06\/27\" \/>\n<meta name=\"citation_volume\" content=\"2\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Michael_Jordan13\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\/links\/53fe1e670cf21edafd14cb6c.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-c059dd7a-cbcb-4eab-a3b0-2dba9d8722a7","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":920,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw49_56ab1c94bcd25"},"id":"rgw49_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-c059dd7a-cbcb-4eab-a3b0-2dba9d8722a7", "76b09011bdab344b8d0306a4831d0dda812aac41");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-c059dd7a-cbcb-4eab-a3b0-2dba9d8722a7", "76b09011bdab344b8d0306a4831d0dda812aac41");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw50_56ab1c94bcd25"},"id":"rgw50_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/228095632_Variational_Bayesian_Inference_with_Stochastic_Search","requestToken":"In8WasW+06UKlPbo82Iu7ekK+CP5jrzi5JAZrBY3sreE+JiumgdjF7NtozUbQ9Z4A68dJk\/DDCxb9EgFilKy5pM8co9YJVamsY5bdAGMXAc3e5PKohyV9vWo1UwLfNGjlezEu8\/235AJ2NlZPDmwzRcYNb9WzHZ8mBtmsqrtywbLbx4qaamq+If7NiU8uQuGc+r19SeFuosI2tcipwO5huhFHBAeMePX1bOaPZmW6vKX7gknIBU6ujWX\/RjTV1pp8oIhWLpSfCLBA67wMj4Wp6+lXyZPnp5anBApE3ZHtoM=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=05W45Kqu6oa0XMnQdji2yaoV1fORA1K7WxQCCFpUts0nLF561CfsUbvkFVA6yBGs","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjI4MDk1NjMyX1ZhcmlhdGlvbmFsX0JheWVzaWFuX0luZmVyZW5jZV93aXRoX1N0b2NoYXN0aWNfU2VhcmNo","signupCallToAction":"Join for free","widgetId":"rgw52_56ab1c94bcd25"},"id":"rgw52_56ab1c94bcd25","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw51_56ab1c94bcd25"},"id":"rgw51_56ab1c94bcd25","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw53_56ab1c94bcd25"},"id":"rgw53_56ab1c94bcd25","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
