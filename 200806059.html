<!DOCTYPE html> <html lang="en" class="" id="rgw28_56aba196a5cf3"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="DCu7DFSDWF3uHz/uN/Ab9bg6JbIQR8eywtC9Ti67Cf5gHXG+w4CmQuxWtmS9ChcwWu2DSrxhh/IkmKesFerm7WURm6FBKi/pvKWO8X5C9XU5fbiaICL4xUl5uz7oKyWCp6ofLSL43Flanu+xHesVIkIo4ajqfkveBObqaBWVOvcil69X8Ram+UAIq+STmFiDSvG8Dp3jBw8Vw9qYq5Iwp3VDLzobw5/thDFzGdSIPgL+/JS/puBugRtDbODFPY7c8rT4PvVyXfaKnNO0jpWrFA3sz2pds0g9xYfTiQPjEQE="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-737da285-0220-4653-a418-168e7988304d",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/200806059_An_Analysis_of_Transformation" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="An Analysis of Transformation" />
<meta property="og:description" content="" />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/200806059_An_Analysis_of_Transformation/links/00afa1950cf245659d00146a/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/200806059_An_Analysis_of_Transformation" />
<meta property="rg:id" content="PB:200806059" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="An Analysis of Transformation" />
<meta name="citation_author" content="George E. P. Box" />
<meta name="citation_author" content="David R. Cox" />
<meta name="citation_publication_date" content="1964/01/01" />
<meta name="citation_journal_title" content="Stat" />
<meta name="citation_issn" content="0038-9986" />
<meta name="citation_volume" content="Series B" />
<meta name="citation_issue" content="2" />
<meta name="citation_firstpage" content="211" />
<meta name="citation_lastpage" content="-246" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/200806059_An_Analysis_of_Transformation" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/200806059_An_Analysis_of_Transformation" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>An Analysis of Transformation</title>
<meta name="description" content="An Analysis of Transformation on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba196a5cf3" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba196a5cf3" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56aba196a5cf3">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=An%20Analysis%20of%20Transformation&rft.title=Journal%20of%20the%20Royal%20Statistical%20Society&rft.jtitle=Journal%20of%20the%20Royal%20Statistical%20Society&rft.volume=Series%20B&rft.issue=2&rft.date=1964&rft.pages=211--246&rft.issn=0038-9986&rft.au=George%20E.%20P.%20Box%2CDavid%20R.%20Cox&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">An Analysis of Transformation</h1> <meta itemprop="headline" content="An Analysis of Transformation">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/200806059_An_Analysis_of_Transformation/links/00afa1950cf245659d00146a/smallpreview.png">  <div id="rgw10_56aba196a5cf3" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw11_56aba196a5cf3"> <a href="researcher/2081050688_George_E_P_Box" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="George E. P. Box" alt="George E. P. Box" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">George E. P. Box</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56aba196a5cf3">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2081050688_George_E_P_Box"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="George E. P. Box" alt="George E. P. Box" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2081050688_George_E_P_Box" class="display-name">George E. P. Box</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56aba196a5cf3"> <a href="researcher/61687043_David_R_Cox" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="David R. Cox" alt="David R. Cox" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">David R. Cox</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56aba196a5cf3">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/61687043_David_R_Cox"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="David R. Cox" alt="David R. Cox" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/61687043_David_R_Cox" class="display-name">David R. Cox</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0038-9986_Stat"><span itemprop="name">Stat</span></a> </span>        <meta itemprop="datePublished" content="1964-01">  01/1964;  Series B(2):211--246.             </div>       <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56aba196a5cf3">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw26_56aba196a5cf3"  itemprop="articleBody">  <p>Page 1</p> <p>An Analysis of Transformations<br />G. E. P. Box; D. R. Cox<br />Journal of the Royal Statistical Society. Series B (Methodological), Vol. 26, No. 2. (1964), pp.<br />211-252.<br />Stable URL:<br />http://links.jstor.org/sici?sici=0035-9246%281964%2926%3A2%3C211%3AAAOT%3E2.0.CO%3B2-6<br />Journal of the Royal Statistical Society. Series B (Methodological) is currently published by Royal Statistical Society.<br />Your use of the JSTOR archive indicates your acceptance of JSTOR&#39;s Terms and Conditions of Use, available at<br />http://www.jstor.org/about/terms.html. JSTOR&#39;s Terms and Conditions of Use provides, in part, that unless you have obtained<br />prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in<br />the JSTOR archive only for your personal, non-commercial use.<br />Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at<br />http://www.jstor.org/journals/rss.html.<br />Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed<br />page of such transmission.<br />The JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic<br />journals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers,<br />and foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take<br />advantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.<br />http://www.jstor.org<br />Sat Sep 29 20:01:42 2007</p>  <p>Page 2</p> <p>An Analysis of Transformations <br />By G. E. P. Box and <br />D. R. Cox <br />University of Wisconsin  Birkbeck College, University of London <br />[Read at a RESEARCH MEETING  April 8th, 1964, METHODS <br />Professor D. V. LINDLEY in the Chair] <br />of the SOCIETY, <br />In the analysis of data it is often assumed that observations y,, y,, ...,y, <br />are independently normally distributed with constant variance and with <br />expectations specified by a model linear in a set of parameters 0. In this <br />paper we make the less restrictive assumption that such a normal, homo- <br />scedastic, linear model is appropriate after some suitable transformation <br />has been applied to the y&#39;s. Inferences about the transformation and about <br />the parameters of the linear model are made by computing the likelihood <br />function and the relevant posterior distribution. The contributions of <br />normality, homoscedasticity and additivity to the transformation are <br />separated. The relation of the present methods to earlier procedures for <br />finding transformations is discussed. The methods are illustrated with <br />examples. <br />1. INTRODUCTION <br />THE usual techniques for the analysis of linear models as exemplified by the analysis of <br />variance and by multiple regression analysis are usually justified by assuming <br />(i) simplicity of structure for &#39;E(y); <br />(ii) constancy of error variance; <br />(iii) normality of distributions; <br />(iv) independence of observations. <br />In analysis of variance applications a very important example of (i) is the assumption <br />of additivity, i.e. absence of interaction. For example, in a two-way table it may be <br />possible to regresent E(y) by additive constants associated with rows and columns. <br />If the assumptions (i)-(iii) are not satisfied in terms of the original observations, <br />y, a non-linear transformation of y may improve matters. With this in mind, numerous <br />special transformations for use in the analysis of variance have been examined in the <br />literature; see, in particular, Bartlett (1947). The main emphasis in these studies has <br />tended to be on obtaining a constant error variance, especially when the variance <br />of y is a known function of the mean, as with binomial and Poisson variates. <br />In multiple regression problems, and in particular in the analysis of response <br />surfaces, assumption (i) might be that E(y) is adequately represented by a rather <br />simple empirical function of the independent variables x,, x,, ...,xt and we would <br />want to transform so that this assumption, together with assumptions (ii) and (iii), <br />is approximately satisfied. In some cases transformation of independent as well as <br />of dependent variables might be desirable to produce the simplest possible regression <br />model in the transformed variables. In all cases we are concerned not merely to find <br />a transformation which will justify assumptions but rather to find, where possible, <br />a metric in terms of which the findings may be succinctly expressed.</p>  <p>Page 3</p> <p>212 Box AND COX-An Analysis o f  Transformations <br />[No. 2, <br />Each of the considerations (i)-(iii) can, and has been, used separately to select a <br />suitable candidate from a parametric family of transformations. For example, to <br />achieve additivity in the analysis of variance, selection might be based on <br />(a) minimization of the ?F value for the degree of freedom for non-additivity <br />(Tukey, 1949); or <br />(b) minimization of the Fratio for interaction versus error; or <br />(c) maximization of the F ratio for treatments versus error (Tukey, 1950). <br />Tukey and Moore (1954) used method (a) in a numerical example, plotting <br />contours of Fagainst (A,, A , )  for transformations in the family (y +A,)&quot;. <br />that in their particular example the minimizing values were very imprecisely determined. <br />In both (a) and (b) the general object is to look for a scale on which effects are <br />additive, i.e. to see whether an apparent interaction is removable by a transformation. <br />Of course, only a particular type of interaction is so removable. Whereas (a) can be <br />applied, for example, to a two-way classification without replication, method (b) <br />requires the availability of an error term separated from the interaction term. Thus, <br />if applied to a two-way classification, method (b) could only be used when there was <br />some replication within cells. Finally, method (c) can be used even in a one-way <br />analysis to find the scale on which treatment effects are in some sense most sensitively <br />expressed. In particular, Tukey (1950) suggested multivariate canonical analysis of <br />(y,y2) to find the linear combination y+ Ay2 most sensitive to treatment effects. <br />Incidentally, care is necessary in using y+ Ay2 over the wide ranges commonly <br />encountered with data being considered for transformation, for such a transformation <br />is sensible only so long as the value of A and the values of y are such that the <br />transformation is monotonic. <br />For transformation to stabilize variance, the usual method (Bartlett, 1947) is to <br />determine empirically or theoretically the relation between variance and mean. An <br />adequate empirical relation may often be found by plotting log of the within-cell <br />variance against log of the cell mean. Another method would be to choose a trans- <br />formation, within a restricted family, to minimize some measure of the heterogeneity <br />of variance, such as Bartlett&#39;s criterion. We are grateful to a referee for pointing <br />out also the paper of Kleczkowski (1949) in which, in particular, approximate fiducial <br />limits for the parameter A in the transformation of y to log(y+ A )  are obtained. The <br />method is to compute fiducial limits for the parameters in the linear relation observed <br />to hold when the within-cell standard deviation is regressed on&#39;the cell mean. <br />Finally, while there is much work on transforming a single distribution to <br />normality, constructive methods of finding transformations to produce normality in <br />analysis of variance problems do not seem to have been considered. <br />While Anscombe (1961) and Anscombe and Tukey (1963) have employed the <br />analysis of residuals as a means of detecting departures from the standard assumptions, <br />they have also indicated how transformations might be constructed from certain <br />functions of the residuals. <br />In regression problems, where both dependent and independent variables can be <br />transformed, there are more possibilities to be considered. Transformation of the <br />independent variables (Box and Tidwell, 1962) can be applied without affecting the <br />constancy of variance and normality of error distributions. An important application <br />is to convert a monotonic non-linear regression relation into a linear one. Obviously <br />it is useless to try to linearize a relation which is not monotonic, but a transformation <br />is sometimes useful in such cases, for example, to make a regression relation more <br />nearly quadratic around its maximum. <br />They found</p>  <p>Page 4</p> <p>19641 Box AND COX-An Analysis of Transformations <br />213 <br />2. GENERAL <br />ON TRANSFORMATIONS REMARKS <br />The main emphasis in this paper is on transformations of the dependent variable. <br />The general idea is to restrict attention to transformations indexed by unknown <br />parameters A ,  and then to estimate X and the other parameters of the model by <br />standard methods of inference. Usually X will be a one-, or at most two-, dimensional <br />parameter, although there is no restriction in principle. Our procedure then leads <br />to an interesting synthesis of the procedures reviewed in Section .I. It is convenient <br />to make first a few general points about transformations. <br />First, we can distinguish between analyses in which either (E) the particular <br />transformation, A ,  is of direct interest, the detailed study of the factor effects, etc., <br />being of secondary concern; or (b) the main interest is in the factor effects, the choice <br />of X being only a,preliminary step. Type (b) is likely to be much the more common. <br />Nevertheless, (a) can arise, for example, in the analysis of a preliminary set of data. <br />Or, again, we may have two factors, A and B, whose main effects are broadly under- <br />stood, it being required to study the A ,  if any, for which there is no interaction between <br />the factors. Here the primary interest is in A. In case (b), however, we shall need to <br />fix one, or possibly a small number, of X&#39;s and go ahead with the detailed estimation <br />and interpretation of the factor effects on this particular transformed scale. We <br />shall choose X partly in the light of the information provided by the data and partly <br />from general considerations of simplicity, ease of interpretation, etc. For instance, <br />it would be quite possible for the formal analysis to show that say ,/y is the best <br />scale for normality and constancy of variance, but for us to decide that there are <br />compelling arguments of ease of interpretation for working say with logy. The <br />formal analysis will warn us, however, that changes of variance and non-normality <br />may need attention in a refined and efficient analysis of logy. That is, the method <br />developed below for finding a transformation is useful as a guide, but is, of course, <br />not to be followed blindly. In Section 7&#39; we discuss briefly some of the consequences <br />of interpreting factor effects on a scale chosen in the light of the data. <br />In regression studies, it is sometimes necessary to take an entirely empirical <br />approach to the choice of a relation. In other cases, physical laws, dimensional <br />analysis, etc., may suggest a particular functional form. Thus, in a study of a chemical <br />system one would expect reaction rate to be proportional to some power of the <br />concentration and to the antilog of the reciprocal of absolute temperature. Again, <br />in many fields of technology relationships of the form <br />y K x&lt;?. .x$ <br />are very common, suggesting a log transformation of all variables. In such cases <br />the reasonable thing will often be first to apply the transformations suggested by the <br />prior reasoning, and after that consider what further modifications, if any, are needed. <br />Finally, we may know the behaviour of y when the independent variables xi tend <br />to zero or infinity, and certainly, if we are hopeful that the model might apply over a <br />wide range, we should consider models that are consistent with such limiting properties <br />of the system. <br />We can distinguish broadly two types of dependent variable, extensive and non- <br />extensive. The former have a relevant property of physical additivity, the latter not. <br />Thus yield of product per batch is extensive. The failure time of a component would <br />be considered extensive if components are replaced on failure, the main thing of <br />interest being the number of components used in a long time. Properties like <br />temperature, viscosity, quality of product, etc., are not extensive. In the absence of</p>  <p>Page 5</p> <p>214  Box AND COX-An Analysis of Transformations <br />[No. 2, <br />the sort of prior consideration mentioned in the previous paragraph there is no <br />reason to prefer the initial form of a non-extensive variable to any monotonic function <br />of it. Hence, transformations can be applied freely to non-extensive variables. For <br />extensive variables, however, the population mean of y is the parameter determining <br />the long-run behaviour of the system. Thus in the two examples mentioned above, <br />the total yield of product in a long period and the total number of components used <br />in a very long time are determined respectively by the population mean of yield per <br />batch and the mean failure time per component, irrespective of distributional form. <br />In a narrowly technological sense, therefore, we are interested in the population <br />mean of y, not of some function of y. Hence we either analyse linearly the untrans- <br />formed data or; if we do apply a transformation in order to make a more efficient and <br />valid analysis, we convert the conclusions back to the original scale. Even in circum- <br />stances where, for immediate application, the original scale y is required, it may be <br />better to think in terms of transformed values in which, say, interactions have been <br />removed. <br />In general, we can regard the usual formal linear models as doing two things: <br />(a) specifying the questions to be asked, by defining explicitly the parameters <br />which it is the main object of the analysis to estimate; <br />(b) specifying assumptions under which the above parameters can be simply and <br />effectively estimated. <br />If there should be conflict between the requirements for (a) and for (b), it is best to <br />pay most attention to (a), since approximate inference about the most meaningful <br />parameters is clearly preferable to formally &quot;exact&quot; inference about parameters <br />whose definition is in some way artificial. Therefore in selecting a transformation we <br />might often give first attention to simplicity of the model structure, for example to <br />additivity in the analysis of variance. This allows simplicity of description and also <br />the main effect of a factor A, measured on a scale for which there appears to be <br />no interaction with a factor B, often has a reasonable possibility of being valid for <br />levels of B outside those of the initial experiment. <br />3. TRANSFORMATION VARIABLE<br />OF THE DEPENDENT <br />We work with a parametric family of transformations from y to y(*), the <br />parameter A ,  possibly a vector, defining a particular transformation. Two important <br />examples considered here are <br />and <br />The transformations (1) hold for y &gt;0 and (2) for y &gt; -A,. Note that since an analysis <br />of variance is unchanged by a linear transformation (1) is equivalent to <br />/I:*<br />logy (A=O); <br />y&#39;&quot; &#39;-<br />(Af O),</p>  <p>Page 6</p> <p>19641 <br />Box AND COX-An Analysis o f  Transformations 215 <br />the form (1) is slightly preferable for theoretical analysis because it is continuous at <br />A = 0. In general, it is assumed that for each A ,  ycA) is a monotonic function of y <br />over the admissible range. Suppose that we observe an n x 1 vector of observations <br />y = {yl, . . .,yn}, and that the appropriate linear model for the problem is specified by <br />where y(&quot; is the column vector of transformed observations, a is a known matrix <br />and 0 a vector of unknown parameters associated with the transformed observations. <br />We now assume that for some unknown A ,  the transformed observations <br />yiA) (i = 1, . . .,n) satisfy the full normal theory assumptions, i.e. are independently <br />normally distributed with constant variance u2, and with expectations (4). The <br />probability density for the untransformed observations, and hence the likelihood <br />in relation to these original observations, is obtained by multiplying the normal <br />density by the Jacobian of the transformation. <br />The likelihood in relation to the original observations y is thus <br />where <br />We shall examine two ways in which inferences about the parameters in (5) can <br />be made. In the first, we apply &quot;orthodox&quot; large-sample maximum-likelihood <br />theory to (5). This approach leads directly to point estimates of the parameters and <br />to approximate tests and confidence intervals based on the chi-squared distribution. <br />In the second approach, via Bayes&#39;s theorem, we assume that the prior distributions <br />of the 0&#39;s and logu can be taken as essentially uniform over the region in which the <br />likelihood is appreciable and we integrate over the parameters to obtain a posterior <br />distribution for A; for general discussion of this approach, see, in particular, Jeffreys <br />(1961). <br />We find the maximum-likelihood estimates in two steps. First, for given A ,  (5) is, <br />except for a constant factor, the likelihood for a standard least-squares problem. <br />Hence the maximum-likelihood estimates of the 0&#39;s are the least-squares estimates <br />for the dependent variable y(&quot; and the estimate of u2, denoted for fixed A by e2(A), is <br />where, when a is of full rank, <br />a, = I-a(afa)-I a&#39;, <br />and S(A) is the residual sum of squares in the analysis of variance of ycA). <br />Thus for fixed A ,  the maximized log likelihood is, except for a constant, <br />= -+n log G2(A) +log J(A; y). <br />In the important special case (1) of the simple power transformation, the second term <br />in (8) is <br />(A-1)xlogy,. <br />In (2), when an unknown origin A, is included, the term becomes <br />L,,,(A) <br />(8) <br />(9)</p>  <p>Page 7</p> <p>216 <br />Box AND COX-An Analysis of Transformations <br />[No. 2, <br />It will now be informative to plot the maximized log likelihood Lmax(h)against h <br />for a trial series of values. From this plot the maximizing value 3 may be read off <br />and we can obtain an approximate 100(1- ol) per cent confidence region from <br />where vh is the number of independent components in A. The main arithmetic <br />consists in doing the analysis of variance of ych)for each chosen h. <br />If it were ever desired to determine 3more precisely this could be done by determin- <br />ing numerically the value 3 for which the derivatives with respect to X are all zero. <br />In the special case of the one parameter power transformation ych)= ( Y ~ - l)/X, <br />where u(&quot; is the vector of components (h-ly,&quot;logy,). The numerator in (12) is the <br />residual sum of products in the analysis of covariance of y(h)and u(&quot;. <br />The above results can be expressed very simply if we work with the normalized <br />transformation <br />Z(h)= Y( A )lJlln <br />where J =J(X; y). Then <br />Lmax(X)= -&amp;nlog a2(h;z), <br />where <br />Z(h)&#39;arZ(h) S(X; z) <br />a2(x;Z) = <br />n <br />3 <br />--<br />-<br />n&#39; <br />where S(X; z) is the residual sum of squares of ~<br />proportional to (S(X;z ) ) - ~and the maximum-likelihood estimate is obtained by <br />minimizing S(X; z) with respect to A. <br />For the simple power transformation <br />( ~ The maximized likelihood is thus <br />1 . <br />where 3 is the geometric mean of the observations. <br />For the power transformation with shifted location <br />where gm (y+A,) is the sample geometric mean of the (y+h2)&#39;s. <br />Consider now the corresponding Bayesian analysis. Let the degrees of freedom <br />for residual be v, = n -rank (a),and let <br />be the residual mean square in the analysis of variance of ycA);note the distinction <br />between a2(X),the maximum-likelihood estimate with divisor n, and s2(X)the &quot;usual&quot;</p>  <p>Page 8</p> <p>19641 <br />Box AND COX-An Analysis o f  Transformations  217 <br />estimate, with divisor the degrees of freedom v,. We first rewrite the likelihood (5), <br />i.e. the conditional probability density function of the y&#39;s given 8, 02, A ,  in the form <br />I <br />( .,s~(A)+(8-B,)&#39;a&#39;a(8 -6 <br />exp -<br />P(Y~ 0, 02, A )  = ( 2 7 ~ ) ~ ~  <br />(3&quot; <br />where 6, is the least-squares estimate of 8 for given A. <br />Now consider the choice of the joint prior distribution for the unknown para- <br />meters. We first parametrize so that the 8&#39;s are linearly independent and hence <br />n- v, in number. Let p,(A) denote the marginal prior density of A. We assume that it <br />is reasonable, when making inferences about A ,  to take the conditional prior distri- <br />bution of the 8&#39;s and logo, given A ,  to be effectively uniform over the range for which <br />the likelihood is appreciable. That is, the conditional prior element given A is <br />202<br />where, for definiteness, we for the moment denote the effects and variance measured <br />in terms of y(&quot; by a suffix A. The factor g(A) is included because the general size <br />and range of the transformed observations y(&quot; may depend strongly on A. If the <br />conditional prior distribution (15) were assumed independent of A ,  nonsensical <br />results would be obtained. <br />To determine g(A) we argue as follows. Fix a standard reference value of A ,  say A,. <br />Suppose provisionally that, for fixed A ,  the relation between y(&quot; and y(&quot;) over the <br />range of the observations is effectively linear, say <br />We can then choose g(A) so that when (16) holds, the conditional prior distributions <br />(15) are consistent with one another for different values of A. In fact, we shall need <br />to apply the answer when the transformations are appreciably non-linear, so that <br />(16) does not hold. There may be a better approach to the choice of a prior distribution <br />than the present one. <br />It follows from (16) that <br />log a :  = const+log o : *  <br />and hence, to this order, the prior density of 02, is independent of A. However, the <br />8,&#39;s are linear combinations of the expected values of the y(n)&#39;s, so that <br />(17) <br />Since there are n -v, independent components to 8, it follows that g(A) is proportional <br />to l/lY-vr. <br />Finally we need to choose I,. In passing from A, to A ,  a small element of volume <br />of the n dimensional sample space is multiplied by J(A; y)/J(A,; y). An average scale <br />change for a single y component is the nth root of this and, since A, is only a standard <br />reference value, we have approximately <br />Thus, approximately, the conditional prior density (15) is</p>  <p>Page 9</p> <p>218 Box AND COX-An Analysis o f  Transformations [No. 2, <br />The combined prior element of probability is thus <br />where we now suppress the suffix X on 8 and a. <br />This is only an approximate result. In particular, the choice of (18) is somewhat <br />arbitrary. However, when a useful amount of information is actually available from <br />the data about the transformation, the likelihood will dominate and the exact choice <br />of (19) is not critical. The prior distribution (19) is interesting in that the observations <br />enter the approximate standardizing coefficient J(X; y). <br />We now have the likelihood (14) and the prior density (19) and can apply Bayes&#39;s <br />theorem to obtain the marginal posterior distribution of X in the form <br />where Kh is a normalizing constant independent of A ,  chosen so that (20) integrates <br />to one with respect to A ,  and <br />The integral (21) can be evaluated to give <br />Substituting into (20); we have that the posterior distribution of X is <br />where K, is a normalizing constant independent of X. <br />Thus the contribution of the observations to the posterior distribution of X is <br />represented by the factor <br />{J(X;y)}vr~?b/{sz(X)}~v~ <br />or, on a log scale, by the addition of a term <br />Lb(X)= -+vTlog sz(X) +(vr/n) log J(h ;y) <br />(22) <br />to logpo(4. <br />Once again if we work with the normalized transformation <br />result is expressed with great simplicity, for <br />=y(&quot;/J1In, the <br />and the posterior density is</p>  <p>Page 10</p> <p>19641 Box AND COX-An Analysis o f  Transformations  219 <br />In practice we can plot (S(h; z))-*&quot;. against A ,  combining it with any prior <br />information about A. When the prior density of h can be taken as locally uniform, <br />the posterior distribution is obtained directly by plotting <br />p,(h) = k(S(h; z)}-tv~, <br />where k is chosen to make the total area under the curve unity. <br />We normally end by selecting a value of h in the light both of this plot and of <br />other relevant considerations discussed in Section 2. We then proceed to a standard <br />analysis using the indicated transformation. <br />The maximized log likelihood and the log of the contribution to the posterior <br />distribution of h may be written respectively as <br />L,,,(h) <br />= -&amp;n log (S(h ;z)/n}, Lb(X)= -&amp;vrlog {S(X; z)/v,}. <br />They differ only by substitution of v, for n. They are both monotonic functions of <br />S(X; z) and their maxima both occur when the sum of squares S(h; z) is minimized. <br />For general description, L,,,(h)  and Lb(X) are substantially equivalent. However, <br />it can easily happen that v,/n is appreciably less than one, even when n is quite large. <br />Therefore, in applications, the difference cannot always be ignored, especially when <br />a number of models are simultaneously considered. <br />There are some reasons for thinking Lb(h) preferable to L,,,(h) <br />Bayesian as well as from a Bayesian point of view; see, for example, the introduction <br />by Bartlett (1937) of degrees of freedom into his test for the homogeneity of variance. <br />The general large-sample theorems about the sampling distributions of maximum- <br />likelihood estimates, and the maximum-likelihood ratio chi-squared test, apply just <br />as much to Lb(h) as to L,,,(h). <br />(24) <br />from a non- <br />4. Two EXAMPLES <br />We have supposed that after suitable transformation from y to y(&quot;, (a) the <br />expected values of the transformed observations are described by a model of simple <br />structure; (b) the error variance is constant; (c) the observations are normally <br />distributed. Then we have shown that the maximized likelihood for h, and also the <br />approximate contribution to the posterior distribution of A ,  are each proportional <br />to a negative power of the residual sum of squares for the variate dh) =Y(~)/J~/&quot;. <br />The &quot;overall&quot; procedure seeks a set of transformation parameters h for which <br />(a), (b) and (c) are simultaneously satisfied, and sample information on all three <br />aspects goes into the choice. In this Section we now apply this overall procedure to <br />two examples. In Section 5 we shall show how further analysis can show the separate <br />contributions of (a), (b) and (c) in the choice of the transformation. We shall then <br />illustrate this separation using the same two examples. <br />The above procedure depends on specific assumptions, but it would be quite <br />wrong for fruitful application to regard the assumptions as final. The proper attitude <br />of sceptical optimism is accurately expressed by saying that we tentatively entertain <br />the basis for analysis, rather than that we assume it. The checking of the plausibility <br />of the present procedure will be discussed in Section 5. <br />A Biological Experiment using a 3x 4 Factorial Design with Replication <br />Table 1 gives the survival times of animals in a 3 x 4 factorial experiment, the <br />factors being (a) three poisons and (b) four. treatments. Each combination of the <br />two factors is used for four animals, the allocation to animals being completely <br />randomized.</p>  <p>Page 11</p> <p>- -- - - <br />220 <br />Box AND COX-An ArzulysB oj&quot; Transformations <br />[No. 2, <br />We consider the application of a simple power transformation y(&quot; = (y&quot; <br />Equivalently we shall actually analyse the standardized variate zch) = (yh-l)/(hjh-I). <br />1))h. <br />TABLE 1 <br />Survival times (unit, 10hr) o f  animals in a 3x 4 factorial experiment <br />Treatment <br />Poison <br />A <br />B <br />C <br />D <br />We are tentatively entertaining the model that after such transformation <br />(a) the expected value of the transformed variate in any cell can be represented <br />by additive row and column constants, i.e. that no interaction terms are needed, <br />(b) the error variance is constant, <br />(c) the observations are normally distributed. <br />The maximized likelihood and the posterior distribution are functions of the residual <br />sum of squares for zch) after eliminating row and column effects. This sum of squares <br />is denoted S(h; z). It has 42 degrees of freedom and is the result of pooling the <br />&quot;within groups&quot; and the &quot;interaction&quot; sums of squares. <br />Table 2 gives S(A; z) together with Lm,,(h) and pu(h) over the interesting ranges. <br />The constant k in keLb(h) =pu(h) is the reciprocal of the area under the curve <br />Y = eLb(h) determined by numerical integration. Graphs of Lm,,(A) and of pu(A) <br />are shown in Fig. 1. This analysis points to an optimal value of about = -0.75. <br />Using (11) the curve of maximized likelihood gives an approximate 95 per cent <br />confidence interval for A extending from about -1.13 to -0.37. <br />The posterior distribution pu(A) is approximately normal with mean -0.75 and <br />standard deviation 0.22. About 95 per cent of this posterior distribution is included <br />within the limits -1.18 and -0.32. <br />The reciprocal transformation has a natural appeal for the analysis of survival <br />times since it is open to the simple interpretation that it is the rate o f  dying which is <br />to be considered. Our analysis shows that it would, in fact, embody most of the <br />advantages obtainable. The complete analysis of variance for the untransformed <br />data and for the reciprocal transformation (taken in the z form) is shown in Table 3. <br />Whereas no great change occurs on transformation in the mean squares associated <br />with poisons and treatments, the within groups mean square has shrunk to a third of</p>  <p>Page 12</p> <p>19641 <br />BOXAND COX-An Analysis of Transformations <br />TABLE 2 <br />Biological data. Calculations based on an additive, homoscedastic, <br />normal model in the transformed observations <br />Lmax(h) = -24 log B2(h; Z) = <br />-k 92.91 ; P,(h) = k eLa(A)= 0.866 x<br />log {S(h;z ) ) - ~ ~  <br />10-10{S(h;= ) } - 2 1 .  <br />&#39; I  <br />FIG. 1. Biological data. Functions Lma,(h) and p,(h). Arrows show approximate <br />95 per cent. confidence interval for h. <br />9</p>  <p>Page 13</p> <p>- -- - <br />Mean squares x 1000 <br />222  Box AND COX-An Analysis o f  Transformations <br />[No. 2, <br />its value and the interaction mean square is now much closer in size to that within <br />groups. Thus, in the transformed metric, not only is greater simplicity of interpre- <br />tation possible but also the sensitivity of the experiment, as measured by the ratios <br />TABLE 3 <br />Analyses o f  variance o f  biological data <br />Degrees <br />of <br />freedom <br />Untransformed <br />Reciprocal <br />transformation <br />(2form) <br />Poisons <br />Treatments . <br />P x T .  <br />Within groups <br />. <br />. <br />2 <br />3 <br />6 <br />36 <br />516.5 <br />307.1 <br />41.7 <br />22.2 <br />568.7 <br />221.9 <br />8.5 <br />7.8 <br />of the poisons and the treatments mean squares to the residual square, has been <br />increased almost threefold. We shall not here consider the detailed interpretation of <br />the factor effects. <br />A Textile Experiment using a Single Replicate o f  a 3, Design <br />In an unpublished report to the Technical Committee, International Wool Textile <br />Organization, Drs A. Barella and A. Sust described some experiments on the behaviour <br />of worsted yarn under cycles of repeated loading. Table 4 gives the numbers of <br />cycles to failure, y, obtained in a single replicate of a 3, experiment in which the <br />factors are <br />x1: length of test specimen (250, 300, 350 mm.), <br />x, : amplitude of loading cycle (8, 9, 10 mm.), <br />x, : load (40, 45, 50 gm.). <br />In Table 4 the levels of the x&#39;s are denoted conventionally by -1, 0, 1. <br />It is useful to describe first the results of a rather informal analysis of Table 4. <br />Barella and Sust fitted a full equation of second degree in x,, x, and x,, but the <br />conclusions were very complicated and messy. In view of the wide relative range of <br />variation of y, it is natural to try analysing instead log y, and there results a great <br />simplification. All linear regression terms are very highly significant and all second- <br />degree terms are small. Further, it is natural to take logs also for the independent <br />variables, i.e. to think in terms of relationships like <br />The estimates of the P&#39;s, from the linear regression coefficients of log y on the <br />log x&#39;s, are, with their estimated standard errors, <br />Since p11 :  -p,, the combination log x, -log x, = log (x,/x,) is suggested by the <br />data as of possible importance. In fact, x,/xl is just the fractional amplitude of the <br />loading cycle; indeed, naPve dimensional considerations suggest this as a possible <br />factor, although there are in fact other relevant lengths, so that dependence on x1</p>  <p>Page 14</p> <p>19641 <br />Box AND COX -An Analysis of Transformations <br />223 <br />and x, separately is not inconsistent with dimensional considerations. If, however, <br />we write x,/x, = x, and round the regression coefficients, we have the simple formula <br />y Cc xy5 x33 <br />which fits the data remarkably well. <br />TABLE 4 <br />Cycles to failure of worsted yarn: 33factorial experiment <br />Factor levels <br />Cycles to failure, y <br />x1 <br />x2 <br />x3 ?<br />In this case, there seem strong general arguments for starting with a log trans- <br />formation of all variables. Power laws are frequently effective in the physical sciences; <br />also, provided that the signs of the p&#39;s are right, (25) has sensible limiting behaviour <br />for x2,x3+0,co; finally, the obvious normal theory model based on transforming <br />(25) gives distributions over positive values of y only.</p>  <p>Page 15</p> <p>224 <br />Box AND COX-An Analysis of Transformations <br />[No. 2, <br />Nevertheless, it is interesting to see whether the method of the present paper <br />applied directly to the data of Table 4 produces the log transformation. In this <br />paper, transformations of the dependent variable alone are considered; in fact, since <br />the relative range of the x&#39;s is not very great, transformation of the x&#39;s does not have <br />a big effect on the linearity of the regression. <br />We first consider the application of a simple power transformation in terms, as <br />before, of the standardized variate z ( ~ ) = (yh- ~)/(XJP-~). We tentatively suppose <br />that after such transformation <br />(a) the expected value of the transformed response can be represented merely by <br />a model linear in the x&#39;s, . <br />(b) the error variance is constant, <br />(c) the observations are normally distributed. <br />The maximized likelihood and the posterior distribution are functions of the residual <br />sum of squares for z(&quot; after fitting only a linear model to the x&#39;s. Since there are <br />four constants in the linear regression model this residual sum of squares has <br />27-4 = 23 degrees of freedom; we denote it by S(h; 2). <br />Table 5 shows S(X; z) together with L,,,(h) <br />and the results are plotted in Fig. 2. The optimal value for the transformation para- <br />meter is ^h = -0.06. The transformation is determined remarkably closely in this <br />and p,(X) over the interesting ranges <br />TABLE 5 <br />Textile data. Calculations based on normal linear model in the <br />transformed observations <br />L,,,(h) <br />pu(h) = k eLacn) = 0.540 x <br />= -13.5 log B2(h;z) = {S(h; ~)}-~~&#39;~+44.49. ?<br />{S(h; ~)}-ll&#39;~. ?<br />example, the approximate 95 per cent confidence range extending only from -0.18 <br />to +0.06. The posterior distribution p,(X) has its mean at -0.06. About 95 per cent <br />of the distribution is included between -0.20 and +0.08. As we have mentioned, <br />the advantages of a log transformation corresponding to the choice X = 0 are very <br />great and such a choice is now seen to be strongly supported by the data.</p>  <p>Page 16</p> <p>19641 <br />BOXAND COX-An Analysis of Transformations <br />225 <br />The complete analysis of variance for the untransformed and the log trans-<br />formation, taken in the z form, is shown in Table 6. <br />- I  <br />0 <br />I <br />X <br />FIG. 2. Textile data. Functions L,,(h) <br />and p,(h). <br />Arrows show approximate <br />95 per cent confidence interval for h. <br />TABLE 6 <br />Analyses of variance of textile data <br />Mean squares x 1000 <br />Degrees <br />of <br />freedom <br />Logarithmic <br />transformation <br />(Z form) <br />Untransformed <br />Linear <br />Quadratic . <br />Residual <br />. <br />. <br />3 <br />6 <br />4,916.2 <br />704.1 <br />73.9 <br />2,374.4 <br />8.1 <br />11.9 <br />. <br />17 <br />The transformation eliminates the need for second-order terms in the regression <br />equation while at the same time increasing the sensitivity of the analysis by about <br />three, as judged by the ratio of linear and residual mean squares. <br />For this example we have also tried out the procedures we have discussed using <br />the two parameter transformation ych) ={(y+h,)hl- 1)/X, or in the z form actually</p>  <p>Page 17</p> <p>226 <br />used here zch)= {(y+ <br />print out of 77 analysis of variance tables, involving in each case the fitting of a <br />general equation of second degree, and calculation of residuals and fitted values <br />took 2 min. 6 sec. on the C.D.C. 1604 electronic computer. The full numerical <br />results can be obtained from the authors, but are not given here. Instead approximate <br />contours of -11.5 log S(A; z), and hence of S(A; z) itself, of the maximized likelihood <br />and of p,(Al, A,), are shown in Fig. 3. If the joint posterior distribution p,(Al, A,) <br />were normal then a region which excluded 100a per cent of the total posterior <br />probability could be given by <br />Box AND COX-An Analysis o f  Transformations <br />-l)/{Al gm (y +AZ))hl-l. Incidentally the calculation and <br />[No. 2, <br />The shape of the contours indicates that the normal assumption is not very exact. <br />Nevertheless, the quantity 100a obtained from (26) has been used to label the contours <br />in Fig. 3 which thus roughly indicates the posterior probability distribution. For this <br />example no appreciable improvement results from the addition of the further <br />transformation parameter A,. <br />300 <br />.200 <br />i s  <br />100 <br />0 <br />0.2  0 -0.2 -0.4 -0.6 <br />A, <br />-0.8 <br />FIG.3. Textile data. Transformation to (yS A s ) &quot; .  <br />with approximate percentage of posterior distribution excluded. <br />Contours of p,(h,, h,) labelled <br />5. FURTHER <br />5.1. General Procedure for Further Analysis <br />The general procedure discussed above seeks to achieve simultaneously a model <br />with (a) simple structure for the expectations, (b) constant variance and (c) normal <br />distributions. Further analysis is sometimes profitable to see the separate contri- <br />butions of these three elements to the transformation. Such analysis may indicate <br />(i) how simple a model we are justified in using; <br />(ii) what weight is given to the considerations (a) - (c) in choosing A; <br />(iii) whether different transformations are really needed to achieve the different <br />objectives and hence whether or not the value of A chosen using the overall <br />procedure is a compatible compromise. <br />Of course, quite often careful inspection of the data will answer (i)-(iii) adequately <br />for practical purposes. Nevertheless, a further analysis is of interest. <br />OF THE TRANSFORMATIONANALYSIS</p>  <p>Page 18</p> <p>19641 Box AND COX-An Analysis of Transformations 227 <br />We aim at simplicity both to achieve ease of understanding and&#39; to allow an <br />efficient analysis. Validity of the formal tests associated with analysis of variance may, <br />in virtue of the robustness of these tests, often hold to a good enough approximation <br />even with the untransformed data. We stress, however, that such approximate validity <br />is not by itself enough to justify an analysis; sensitivity must be considered as well as <br />robustness. Thus in the biological example we have about one-third the sensitivity on <br />the original scale as on the transformed scale. The approximate validity of significance <br />tests on the original scale would be very poor consolation for the substantial loss of <br />information involved in using the untransformed analysis. In any case even such <br />validity is usually only preserved under the null hypothesis that all treatment effects <br />are zero. <br />For the further analysis we again explore two approaches, one via maximum <br />likelihood and the other via Bayes&#39;s theorem. Consider a general model to which a <br />constraint C can be applied or relaxed, so that the relative merits of the simple and <br />of the more complex model can be assessed. For example, the general model may <br />include interaction terms, the constraint C being that the interaction terms are zero. <br />If Lmax(A)and Lmax(AI C) denote maximized log likelihoods for the general model <br />and for the constrained model, then <br />Here the second term on the right-hand side is a statistic for testing for the presence <br />of the constraint. <br />More generally, with a succession of constraints, we have <br />and the three terms on the right of (28) can be examined separately. The detailed <br />procedure should be clear from the examples to follow. <br />To apply the Bayesian approach, we write the posterior density of A <br />where p(C) = E,{p(CI A)) is a constant independent of A. That is, the posterior <br />density of A under the constrained model is the posterior density under the general <br />model multiplied by a factor proportional to the conditional probability of the <br />constraint given A. Successive factorization can be applied when there is a series of <br />successively applied constraints, giving, for example, <br />where p(C, I C,) = E,{p(C2 I A, C,)) is a further constant independent of A. Note that <br />we are concerned here not with the probabilities that the constraints are true, but with <br />the contributions of the constraints to the final function p(AI C,, C,). <br />5.2. Structure of the Expectation <br />Now very often the most important question is: how simple a form can we use <br />for E{Y(~))? Thus in the analysis of the biological example in Section 4, we assumed, <br />among other things, that additivity can be achieved by transformation. In fact,</p>  <p>Page 19</p> <p>228 <br />Box AND COX -An Analysis o f  Transformations [No. 2, <br />interaction terms may or may not be needed. Similarly, in our analysis of the textile <br />example we took a linear model with four parameters; the full second-degree model <br />with ten parameters may or may not be necessary. <br />Now let A, Hand N denote respectively the constraints to the simpler linear model <br />(without interaction or second-degree terms), to a heteroscedastic model and to a <br />model with normal distributions. Then, <br />Lmax(h) A, H, N) = Lmax(hl H, N) +{Lrnax(XIA, H, N)-Lmax(hI H, N)). <br />Let the parameter 8 in the expectation under the general linear model be partitioned <br />(O,, 8,) where 8, = 0 is the constraint A. Denote the degrees of freedom associated <br />with 8, and 8, by V, and v,. If v, is the number of degrees of freedom for residual <br />in the complex model, the number in the simpler model is thus v,+ v,. <br />As before, we work with the standardized variable z ( ~ ) = Y(~)/J~/*. If we identify <br />residual sums of squares by their degrees of freedom, we have <br />Lmax(X 1 8, = 0, H, N) = -Sn log {S,p+,2(h ;z)ln), <br />whereas <br />Lma,(X I H, N) = -Sn log {Svv(X; z)ln). <br />Thus, in the textile example, Svr refers to the residual sum of squares from a second- <br />degree model and S,~+,a<br />refers to the residual sum of squares from a first-degree model. <br />Quite generally <br />s,~+,z(~; Sur(X; z)+S,~,~(X; 4 ,  <br />z) = <br />where S,2,,1(X; z) denotes the extra sum of squares of ztA) for fitting 8,, adjusting for <br />el, and has v, degrees of freedom. <br />Thus with (32) and (33) the decomposition (31) becomes <br />(31) <br />(32) <br />(33) <br />where <br />is the standard F ratio, in the analysis of variance of z(~), <br />the simpler model. <br />Equation (34) thus provides an analysis of the overall criterion into a part taking <br />account only of homoscedasticity (H) and normality (N) plus a part representing <br />the additional requirement of a simple linear model, given that H and N have been <br />achieved. <br />In the corresponding Bayesian analysis (30) gives <br />p(hl 8, = O ,  H, N) =p(hl H, N) x k,p(e2 = 01 A ,  H, N), <br />where <br />Ilk, = E A  1 ,,,{P(~z <br />the expectation being taken over the distribution p(XI H, N). <br />Note that since the condition 8, = 0 is given, there is no component for these <br />parameters in the prior distribution, so that the left-hand side of (36) is the posterior <br />density obtained previously assuming A. Thus, in terms of the standardized variable <br />dA), the left-hand side is <br />for testing the restriction to <br />(36) <br />= 01 A ,  H, N)),</p>  <p>Page 20</p> <p>19641 <br />Box AND COX-An Analysis o f  Transformations <br />where the normalizing constant is given by <br />Similarly, in the general model with 8, and 82 both free to vary, we obtain the first <br />factor on the right-hand side of (36) as <br />P(A IH, N) = PO(^ Cvr {Svv(A ;Z))-*~T, <br />with <br />(38)<br />C;;&#39; <br />= /po(h) {S,~(A ;z)}-ivr dA. <br />Thus, from (37) and (38), the second factor on the right-hand side of (36) must be <br />Now the general equation (36) shows that this last expression must be proportional <br />to p(02 = 01 A ,  H, N). It is worth proving this directly. To do this, consider a trans- <br />formed scale on which constant variance and normality have been attained and the <br />standard estimates 8, and s2 calculated. For the moment, we need not indicate <br />explicitly the dependence on A and z. We denote the matrix of the reduced least- <br />squares equations for 8,, eliminating el, by b, so that the covariance matrix of 8, is <br />02b-I. The elements of b and bL1 are denoted bij and bij. Also we write <br />pij = bij/d(bii bjj) and {pij) for the matrix inverse to {pij). Then the joint distribution of <br />is (Cornish, 1954; Dunnett and Sobel, 1954) <br />where here and later the constant involves neither the parameters nor the observations. <br />With uniform prior distributions for-the 0&#39;s and for log a, this is also the posterior <br />distribution of the quantities (dZi- dZi)/(s/Jbii), where now the dZi are the random <br />variables. Transforming from the ti&#39;s to the d2,&#39;s we have that <br />whence <br />If now we restore in our notation the dependence on A, comparison of (40) with <br />(39) proves the required result; the appropriateness of the constant is easily checked. <br />Thus (36) provides an analysis of the overall density into a part p(AI H,N) taking <br />account only of homoscedasticity and normality, and a second part, (39), in which <br />the influence of the simplifying constraint is measured.</p>  <p>Page 21</p> <p>230 <br />Box AND COX -An Analysis o f  Transformations <br />[No. 2, <br />Equation (39) can be rewritten <br />Now, by (34), the corresponding expression in the maximum-likelihood approach is <br />given, in a logarithmic version, by <br />The essential difference between (41) and (42) is the occurrence of the term in <br />SJX; z) in (41). In conventional large sample theory, vr is supposed large compared <br />with v, and then in the limit the variation with h of the additional term is negligible, <br />and the effect of both terms can be represented by plotting the standard F ratio as a <br />function of A. In applications, however, v,/v,may well be appreciable; thus in the <br />textile example v,/v,= 6/17. <br />Hence (41) and (42) could lead to appreciably different conclusions, for example, <br />if we found a particular value of X giving a low value of F(h; z) but a relatively <br />high value of SJh; z). <br />The distinction between (41) and (42) from a Bayesian point of view can be <br />expressed as follows. In (41) there occurs the ordinate of the posterior distribution <br />of 8, at 8, = 0. On the other hand, the Fratio, which determines (42), is a monotonic <br />function of the probability mass outside the contour of the posterior distribution <br />passing through 8, = O: Alternatively, a calculation of the posterior probability of a <br />small region near 8, =0 having a length proportional to o ,  in each of the v, <br />component directions gives an expression equivalent to (42). The difference between <br />(41) and (42) will be most pronounced if there exists an extreme transformation <br />producing a low value of F(h; z) but a large value of S,,(X; z), corresponding to a <br />large spread of the posterior distribution of 8,. Expression (42) would give an <br />answer tending to favour this transformation, whereas (41) would not. <br />5.3. Application to Textile Example <br />We now illustrate the above analysis using the textile data. The calculations are <br />set out in Table 7 and displayed in Figs. 4 and 5. We discuss the conclusions in some <br />detail here. In practice, however, the most useful aspect of this approach is the <br />opportunity for graphical assessment. <br />Fig. 4 shows that the curvature of L,,,(hl <br />L,,,(XI A, H, N) previously given in Fig. 2, the constraint A here being that the <br />second-degree terms are supposed zero. The inequality <br />H, N) is much jess than that of <br />thus gives the much wider approximate 95 per cent confidence interval (-0.48, 0.13) <br />for h indicated by HN in Fig. 4 and compared with the previous interval, marked <br />AHN. Since the constraint has 6 degrees of freedom the sampling distribution of <br />for fixed normalizing h is asymptotically xi. Alternatively, (44, being a monotonic <br />function of F, can be tested exactly. Thus we can decide for which X&#39;s, if any, the <br />inclusion of the constraint is compatible with the data. In Fig. 5, F(h; z) is close to</p>  <p>Page 22</p> <p>19641 <br />Box AND COX-An Analysis of Transformations <br />231 <br />unity over the interesting range of h close to zero, so that we can use the simpler model <br />in this neighbourhood. The range indicated by C in Fig. 4 is that for which Fis less <br />than 2-70, the 5 per cent significance point. <br />TABLE 7 <br />Textile data. Calculations for the analysis of the transformation <br />Diference = -13.5 x <br />h <br />Lmax(h I A, H, N )  Lmax(h I H, N )  <br />log (1s &amp; F ( ~ ;z)) <br />F(X; 2) <br />The Bayesian analysis follows parallel lines. In Fig. 4, pu(hI H,N )  has a much <br />greater spread than ~,(hl A, H,N). Fig. 5 shows pu(XI H, N )  with the component <br />kAp(AI A, H, N )  from the constraint. When multiplied together they give the overall <br />density pu(hIA, H, N). A value of h near zero maximizes the posterior density <br />assuming the constraint and is consistent with the information in pu(hj H, N). <br />There is, however, nothing in our Bayesian analysis itself to tell us whether the <br />simplified model with the constraint is compatible with the data, even for the best <br />possible A. There is an important general point here. All probability calculations in <br />statistical inference are conditional in one way or another. In particular, Bayesian <br />posterior distributions such as p,(hIA, H, N )  are conditional on the model, in <br />particular here on assumption A. It could easily happen that there is no value of h <br />for which A is at all reasonable, but to check on this we need to supplement the</p>  <p>Page 23</p> <p>232 <br />Box AND COX-An Analysis o f  Transformations <br />[No. 2, <br />Bayesian argument (Anscombe, 1961). Here we can do this by a significance test <br />based on the sampling distribution of a suitable function of the observations, namely <br />P(h; z). For h around zero the value of P(h; z) is, in fact, well within the significance <br />limits, so that we can reasonably use the posterior distribution of h in question. <br />X <br />FIG. 4. Textile data. Functions Z,,,(h) <br />H: homogeneity of variance. N: normality. Arrows HN, AHN show approximate 95 per <br />cent confidence intervals for h. Arrows C show range for which F for second-degree <br />terms is not significant at 5 per cent level. <br />andp,(h) under different models. A: additivity. <br />5.4. Homogeneity o f  Variance <br />Suppose that we have k groups of data, the expectation and variance being <br />constant within each group. In the Zth group, let the variance be a? and let S(l) <br />denote the sum of squares of deviations, having vl =nl- 1 degrees of freedom. <br />Write Xnl =n, Cvl = n -k. Thus in our biological example, k = 12, v, = ... =v,, = 3, <br />n,= ...=nI,=4andv=36,n=48. <br />Now suppose that a transformation to y(&quot; exists which induces normality simul- <br />taneously in all groups. Then in terms of the standardized variable z(&quot;, the maximized <br />log likelihood is <br />I N) = -&amp;Cnl log {S(l) (A; z)/n,}, <br />L,,,(X</p>  <p>Page 24</p> <p>19641 <br />Box AND COX-An Analysis of Transformations <br />233 <br />where S(l)(X; z) is the sum of squares S&#39;l), considered as a function of X and <br />calculated from the standardized variable z(&quot;. <br />?,,(A.IH,Nf <br />and <br />k~ ?<br />p(AlM,N) ?<br />X <br />Textile data. - Components of posterior distribution. ----- Variance <br />ratio, F(X; 2). Arrow gives 5 per cent significance level. <br />We now consider the constraint H, a! = ... = a;, i.e. look at the possibility that <br />a transformation exists simultaneously achieving normality and constant variance. <br />Then if S, = XS&#39;l) is the pooled sum of squares within groups <br />Lmax(XI H, N )  = -i!n log {S,(X ;z)ln). <br />Therefore <br />= Lm,, ( AI N )+log L,(X; z), <br />(47) <br />say. Here the second factor is the log of the Neyman-Pearson L, criterion for testing <br />the hypothesis a: = ... = a;. <br />In the corresponding Bayesian analysis, (29)gives <br />p(hIH,N) =p(XIN)xkHp(u,2 = ... = <br />where <br />kg1= EAIN{p(a: = ... = U; 1 A, N)). <br />For the general model in which a!, ..., o; may be different, the prior distribution is <br />po(X)(n do,)(n dlog o,)J -&quot;in <br />h,N), (48)</p>  <p>Page 25</p> <p>234 <br />Box AND COX -An Analysis o f  Transformations <br />[No. 2, <br />and <br />with <br />(49) <br />For the restricted model in which the variances are all equal to a2, the appropriate <br />prior distribution is <br />Po(~) (rI&#39;d0,) (dlog a) J-vln <br />and <br />P(X I H, N) = {P,(X) c,(X ;z))-tv. <br />Hence, on dividing (50) by (49), we have that the second factor in (48) is <br />(50) <br />where (Bartlett, 1937) <br />(S (9&#39; z)) <br />(s&quot;&#39;;;&quot;)<br />M(X;. z) = v log --Zvl log <br />is the modification of the L, statistic for testing homogeneity of variance, replacing <br />sample sizes by degrees of freedom. <br />From our general argument, (51) must be proportional to p(u! = ... = oil X ,  N). <br />This can be verified directly by finding the joint posterior distribution of a ! ,  ...,a;, <br />transforming to new variables u,2, oi/a!, ...,ui]u,2, integrating out u,2, and then taking <br />unit values of the remaining arguments. <br />5.5. Application to ~iolo~ical <br />Example <br />In the biological example, we can now factorize the overall criterion into three <br />parts. These correspond to the possibilities that in a$dition to normality within <br />each group, we may be able to get constant variance and that it may be unnecessary to <br />include interaction terms in the model, i.e. that additivity is achievable. <br />In terms of maximized likelihoods, <br />where L,(A; z) is the criterion for testing constancy of variance given normality and <br />F(X; z) is the criterion for absence of interaction given normality and constancy of <br />variance. <br />The correspondiilg Bayesian analysis is <br />The results are set out in Table 8 and in Figs. 6-8. The graphs of Lrna,(XI N) and <br />p,(XI N) in Fig. 6 show that the information about X comihg from within group <br />normality is very slight, values of X as far apart as -1 and 2 being acceptable on this</p>  <p>Page 26</p> <p>235 <br />19641 <br />Box AND COX-An Analysis of Transformations <br />basis. The requirement of constant variance, however, has a major effect on the <br />choice of A; further, some information is contributed by the requirement of additivity. <br />TABLE 8 <br />Biological data. Calculations for analysis of the transformation <br />From Fig. 7, which shows the detailed separation of the maximum-likelihood and <br />Bayesian components; any transformation in the region y-I to y-* gives a compatible <br />compromise.</p>  <p>Page 27</p> <p>FIG. 6. Biological data. <br />A: additivity. H: homogeneity of variance. N: normality.. Arrows N, HN, AHN show <br />approximate 95 per cent confidence intervals for A. <br />Functions ~,,,(h) <br />and P,(X) under different models. <br />h <br />FIG. 7. Biological data. Components of posterior distribution.</p>  <p>Page 28</p> <p>19641 <br />Box AND COX-An Analysis of Transformations <br />Since the groups all contain four observations <br />and the graph of M(X; z) in Fig. 8 is equivalent to one of &amp;(A; z). Since on the null <br />hypothesis the distribution of M(h; z) is approximately x:,, we can use Fig. 8 to <br />FIG. 8. Biological data. Variance ratio, F(h; z), for interaction against error as a <br />function of h. Bajtlett&#39;s criterion, M(h; z), for equality of cell variances as a function of h. <br />Dotted lines give 5 per cent significance limits. <br />find the range in which the data are consistent with homoscedasticity. Similarly <br />the graph of F(h; z) indicates the range within which the data are consistent with <br />additivity. The dotted lines indicate the 5 per cent significance levels of M and of F. <br />The minimum of M(h; z) is very near h = -1. It is of interest that the regression <br />coefficient of log(samp1e variance) on log(samp1e mean) is nearly 4, so that the <br />reciprocal transformation is suggested also by the usual approximate argument for <br />stabilizing variance. <br />6. ANALYSIS<br />OF RESIDUALST <br />We now examine briefly a connection between the methods of the present paper <br />and those based on the analysis of residuals. The analysis of residuals is intended <br />t We are greatly indebted to Professor F. J. Anscombe for pointing out an error in the <br />approximation for a as we originally gave it. In the present modified version terms originally <br />neglected in this Section have been included to correct the discrepancy.</p>  <p>Page 29</p> <p>238 <br />Box AND COX-An Analysis o f  Transformations <br />[No. 2, <br />primarily to examine what happens on one particular scale, although its use to <br />indicate a transformation has been suggested (Anscombe and Tukey, 1963). Corre- <br />sponding to an observation y, let Y be the deviation j-j of the fitted value j from <br />the sample mean and let r = y -j be the residual. If the ideal assumptions are satisfied <br />r and Y will be distributed independently. Different sorts of departures from ideal <br />assumptions can be measured, therefore, by studying the deviations of the statistics <br />= Cri Yj from nE(ri)E(Yj). In addition to graphical analysis, a number of such <br />functions have indeed been proposed for particular study (Anscombe, 1961; <br />Anscombe and Tukey, 1963). <br />Specifically, the statistics <br />were put forward as measures respectively of skewness, kurtosis, heterogeneity of <br />variance and non-additivity. Tukey&#39;s degree of freedom for non-additivity (Tukey, <br />1949) involves the sum of squares corresponding to TI, considered as a contrast of <br />residuals with &quot;fixed&quot; coefficients Y2. <br />Suppose now that we consider the family of power transformations and, writing <br />z = y/j, and w = z- 1, make the expansion <br />where w, = w2, w3 = w3 and a! = 1-A. <br />Now, L,,,(X) <br />which is approximately <br />and L,(X) are determined by the residual sum of squares of z(~), <br />If we take terms up to the fourth degree in w and then differentiate with respect to a!, <br />we have that the maximum-likelihood estimate of a! is approximately <br />3w1a, w, -w&#39;a, w3 <br />A <br />a!= 3wk a, w, + 4w!a, w3 &#39; <br />If we write y, = y -);, y, = (y -j),, y3 = (Y-);)~ and denote by 9,,9,,j3 the values <br />obtained by fitting y,, y, and y3 to the model, the above approximation may be <br />expressed in terms of the original observations as <br />To see the relation between this expression and the T statistics, write d = j-3. <br />Then y, = y-); = r+ Y+d. Bearing in mind that a,Y = O,a,r = r, Y&#39;r = O,a,1 = 0, <br />l&#39;r = 0, where 1denotes a vector of ones, terms such as y; a, y, can easily be expressed <br />in terms of sums of powers and products of r, Y and d. In particular, on writing S <br />for Cr2, we find the numerator of (58) to be <br />To this order of approximation the maximum-likelihood estimate of a! thus <br />involves all the T statistics of orders 3 and 4.</p>  <p>Page 30</p> <p>19641 Box AND COX-An Analysis o f  Transformations 239 <br />As a very special case, for data assumed to form a single random sample <br />Here questions such as non-additivity and non-constancy of variance do not arise and <br />the transformation is attempting only to produce normality. Correspondingly in (59), <br />T,, = TI, = T3, = T,, = TI, = 0, since Y = 9-J = 0. In fact if we write m, =J, <br />m, = n-l C(y -J)p (p = 2,3, ...) and make the approximation d = *m,/m,, we have <br />that <br />For distributions in which m,, m,, m, and m,- 3mi are of the same order of magnitude, <br />the terms in curly brackets are of one order higher in l/m,,than are the other terms of <br />the numerator and denominator. If we ignore the higher-order terms, we, have <br />A useful check suggested by Anscombe is to consider the X2 distribution for moderate <br />degrees of freedom and the Poisson distribution for not too small a mean. For <br />x2 we find a-4, whence A- 4,corresponding to the well-known Wilson-Hilferty <br />transformation. For the Poisson distribution, a- 4, whence A- 3. <br />In Section 2 we suggested that, having chosen a suitable X ,  we should make the <br />usual detailed estimation and interpretation of effects on this transformed scale. Thus <br />in our two examples we recommended that the detailed interpretation should be in <br />terms of a standard analysis of respectively l/y and log y. Since the value of A used <br />is selected at least partly in the light of the data, the question arises of a possible <br />need to allow for this selection when interpreting the factor effects. <br />To investigate an appropriate allowance, we regard X as an unknown parameter <br />with &quot;true&quot; value A , ,  say, and suppose the true factor effects to be measured in terms <br />of the scale A,,. <br />If we were, for instance, to analye the factor effects on the scale <br />corresponding to the maximum-likelihood estimate A ,  we might expect some additional <br />error arising from the difference between 2 and A,. <br />although the present formulation of the problem is not always completely realistic. <br />For example, in our biological example, having decided to work with lly, we shall <br />probably be interested in factor effects measured on this scale and not those measured <br />in some unknown scale corresponding to an unknown &quot;true&quot; A , .  On the other hand, <br />if we are interested in whether there is interaction between two fa%tors, it iz possibly <br />dangerous to answer this by testing for interaction Qn the scale A ,  since X may be <br />selected at least in part to minimize the sample interaction. A more reasonable <br />formulation here may often be: on some unknown &quot;true&quot; scale A,, are interaction <br />terms necessary in the model? <br />We now investigate this matter,</p>  <p>Page 31</p> <p>240 <br />Box AND COX-An Analysis of Transformations <br />[No. 2, <br />From the maximum-likelihood:approach, the most useful result is that significance <br />tests for null hypotheses, such as that just mentioned about the absence of interaction, <br />can be obtained in a straightforward way in terms of the usual large-sample chi- <br />squared test. Thus, in the textile example, we could test the null hypothesis that <br />second-degree terms are absent for some unknown &quot;true&quot; A,, by testing twice the <br />difference of the maxima of the two curves of L,,,(X) <br />maxima occur at different values of A. In this particular example, such a test is hardly <br />necessary. <br />It would be possible to obtain more detailed results by evaluating the usual large- <br />sample information matrix for the joint estimation of A ,  u2 and 8. Since, however, <br />more specific results can be obtained from the Bayesian analysis, we shall present <br />only those. The general conclusion will be that to allow for the effect of analysing in <br />terms of 2 rather than A , ,  the residual degrees of freedom need only be reduced by <br />v,, the number of component parameters in A. This result applies provided that the <br />population and sample effects are measured in terms of the normalized variables z(,). <br />Consider locally uniform prior densities for 8, log u and A. Then the posterior <br />density for 8 is <br />in Fig. 4 as x,2. Note that the <br />Approximate evaluation of the integral in (61) is done by expansion around the <br />maxima of the integrands. The ~aximum of the integrand in the denominator is at <br />the maximum-likelihood estimate A, and that of the numerator is near 2, so long as 8 <br />is near its maximum-likelihood value. The answer is that (61) is approximately <br />This is exactly the posterior density of 8 for some known fixed X with the degrees of <br />freedom reduced by v,. <br />To derive (62) from (61), we need to evaluate integrals of the form <br />w h e ~ v is large, and q(A) is assumed positive and to have a unique minimum at <br />A = A ,  with a finite Hessian determinant A, at the minimum. We can then make a <br />Laplace expansion, writing <br />{q(&gt;)}-*v-*v~?<br />N <br />-<br />x const; <br />A: <br />for this we expand the second logarithmic term as far as the quadratic terms and then <br />integrate over the whole v,-dimensional space of A. In our application the terms <br />A: in numerator and denominator are equal to the first order.</p>  <p>Page 32</p> <p>19641 Box AND COX-An Analysis o f  Transformations <br />241 <br />Finally, we can obtain an approximation to the posterior distribution p,(A) of A <br />that is better than the usual type of asymptotic normal approximation. For an <br />expansion about A gives that <br />const <br />Here <br />with d(A) being the n x v, matrix with elements <br />-<br />ax, <br />azp <br />(i= 1,...,n; j= 1,...,v,).<br />The matrix b determines the quadratic terms in the expansion of s2(A; Z) around ?,. <br />Thus the quantities (Aj-&amp;)/{s@; z)dbii) have approximately a posterior multi- <br />variate t distribution and <br />(A- X)&#39;b(A-X) <br />a posterior F distribution. In fact, however, it will usually be better to examine the <br />posterior distribution of A directly, as we have done in the numerical examples. <br />8. FURTHER DEVELOPMENTS <br />We now consider in much less detail a number of possible developments of the <br />methods proposed in this paper. Of these, the most important is probably the simul- <br />taneous transformation of independent and dependent variables in a regression <br />problem. Some general remarks on this have been made in Section 1. <br />Denote the dependent variable by y and the independent variables by x,, ...,x,. <br />Consider a family of transformations from y into y(,) and x,, ...,x, into xp), ...,xjKz), <br />the whole transformation being thus indexed by the parameters (A; K,, ...,K,). It is <br />not necessary that the family of transformations of say x, into x p )  and x, into <br />x$J should be the same, although this would often be the case. <br />We now assume that for some unknown (A; K,, ...,K,) the usual normal theory <br />assumptions of linear regression theory hold. We can then compute say the maximized <br />log likelihood for given (A; K,, ...,K,), obtaining exactly as in (8) <br />Kl, ...,KJ = -8log G2(A ; Kl, . . . ,K 1 )+10gJ(A; y), <br />where G2(A; K,, ...,K,) is the maximum-likelihood estimate of residual variance in <br />the standard multiple regression analysis of the transformed variable. The corre- <br />sponding expression from the Bayesian approach is <br />L,,,(A; <br />(67)</p>  <p>Page 33</p> <p>242 <br />Box AND COX -An Analysis of Transformations <br />[No. 2, <br />The straightforward extension of the procedure of Section 3 is to compute (67) or <br />(68) for a suitable set of (A; K,, ...,K,) and to examine the resulting surface especially <br />near its maximum. This is, however, a tedious procedure, except perhaps for 1= 1. <br />Further, graphical presentation of the conclusions will not be easy if 1&gt; 1;for 1= 1 <br />we can plot contours of the functions (67) and (68). <br />When X is fixed, i.e. transformations of the independent variables only are involved, <br />Box and Tidwell (1962) developed an iterative procedure for the corresponding non- <br />linear least-squares problem. In this the independent variables are, if necessary, first <br />transformed to near the optimum form. Then two terms of the Taylor expansion <br />of x?), . . .,~ ~ ( ~ 1 )  <br />For example if x?) = xKl and the best value for K,<br />are taken. <br />thought to be near 1, we write <br />is <br />x? = x,+(K,- 1)xllogx1. (69) <br />A linear regression term p, xyl can then be written approximately <br />PI XI+P1(~1- 1) xl logx, = Plx1+ YIx1 1% x1, <br />say. If the linear model involves linear regression on x,, .. . ,x, and if all the transfor- <br />mations of the independent variable are to powers, we can therefore take the linear <br />regression on x,, ...,x,, x,logx,, ...,x,logx, in order to estimate the p&#39;s and y&#39;s and <br />hence also the K&#39;S. <br />The procedure can then be iterated. Transformation of the <br />dependent variable will usually be the more critical. Therefore, a reasonable practical <br />procedure will often be to combine straightforward investigation of transformation <br />of the dependent variable with Box and Tidwell&#39;s method applied to the independent <br />variables. <br />It is possible also to consider simplifications of the procedure for determining a <br />transformation of the dependent variable. The main labour in straightforward <br />application of the method of Section 3 is in applying the transformation for various <br />values of h and then computing the standard analysis of variance for each set of <br />transformed data. Such a sequence of similar calculations is straightforward on an <br />electronic computer. It is perfectly practicable also for occasional desk calculation, <br />although probably not for routine use. There are a number of possible simplifications <br />based, for example, on expansions like (69) or even (55), but they have to be used <br />very cautiously. <br />In the present paper we have concentrated largely on transformations for those <br />standard &quot;fixed-effects&quot; analysis of variance situations where the response can be <br />treated as a continuous variable. The same general approach could be adopted in <br />dealing with &quot;random-effects&quot; models, and with various problems in multivariate <br />analysis and in the analysis of time series. We shall not go into these applications <br />here. <br />An important omission from our discussion concerns transformations specifically <br />for data suspected of following the Poisson or binomial distributions. There are two <br />difficulties here. One is purely computational. Suppose we assume that our obser- <br />vations, y, follow, for example, Poisson distributions with means that obey an <br />additive law on an unknown transformed scale. Thus, in a row-column arrangement, <br />it might be assumed that the Poisson mean in row i and column j has the form <br />(P+ai+pj&gt;llh (XfO), <br />Pj <br />(A = O),</p>  <p>Page 34</p> <p>19641 Box AND COX-An Analysis o f  Transformations 243 <br />where h is unknown. Then h and the other parameters of the model can be estimated <br />by maximum likelihood (Cochran, 1940). It would probably be possible to develop <br />reasonable approximations to this procedure although we have not investigated <br />this matter. <br />An essential distinction between this situation and the one considered in Section 3 <br />is that here the untransformed observations y have known distributional properties. <br />The analogous normal theory situation would involve observations y normally <br />distributed with constant variance on the untransformed scale, but for which the <br />population means are. additive on a transformed scale. The maximum-likelihood <br />solution in this case would involve, at least in principle, a straightforward non-linear <br />least-squares problem. However, this situation does not seem likely to arise often; <br />certainly, it is inappropriate in our examples. <br />An important possible complication of the analysis of data connected with <br />Poisson and binomial distributions has been particularly stressed by Bartlett (1947). <br />This is the presence of an additional component of variance of unknown form on <br />top of the Poisson or binomial variation. If inspection of the data shows that such <br />additional variation is &#39;substantial, it may be adequate to apply the methods of <br />Section 3. For integer data with range (0, 1, ...) it will often be reasonable to consider <br />power transformations. For data in the form of proportions of &quot;successes&quot; in which <br />&quot;SUCC~SS~S&quot; and &quot;failures&quot; are to be treated symmetrically, Professor J. W. Tukey <br />has, in an unpublished paper, suggested the family of transformations from y to <br />yh-(1 -y)h. <br />For suitable X&#39;s this approximates closely to the standard transforms of proportions, <br />the probit, logistic and angular transformations. The methods of the present paper <br />could be applied with this family of transformations. <br />ACKNOWLEDGEMENT ?<br />We thank many friends for remarks leading to the writing of this paper. ?<br />REFERENCES <br />of residuals&quot;, Proc. <br />F. J. &quot;Examination <br />Fourth<br />ANSCOMBE, <br />Statist. and Prob., 1, 1-36. <br />-and TUKEY, J. W. (1963), &quot;The examination and analysis of residuals&quot;, Technometrics, 5, <br />141-160. <br />BARTLETT, M. S. (1937), &quot;Properties of sufficiency and statistical tests&quot;, Proc. Roy. Soc. A, 160, <br />268-282. <br />-(1947), &quot;The use of transformations&quot;, Biometries, 3, 39-52. ?<br />Box, G. E. P. and&#39; TIDWELL, P. W. (1962), &quot;Transformation of the independent variables&quot;, ?<br />Technometrics, 4, 531-550. <br />COCHRAN, W. G. (1940), &quot;The analysis of variance when experimental errors follow the Poisson <br />or, binomial laws&quot;, Ann. math. Statist., 11, 335-347. <br />CORNISH, E. A. (1954), &quot;The multivariate t distribution associated with a set of normal sample <br />deviates&quot;, Austral. J. Physics, 7, 531-542. <br />DUNNETT, C. W. and SOBEL, M. (1954), &quot;A bivariate generalization of Student&#39;s t distribution&quot;, <br />Biometrika, 41, 153-169. <br />JEFFREYS, <br />H. (1961), Theory of Probability, 3rd ed. <br />KLECZKOWSKI,<br />A. (1949), &quot;The transformation of local lesion counts for statistical analysis&quot;, <br />Ann. appl. Biol., 36, 139-152. <br />TUKEY, J. W. (1949), &quot;One degree of freedom for non-additivity&quot;, Biornetrics, 5, 232-242. <br />-(1950), &quot;Dyadic anova, an analysis of variance for vectors&quot;, Human Biology, 21, 65-110. <br />-and MOORE, P. G. (1954), &quot;Answer to query 112&quot;, Biometrics, 10, 562-568. <br />(1961), <br />Berkeley Symp. Math. <br />Oxford University Press.</p>  <p>Page 35</p> <p>244 <br />Discussion on Paper by Professor Box and Professor Cox <br />[No. 2, <br />Mr J. A. NELDER: May I begin with a definition (from the Concise Oxford Dictionary) : <br />&quot;Box and Cox-two persons who take turns in sustaining a part.&quot; I must admit to having <br />spent some time in trying to deduce which person was sustaining which part of this most <br />interesting paper. I do not think the exercise was very successful, and this testifies to some <br />sound collaboration on the part of the authors. <br />It seems to me that there are two basic problems besetting all conscientious data <br />analysts (to borrow Professor Tukey&#39;s term). One is how to check that the data are not <br />contaminated with rogue observations and what action to take if they are. The other is <br />how to check that the model being used to analyse the data is substantially the right one. <br />Looking through the corpus of statistical writings one must be struck, I think, by how <br />relatively little effort has been devoted to these problems. The overwhelming preponder- <br />ance of the literature consists of deductive exercises from a priori starting points. Now, <br />of course, there must always be some assumptions made a priori; in data analysis the <br />important thing is that they should not be much stronger than previous evidence justifies. <br />The first of the two problems, that of gross errors or rogue observations, we are not <br />directly concerned with now, but the question of scale for analysis, which is discussed <br />here, is fundamental to the second. One sees not infrequently remarks to the effect that <br />the design of an experiment determines the analysis. Life would be easier if this were <br />true. To the information from the design we must add the analyst&#39;s prior judgements, <br />preconceptions or prejudices (call them what you will) about questions of additivity, <br />homoscedasticity and the like. Frequently these prior assumptions are unjustifiably strong, <br />and amount to an assertion that the scale adopted will give the required additivity, etc. <br />The great virtue of this paper lies in its showing us how to weaken these prior assumptions <br />and allow the data to speak for themselves in these matters. The data analyst&#39;s two <br />problems are closely intertwined, however; for if rogue observations are present their <br />residuals tend to dominate the residual sum of squares, and may thus seriously affect the <br />estimation of h. <br />The two approaches, via likelihood and via Bayes theorem, run side by side, and give <br />results which will often be very similar. I am not entirely happy about the derivation of <br />equation (19) and wonder whether the appearance of the observations in the prior proba- <br />bility is not only &quot;interesting&quot;, as the authors state,&#39;but also illegal. They remark (on <br />p. 219) that, &quot;There are some reasons for thinking L,(h) preferable to L,,,(h) <br />Bayesian as well as from a Bayesian point of view.&quot; I agree and, furthermore, I believe <br />that a suitable modification of the likelihood approach may be found to produce just this <br />result. The starting point is that fixed effects are unrealistic in a model. If we measure a <br />treatment effect in an experiment, it is common experience that a further experiment will <br />give us a further estimate of the effect which often differs from the original estimate by <br />more than the internal standard errors of the experiments would lead us to expect. If we <br />construct a model with this in mind, then for a single normal sample of n we might obtain <br />from anon- <br />where m = N(p, ut2) and ei = N(0, u2). If we now do an orthogonal transformation of <br />the data z = Hy where H is an orthogonal matrix of known coefficients having its first row <br />with elements n-4, then the log likelihood is given by <br />n <br />L = const -: In V- (z, -,u Jn)2/2V- $(n -1) log u2 -2z,2/2u2, <br />where V = u2+ nu&#39;2. Clearly we cannot estimate V unless , u  is known, which in general <br />it is not. However, for any fixed but unknown V,we have L maximized by taking <br />P = 8, and B2 = C(y-jj)2/(n-1).</p>  <p>Page 36</p> <p>Discussion on Paper by Professor Box and Professor Cox <br />&#39;Thus L,,,(h) <br />L,(h). By extensions of this argument we obtain Bartlett&#39;s criterion for testing the homo- <br />geneity of variances instead of the L, criterion, and the likelihood criterion for a restricted <br />hypothesis on the means (equation (35)) becomes the same (apart from an unknown <br />constant factor) as the Bayesian one. Thus some of the apparent differences between the <br />two approaches may result from the restrictions implied by fixed effects in a model, these <br />being equivalent to assertions of zero variance in repetitions of the experiment. <br />Taken with the work of Tukey, Daniel and others, on the detection of rogue obser- <br />vations, the results of this paper should lead before long to substantial improvements in <br />computer programmes for the analysis of experiments. &quot;First generation&quot; programmes, <br />which largely behave as though the design did wholly define the analysis, will be replaced <br />by new second-generation programmes capable of checking the additional assumptions <br />and taking appropriate action. It is hardly necessary to stress what an advance this <br />would be. <br />I suppose that the converse of &quot;two persons who take turns in sustaining a part&quot; <br />would be &quot;one person who takes turns in sustaining two parts&quot;. Such a person is often <br />the proposer of the vote of thanks, the parts being those of congratulator and critic; <br />the latter has been known to overwhelm the former, but not, I hope, today. We must <br />all be grateful for the clear exposition of an important problem, for the practical value <br />of the results obtained and for the possibilities opened up for future investigations.. It is <br />a real pleasure, therefore, for me to propose the vote of thanks today. <br />following equation (24) is replaced (apart from an unknown constant) by <br />Dr J. HARTIGAN:<br />problem. Suppose in the ith experiment we observe yi under conditions xi and that it is <br />desired to find the probability distribution of y given x for various x. The only general <br />principle that seems to apply is a similarity principle-&quot;What <br />circumstances will probably be similar to what happened under similar circumstances in <br />the past&quot; or more simply &quot;like equals likely&quot;. The Meteorological Office does seem to be <br />acting according to this principle in its long-range forecasts, where the procedure is to <br />look at this month&#39;s weather, look in the records for a similar month, see what happened <br />the following month then, and predict the same thing will happen next month, now- <br />they would say, to predict what yo will be under conditions x,, look among the (y,, xi) <br />for an xi close to x,, then predict yo = y,. <br />It does seem possible to offer a non-parametric method for predicting a new y at x,; <br />in least squares theory this would be the fitted value Yo. The general procedure is to <br />smooth from the various readings (y, x) in the neighbourhood of x,, values of y being <br />given greater or less weight according to x&#39;s &quot;similarity&quot; to x,; just how the weights are <br />to be chosen, or how the y&#39;s are to be combined is an open question; the least squares <br />answer is Yo = Xa, y,, where the weights a, (possibly negative, but not very, and nearly <br />always adding to one) are calculated from the linear model. <br />Box and Cox are assuming that for some transformed set of observations f(yi), the <br />model is valid, and their smoothed value would be given by <br />I would like to suggest a non-parametric approach to Box and Cox&#39;s <br />will happen under present <br />A &quot;non-parametric&quot; approach would be to order the observations y,,,, ...,y,;) and <br />select Yo such that <br />Essentially, Yo is the median of the distribution consisting of points y(i, with probability a, <br />(possible negative values confuse this interpretation). The justification of this procedure <br />is that Yo should not be too far from the value obtained by Box and Cox&#39;s procedure, <br />since the median of the f(yi)&#39;s will be approximately equal to the mean of the f(yi)&#39;s; <br />but this procedure is invariant under any monotonic transformation of the observations.</p>  <p>Page 37</p> <p>246 <br />Discussion on Paper by Professor Box and Professor Cox <br />[No. 2, <br />I have tried this with Box and Cox&#39;s 33 experiment, when x, is at the centre of the <br />cube (O,O, 0). The weights a, will depend on the linear model; for a complete factorial <br />model a, = 1 at (0, 0, 0) and 0 elsewhere so that no smoothing takes place; for the second- <br />degree polynomial model a, = 7 at the centre, 4 at the midpoint of a face, 1 at the midpoint <br />of an edge and -2 at a vertex; for the first- and zero-degree polynomials, a, = 1 every- <br />where and the smoothing is excessive. <br />The smoothed values with various similarity coefficients (we may regard ai as the <br />relevance of the ith observation to Yo) and various methods of combination are <br />Degree of <br />Polynomial Mean <br />0,1 861 <br />2, 724 <br />C F 620 <br />Negative weights are a,nuisance, and, also, we would like the similarity coefficients to <br />decrease with distance. However, least squares is the only general way of generating the <br />coefficients at present. <br />I wonder if the interquartile range of the distribution over the y, with weights a, would <br />be a reasonable (transformation invariant) measure of dispersion of a new observation y <br />about Yo. In general this would tend to be large if yi&#39;s which were observed under highly <br />similar conditions were a long way from the predicted Yoat xo. <br />A preliminary analysis of the above type based on the order statistics would be invariant <br />under monotonic transformation, and so would seem an appropriate method of finding <br />a transformation in which an ordinary &quot;metric&quot; analysis might be performed. <br />I have found this paper extremely informative and stimulating and it gives me great <br />pleasure to second the vote of thanks to Professors Box and Cox. <br />Mean log <br />564 <br />610 <br />620 <br />Median <br />566 <br />604 <br />620 <br />The vote of thanks was put to the meeting and carried unanimously. <br />The following written contribution was read by Professor D. G. Kendall. <br />Professor J. W. TUKEY: The results reported by Professors Box and Cox clearly <br />represent a substantial step forward; all those concerned with the actual analysis of data <br />should be pleased to know that they do exist, both because of the new and modified <br />techniques which they urge us to try, and because these results were obtained by using <br />almost &quot;all the allowed principles of witchcraft&quot; as of the year 1964: normality assumptions, <br />maximum-likelihood estimation, Bayesian inference and a priori distributions invariant <br />under natural, transitive groups. This last fact makes it inevitable that intelligent choice <br />of modes of expression for the observed responses will become both socially acceptable <br />and widely taught and that the long-run consequences for the analysis of data will be <br />very desirable. <br />While this is a useful step forward, it is, I think, important not to overestimate its <br />conclusiveness. From the point of view of the man who does indeed have data to analyse, <br />these results are merely further guidance about a situation only reasonably close to the <br />one he actually faces. This is, of course, no novelty in statistics, but some aspects of the <br />present discussion make it important to re-emphasize some things that should be familiar <br />to all of us. In the authors&#39; discussion, as in all to nearly all of our presently available <br />theory, all the approaches are at least formally based upon a model involving normality- <br />or, as I would rather say, Gaussiahity. I think that this is stressed by the discussion in <br />Section 5 where one is asked to look first at the evidence from assumed Gaussianity, then <br />at the evidence from an additional assumption of constancy of variance in the presence of <br />Gaussianity and, finally, at the evidence from a further assumption of additivity in the <br />presence of both other assumptions. So long as we are going to work with tight specifi- <br />cations, where only a few parameters can be allowed to enter, it is hard to see how things <br />can be done in any other way than this. But from the point of view of the man with the</p>  <p>Page 38</p> <p>247 <br />19641 <br />Discussion on Paper by Professor Box and Professor Cox <br />actual data, it would make much more sense to ask-possibly <br />which one could examine first the evidence derived from assumed additivity in the absence <br />of other assumptions, secondly (in those situations where this was appropriate) the evidence <br />provided by an additional assumption of constant variance in the presence of additivity, <br />and thirdly (in perhaps a few cases) the additional evidence provided by assumed <br />Gaussianity, in the presence of both additivity and constancy of variance. (If additivity <br />-or, more generally, parsimony-is at issue, considerations of constancy of variance and <br />Gaussianity of distribution are usually negligible, at least so far as the choice of a mode <br />of expression is concerned. If additivity is not at issue, constancy of variance usually <br />dominates Gaussianity of distribution.) If all of us can have enough good ideas over a long <br />enough period of time, perhaps we can come, eventually, to a theory which corresponds <br />more directly to what we desire. It may well be that, with the exception of very rare <br />instances, the differences in practice associated with such an approach would be in- <br />appreciably different from those suggested by the present approach. The widespread <br />tendency for additivity, constancy of variance and Gaussianity of distribution to come <br />and go as a group offers us such a hope. It would be nice to know whether or not this hope <br />is justified. <br />We are all used to having maximum-likelihood estimation combine different bits of <br />evidence with quite appropriate weights. Accordingly, we may hope that this is still the <br />case in the present situation, but I must report that the relative weighting of the evidence <br />provided by interaction sums of squares and error sums of squares does not feel as if it <br />were being quite fairly weighted when one merely looks, as in Table 3, at the total of these <br />two sums of squares. Perhaps the decomposition into the three parts mentioned above, and <br />concentration upon the part associated with the additivity assumption, might produce a <br />much heavier weighting of the interaction sums of squares. Again it would be interesting <br />to know whether or not this is true. <br />In most circumstances one is going to be more interested in reaching additivity than <br />in maximizing the formal sensitivity of the main effects. There will be, however, a few <br />instances where the reverse is true. I am not clear, from the discussion of Table 6, to what <br />extent the results of applying the proposed approach rigorously and without thought <br />will differ from the results obtained by seeking maximum sensitivity. If there should be <br />differences which persist as the amount of data is increased without limit, I think one <br />will have, in the long run, to look more carefully into the choice of criterion, where a <br />decision to look need not imply an ultimate decision to adopt a different criterion. <br />Clearly Box and Cox have made a major step forward in the succession of approxi- <br />mations which give us better and better answers to an important problem of practice. <br />in vain-for  an analysis in <br />The following written contribution was read by the Honorary Secretary. <br />Professor R. L. PLACKETT: The authors have come up with the interesting ideas we <br />would have expected from them, and deserve our congratulations for a paper which will <br />be widely appreciated. They have made full use of modern computational facilities and <br />the two systems of inference which are currently competing for our attention. An <br />impression ltft by reading their paper is that the data should be fed into a large and <br />powerful machine which will very quickly draw all the necessary graphs and print out the <br />best analysis of variance available in the circumstances. Those accustomed to the blissful <br />ease of the standard analysis of variance calculations will need to be convinced that such <br />hard work is really necessary, and will ask for assuran&#39;ce that too much responsibility has <br />not been delegated. <br />So much has recently been said on Bayesian procedures that it is a relief to find that <br />the authors are not really Bayesians at all, but have been very ingenious in using Bayesian <br />arguments without ever becoming fully committed to them. Thus they call for uniform <br />distributions, but only over the region where the likelihood is appreciable, and they justify <br />their preference for a Bayesian procedure on the grounds that the confidence coefficients</p>  <p>Page 39</p> <p>248 <br />Discussion on Paper by Professor Box and Professor Cox <br />[No. 2, <br />from asymptotic distribution theory are closer to their nominal values if Lb is used instead <br />of L,,,. <br />It is true that in the further analysis separating out A and H they suggest that <br />the two procedures may lead to appreciably different conclusions, but the circumstances <br />in which this might occur are not closely defined. Surely it is not the magnitude of either <br />S,,(h; z) or F(h; z) which is relevant, but that of the derivatives of these quantities with <br />respect to A. In any case, the authors do not tell us what they would do if the conclusions <br />differ markedly; but it accords with the spirit of this long-awaited collaboration that we <br />should be left in doubt as to which method of inference to follow. <br />Likelihood procedures have also been well publicized and discussed, but there is a <br />practical point which seems not to have been emphasized in the midst of a good deal of <br />mathematical and logical argument. It arises because the likelihood function contains <br />much that is taken for granted in the way of distributional forms, and is no substitute <br />for an inspection of the data. As a simple illustration, consider a large sample of measure- <br />ments in which half are clustered round the value a and half round the value b (a# b). <br />The assumption that this constitutes a sample from a normal distribution with mean ,u <br />and standard deviation a leads to an exactly parabolic log likelihood function for ,u, but <br />the inferences that this would suggest conflict with those obtained directly from the data. <br />It is. tempting to contrast the smooth and deceptive character of a likelihood function <br />with the spotty but straightforward nature of Anscombe and Tukey&#39;s procedures. They <br />fit a full linear model to the original data and plot residuals against fitted values. Residuals <br />are something which the authors have not calculated, but it would have been interesting <br />to see other methods at work on the same examples. One might consider a modification <br />of the Anscombe-Tukey procedure in which the predicted value Y is plotted against the <br />observed value y. This will lead to a linearizing transformation Y=f(y) (e.g. by Dolby&#39;s, <br />1963, analysis of the simple family); the procedure can be iterated if necessary and should <br />converge under reasonable conditions. It may be objected that the possibility of differing <br />variances isnot taken into account, but the usual argument is that the same transformation <br />does for both. If a greatly differing transformation is necessary to equalize the variances, <br />then the experiment is unlikely to be very successful. <br />In the second part of their paper, the authors separate the contributions of linearity, <br />constant variance and normality, but the place of normality in their analysis is logically <br />different from that occupied by the other two, since normality is not a constraint which <br />they either apply or relax. For that, they would presumably need to carry through the <br />entire analysis with some other distribution. <br />Professor M. S. BARTLETT: <br />a major step forward in this paper on the theory of transformations. I think also, like <br />Professor Plackett, I was a little uneasy about the extent to which complicated analysis <br />might seem necessary. <br />Again, like Mr Nelder, I found myself wondering about the Box and Cox nature of the <br />paper and in particular whether this kind of oscillatory character between likelihood and <br />Bayes analysis had any relevance to the Box and Cox aspect! Perhaps Professor Cox <br />may wish to comment on this; on this point of Bayes versus likelihood I would especially <br />welcome his views on whether he is advocating them as equally useful or whether he has <br />reached any conclusions as to whether one is better than the other. In particular I would <br />certainly draw attention to the point made in the paper, and I think Professor Plackett <br />made this point also, that whichever analysis you make, the inference is very conditional <br />on your set of assumptions from which you start. <br />Now to come to other minor points, I think I have only two to make. One was in the <br />approximation used for the log likelihood, the max log likelihood and the use of x2with <br />this, and I wondered whether Professor Cox, or for that matter, Professor Box, could make <br />any comment on the accuracy in this in other than very large samples. One knows that <br />the distribution is valid up to but not including order lln, and one knows, for example, <br />from Professor Box&#39;s work, that if you want to go to order lln you have to bring in a <br />Like Professor Tukey, I think that the authors have made</p>  <p>Page 40</p> <p>249 <br />19641 <br />Discussion on Paper by Professor Box and Professor Cox <br />different multiplying factor to your x2approximation. And it would help to know whether <br />there is any possibility of getting the sort of confidence limits based on the x2 analysis a <br />bit more exact, and if not, how misleading they might occasionally be. <br />I think my last point is one that was raised by Professor Tukey and that is, I did wonder <br />about the uniqueness of this order of taking the various factors, normality, additivity and <br />homogeneity of variances, and whether you Would reach anything like the same sort of <br />conclusion if you tried to take them in a different order. <br />Dr M. R. SAMPFORD:<br />assumed normality of the transformed variable on the additivity, in particular, and to a <br />lesser extent on the homogeneity of variance, when in fact no single transformation will <br />achieve all three properties. The relatively small amount of information about h obtained <br />from the normality assumption in the example (Table 8) seems to be reassuring on this <br />point, but the possible effects when the transformed distribution is rather far from normal <br />might still be serious. Of course, one can sometimes advance a more plausible distributional <br />model, and in this context it may be worth suggesting that, though the title of this paper <br />should more properly be &quot;An Analysis of Transformations to Normality&quot;, the ingenious <br />approach on which it is based could perfectly well be applied to other distributions. For <br />example, I have several times encountered response-time distributions-in <br />distributions of time to death-that appear log-normal at the lower end of the scale, but <br />have a secondary mode in the upper tail. This might suggest that some animals die as a <br />direct result of damage caused by the treatment, but that others, having a high tolerance <br />or being, by chance, little damaged, may survive the initial shock, only to die later <br />as a result of physiological disturbance caused by the damage. One might, by making <br />some assumptions about distributions of damage and tolerances, derive a more or less <br />plausible class of distributions for transformed times that might be expected to be con- <br />sistent with variance homogeneity and at least approximate additivity. The method of <br />this paper could then be applied to determine the most satisfactory transformation leading <br />to a distribution in this class. This is perhaps a rather extreme example, but I hope suggests <br />the potential value of the authors&#39; approach in situations where additivity need not be <br />expected to involve, as it often does, near-normality. <br />Like Professor Tukey, I am rather nervous about the effect of the <br />particular, <br />Dr C. A. B. SMITH: I merely wish to draw attention to a recent paper by A. F. Naylor <br />(1964). He applied the arcsine, logit, log-log and normal equivalent deviate transformations <br />to four sets of biological data. He concluded that for all practical purposes they could be <br />considered as equivalent. For example, in most of the entries the expected numbers <br />calculated from the four transformations differ only slightly in the first decimal place. <br />Mr D. KERRIDGE: <br />general comment is that it is very pleasant to have a paper in which the idea is obvious. <br />I am not saying this in any derogatory sense. I think all the great ideas were obvious <br />ones. Nothing could be more obvious than the idea of taking a parametric family and <br />estimating the parameter. It is strange that such an obvious idea should take such a long <br />time to be seen, but in many ways, the simpler the idea, the greater the discovery. There is, <br />for example, much more chance that a simple idea will be used in practice. The particular <br />comment concerns the rather strange prior distribution which has the interesting property <br />that it contains the observations. We cannot let the night go without saying something <br />about that. Clearly this is not an expression of belief, so some people would not call it a <br />probability. It is not prior, because it is determined a posteriori, and so it is a pseudo-prior <br />pseudo-probability. Now I am not against it because of its strangeness, since obviously <br />the authors have extremely good reasons for using it. They use it because it works. It is <br />very interesting indeed to find a practical example in which you have to use something <br />which clearly is a pseudo-probability. I believe that as we get to use Bayes&#39;s theorem <br />instead of talking about it, as I hope we are going to do in the future, we are going to come <br />TheI have two comments to make, one general and one particular.</p>  <p>Page 41</p> <p>250 <br />Discussion on Paper by Professor Box and Professor Cox <br />[No. 2, <br />up against many more of these peculiar things. For example, I think that to get sensible <br />significance tests in Bayesian theory we are going to have to use prior probabilities which <br />depend on the number of observations. These again will be pseudo-probabilities, in a <br />sense pseudo-prior too. So this is a very interesting first example of something which will <br />eventually, I think, shed some light on what probabilities really are. My view is that <br />they do not express beliefs. They are a convenient figment introduced to do something <br />we do not really understand yet, but by examining examples of this sort I hope that one <br />day we will achieve understanding. <br />Mr E. M. L. BEALE: I should like to add my thanks to Professors Box and Cox for a <br />most valuable paper, and to ask one question. Would the authors ever consider using a <br />transformation of the type (1) when some y&#39;s are negative, or one of type (2) where some <br />y,+ h, is negative? Such a transformation obviously has strange arithmetic properties. <br />It gives a real answer if A, is integral, and I think one can always overcome any problems <br />created by the fact that y may not be uniquely determined by the value of y(&quot;. But would <br />the transformation ever make sense statistically? <br />The following written contribution was received after the meeting: ?<br />Professor F. J. ANSCOMBE:?The authors are to be congratulated on a most remarkable <br />paper. The basic idea is highly original, and the tackling of horrendous difficulties is <br />breath-taking. The examples are illuminating, and the preliminary &quot;rather informal&quot; <br />analysis of the textile example is statistry in the grand manner-but, <br />paper is that. <br />Because of my own efforts with residuals, I have been particularly interested by <br />Section 6. In my 1961 paper I gave a formula for roughly estimating the power transforma- <br />tion that would remove Tukey&#39;s type of removable non-additivity, and also one for <br />estimating the power transformation that would remove an exponential dependence of <br />error variance on the mean. The formulas were based essentially on the statistics denoted <br />by TI,and T,,, respectively, in this paper. I did not also give a formula aimed at removing <br />skewness of the error distribution, based on the statistic here denoted by T,,, though I <br />have since used such a formula; in the notation of my 1961 paper the formula goes <br />indeed, the whole <br />(My p is Box and Cox&#39;s h, 8 is the overall sample mean, s the residual root mean square, <br />and g, and g, are analogues of Fisher&#39;s g-statistics.) It was my thought that one would <br />calculate one or more of these expressions, and (if more than one) hope they would <br />somewhat agree. No doubt, with factorial data showing pronounced effects for at least <br />two factors, one would attach primary importance to additivity. With only one effective <br />factor, there would be no question of additivity, and one would attach primary importance <br />to constancy of variance. With no effective factors, and in particular with a simple homo- <br />geneous sample, there would be nothing to worry about except skewness. <br />Now, Professors Box and Cox have shown that these three separate estimates should <br />(very nearly) be averaged in a certain proportion to yield a best estimate of the power. <br />This result, for the relatively simple calculations based on residuals from a least-squares <br />analysis on one scale, parallels the subtle decomposition of the likelihood function into <br />three parts in Section 5. <br />Professor Cox replied briefly at the meeting and the authors subsequently replied <br />more fully in writing as follows: <br />We are very grateful to the speakers for their encouraging and helpful remarks. <br />One important general issue raised by Professors Tukey, Plackett, Bartlett and <br />Dr Sampford cbncerns priorities for the criteria of simplicity of the model and specifically <br />of additivity, A, homogeneity of variance, H, and normality, N. We certainly agree on</p>  <p>Page 42</p> <p>19641 <br />Discussion on Paper by Professor Box and Professor Cox <br />251 <br />the importance of the first of these, as indeed we indicate in our remarks at the end of <br />Section 2. In the formal analysis of Section 5 we have considered N, HN, AHN as three <br />models in that order. If one is to employ a parametric approach one must, it seems, start <br />from some distributional assumption although, of course, if desired this could be broader <br />than that adopted here. Furthermore, there is no reason in principle why A should not <br />have been taken before H in discussing the biological example. We would then have to <br />fit an additive model with separate within-cell variances. The rough justification for <br />thinking that the procedure given in the paper genuinely separates out the effects of <br />N,H and A is that M(h; z), on which (47) and (51) depend, is a valid descriptive <br />measure of heterogeneity of variance independently of N. Likewise P(h; z) is a descriptive <br />measure of non-additivity independently of H and N. If we started from a non-normal <br />model, we would get a different measure of heterogeneity of variance, but except in <br />extreme circumstances it is unlikely that it would be minimized by a value of h very <br />different from that minimizing M(h; z). An analogous remark applies to F(h; z). Under <br />non-normality the weighting of the different requirements will be different, but it is hard <br />to see how a radically different value of h could emerge from the final analysis. <br />Concerning Professor Tukey&#39;s point about the appropriateness of the weighting given <br />by the likelihood in the biological example, the truth seems to be that in this example <br />non-additivity is not in fact the major contribution in determining h. The sizes of the <br />mean squares in Table 3 seem rather to bear this out than to contradict it. Concerning <br />Tables 3 and 6 a striking thing is not only the removal of non-additivity, or correspondingly <br />in Table 6 the simplification of the model, but also the large increase in sensitivity of the <br />experiment. The result achieved by transformation is in fact equivalent to threefold <br />increase in experimental effort. <br />In the. paper we were at pains to stress that, where the procedures do seem relevant, <br />we recommend using them in a flexible way, and that the assumptions on which they are <br />based are a tentative working basis for the analysis rather than anything to be adopted <br />irrevocably. In particular, in the discussion of the textile example we deliberately gave <br />first the &quot;common-sense&quot; analysis before the more elaborate one. As Mr Kerridge has <br />very rightly stressed, the basic idea is an extremely simple one; in particular, the absence <br />of iterative calculations is a considerable practical advantage. We hope that this will <br />reassure Professor Plackett that we are not advocating unnecessary elaboration. Mr Nelder <br />has stated extremely clearly the need for a more searching examination of &quot;assumptions&quot;. <br />We have not specifically investigated the point raised by Professor Bartlett concerning <br />the adequacy of the chi-squared approximation for confidence intervals for h. However, <br />the line we have followed in finding a closer approximation to the posterior density of h <br />leads to posterior intervals based on the Fdistribution and a similar approximation might <br />be found for confidence intervals. The use of L,(h) instead of L,,,(h) <br />analogy with Bartlett&#39;s (1937) procedure of applying the likelihood-ratio procedure after <br />suitable contrasts have been removed by transformation. The difficulty when h is unknown <br />is that the transformations to remove the parameters 8 depend on A ,  so that the argument <br />is at best approximate. We were most interested in Mr Nelder&#39;s remarks on this point <br />and hope that he will develop his ideas further. <br />The maximum-likelihood approach and the Bayesian approach have deliberately been <br />given as entirely separate but parallel developments. Professor Plackett suggests that we <br />justify the Bayesian approach only because it leads to &quot;better&quot; confidence intervals; this <br />is not so. Several speakers have commented on the special prior distribution (19) which <br />involves the observations. As we remarked in the paper, it is possible that there is an <br />alternative and better approach to this; one way may be to make the prior distributions <br />for the contrasts depend on the general population mean. However, the observations <br />enter (19) only in a mild way in establishing the overall level of the observations, usually <br />the overall geometric mean in our special cases. It is essential that some allowance should <br />be made for the fact that the prior distribution for the magnitude of the contrasts depends <br />on the overall magnitude of the observations. <br />was suggested by</p>  <p>Page 43</p> <p>Discussion on Paper by Professor Box and Professor Cox <br />[No. 2, <br />In answer to Mr Beale&#39;s question, we feel that, while it is probably possible to develop <br />the theory for non-monotonic transformations of the dependent variable, we cannot think <br />of any situations where such transformations would be physically allowable. <br />We are grateful to Dr Smith for his reference to Naylor&#39;s work. However, Naylor <br />seems to be considering situations where the transformations are, over the relevant range, <br />practically linear functions of one another. In our examples the relative range of variation <br />of the observations is high, the transformations are very non-linear and this is of course <br />why we are able to obtain fairly sharp discrimination between the different values of A. <br />In the quanta1 response case, the transformations in question become essentially different <br />only in the tails of the response curve, and observations there would be required for the <br />differences to be detectable and of practical importance. <br />We are very interested in Professor Anscombe&#39;s remarks on residuals. Further <br />comparisons of the analysis of residuals with the methods of our paper would be of value. <br />We are interested in Dr Hartigan&#39;s problem and formulation. However, this seems <br />essentially different from ours, partly because in our applications we are primarily interested <br />in changes in response, rather than in absolute responses, and partly because one of our <br />primary objectives is to find a scale on which the factor effects are succinctly characterized <br />by a few parameters. Even if the distributional assumptions were to be phrased non- <br />parametrically (which we would in any case not wish to do), we must have parameters <br />in order to describe at all concisely the changes in response in a complex system. <br />REFERENCES<br />IN THE DISCUSSION <br />DOLBY, J. L. (1963), &quot;A quick method for choosing a transformation&quot;, Technometrics, 5, 317-326. <br />NAYLOR, A. F. (1964), &quot;Comparisons of regression constants fitted by maximum likelihood to <br />four common transformations of binomial data&quot;, Ann. hum. Genet., Lond., 27, 241-246.</p>   </div> <div id="rgw19_56aba196a5cf3" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56aba196a5cf3">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56aba196a5cf3"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://www.ime.usp.br/~abe/lista/pdfQWaCMboK68.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="An Analysis of Transformation">An Analysis of Transformation</a> </div>  <div class="details">   Available from <a href="http://www.ime.usp.br/~abe/lista/pdfQWaCMboK68.pdf" target="_blank" rel="nofollow">usp.br</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw23_56aba196a5cf3" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56aba196a5cf3">  </ul> </div> </div>   <div id="rgw15_56aba196a5cf3" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56aba196a5cf3"> <div> <h5> <a href="publication/267299897_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transformation_Characteristics_and_Approaches_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transforma" class="color-inherit ga-similar-publication-title"><span class="publication-title">Beyond Distance Teaching Towards Open Learning: A Conceptual Analysis of Transformation, Characteristics and Approaches Beyond Distance Teaching Towards Open Learning: A Conceptual Analysis of Transformation, Characteristics and Approaches</span></a>  </h5>  <div class="authors"> <a href="researcher/2056736228_Meng-Ching_Hu" class="authors ga-similar-publication-author">Meng-Ching Hu</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56aba196a5cf3"> <div> <h5> <a href="publication/237454509_Transformation_Dynamics_in_Utility_Systems_An_integrated_approach_to_the_analysis_of_transformation_processes_drawing_on_transition_theory" class="color-inherit ga-similar-publication-title"><span class="publication-title">Transformation Dynamics in Utility Systems An integrated approach to the analysis of transformation processes drawing on transition theory</span></a>  </h5>  <div class="authors"> <a href="researcher/23414679_Kornelia_Konrad" class="authors ga-similar-publication-author">Kornelia Konrad</a>, <a href="researcher/357909_Bernhard_Truffer" class="authors ga-similar-publication-author">Bernhard Truffer</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56aba196a5cf3"> <div> <h5> <a href="publication/223091326_A_microstructure-based_analysis_for_transformation_induced_plasticity_and_mechanically_induced_martensitic_transformation" class="color-inherit ga-similar-publication-title"><span class="publication-title">A microstructure-based analysis for transformation induced plasticity and mechanically induced martensitic transformation</span></a>  </h5>  <div class="authors"> <a href="researcher/29967820_Heung_Nam_Han" class="authors ga-similar-publication-author">Heung Nam Han</a>, <a href="researcher/28848092_Chang_Gil_Lee" class="authors ga-similar-publication-author">Chang Gil Lee</a>, <a href="researcher/71935923_Dong-Woo_Suh" class="authors ga-similar-publication-author">Dong-Woo Suh</a>, <a href="researcher/11984116_Sung-Joon_Kim" class="authors ga-similar-publication-author">Sung-Joon Kim</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw29_56aba196a5cf3" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw30_56aba196a5cf3">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw31_56aba196a5cf3" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=HAg26Ef5p6G0IDiUS927PzE6gdrT9S5CiobKmIhOR07kAml8jD-YHmEEhIt1MYfg" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="OKYEUN/3JCTqz5+Sca9zMsA7QKRFh2kg01nXC6UVOue8aQnN7uJMOF1blaSTKzxERj8TTbnJI9zC0YS55HZOAJonLLN0wx+lCg9pnrlMsRc9xFWMFEmyM+0o6JTw691YXopcvUesHoo7EA7qfpBVmFnbpiTBqWXAMbnMI2xvFhI0Wk5NG0yahgEiw36++eibGR/pIlMzK1ktQ95/ewSeZJDSt0hJgVDP4OO8g7uz1MKhcL/Xmuw1yD5N+beDIBNjdHvyTq47IQmxBxhEuQCEXNou+T1fpX8jhvlbOe2BBpc="/> <input type="hidden" name="urlAfterLogin" value="publication/200806059_An_Analysis_of_Transformation"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjAwODA2MDU5X0FuX0FuYWx5c2lzX29mX1RyYW5zZm9ybWF0aW9u"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjAwODA2MDU5X0FuX0FuYWx5c2lzX29mX1RyYW5zZm9ybWF0aW9u"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjAwODA2MDU5X0FuX0FuYWx5c2lzX29mX1RyYW5zZm9ybWF0aW9u"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw32_56aba196a5cf3"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 566;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/2081050688_G_E_P_Box","fullname":"G. E. P. Box","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"3.79","widgetId":"rgw5_56aba196a5cf3"},"id":"rgw5_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=2081050688","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":6,"widgetId":"rgw6_56aba196a5cf3"},"id":"rgw6_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=2081050688","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56aba196a5cf3"},"id":"rgw4_56aba196a5cf3","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=2081050688","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56aba196a5cf3"},"id":"rgw3_56aba196a5cf3","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=200806059","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":200806059,"title":"An Analysis of Transformation","journalTitle":"Stat","journalDetailsTooltip":{"data":{"journalTitle":"Stat","journalAbbrev":"Stat","publisher":"Wisconsin Nurses Association","issn":"0038-9986","impactFactor":"0.00","fiveYearImpactFactor":"0.00","citedHalfLife":"0.00","immediacyIndex":"0.00","eigenFactor":"0.00","articleInfluence":"0.00","widgetId":"rgw8_56aba196a5cf3"},"id":"rgw8_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0038-9986","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"01\/1964;","publicationDateRobot":"1964-01","article":"Series B(2):211--246.","journalTitle":"Stat","journalUrl":"journal\/0038-9986_Stat"}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"An Analysis of Transformation"},{"key":"rft.title","value":"Journal of the Royal Statistical Society"},{"key":"rft.jtitle","value":"Journal of the Royal Statistical Society"},{"key":"rft.volume","value":"Series B"},{"key":"rft.issue","value":"2"},{"key":"rft.date","value":"1964"},{"key":"rft.pages","value":"211--246"},{"key":"rft.issn","value":"0038-9986"},{"key":"rft.au","value":"George E. P. Box,David R. Cox"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw9_56aba196a5cf3"},"id":"rgw9_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=200806059","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":200806059,"peopleItems":[{"data":{"authorUrl":"researcher\/2081050688_George_E_P_Box","authorNameOnPublication":"George E. P. Box","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"George E. P. Box","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2081050688_George_E_P_Box","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56aba196a5cf3"},"id":"rgw12_56aba196a5cf3","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2081050688&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56aba196a5cf3"},"id":"rgw11_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2081050688&authorNameOnPublication=George%20E.%20P.%20Box","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/61687043_David_R_Cox","authorNameOnPublication":"David R. Cox","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"David R. Cox","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/61687043_David_R_Cox","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56aba196a5cf3"},"id":"rgw14_56aba196a5cf3","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=61687043&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56aba196a5cf3"},"id":"rgw13_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=61687043&authorNameOnPublication=David%20R.%20Cox","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw10_56aba196a5cf3"},"id":"rgw10_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=200806059&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":null,"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/200806059_An_Analysis_of_Transformation\/links\/00afa1950cf245659d00146a\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56aba196a5cf3"},"id":"rgw7_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=200806059&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2056736228,"url":"researcher\/2056736228_Meng-Ching_Hu","fullname":"Meng-Ching Hu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/267299897_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transformation_Characteristics_and_Approaches_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transforma","usePlainButton":true,"publicationUid":267299897,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/267299897_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transformation_Characteristics_and_Approaches_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transforma","title":"Beyond Distance Teaching Towards Open Learning: A Conceptual Analysis of Transformation, Characteristics and Approaches Beyond Distance Teaching Towards Open Learning: A Conceptual Analysis of Transformation, Characteristics and Approaches","displayTitleAsLink":true,"authors":[{"id":2056736228,"url":"researcher\/2056736228_Meng-Ching_Hu","fullname":"Meng-Ching Hu","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/267299897_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transformation_Characteristics_and_Approaches_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transforma","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/267299897_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transformation_Characteristics_and_Approaches_Beyond_Distance_Teaching_Towards_Open_Learning_A_Conceptual_Analysis_of_Transforma\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56aba196a5cf3"},"id":"rgw16_56aba196a5cf3","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=267299897","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":23414679,"url":"researcher\/23414679_Kornelia_Konrad","fullname":"Kornelia Konrad","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":357909,"url":"researcher\/357909_Bernhard_Truffer","fullname":"Bernhard Truffer","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/237454509_Transformation_Dynamics_in_Utility_Systems_An_integrated_approach_to_the_analysis_of_transformation_processes_drawing_on_transition_theory","usePlainButton":true,"publicationUid":237454509,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/237454509_Transformation_Dynamics_in_Utility_Systems_An_integrated_approach_to_the_analysis_of_transformation_processes_drawing_on_transition_theory","title":"Transformation Dynamics in Utility Systems An integrated approach to the analysis of transformation processes drawing on transition theory","displayTitleAsLink":true,"authors":[{"id":23414679,"url":"researcher\/23414679_Kornelia_Konrad","fullname":"Kornelia Konrad","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":357909,"url":"researcher\/357909_Bernhard_Truffer","fullname":"Bernhard Truffer","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/237454509_Transformation_Dynamics_in_Utility_Systems_An_integrated_approach_to_the_analysis_of_transformation_processes_drawing_on_transition_theory","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/237454509_Transformation_Dynamics_in_Utility_Systems_An_integrated_approach_to_the_analysis_of_transformation_processes_drawing_on_transition_theory\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba196a5cf3"},"id":"rgw17_56aba196a5cf3","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=237454509","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":29967820,"url":"researcher\/29967820_Heung_Nam_Han","fullname":"Heung Nam Han","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":28848092,"url":"researcher\/28848092_Chang_Gil_Lee","fullname":"Chang Gil Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71935923,"url":"researcher\/71935923_Dong-Woo_Suh","fullname":"Dong-Woo Suh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":11984116,"url":"researcher\/11984116_Sung-Joon_Kim","fullname":"Sung-Joon Kim","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2008","journal":"Materials Science and Engineering A","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/223091326_A_microstructure-based_analysis_for_transformation_induced_plasticity_and_mechanically_induced_martensitic_transformation","usePlainButton":true,"publicationUid":223091326,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.57","url":"publication\/223091326_A_microstructure-based_analysis_for_transformation_induced_plasticity_and_mechanically_induced_martensitic_transformation","title":"A microstructure-based analysis for transformation induced plasticity and mechanically induced martensitic transformation","displayTitleAsLink":true,"authors":[{"id":29967820,"url":"researcher\/29967820_Heung_Nam_Han","fullname":"Heung Nam Han","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":28848092,"url":"researcher\/28848092_Chang_Gil_Lee","fullname":"Chang Gil Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":71935923,"url":"researcher\/71935923_Dong-Woo_Suh","fullname":"Dong-Woo Suh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":11984116,"url":"researcher\/11984116_Sung-Joon_Kim","fullname":"Sung-Joon Kim","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Materials Science and Engineering A 06\/2008; 485(1-2-485):224-233. DOI:10.1016\/j.msea.2007.08.022"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/223091326_A_microstructure-based_analysis_for_transformation_induced_plasticity_and_mechanically_induced_martensitic_transformation","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/223091326_A_microstructure-based_analysis_for_transformation_induced_plasticity_and_mechanically_induced_martensitic_transformation\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba196a5cf3"},"id":"rgw18_56aba196a5cf3","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=223091326","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56aba196a5cf3"},"id":"rgw15_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=200806059&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":200806059,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":200806059,"publicationType":"article","linkId":"00afa1950cf245659d00146a","fileName":"An Analysis of Transformation","fileUrl":"http:\/\/www.ime.usp.br\/~abe\/lista\/pdfQWaCMboK68.pdf","name":"usp.br","nameUrl":"http:\/\/www.ime.usp.br\/~abe\/lista\/pdfQWaCMboK68.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw21_56aba196a5cf3"},"id":"rgw21_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=200806059&linkId=00afa1950cf245659d00146a&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56aba196a5cf3"},"id":"rgw20_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=200806059&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":9,"valueFormatted":"9","widgetId":"rgw22_56aba196a5cf3"},"id":"rgw22_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=200806059","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56aba196a5cf3"},"id":"rgw19_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=200806059&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":200806059,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56aba196a5cf3"},"id":"rgw24_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=200806059&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":9,"valueFormatted":"9","widgetId":"rgw25_56aba196a5cf3"},"id":"rgw25_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=200806059","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56aba196a5cf3"},"id":"rgw23_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=200806059&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"An Analysis of Transformations\nG. E. P. Box; D. R. Cox\nJournal of the Royal Statistical Society. Series B (Methodological), Vol. 26, No. 2. (1964), pp.\n211-252.\nStable URL:\nhttp:\/\/links.jstor.org\/sici?sici=0035-9246%281964%2926%3A2%3C211%3AAAOT%3E2.0.CO%3B2-6\nJournal of the Royal Statistical Society. Series B (Methodological) is currently published by Royal Statistical Society.\nYour use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at\nhttp:\/\/www.jstor.org\/about\/terms.html. JSTOR's Terms and Conditions of Use provides, in part, that unless you have obtained\nprior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in\nthe JSTOR archive only for your personal, non-commercial use.\nPlease contact the publisher regarding any further use of this work. Publisher contact information may be obtained at\nhttp:\/\/www.jstor.org\/journals\/rss.html.\nEach copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed\npage of such transmission.\nThe JSTOR Archive is a trusted digital repository providing for long-term preservation and access to leading academic\njournals and scholarly literature from around the world. The Archive is supported by libraries, scholarly societies, publishers,\nand foundations. It is an initiative of JSTOR, a not-for-profit organization with a mission to help the scholarly community take\nadvantage of advances in technology. For more information regarding JSTOR, please contact support@jstor.org.\nhttp:\/\/www.jstor.org\nSat Sep 29 20:01:42 2007"},{"page":2,"text":"An Analysis of Transformations \nBy G. E. P. Box and \nD. R. Cox \nUniversity of Wisconsin  Birkbeck College, University of London \n[Read at a RESEARCH MEETING  April 8th, 1964, METHODS \nProfessor D. V. LINDLEY in the Chair] \nof the SOCIETY, \nIn the analysis of data it is often assumed that observations y,, y,, ...,y, \nare independently normally distributed with constant variance and with \nexpectations specified by a model linear in a set of parameters 0. In this \npaper we make the less restrictive assumption that such a normal, homo- \nscedastic, linear model is appropriate after some suitable transformation \nhas been applied to the y's. Inferences about the transformation and about \nthe parameters of the linear model are made by computing the likelihood \nfunction and the relevant posterior distribution. The contributions of \nnormality, homoscedasticity and additivity to the transformation are \nseparated. The relation of the present methods to earlier procedures for \nfinding transformations is discussed. The methods are illustrated with \nexamples. \n1. INTRODUCTION \nTHE usual techniques for the analysis of linear models as exemplified by the analysis of \nvariance and by multiple regression analysis are usually justified by assuming \n(i) simplicity of structure for 'E(y); \n(ii) constancy of error variance; \n(iii) normality of distributions; \n(iv) independence of observations. \nIn analysis of variance applications a very important example of (i) is the assumption \nof additivity, i.e. absence of interaction. For example, in a two-way table it may be \npossible to regresent E(y) by additive constants associated with rows and columns. \nIf the assumptions (i)-(iii) are not satisfied in terms of the original observations, \ny, a non-linear transformation of y may improve matters. With this in mind, numerous \nspecial transformations for use in the analysis of variance have been examined in the \nliterature; see, in particular, Bartlett (1947). The main emphasis in these studies has \ntended to be on obtaining a constant error variance, especially when the variance \nof y is a known function of the mean, as with binomial and Poisson variates. \nIn multiple regression problems, and in particular in the analysis of response \nsurfaces, assumption (i) might be that E(y) is adequately represented by a rather \nsimple empirical function of the independent variables x,, x,, ...,xt and we would \nwant to transform so that this assumption, together with assumptions (ii) and (iii), \nis approximately satisfied. In some cases transformation of independent as well as \nof dependent variables might be desirable to produce the simplest possible regression \nmodel in the transformed variables. In all cases we are concerned not merely to find \na transformation which will justify assumptions but rather to find, where possible, \na metric in terms of which the findings may be succinctly expressed."},{"page":3,"text":"212 Box AND COX-An Analysis o f  Transformations \n[No. 2, \nEach of the considerations (i)-(iii) can, and has been, used separately to select a \nsuitable candidate from a parametric family of transformations. For example, to \nachieve additivity in the analysis of variance, selection might be based on \n(a) minimization of the ?F value for the degree of freedom for non-additivity \n(Tukey, 1949); or \n(b) minimization of the Fratio for interaction versus error; or \n(c) maximization of the F ratio for treatments versus error (Tukey, 1950). \nTukey and Moore (1954) used method (a) in a numerical example, plotting \ncontours of Fagainst (A,, A , )  for transformations in the family (y +A,)\". \nthat in their particular example the minimizing values were very imprecisely determined. \nIn both (a) and (b) the general object is to look for a scale on which effects are \nadditive, i.e. to see whether an apparent interaction is removable by a transformation. \nOf course, only a particular type of interaction is so removable. Whereas (a) can be \napplied, for example, to a two-way classification without replication, method (b) \nrequires the availability of an error term separated from the interaction term. Thus, \nif applied to a two-way classification, method (b) could only be used when there was \nsome replication within cells. Finally, method (c) can be used even in a one-way \nanalysis to find the scale on which treatment effects are in some sense most sensitively \nexpressed. In particular, Tukey (1950) suggested multivariate canonical analysis of \n(y,y2) to find the linear combination y+ Ay2 most sensitive to treatment effects. \nIncidentally, care is necessary in using y+ Ay2 over the wide ranges commonly \nencountered with data being considered for transformation, for such a transformation \nis sensible only so long as the value of A and the values of y are such that the \ntransformation is monotonic. \nFor transformation to stabilize variance, the usual method (Bartlett, 1947) is to \ndetermine empirically or theoretically the relation between variance and mean. An \nadequate empirical relation may often be found by plotting log of the within-cell \nvariance against log of the cell mean. Another method would be to choose a trans- \nformation, within a restricted family, to minimize some measure of the heterogeneity \nof variance, such as Bartlett's criterion. We are grateful to a referee for pointing \nout also the paper of Kleczkowski (1949) in which, in particular, approximate fiducial \nlimits for the parameter A in the transformation of y to log(y+ A )  are obtained. The \nmethod is to compute fiducial limits for the parameters in the linear relation observed \nto hold when the within-cell standard deviation is regressed on'the cell mean. \nFinally, while there is much work on transforming a single distribution to \nnormality, constructive methods of finding transformations to produce normality in \nanalysis of variance problems do not seem to have been considered. \nWhile Anscombe (1961) and Anscombe and Tukey (1963) have employed the \nanalysis of residuals as a means of detecting departures from the standard assumptions, \nthey have also indicated how transformations might be constructed from certain \nfunctions of the residuals. \nIn regression problems, where both dependent and independent variables can be \ntransformed, there are more possibilities to be considered. Transformation of the \nindependent variables (Box and Tidwell, 1962) can be applied without affecting the \nconstancy of variance and normality of error distributions. An important application \nis to convert a monotonic non-linear regression relation into a linear one. Obviously \nit is useless to try to linearize a relation which is not monotonic, but a transformation \nis sometimes useful in such cases, for example, to make a regression relation more \nnearly quadratic around its maximum. \nThey found"},{"page":4,"text":"19641 Box AND COX-An Analysis of Transformations \n213 \n2. GENERAL \nON TRANSFORMATIONS REMARKS \nThe main emphasis in this paper is on transformations of the dependent variable. \nThe general idea is to restrict attention to transformations indexed by unknown \nparameters A ,  and then to estimate X and the other parameters of the model by \nstandard methods of inference. Usually X will be a one-, or at most two-, dimensional \nparameter, although there is no restriction in principle. Our procedure then leads \nto an interesting synthesis of the procedures reviewed in Section .I. It is convenient \nto make first a few general points about transformations. \nFirst, we can distinguish between analyses in which either (E) the particular \ntransformation, A ,  is of direct interest, the detailed study of the factor effects, etc., \nbeing of secondary concern; or (b) the main interest is in the factor effects, the choice \nof X being only a,preliminary step. Type (b) is likely to be much the more common. \nNevertheless, (a) can arise, for example, in the analysis of a preliminary set of data. \nOr, again, we may have two factors, A and B, whose main effects are broadly under- \nstood, it being required to study the A ,  if any, for which there is no interaction between \nthe factors. Here the primary interest is in A. In case (b), however, we shall need to \nfix one, or possibly a small number, of X's and go ahead with the detailed estimation \nand interpretation of the factor effects on this particular transformed scale. We \nshall choose X partly in the light of the information provided by the data and partly \nfrom general considerations of simplicity, ease of interpretation, etc. For instance, \nit would be quite possible for the formal analysis to show that say ,\/y is the best \nscale for normality and constancy of variance, but for us to decide that there are \ncompelling arguments of ease of interpretation for working say with logy. The \nformal analysis will warn us, however, that changes of variance and non-normality \nmay need attention in a refined and efficient analysis of logy. That is, the method \ndeveloped below for finding a transformation is useful as a guide, but is, of course, \nnot to be followed blindly. In Section 7' we discuss briefly some of the consequences \nof interpreting factor effects on a scale chosen in the light of the data. \nIn regression studies, it is sometimes necessary to take an entirely empirical \napproach to the choice of a relation. In other cases, physical laws, dimensional \nanalysis, etc., may suggest a particular functional form. Thus, in a study of a chemical \nsystem one would expect reaction rate to be proportional to some power of the \nconcentration and to the antilog of the reciprocal of absolute temperature. Again, \nin many fields of technology relationships of the form \ny K x<?. .x$ \nare very common, suggesting a log transformation of all variables. In such cases \nthe reasonable thing will often be first to apply the transformations suggested by the \nprior reasoning, and after that consider what further modifications, if any, are needed. \nFinally, we may know the behaviour of y when the independent variables xi tend \nto zero or infinity, and certainly, if we are hopeful that the model might apply over a \nwide range, we should consider models that are consistent with such limiting properties \nof the system. \nWe can distinguish broadly two types of dependent variable, extensive and non- \nextensive. The former have a relevant property of physical additivity, the latter not. \nThus yield of product per batch is extensive. The failure time of a component would \nbe considered extensive if components are replaced on failure, the main thing of \ninterest being the number of components used in a long time. Properties like \ntemperature, viscosity, quality of product, etc., are not extensive. In the absence of"},{"page":5,"text":"214  Box AND COX-An Analysis of Transformations \n[No. 2, \nthe sort of prior consideration mentioned in the previous paragraph there is no \nreason to prefer the initial form of a non-extensive variable to any monotonic function \nof it. Hence, transformations can be applied freely to non-extensive variables. For \nextensive variables, however, the population mean of y is the parameter determining \nthe long-run behaviour of the system. Thus in the two examples mentioned above, \nthe total yield of product in a long period and the total number of components used \nin a very long time are determined respectively by the population mean of yield per \nbatch and the mean failure time per component, irrespective of distributional form. \nIn a narrowly technological sense, therefore, we are interested in the population \nmean of y, not of some function of y. Hence we either analyse linearly the untrans- \nformed data or; if we do apply a transformation in order to make a more efficient and \nvalid analysis, we convert the conclusions back to the original scale. Even in circum- \nstances where, for immediate application, the original scale y is required, it may be \nbetter to think in terms of transformed values in which, say, interactions have been \nremoved. \nIn general, we can regard the usual formal linear models as doing two things: \n(a) specifying the questions to be asked, by defining explicitly the parameters \nwhich it is the main object of the analysis to estimate; \n(b) specifying assumptions under which the above parameters can be simply and \neffectively estimated. \nIf there should be conflict between the requirements for (a) and for (b), it is best to \npay most attention to (a), since approximate inference about the most meaningful \nparameters is clearly preferable to formally \"exact\" inference about parameters \nwhose definition is in some way artificial. Therefore in selecting a transformation we \nmight often give first attention to simplicity of the model structure, for example to \nadditivity in the analysis of variance. This allows simplicity of description and also \nthe main effect of a factor A, measured on a scale for which there appears to be \nno interaction with a factor B, often has a reasonable possibility of being valid for \nlevels of B outside those of the initial experiment. \n3. TRANSFORMATION VARIABLE\nOF THE DEPENDENT \nWe work with a parametric family of transformations from y to y(*), the \nparameter A ,  possibly a vector, defining a particular transformation. Two important \nexamples considered here are \nand \nThe transformations (1) hold for y >0 and (2) for y > -A,. Note that since an analysis \nof variance is unchanged by a linear transformation (1) is equivalent to \n\/I:*\nlogy (A=O); \ny'\" '-\n(Af O),"},{"page":6,"text":"19641 \nBox AND COX-An Analysis o f  Transformations 215 \nthe form (1) is slightly preferable for theoretical analysis because it is continuous at \nA = 0. In general, it is assumed that for each A ,  ycA) is a monotonic function of y \nover the admissible range. Suppose that we observe an n x 1 vector of observations \ny = {yl, . . .,yn}, and that the appropriate linear model for the problem is specified by \nwhere y(\" is the column vector of transformed observations, a is a known matrix \nand 0 a vector of unknown parameters associated with the transformed observations. \nWe now assume that for some unknown A ,  the transformed observations \nyiA) (i = 1, . . .,n) satisfy the full normal theory assumptions, i.e. are independently \nnormally distributed with constant variance u2, and with expectations (4). The \nprobability density for the untransformed observations, and hence the likelihood \nin relation to these original observations, is obtained by multiplying the normal \ndensity by the Jacobian of the transformation. \nThe likelihood in relation to the original observations y is thus \nwhere \nWe shall examine two ways in which inferences about the parameters in (5) can \nbe made. In the first, we apply \"orthodox\" large-sample maximum-likelihood \ntheory to (5). This approach leads directly to point estimates of the parameters and \nto approximate tests and confidence intervals based on the chi-squared distribution. \nIn the second approach, via Bayes's theorem, we assume that the prior distributions \nof the 0's and logu can be taken as essentially uniform over the region in which the \nlikelihood is appreciable and we integrate over the parameters to obtain a posterior \ndistribution for A; for general discussion of this approach, see, in particular, Jeffreys \n(1961). \nWe find the maximum-likelihood estimates in two steps. First, for given A ,  (5) is, \nexcept for a constant factor, the likelihood for a standard least-squares problem. \nHence the maximum-likelihood estimates of the 0's are the least-squares estimates \nfor the dependent variable y(\" and the estimate of u2, denoted for fixed A by e2(A), is \nwhere, when a is of full rank, \na, = I-a(afa)-I a', \nand S(A) is the residual sum of squares in the analysis of variance of ycA). \nThus for fixed A ,  the maximized log likelihood is, except for a constant, \n= -+n log G2(A) +log J(A; y). \nIn the important special case (1) of the simple power transformation, the second term \nin (8) is \n(A-1)xlogy,. \nIn (2), when an unknown origin A, is included, the term becomes \nL,,,(A) \n(8) \n(9)"},{"page":7,"text":"216 \nBox AND COX-An Analysis of Transformations \n[No. 2, \nIt will now be informative to plot the maximized log likelihood Lmax(h)against h \nfor a trial series of values. From this plot the maximizing value 3 may be read off \nand we can obtain an approximate 100(1- ol) per cent confidence region from \nwhere vh is the number of independent components in A. The main arithmetic \nconsists in doing the analysis of variance of ych)for each chosen h. \nIf it were ever desired to determine 3more precisely this could be done by determin- \ning numerically the value 3 for which the derivatives with respect to X are all zero. \nIn the special case of the one parameter power transformation ych)= ( Y ~ - l)\/X, \nwhere u(\" is the vector of components (h-ly,\"logy,). The numerator in (12) is the \nresidual sum of products in the analysis of covariance of y(h)and u(\". \nThe above results can be expressed very simply if we work with the normalized \ntransformation \nZ(h)= Y( A )lJlln \nwhere J =J(X; y). Then \nLmax(X)= -&nlog a2(h;z), \nwhere \nZ(h)'arZ(h) S(X; z) \na2(x;Z) = \nn \n3 \n--\n-\nn' \nwhere S(X; z) is the residual sum of squares of ~\nproportional to (S(X;z ) ) - ~and the maximum-likelihood estimate is obtained by \nminimizing S(X; z) with respect to A. \nFor the simple power transformation \n( ~ The maximized likelihood is thus \n1 . \nwhere 3 is the geometric mean of the observations. \nFor the power transformation with shifted location \nwhere gm (y+A,) is the sample geometric mean of the (y+h2)'s. \nConsider now the corresponding Bayesian analysis. Let the degrees of freedom \nfor residual be v, = n -rank (a),and let \nbe the residual mean square in the analysis of variance of ycA);note the distinction \nbetween a2(X),the maximum-likelihood estimate with divisor n, and s2(X)the \"usual\""},{"page":8,"text":"19641 \nBox AND COX-An Analysis o f  Transformations  217 \nestimate, with divisor the degrees of freedom v,. We first rewrite the likelihood (5), \ni.e. the conditional probability density function of the y's given 8, 02, A ,  in the form \nI \n( .,s~(A)+(8-B,)'a'a(8 -6 \nexp -\nP(Y~ 0, 02, A )  = ( 2 7 ~ ) ~ ~  \n(3\" \nwhere 6, is the least-squares estimate of 8 for given A. \nNow consider the choice of the joint prior distribution for the unknown para- \nmeters. We first parametrize so that the 8's are linearly independent and hence \nn- v, in number. Let p,(A) denote the marginal prior density of A. We assume that it \nis reasonable, when making inferences about A ,  to take the conditional prior distri- \nbution of the 8's and logo, given A ,  to be effectively uniform over the range for which \nthe likelihood is appreciable. That is, the conditional prior element given A is \n202\nwhere, for definiteness, we for the moment denote the effects and variance measured \nin terms of y(\" by a suffix A. The factor g(A) is included because the general size \nand range of the transformed observations y(\" may depend strongly on A. If the \nconditional prior distribution (15) were assumed independent of A ,  nonsensical \nresults would be obtained. \nTo determine g(A) we argue as follows. Fix a standard reference value of A ,  say A,. \nSuppose provisionally that, for fixed A ,  the relation between y(\" and y(\") over the \nrange of the observations is effectively linear, say \nWe can then choose g(A) so that when (16) holds, the conditional prior distributions \n(15) are consistent with one another for different values of A. In fact, we shall need \nto apply the answer when the transformations are appreciably non-linear, so that \n(16) does not hold. There may be a better approach to the choice of a prior distribution \nthan the present one. \nIt follows from (16) that \nlog a :  = const+log o : *  \nand hence, to this order, the prior density of 02, is independent of A. However, the \n8,'s are linear combinations of the expected values of the y(n)'s, so that \n(17) \nSince there are n -v, independent components to 8, it follows that g(A) is proportional \nto l\/lY-vr. \nFinally we need to choose I,. In passing from A, to A ,  a small element of volume \nof the n dimensional sample space is multiplied by J(A; y)\/J(A,; y). An average scale \nchange for a single y component is the nth root of this and, since A, is only a standard \nreference value, we have approximately \nThus, approximately, the conditional prior density (15) is"},{"page":9,"text":"218 Box AND COX-An Analysis o f  Transformations [No. 2, \nThe combined prior element of probability is thus \nwhere we now suppress the suffix X on 8 and a. \nThis is only an approximate result. In particular, the choice of (18) is somewhat \narbitrary. However, when a useful amount of information is actually available from \nthe data about the transformation, the likelihood will dominate and the exact choice \nof (19) is not critical. The prior distribution (19) is interesting in that the observations \nenter the approximate standardizing coefficient J(X; y). \nWe now have the likelihood (14) and the prior density (19) and can apply Bayes's \ntheorem to obtain the marginal posterior distribution of X in the form \nwhere Kh is a normalizing constant independent of A ,  chosen so that (20) integrates \nto one with respect to A ,  and \nThe integral (21) can be evaluated to give \nSubstituting into (20); we have that the posterior distribution of X is \nwhere K, is a normalizing constant independent of X. \nThus the contribution of the observations to the posterior distribution of X is \nrepresented by the factor \n{J(X;y)}vr~?b\/{sz(X)}~v~ \nor, on a log scale, by the addition of a term \nLb(X)= -+vTlog sz(X) +(vr\/n) log J(h ;y) \n(22) \nto logpo(4. \nOnce again if we work with the normalized transformation \nresult is expressed with great simplicity, for \n=y(\"\/J1In, the \nand the posterior density is"},{"page":10,"text":"19641 Box AND COX-An Analysis o f  Transformations  219 \nIn practice we can plot (S(h; z))-*\". against A ,  combining it with any prior \ninformation about A. When the prior density of h can be taken as locally uniform, \nthe posterior distribution is obtained directly by plotting \np,(h) = k(S(h; z)}-tv~, \nwhere k is chosen to make the total area under the curve unity. \nWe normally end by selecting a value of h in the light both of this plot and of \nother relevant considerations discussed in Section 2. We then proceed to a standard \nanalysis using the indicated transformation. \nThe maximized log likelihood and the log of the contribution to the posterior \ndistribution of h may be written respectively as \nL,,,(h) \n= -&n log (S(h ;z)\/n}, Lb(X)= -&vrlog {S(X; z)\/v,}. \nThey differ only by substitution of v, for n. They are both monotonic functions of \nS(X; z) and their maxima both occur when the sum of squares S(h; z) is minimized. \nFor general description, L,,,(h)  and Lb(X) are substantially equivalent. However, \nit can easily happen that v,\/n is appreciably less than one, even when n is quite large. \nTherefore, in applications, the difference cannot always be ignored, especially when \na number of models are simultaneously considered. \nThere are some reasons for thinking Lb(h) preferable to L,,,(h) \nBayesian as well as from a Bayesian point of view; see, for example, the introduction \nby Bartlett (1937) of degrees of freedom into his test for the homogeneity of variance. \nThe general large-sample theorems about the sampling distributions of maximum- \nlikelihood estimates, and the maximum-likelihood ratio chi-squared test, apply just \nas much to Lb(h) as to L,,,(h). \n(24) \nfrom a non- \n4. Two EXAMPLES \nWe have supposed that after suitable transformation from y to y(\", (a) the \nexpected values of the transformed observations are described by a model of simple \nstructure; (b) the error variance is constant; (c) the observations are normally \ndistributed. Then we have shown that the maximized likelihood for h, and also the \napproximate contribution to the posterior distribution of A ,  are each proportional \nto a negative power of the residual sum of squares for the variate dh) =Y(~)\/J~\/\". \nThe \"overall\" procedure seeks a set of transformation parameters h for which \n(a), (b) and (c) are simultaneously satisfied, and sample information on all three \naspects goes into the choice. In this Section we now apply this overall procedure to \ntwo examples. In Section 5 we shall show how further analysis can show the separate \ncontributions of (a), (b) and (c) in the choice of the transformation. We shall then \nillustrate this separation using the same two examples. \nThe above procedure depends on specific assumptions, but it would be quite \nwrong for fruitful application to regard the assumptions as final. The proper attitude \nof sceptical optimism is accurately expressed by saying that we tentatively entertain \nthe basis for analysis, rather than that we assume it. The checking of the plausibility \nof the present procedure will be discussed in Section 5. \nA Biological Experiment using a 3x 4 Factorial Design with Replication \nTable 1 gives the survival times of animals in a 3 x 4 factorial experiment, the \nfactors being (a) three poisons and (b) four. treatments. Each combination of the \ntwo factors is used for four animals, the allocation to animals being completely \nrandomized."},{"page":11,"text":"- -- - - \n220 \nBox AND COX-An ArzulysB oj\" Transformations \n[No. 2, \nWe consider the application of a simple power transformation y(\" = (y\" \nEquivalently we shall actually analyse the standardized variate zch) = (yh-l)\/(hjh-I). \n1))h. \nTABLE 1 \nSurvival times (unit, 10hr) o f  animals in a 3x 4 factorial experiment \nTreatment \nPoison \nA \nB \nC \nD \nWe are tentatively entertaining the model that after such transformation \n(a) the expected value of the transformed variate in any cell can be represented \nby additive row and column constants, i.e. that no interaction terms are needed, \n(b) the error variance is constant, \n(c) the observations are normally distributed. \nThe maximized likelihood and the posterior distribution are functions of the residual \nsum of squares for zch) after eliminating row and column effects. This sum of squares \nis denoted S(h; z). It has 42 degrees of freedom and is the result of pooling the \n\"within groups\" and the \"interaction\" sums of squares. \nTable 2 gives S(A; z) together with Lm,,(h) and pu(h) over the interesting ranges. \nThe constant k in keLb(h) =pu(h) is the reciprocal of the area under the curve \nY = eLb(h) determined by numerical integration. Graphs of Lm,,(A) and of pu(A) \nare shown in Fig. 1. This analysis points to an optimal value of about = -0.75. \nUsing (11) the curve of maximized likelihood gives an approximate 95 per cent \nconfidence interval for A extending from about -1.13 to -0.37. \nThe posterior distribution pu(A) is approximately normal with mean -0.75 and \nstandard deviation 0.22. About 95 per cent of this posterior distribution is included \nwithin the limits -1.18 and -0.32. \nThe reciprocal transformation has a natural appeal for the analysis of survival \ntimes since it is open to the simple interpretation that it is the rate o f  dying which is \nto be considered. Our analysis shows that it would, in fact, embody most of the \nadvantages obtainable. The complete analysis of variance for the untransformed \ndata and for the reciprocal transformation (taken in the z form) is shown in Table 3. \nWhereas no great change occurs on transformation in the mean squares associated \nwith poisons and treatments, the within groups mean square has shrunk to a third of"},{"page":12,"text":"19641 \nBOXAND COX-An Analysis of Transformations \nTABLE 2 \nBiological data. Calculations based on an additive, homoscedastic, \nnormal model in the transformed observations \nLmax(h) = -24 log B2(h; Z) = \n-k 92.91 ; P,(h) = k eLa(A)= 0.866 x\nlog {S(h;z ) ) - ~ ~  \n10-10{S(h;= ) } - 2 1 .  \n' I  \nFIG. 1. Biological data. Functions Lma,(h) and p,(h). Arrows show approximate \n95 per cent. confidence interval for h. \n9"},{"page":13,"text":"- -- - \nMean squares x 1000 \n222  Box AND COX-An Analysis o f  Transformations \n[No. 2, \nits value and the interaction mean square is now much closer in size to that within \ngroups. Thus, in the transformed metric, not only is greater simplicity of interpre- \ntation possible but also the sensitivity of the experiment, as measured by the ratios \nTABLE 3 \nAnalyses o f  variance o f  biological data \nDegrees \nof \nfreedom \nUntransformed \nReciprocal \ntransformation \n(2form) \nPoisons \nTreatments . \nP x T .  \nWithin groups \n. \n. \n2 \n3 \n6 \n36 \n516.5 \n307.1 \n41.7 \n22.2 \n568.7 \n221.9 \n8.5 \n7.8 \nof the poisons and the treatments mean squares to the residual square, has been \nincreased almost threefold. We shall not here consider the detailed interpretation of \nthe factor effects. \nA Textile Experiment using a Single Replicate o f  a 3, Design \nIn an unpublished report to the Technical Committee, International Wool Textile \nOrganization, Drs A. Barella and A. Sust described some experiments on the behaviour \nof worsted yarn under cycles of repeated loading. Table 4 gives the numbers of \ncycles to failure, y, obtained in a single replicate of a 3, experiment in which the \nfactors are \nx1: length of test specimen (250, 300, 350 mm.), \nx, : amplitude of loading cycle (8, 9, 10 mm.), \nx, : load (40, 45, 50 gm.). \nIn Table 4 the levels of the x's are denoted conventionally by -1, 0, 1. \nIt is useful to describe first the results of a rather informal analysis of Table 4. \nBarella and Sust fitted a full equation of second degree in x,, x, and x,, but the \nconclusions were very complicated and messy. In view of the wide relative range of \nvariation of y, it is natural to try analysing instead log y, and there results a great \nsimplification. All linear regression terms are very highly significant and all second- \ndegree terms are small. Further, it is natural to take logs also for the independent \nvariables, i.e. to think in terms of relationships like \nThe estimates of the P's, from the linear regression coefficients of log y on the \nlog x's, are, with their estimated standard errors, \nSince p11 :  -p,, the combination log x, -log x, = log (x,\/x,) is suggested by the \ndata as of possible importance. In fact, x,\/xl is just the fractional amplitude of the \nloading cycle; indeed, naPve dimensional considerations suggest this as a possible \nfactor, although there are in fact other relevant lengths, so that dependence on x1"},{"page":14,"text":"19641 \nBox AND COX -An Analysis of Transformations \n223 \nand x, separately is not inconsistent with dimensional considerations. If, however, \nwe write x,\/x, = x, and round the regression coefficients, we have the simple formula \ny Cc xy5 x33 \nwhich fits the data remarkably well. \nTABLE 4 \nCycles to failure of worsted yarn: 33factorial experiment \nFactor levels \nCycles to failure, y \nx1 \nx2 \nx3 ?\nIn this case, there seem strong general arguments for starting with a log trans- \nformation of all variables. Power laws are frequently effective in the physical sciences; \nalso, provided that the signs of the p's are right, (25) has sensible limiting behaviour \nfor x2,x3+0,co; finally, the obvious normal theory model based on transforming \n(25) gives distributions over positive values of y only."},{"page":15,"text":"224 \nBox AND COX-An Analysis of Transformations \n[No. 2, \nNevertheless, it is interesting to see whether the method of the present paper \napplied directly to the data of Table 4 produces the log transformation. In this \npaper, transformations of the dependent variable alone are considered; in fact, since \nthe relative range of the x's is not very great, transformation of the x's does not have \na big effect on the linearity of the regression. \nWe first consider the application of a simple power transformation in terms, as \nbefore, of the standardized variate z ( ~ ) = (yh- ~)\/(XJP-~). We tentatively suppose \nthat after such transformation \n(a) the expected value of the transformed response can be represented merely by \na model linear in the x's, . \n(b) the error variance is constant, \n(c) the observations are normally distributed. \nThe maximized likelihood and the posterior distribution are functions of the residual \nsum of squares for z(\" after fitting only a linear model to the x's. Since there are \nfour constants in the linear regression model this residual sum of squares has \n27-4 = 23 degrees of freedom; we denote it by S(h; 2). \nTable 5 shows S(X; z) together with L,,,(h) \nand the results are plotted in Fig. 2. The optimal value for the transformation para- \nmeter is ^h = -0.06. The transformation is determined remarkably closely in this \nand p,(X) over the interesting ranges \nTABLE 5 \nTextile data. Calculations based on normal linear model in the \ntransformed observations \nL,,,(h) \npu(h) = k eLacn) = 0.540 x \n= -13.5 log B2(h;z) = {S(h; ~)}-~~'~+44.49. ?\n{S(h; ~)}-ll'~. ?\nexample, the approximate 95 per cent confidence range extending only from -0.18 \nto +0.06. The posterior distribution p,(X) has its mean at -0.06. About 95 per cent \nof the distribution is included between -0.20 and +0.08. As we have mentioned, \nthe advantages of a log transformation corresponding to the choice X = 0 are very \ngreat and such a choice is now seen to be strongly supported by the data."},{"page":16,"text":"19641 \nBOXAND COX-An Analysis of Transformations \n225 \nThe complete analysis of variance for the untransformed and the log trans-\nformation, taken in the z form, is shown in Table 6. \n- I  \n0 \nI \nX \nFIG. 2. Textile data. Functions L,,(h) \nand p,(h). \nArrows show approximate \n95 per cent confidence interval for h. \nTABLE 6 \nAnalyses of variance of textile data \nMean squares x 1000 \nDegrees \nof \nfreedom \nLogarithmic \ntransformation \n(Z form) \nUntransformed \nLinear \nQuadratic . \nResidual \n. \n. \n3 \n6 \n4,916.2 \n704.1 \n73.9 \n2,374.4 \n8.1 \n11.9 \n. \n17 \nThe transformation eliminates the need for second-order terms in the regression \nequation while at the same time increasing the sensitivity of the analysis by about \nthree, as judged by the ratio of linear and residual mean squares. \nFor this example we have also tried out the procedures we have discussed using \nthe two parameter transformation ych) ={(y+h,)hl- 1)\/X, or in the z form actually"},{"page":17,"text":"226 \nused here zch)= {(y+ \nprint out of 77 analysis of variance tables, involving in each case the fitting of a \ngeneral equation of second degree, and calculation of residuals and fitted values \ntook 2 min. 6 sec. on the C.D.C. 1604 electronic computer. The full numerical \nresults can be obtained from the authors, but are not given here. Instead approximate \ncontours of -11.5 log S(A; z), and hence of S(A; z) itself, of the maximized likelihood \nand of p,(Al, A,), are shown in Fig. 3. If the joint posterior distribution p,(Al, A,) \nwere normal then a region which excluded 100a per cent of the total posterior \nprobability could be given by \nBox AND COX-An Analysis o f  Transformations \n-l)\/{Al gm (y +AZ))hl-l. Incidentally the calculation and \n[No. 2, \nThe shape of the contours indicates that the normal assumption is not very exact. \nNevertheless, the quantity 100a obtained from (26) has been used to label the contours \nin Fig. 3 which thus roughly indicates the posterior probability distribution. For this \nexample no appreciable improvement results from the addition of the further \ntransformation parameter A,. \n300 \n.200 \ni s  \n100 \n0 \n0.2  0 -0.2 -0.4 -0.6 \nA, \n-0.8 \nFIG.3. Textile data. Transformation to (yS A s ) \" .  \nwith approximate percentage of posterior distribution excluded. \nContours of p,(h,, h,) labelled \n5. FURTHER \n5.1. General Procedure for Further Analysis \nThe general procedure discussed above seeks to achieve simultaneously a model \nwith (a) simple structure for the expectations, (b) constant variance and (c) normal \ndistributions. Further analysis is sometimes profitable to see the separate contri- \nbutions of these three elements to the transformation. Such analysis may indicate \n(i) how simple a model we are justified in using; \n(ii) what weight is given to the considerations (a) - (c) in choosing A; \n(iii) whether different transformations are really needed to achieve the different \nobjectives and hence whether or not the value of A chosen using the overall \nprocedure is a compatible compromise. \nOf course, quite often careful inspection of the data will answer (i)-(iii) adequately \nfor practical purposes. Nevertheless, a further analysis is of interest. \nOF THE TRANSFORMATIONANALYSIS"},{"page":18,"text":"19641 Box AND COX-An Analysis of Transformations 227 \nWe aim at simplicity both to achieve ease of understanding and' to allow an \nefficient analysis. Validity of the formal tests associated with analysis of variance may, \nin virtue of the robustness of these tests, often hold to a good enough approximation \neven with the untransformed data. We stress, however, that such approximate validity \nis not by itself enough to justify an analysis; sensitivity must be considered as well as \nrobustness. Thus in the biological example we have about one-third the sensitivity on \nthe original scale as on the transformed scale. The approximate validity of significance \ntests on the original scale would be very poor consolation for the substantial loss of \ninformation involved in using the untransformed analysis. In any case even such \nvalidity is usually only preserved under the null hypothesis that all treatment effects \nare zero. \nFor the further analysis we again explore two approaches, one via maximum \nlikelihood and the other via Bayes's theorem. Consider a general model to which a \nconstraint C can be applied or relaxed, so that the relative merits of the simple and \nof the more complex model can be assessed. For example, the general model may \ninclude interaction terms, the constraint C being that the interaction terms are zero. \nIf Lmax(A)and Lmax(AI C) denote maximized log likelihoods for the general model \nand for the constrained model, then \nHere the second term on the right-hand side is a statistic for testing for the presence \nof the constraint. \nMore generally, with a succession of constraints, we have \nand the three terms on the right of (28) can be examined separately. The detailed \nprocedure should be clear from the examples to follow. \nTo apply the Bayesian approach, we write the posterior density of A \nwhere p(C) = E,{p(CI A)) is a constant independent of A. That is, the posterior \ndensity of A under the constrained model is the posterior density under the general \nmodel multiplied by a factor proportional to the conditional probability of the \nconstraint given A. Successive factorization can be applied when there is a series of \nsuccessively applied constraints, giving, for example, \nwhere p(C, I C,) = E,{p(C2 I A, C,)) is a further constant independent of A. Note that \nwe are concerned here not with the probabilities that the constraints are true, but with \nthe contributions of the constraints to the final function p(AI C,, C,). \n5.2. Structure of the Expectation \nNow very often the most important question is: how simple a form can we use \nfor E{Y(~))? Thus in the analysis of the biological example in Section 4, we assumed, \namong other things, that additivity can be achieved by transformation. In fact,"},{"page":19,"text":"228 \nBox AND COX -An Analysis o f  Transformations [No. 2, \ninteraction terms may or may not be needed. Similarly, in our analysis of the textile \nexample we took a linear model with four parameters; the full second-degree model \nwith ten parameters may or may not be necessary. \nNow let A, Hand N denote respectively the constraints to the simpler linear model \n(without interaction or second-degree terms), to a heteroscedastic model and to a \nmodel with normal distributions. Then, \nLmax(h) A, H, N) = Lmax(hl H, N) +{Lrnax(XIA, H, N)-Lmax(hI H, N)). \nLet the parameter 8 in the expectation under the general linear model be partitioned \n(O,, 8,) where 8, = 0 is the constraint A. Denote the degrees of freedom associated \nwith 8, and 8, by V, and v,. If v, is the number of degrees of freedom for residual \nin the complex model, the number in the simpler model is thus v,+ v,. \nAs before, we work with the standardized variable z ( ~ ) = Y(~)\/J~\/*. If we identify \nresidual sums of squares by their degrees of freedom, we have \nLmax(X 1 8, = 0, H, N) = -Sn log {S,p+,2(h ;z)ln), \nwhereas \nLma,(X I H, N) = -Sn log {Svv(X; z)ln). \nThus, in the textile example, Svr refers to the residual sum of squares from a second- \ndegree model and S,~+,a\nrefers to the residual sum of squares from a first-degree model. \nQuite generally \ns,~+,z(~; Sur(X; z)+S,~,~(X; 4 ,  \nz) = \nwhere S,2,,1(X; z) denotes the extra sum of squares of ztA) for fitting 8,, adjusting for \nel, and has v, degrees of freedom. \nThus with (32) and (33) the decomposition (31) becomes \n(31) \n(32) \n(33) \nwhere \nis the standard F ratio, in the analysis of variance of z(~), \nthe simpler model. \nEquation (34) thus provides an analysis of the overall criterion into a part taking \naccount only of homoscedasticity (H) and normality (N) plus a part representing \nthe additional requirement of a simple linear model, given that H and N have been \nachieved. \nIn the corresponding Bayesian analysis (30) gives \np(hl 8, = O ,  H, N) =p(hl H, N) x k,p(e2 = 01 A ,  H, N), \nwhere \nIlk, = E A  1 ,,,{P(~z \nthe expectation being taken over the distribution p(XI H, N). \nNote that since the condition 8, = 0 is given, there is no component for these \nparameters in the prior distribution, so that the left-hand side of (36) is the posterior \ndensity obtained previously assuming A. Thus, in terms of the standardized variable \ndA), the left-hand side is \nfor testing the restriction to \n(36) \n= 01 A ,  H, N)),"},{"page":20,"text":"19641 \nBox AND COX-An Analysis o f  Transformations \nwhere the normalizing constant is given by \nSimilarly, in the general model with 8, and 82 both free to vary, we obtain the first \nfactor on the right-hand side of (36) as \nP(A IH, N) = PO(^ Cvr {Svv(A ;Z))-*~T, \nwith \n(38)\nC;;' \n= \/po(h) {S,~(A ;z)}-ivr dA. \nThus, from (37) and (38), the second factor on the right-hand side of (36) must be \nNow the general equation (36) shows that this last expression must be proportional \nto p(02 = 01 A ,  H, N). It is worth proving this directly. To do this, consider a trans- \nformed scale on which constant variance and normality have been attained and the \nstandard estimates 8, and s2 calculated. For the moment, we need not indicate \nexplicitly the dependence on A and z. We denote the matrix of the reduced least- \nsquares equations for 8,, eliminating el, by b, so that the covariance matrix of 8, is \n02b-I. The elements of b and bL1 are denoted bij and bij. Also we write \npij = bij\/d(bii bjj) and {pij) for the matrix inverse to {pij). Then the joint distribution of \nis (Cornish, 1954; Dunnett and Sobel, 1954) \nwhere here and later the constant involves neither the parameters nor the observations. \nWith uniform prior distributions for-the 0's and for log a, this is also the posterior \ndistribution of the quantities (dZi- dZi)\/(s\/Jbii), where now the dZi are the random \nvariables. Transforming from the ti's to the d2,'s we have that \nwhence \nIf now we restore in our notation the dependence on A, comparison of (40) with \n(39) proves the required result; the appropriateness of the constant is easily checked. \nThus (36) provides an analysis of the overall density into a part p(AI H,N) taking \naccount only of homoscedasticity and normality, and a second part, (39), in which \nthe influence of the simplifying constraint is measured."},{"page":21,"text":"230 \nBox AND COX -An Analysis o f  Transformations \n[No. 2, \nEquation (39) can be rewritten \nNow, by (34), the corresponding expression in the maximum-likelihood approach is \ngiven, in a logarithmic version, by \nThe essential difference between (41) and (42) is the occurrence of the term in \nSJX; z) in (41). In conventional large sample theory, vr is supposed large compared \nwith v, and then in the limit the variation with h of the additional term is negligible, \nand the effect of both terms can be represented by plotting the standard F ratio as a \nfunction of A. In applications, however, v,\/v,may well be appreciable; thus in the \ntextile example v,\/v,= 6\/17. \nHence (41) and (42) could lead to appreciably different conclusions, for example, \nif we found a particular value of X giving a low value of F(h; z) but a relatively \nhigh value of SJh; z). \nThe distinction between (41) and (42) from a Bayesian point of view can be \nexpressed as follows. In (41) there occurs the ordinate of the posterior distribution \nof 8, at 8, = 0. On the other hand, the Fratio, which determines (42), is a monotonic \nfunction of the probability mass outside the contour of the posterior distribution \npassing through 8, = O: Alternatively, a calculation of the posterior probability of a \nsmall region near 8, =0 having a length proportional to o ,  in each of the v, \ncomponent directions gives an expression equivalent to (42). The difference between \n(41) and (42) will be most pronounced if there exists an extreme transformation \nproducing a low value of F(h; z) but a large value of S,,(X; z), corresponding to a \nlarge spread of the posterior distribution of 8,. Expression (42) would give an \nanswer tending to favour this transformation, whereas (41) would not. \n5.3. Application to Textile Example \nWe now illustrate the above analysis using the textile data. The calculations are \nset out in Table 7 and displayed in Figs. 4 and 5. We discuss the conclusions in some \ndetail here. In practice, however, the most useful aspect of this approach is the \nopportunity for graphical assessment. \nFig. 4 shows that the curvature of L,,,(hl \nL,,,(XI A, H, N) previously given in Fig. 2, the constraint A here being that the \nsecond-degree terms are supposed zero. The inequality \nH, N) is much jess than that of \nthus gives the much wider approximate 95 per cent confidence interval (-0.48, 0.13) \nfor h indicated by HN in Fig. 4 and compared with the previous interval, marked \nAHN. Since the constraint has 6 degrees of freedom the sampling distribution of \nfor fixed normalizing h is asymptotically xi. Alternatively, (44, being a monotonic \nfunction of F, can be tested exactly. Thus we can decide for which X's, if any, the \ninclusion of the constraint is compatible with the data. In Fig. 5, F(h; z) is close to"},{"page":22,"text":"19641 \nBox AND COX-An Analysis of Transformations \n231 \nunity over the interesting range of h close to zero, so that we can use the simpler model \nin this neighbourhood. The range indicated by C in Fig. 4 is that for which Fis less \nthan 2-70, the 5 per cent significance point. \nTABLE 7 \nTextile data. Calculations for the analysis of the transformation \nDiference = -13.5 x \nh \nLmax(h I A, H, N )  Lmax(h I H, N )  \nlog (1s & F ( ~ ;z)) \nF(X; 2) \nThe Bayesian analysis follows parallel lines. In Fig. 4, pu(hI H,N )  has a much \ngreater spread than ~,(hl A, H,N). Fig. 5 shows pu(XI H, N )  with the component \nkAp(AI A, H, N )  from the constraint. When multiplied together they give the overall \ndensity pu(hIA, H, N). A value of h near zero maximizes the posterior density \nassuming the constraint and is consistent with the information in pu(hj H, N). \nThere is, however, nothing in our Bayesian analysis itself to tell us whether the \nsimplified model with the constraint is compatible with the data, even for the best \npossible A. There is an important general point here. All probability calculations in \nstatistical inference are conditional in one way or another. In particular, Bayesian \nposterior distributions such as p,(hIA, H, N )  are conditional on the model, in \nparticular here on assumption A. It could easily happen that there is no value of h \nfor which A is at all reasonable, but to check on this we need to supplement the"},{"page":23,"text":"232 \nBox AND COX-An Analysis o f  Transformations \n[No. 2, \nBayesian argument (Anscombe, 1961). Here we can do this by a significance test \nbased on the sampling distribution of a suitable function of the observations, namely \nP(h; z). For h around zero the value of P(h; z) is, in fact, well within the significance \nlimits, so that we can reasonably use the posterior distribution of h in question. \nX \nFIG. 4. Textile data. Functions Z,,,(h) \nH: homogeneity of variance. N: normality. Arrows HN, AHN show approximate 95 per \ncent confidence intervals for h. Arrows C show range for which F for second-degree \nterms is not significant at 5 per cent level. \nandp,(h) under different models. A: additivity. \n5.4. Homogeneity o f  Variance \nSuppose that we have k groups of data, the expectation and variance being \nconstant within each group. In the Zth group, let the variance be a? and let S(l) \ndenote the sum of squares of deviations, having vl =nl- 1 degrees of freedom. \nWrite Xnl =n, Cvl = n -k. Thus in our biological example, k = 12, v, = ... =v,, = 3, \nn,= ...=nI,=4andv=36,n=48. \nNow suppose that a transformation to y(\" exists which induces normality simul- \ntaneously in all groups. Then in terms of the standardized variable z(\", the maximized \nlog likelihood is \nI N) = -&Cnl log {S(l) (A; z)\/n,}, \nL,,,(X"},{"page":24,"text":"19641 \nBox AND COX-An Analysis of Transformations \n233 \nwhere S(l)(X; z) is the sum of squares S'l), considered as a function of X and \ncalculated from the standardized variable z(\". \n?,,(A.IH,Nf \nand \nk~ ?\np(AlM,N) ?\nX \nTextile data. - Components of posterior distribution. ----- Variance \nratio, F(X; 2). Arrow gives 5 per cent significance level. \nWe now consider the constraint H, a! = ... = a;, i.e. look at the possibility that \na transformation exists simultaneously achieving normality and constant variance. \nThen if S, = XS'l) is the pooled sum of squares within groups \nLmax(XI H, N )  = -i!n log {S,(X ;z)ln). \nTherefore \n= Lm,, ( AI N )+log L,(X; z), \n(47) \nsay. Here the second factor is the log of the Neyman-Pearson L, criterion for testing \nthe hypothesis a: = ... = a;. \nIn the corresponding Bayesian analysis, (29)gives \np(hIH,N) =p(XIN)xkHp(u,2 = ... = \nwhere \nkg1= EAIN{p(a: = ... = U; 1 A, N)). \nFor the general model in which a!, ..., o; may be different, the prior distribution is \npo(X)(n do,)(n dlog o,)J -\"in \nh,N), (48)"},{"page":25,"text":"234 \nBox AND COX -An Analysis o f  Transformations \n[No. 2, \nand \nwith \n(49) \nFor the restricted model in which the variances are all equal to a2, the appropriate \nprior distribution is \nPo(~) (rI'd0,) (dlog a) J-vln \nand \nP(X I H, N) = {P,(X) c,(X ;z))-tv. \nHence, on dividing (50) by (49), we have that the second factor in (48) is \n(50) \nwhere (Bartlett, 1937) \n(S (9' z)) \n(s\"';;\")\nM(X;. z) = v log --Zvl log \nis the modification of the L, statistic for testing homogeneity of variance, replacing \nsample sizes by degrees of freedom. \nFrom our general argument, (51) must be proportional to p(u! = ... = oil X ,  N). \nThis can be verified directly by finding the joint posterior distribution of a ! ,  ...,a;, \ntransforming to new variables u,2, oi\/a!, ...,ui]u,2, integrating out u,2, and then taking \nunit values of the remaining arguments. \n5.5. Application to ~iolo~ical \nExample \nIn the biological example, we can now factorize the overall criterion into three \nparts. These correspond to the possibilities that in a$dition to normality within \neach group, we may be able to get constant variance and that it may be unnecessary to \ninclude interaction terms in the model, i.e. that additivity is achievable. \nIn terms of maximized likelihoods, \nwhere L,(A; z) is the criterion for testing constancy of variance given normality and \nF(X; z) is the criterion for absence of interaction given normality and constancy of \nvariance. \nThe correspondiilg Bayesian analysis is \nThe results are set out in Table 8 and in Figs. 6-8. The graphs of Lrna,(XI N) and \np,(XI N) in Fig. 6 show that the information about X comihg from within group \nnormality is very slight, values of X as far apart as -1 and 2 being acceptable on this"},{"page":26,"text":"235 \n19641 \nBox AND COX-An Analysis of Transformations \nbasis. The requirement of constant variance, however, has a major effect on the \nchoice of A; further, some information is contributed by the requirement of additivity. \nTABLE 8 \nBiological data. Calculations for analysis of the transformation \nFrom Fig. 7, which shows the detailed separation of the maximum-likelihood and \nBayesian components; any transformation in the region y-I to y-* gives a compatible \ncompromise."},{"page":27,"text":"FIG. 6. Biological data. \nA: additivity. H: homogeneity of variance. N: normality.. Arrows N, HN, AHN show \napproximate 95 per cent confidence intervals for A. \nFunctions ~,,,(h) \nand P,(X) under different models. \nh \nFIG. 7. Biological data. Components of posterior distribution."},{"page":28,"text":"19641 \nBox AND COX-An Analysis of Transformations \nSince the groups all contain four observations \nand the graph of M(X; z) in Fig. 8 is equivalent to one of &(A; z). Since on the null \nhypothesis the distribution of M(h; z) is approximately x:,, we can use Fig. 8 to \nFIG. 8. Biological data. Variance ratio, F(h; z), for interaction against error as a \nfunction of h. Bajtlett's criterion, M(h; z), for equality of cell variances as a function of h. \nDotted lines give 5 per cent significance limits. \nfind the range in which the data are consistent with homoscedasticity. Similarly \nthe graph of F(h; z) indicates the range within which the data are consistent with \nadditivity. The dotted lines indicate the 5 per cent significance levels of M and of F. \nThe minimum of M(h; z) is very near h = -1. It is of interest that the regression \ncoefficient of log(samp1e variance) on log(samp1e mean) is nearly 4, so that the \nreciprocal transformation is suggested also by the usual approximate argument for \nstabilizing variance. \n6. ANALYSIS\nOF RESIDUALST \nWe now examine briefly a connection between the methods of the present paper \nand those based on the analysis of residuals. The analysis of residuals is intended \nt We are greatly indebted to Professor F. J. Anscombe for pointing out an error in the \napproximation for a as we originally gave it. In the present modified version terms originally \nneglected in this Section have been included to correct the discrepancy."},{"page":29,"text":"238 \nBox AND COX-An Analysis o f  Transformations \n[No. 2, \nprimarily to examine what happens on one particular scale, although its use to \nindicate a transformation has been suggested (Anscombe and Tukey, 1963). Corre- \nsponding to an observation y, let Y be the deviation j-j of the fitted value j from \nthe sample mean and let r = y -j be the residual. If the ideal assumptions are satisfied \nr and Y will be distributed independently. Different sorts of departures from ideal \nassumptions can be measured, therefore, by studying the deviations of the statistics \n= Cri Yj from nE(ri)E(Yj). In addition to graphical analysis, a number of such \nfunctions have indeed been proposed for particular study (Anscombe, 1961; \nAnscombe and Tukey, 1963). \nSpecifically, the statistics \nwere put forward as measures respectively of skewness, kurtosis, heterogeneity of \nvariance and non-additivity. Tukey's degree of freedom for non-additivity (Tukey, \n1949) involves the sum of squares corresponding to TI, considered as a contrast of \nresiduals with \"fixed\" coefficients Y2. \nSuppose now that we consider the family of power transformations and, writing \nz = y\/j, and w = z- 1, make the expansion \nwhere w, = w2, w3 = w3 and a! = 1-A. \nNow, L,,,(X) \nwhich is approximately \nand L,(X) are determined by the residual sum of squares of z(~), \nIf we take terms up to the fourth degree in w and then differentiate with respect to a!, \nwe have that the maximum-likelihood estimate of a! is approximately \n3w1a, w, -w'a, w3 \nA \na!= 3wk a, w, + 4w!a, w3 ' \nIf we write y, = y -);, y, = (y -j),, y3 = (Y-);)~ and denote by 9,,9,,j3 the values \nobtained by fitting y,, y, and y3 to the model, the above approximation may be \nexpressed in terms of the original observations as \nTo see the relation between this expression and the T statistics, write d = j-3. \nThen y, = y-); = r+ Y+d. Bearing in mind that a,Y = O,a,r = r, Y'r = O,a,1 = 0, \nl'r = 0, where 1denotes a vector of ones, terms such as y; a, y, can easily be expressed \nin terms of sums of powers and products of r, Y and d. In particular, on writing S \nfor Cr2, we find the numerator of (58) to be \nTo this order of approximation the maximum-likelihood estimate of a! thus \ninvolves all the T statistics of orders 3 and 4."},{"page":30,"text":"19641 Box AND COX-An Analysis o f  Transformations 239 \nAs a very special case, for data assumed to form a single random sample \nHere questions such as non-additivity and non-constancy of variance do not arise and \nthe transformation is attempting only to produce normality. Correspondingly in (59), \nT,, = TI, = T3, = T,, = TI, = 0, since Y = 9-J = 0. In fact if we write m, =J, \nm, = n-l C(y -J)p (p = 2,3, ...) and make the approximation d = *m,\/m,, we have \nthat \nFor distributions in which m,, m,, m, and m,- 3mi are of the same order of magnitude, \nthe terms in curly brackets are of one order higher in l\/m,,than are the other terms of \nthe numerator and denominator. If we ignore the higher-order terms, we, have \nA useful check suggested by Anscombe is to consider the X2 distribution for moderate \ndegrees of freedom and the Poisson distribution for not too small a mean. For \nx2 we find a-4, whence A- 4,corresponding to the well-known Wilson-Hilferty \ntransformation. For the Poisson distribution, a- 4, whence A- 3. \nIn Section 2 we suggested that, having chosen a suitable X ,  we should make the \nusual detailed estimation and interpretation of effects on this transformed scale. Thus \nin our two examples we recommended that the detailed interpretation should be in \nterms of a standard analysis of respectively l\/y and log y. Since the value of A used \nis selected at least partly in the light of the data, the question arises of a possible \nneed to allow for this selection when interpreting the factor effects. \nTo investigate an appropriate allowance, we regard X as an unknown parameter \nwith \"true\" value A , ,  say, and suppose the true factor effects to be measured in terms \nof the scale A,,. \nIf we were, for instance, to analye the factor effects on the scale \ncorresponding to the maximum-likelihood estimate A ,  we might expect some additional \nerror arising from the difference between 2 and A,. \nalthough the present formulation of the problem is not always completely realistic. \nFor example, in our biological example, having decided to work with lly, we shall \nprobably be interested in factor effects measured on this scale and not those measured \nin some unknown scale corresponding to an unknown \"true\" A , .  On the other hand, \nif we are interested in whether there is interaction between two fa%tors, it iz possibly \ndangerous to answer this by testing for interaction Qn the scale A ,  since X may be \nselected at least in part to minimize the sample interaction. A more reasonable \nformulation here may often be: on some unknown \"true\" scale A,, are interaction \nterms necessary in the model? \nWe now investigate this matter,"},{"page":31,"text":"240 \nBox AND COX-An Analysis of Transformations \n[No. 2, \nFrom the maximum-likelihood:approach, the most useful result is that significance \ntests for null hypotheses, such as that just mentioned about the absence of interaction, \ncan be obtained in a straightforward way in terms of the usual large-sample chi- \nsquared test. Thus, in the textile example, we could test the null hypothesis that \nsecond-degree terms are absent for some unknown \"true\" A,, by testing twice the \ndifference of the maxima of the two curves of L,,,(X) \nmaxima occur at different values of A. In this particular example, such a test is hardly \nnecessary. \nIt would be possible to obtain more detailed results by evaluating the usual large- \nsample information matrix for the joint estimation of A ,  u2 and 8. Since, however, \nmore specific results can be obtained from the Bayesian analysis, we shall present \nonly those. The general conclusion will be that to allow for the effect of analysing in \nterms of 2 rather than A , ,  the residual degrees of freedom need only be reduced by \nv,, the number of component parameters in A. This result applies provided that the \npopulation and sample effects are measured in terms of the normalized variables z(,). \nConsider locally uniform prior densities for 8, log u and A. Then the posterior \ndensity for 8 is \nin Fig. 4 as x,2. Note that the \nApproximate evaluation of the integral in (61) is done by expansion around the \nmaxima of the integrands. The ~aximum of the integrand in the denominator is at \nthe maximum-likelihood estimate A, and that of the numerator is near 2, so long as 8 \nis near its maximum-likelihood value. The answer is that (61) is approximately \nThis is exactly the posterior density of 8 for some known fixed X with the degrees of \nfreedom reduced by v,. \nTo derive (62) from (61), we need to evaluate integrals of the form \nw h e ~ v is large, and q(A) is assumed positive and to have a unique minimum at \nA = A ,  with a finite Hessian determinant A, at the minimum. We can then make a \nLaplace expansion, writing \n{q(>)}-*v-*v~?\nN \n-\nx const; \nA: \nfor this we expand the second logarithmic term as far as the quadratic terms and then \nintegrate over the whole v,-dimensional space of A. In our application the terms \nA: in numerator and denominator are equal to the first order."},{"page":32,"text":"19641 Box AND COX-An Analysis o f  Transformations \n241 \nFinally, we can obtain an approximation to the posterior distribution p,(A) of A \nthat is better than the usual type of asymptotic normal approximation. For an \nexpansion about A gives that \nconst \nHere \nwith d(A) being the n x v, matrix with elements \n-\nax, \nazp \n(i= 1,...,n; j= 1,...,v,).\nThe matrix b determines the quadratic terms in the expansion of s2(A; Z) around ?,. \nThus the quantities (Aj-&)\/{s@; z)dbii) have approximately a posterior multi- \nvariate t distribution and \n(A- X)'b(A-X) \na posterior F distribution. In fact, however, it will usually be better to examine the \nposterior distribution of A directly, as we have done in the numerical examples. \n8. FURTHER DEVELOPMENTS \nWe now consider in much less detail a number of possible developments of the \nmethods proposed in this paper. Of these, the most important is probably the simul- \ntaneous transformation of independent and dependent variables in a regression \nproblem. Some general remarks on this have been made in Section 1. \nDenote the dependent variable by y and the independent variables by x,, ...,x,. \nConsider a family of transformations from y into y(,) and x,, ...,x, into xp), ...,xjKz), \nthe whole transformation being thus indexed by the parameters (A; K,, ...,K,). It is \nnot necessary that the family of transformations of say x, into x p )  and x, into \nx$J should be the same, although this would often be the case. \nWe now assume that for some unknown (A; K,, ...,K,) the usual normal theory \nassumptions of linear regression theory hold. We can then compute say the maximized \nlog likelihood for given (A; K,, ...,K,), obtaining exactly as in (8) \nKl, ...,KJ = -8log G2(A ; Kl, . . . ,K 1 )+10gJ(A; y), \nwhere G2(A; K,, ...,K,) is the maximum-likelihood estimate of residual variance in \nthe standard multiple regression analysis of the transformed variable. The corre- \nsponding expression from the Bayesian approach is \nL,,,(A; \n(67)"},{"page":33,"text":"242 \nBox AND COX -An Analysis of Transformations \n[No. 2, \nThe straightforward extension of the procedure of Section 3 is to compute (67) or \n(68) for a suitable set of (A; K,, ...,K,) and to examine the resulting surface especially \nnear its maximum. This is, however, a tedious procedure, except perhaps for 1= 1. \nFurther, graphical presentation of the conclusions will not be easy if 1> 1;for 1= 1 \nwe can plot contours of the functions (67) and (68). \nWhen X is fixed, i.e. transformations of the independent variables only are involved, \nBox and Tidwell (1962) developed an iterative procedure for the corresponding non- \nlinear least-squares problem. In this the independent variables are, if necessary, first \ntransformed to near the optimum form. Then two terms of the Taylor expansion \nof x?), . . .,~ ~ ( ~ 1 )  \nFor example if x?) = xKl and the best value for K,\nare taken. \nthought to be near 1, we write \nis \nx? = x,+(K,- 1)xllogx1. (69) \nA linear regression term p, xyl can then be written approximately \nPI XI+P1(~1- 1) xl logx, = Plx1+ YIx1 1% x1, \nsay. If the linear model involves linear regression on x,, .. . ,x, and if all the transfor- \nmations of the independent variable are to powers, we can therefore take the linear \nregression on x,, ...,x,, x,logx,, ...,x,logx, in order to estimate the p's and y's and \nhence also the K'S. \nThe procedure can then be iterated. Transformation of the \ndependent variable will usually be the more critical. Therefore, a reasonable practical \nprocedure will often be to combine straightforward investigation of transformation \nof the dependent variable with Box and Tidwell's method applied to the independent \nvariables. \nIt is possible also to consider simplifications of the procedure for determining a \ntransformation of the dependent variable. The main labour in straightforward \napplication of the method of Section 3 is in applying the transformation for various \nvalues of h and then computing the standard analysis of variance for each set of \ntransformed data. Such a sequence of similar calculations is straightforward on an \nelectronic computer. It is perfectly practicable also for occasional desk calculation, \nalthough probably not for routine use. There are a number of possible simplifications \nbased, for example, on expansions like (69) or even (55), but they have to be used \nvery cautiously. \nIn the present paper we have concentrated largely on transformations for those \nstandard \"fixed-effects\" analysis of variance situations where the response can be \ntreated as a continuous variable. The same general approach could be adopted in \ndealing with \"random-effects\" models, and with various problems in multivariate \nanalysis and in the analysis of time series. We shall not go into these applications \nhere. \nAn important omission from our discussion concerns transformations specifically \nfor data suspected of following the Poisson or binomial distributions. There are two \ndifficulties here. One is purely computational. Suppose we assume that our obser- \nvations, y, follow, for example, Poisson distributions with means that obey an \nadditive law on an unknown transformed scale. Thus, in a row-column arrangement, \nit might be assumed that the Poisson mean in row i and column j has the form \n(P+ai+pj>llh (XfO), \nPj \n(A = O),"},{"page":34,"text":"19641 Box AND COX-An Analysis o f  Transformations 243 \nwhere h is unknown. Then h and the other parameters of the model can be estimated \nby maximum likelihood (Cochran, 1940). It would probably be possible to develop \nreasonable approximations to this procedure although we have not investigated \nthis matter. \nAn essential distinction between this situation and the one considered in Section 3 \nis that here the untransformed observations y have known distributional properties. \nThe analogous normal theory situation would involve observations y normally \ndistributed with constant variance on the untransformed scale, but for which the \npopulation means are. additive on a transformed scale. The maximum-likelihood \nsolution in this case would involve, at least in principle, a straightforward non-linear \nleast-squares problem. However, this situation does not seem likely to arise often; \ncertainly, it is inappropriate in our examples. \nAn important possible complication of the analysis of data connected with \nPoisson and binomial distributions has been particularly stressed by Bartlett (1947). \nThis is the presence of an additional component of variance of unknown form on \ntop of the Poisson or binomial variation. If inspection of the data shows that such \nadditional variation is 'substantial, it may be adequate to apply the methods of \nSection 3. For integer data with range (0, 1, ...) it will often be reasonable to consider \npower transformations. For data in the form of proportions of \"successes\" in which \n\"SUCC~SS~S\" and \"failures\" are to be treated symmetrically, Professor J. W. Tukey \nhas, in an unpublished paper, suggested the family of transformations from y to \nyh-(1 -y)h. \nFor suitable X's this approximates closely to the standard transforms of proportions, \nthe probit, logistic and angular transformations. The methods of the present paper \ncould be applied with this family of transformations. \nACKNOWLEDGEMENT ?\nWe thank many friends for remarks leading to the writing of this paper. ?\nREFERENCES \nof residuals\", Proc. \nF. J. \"Examination \nFourth\nANSCOMBE, \nStatist. and Prob., 1, 1-36. \n-and TUKEY, J. W. (1963), \"The examination and analysis of residuals\", Technometrics, 5, \n141-160. \nBARTLETT, M. S. (1937), \"Properties of sufficiency and statistical tests\", Proc. Roy. Soc. A, 160, \n268-282. \n-(1947), \"The use of transformations\", Biometries, 3, 39-52. ?\nBox, G. E. P. and' TIDWELL, P. W. (1962), \"Transformation of the independent variables\", ?\nTechnometrics, 4, 531-550. \nCOCHRAN, W. G. (1940), \"The analysis of variance when experimental errors follow the Poisson \nor, binomial laws\", Ann. math. Statist., 11, 335-347. \nCORNISH, E. A. (1954), \"The multivariate t distribution associated with a set of normal sample \ndeviates\", Austral. J. Physics, 7, 531-542. \nDUNNETT, C. W. and SOBEL, M. (1954), \"A bivariate generalization of Student's t distribution\", \nBiometrika, 41, 153-169. \nJEFFREYS, \nH. (1961), Theory of Probability, 3rd ed. \nKLECZKOWSKI,\nA. (1949), \"The transformation of local lesion counts for statistical analysis\", \nAnn. appl. Biol., 36, 139-152. \nTUKEY, J. W. (1949), \"One degree of freedom for non-additivity\", Biornetrics, 5, 232-242. \n-(1950), \"Dyadic anova, an analysis of variance for vectors\", Human Biology, 21, 65-110. \n-and MOORE, P. G. (1954), \"Answer to query 112\", Biometrics, 10, 562-568. \n(1961), \nBerkeley Symp. Math. \nOxford University Press."},{"page":35,"text":"244 \nDiscussion on Paper by Professor Box and Professor Cox \n[No. 2, \nMr J. A. NELDER: May I begin with a definition (from the Concise Oxford Dictionary) : \n\"Box and Cox-two persons who take turns in sustaining a part.\" I must admit to having \nspent some time in trying to deduce which person was sustaining which part of this most \ninteresting paper. I do not think the exercise was very successful, and this testifies to some \nsound collaboration on the part of the authors. \nIt seems to me that there are two basic problems besetting all conscientious data \nanalysts (to borrow Professor Tukey's term). One is how to check that the data are not \ncontaminated with rogue observations and what action to take if they are. The other is \nhow to check that the model being used to analyse the data is substantially the right one. \nLooking through the corpus of statistical writings one must be struck, I think, by how \nrelatively little effort has been devoted to these problems. The overwhelming preponder- \nance of the literature consists of deductive exercises from a priori starting points. Now, \nof course, there must always be some assumptions made a priori; in data analysis the \nimportant thing is that they should not be much stronger than previous evidence justifies. \nThe first of the two problems, that of gross errors or rogue observations, we are not \ndirectly concerned with now, but the question of scale for analysis, which is discussed \nhere, is fundamental to the second. One sees not infrequently remarks to the effect that \nthe design of an experiment determines the analysis. Life would be easier if this were \ntrue. To the information from the design we must add the analyst's prior judgements, \npreconceptions or prejudices (call them what you will) about questions of additivity, \nhomoscedasticity and the like. Frequently these prior assumptions are unjustifiably strong, \nand amount to an assertion that the scale adopted will give the required additivity, etc. \nThe great virtue of this paper lies in its showing us how to weaken these prior assumptions \nand allow the data to speak for themselves in these matters. The data analyst's two \nproblems are closely intertwined, however; for if rogue observations are present their \nresiduals tend to dominate the residual sum of squares, and may thus seriously affect the \nestimation of h. \nThe two approaches, via likelihood and via Bayes theorem, run side by side, and give \nresults which will often be very similar. I am not entirely happy about the derivation of \nequation (19) and wonder whether the appearance of the observations in the prior proba- \nbility is not only \"interesting\", as the authors state,'but also illegal. They remark (on \np. 219) that, \"There are some reasons for thinking L,(h) preferable to L,,,(h) \nBayesian as well as from a Bayesian point of view.\" I agree and, furthermore, I believe \nthat a suitable modification of the likelihood approach may be found to produce just this \nresult. The starting point is that fixed effects are unrealistic in a model. If we measure a \ntreatment effect in an experiment, it is common experience that a further experiment will \ngive us a further estimate of the effect which often differs from the original estimate by \nmore than the internal standard errors of the experiments would lead us to expect. If we \nconstruct a model with this in mind, then for a single normal sample of n we might obtain \nfrom anon- \nwhere m = N(p, ut2) and ei = N(0, u2). If we now do an orthogonal transformation of \nthe data z = Hy where H is an orthogonal matrix of known coefficients having its first row \nwith elements n-4, then the log likelihood is given by \nn \nL = const -: In V- (z, -,u Jn)2\/2V- $(n -1) log u2 -2z,2\/2u2, \nwhere V = u2+ nu'2. Clearly we cannot estimate V unless , u  is known, which in general \nit is not. However, for any fixed but unknown V,we have L maximized by taking \nP = 8, and B2 = C(y-jj)2\/(n-1)."},{"page":36,"text":"Discussion on Paper by Professor Box and Professor Cox \n'Thus L,,,(h) \nL,(h). By extensions of this argument we obtain Bartlett's criterion for testing the homo- \ngeneity of variances instead of the L, criterion, and the likelihood criterion for a restricted \nhypothesis on the means (equation (35)) becomes the same (apart from an unknown \nconstant factor) as the Bayesian one. Thus some of the apparent differences between the \ntwo approaches may result from the restrictions implied by fixed effects in a model, these \nbeing equivalent to assertions of zero variance in repetitions of the experiment. \nTaken with the work of Tukey, Daniel and others, on the detection of rogue obser- \nvations, the results of this paper should lead before long to substantial improvements in \ncomputer programmes for the analysis of experiments. \"First generation\" programmes, \nwhich largely behave as though the design did wholly define the analysis, will be replaced \nby new second-generation programmes capable of checking the additional assumptions \nand taking appropriate action. It is hardly necessary to stress what an advance this \nwould be. \nI suppose that the converse of \"two persons who take turns in sustaining a part\" \nwould be \"one person who takes turns in sustaining two parts\". Such a person is often \nthe proposer of the vote of thanks, the parts being those of congratulator and critic; \nthe latter has been known to overwhelm the former, but not, I hope, today. We must \nall be grateful for the clear exposition of an important problem, for the practical value \nof the results obtained and for the possibilities opened up for future investigations.. It is \na real pleasure, therefore, for me to propose the vote of thanks today. \nfollowing equation (24) is replaced (apart from an unknown constant) by \nDr J. HARTIGAN:\nproblem. Suppose in the ith experiment we observe yi under conditions xi and that it is \ndesired to find the probability distribution of y given x for various x. The only general \nprinciple that seems to apply is a similarity principle-\"What \ncircumstances will probably be similar to what happened under similar circumstances in \nthe past\" or more simply \"like equals likely\". The Meteorological Office does seem to be \nacting according to this principle in its long-range forecasts, where the procedure is to \nlook at this month's weather, look in the records for a similar month, see what happened \nthe following month then, and predict the same thing will happen next month, now- \nthey would say, to predict what yo will be under conditions x,, look among the (y,, xi) \nfor an xi close to x,, then predict yo = y,. \nIt does seem possible to offer a non-parametric method for predicting a new y at x,; \nin least squares theory this would be the fitted value Yo. The general procedure is to \nsmooth from the various readings (y, x) in the neighbourhood of x,, values of y being \ngiven greater or less weight according to x's \"similarity\" to x,; just how the weights are \nto be chosen, or how the y's are to be combined is an open question; the least squares \nanswer is Yo = Xa, y,, where the weights a, (possibly negative, but not very, and nearly \nalways adding to one) are calculated from the linear model. \nBox and Cox are assuming that for some transformed set of observations f(yi), the \nmodel is valid, and their smoothed value would be given by \nI would like to suggest a non-parametric approach to Box and Cox's \nwill happen under present \nA \"non-parametric\" approach would be to order the observations y,,,, ...,y,;) and \nselect Yo such that \nEssentially, Yo is the median of the distribution consisting of points y(i, with probability a, \n(possible negative values confuse this interpretation). The justification of this procedure \nis that Yo should not be too far from the value obtained by Box and Cox's procedure, \nsince the median of the f(yi)'s will be approximately equal to the mean of the f(yi)'s; \nbut this procedure is invariant under any monotonic transformation of the observations."},{"page":37,"text":"246 \nDiscussion on Paper by Professor Box and Professor Cox \n[No. 2, \nI have tried this with Box and Cox's 33 experiment, when x, is at the centre of the \ncube (O,O, 0). The weights a, will depend on the linear model; for a complete factorial \nmodel a, = 1 at (0, 0, 0) and 0 elsewhere so that no smoothing takes place; for the second- \ndegree polynomial model a, = 7 at the centre, 4 at the midpoint of a face, 1 at the midpoint \nof an edge and -2 at a vertex; for the first- and zero-degree polynomials, a, = 1 every- \nwhere and the smoothing is excessive. \nThe smoothed values with various similarity coefficients (we may regard ai as the \nrelevance of the ith observation to Yo) and various methods of combination are \nDegree of \nPolynomial Mean \n0,1 861 \n2, 724 \nC F 620 \nNegative weights are a,nuisance, and, also, we would like the similarity coefficients to \ndecrease with distance. However, least squares is the only general way of generating the \ncoefficients at present. \nI wonder if the interquartile range of the distribution over the y, with weights a, would \nbe a reasonable (transformation invariant) measure of dispersion of a new observation y \nabout Yo. In general this would tend to be large if yi's which were observed under highly \nsimilar conditions were a long way from the predicted Yoat xo. \nA preliminary analysis of the above type based on the order statistics would be invariant \nunder monotonic transformation, and so would seem an appropriate method of finding \na transformation in which an ordinary \"metric\" analysis might be performed. \nI have found this paper extremely informative and stimulating and it gives me great \npleasure to second the vote of thanks to Professors Box and Cox. \nMean log \n564 \n610 \n620 \nMedian \n566 \n604 \n620 \nThe vote of thanks was put to the meeting and carried unanimously. \nThe following written contribution was read by Professor D. G. Kendall. \nProfessor J. W. TUKEY: The results reported by Professors Box and Cox clearly \nrepresent a substantial step forward; all those concerned with the actual analysis of data \nshould be pleased to know that they do exist, both because of the new and modified \ntechniques which they urge us to try, and because these results were obtained by using \nalmost \"all the allowed principles of witchcraft\" as of the year 1964: normality assumptions, \nmaximum-likelihood estimation, Bayesian inference and a priori distributions invariant \nunder natural, transitive groups. This last fact makes it inevitable that intelligent choice \nof modes of expression for the observed responses will become both socially acceptable \nand widely taught and that the long-run consequences for the analysis of data will be \nvery desirable. \nWhile this is a useful step forward, it is, I think, important not to overestimate its \nconclusiveness. From the point of view of the man who does indeed have data to analyse, \nthese results are merely further guidance about a situation only reasonably close to the \none he actually faces. This is, of course, no novelty in statistics, but some aspects of the \npresent discussion make it important to re-emphasize some things that should be familiar \nto all of us. In the authors' discussion, as in all to nearly all of our presently available \ntheory, all the approaches are at least formally based upon a model involving normality- \nor, as I would rather say, Gaussiahity. I think that this is stressed by the discussion in \nSection 5 where one is asked to look first at the evidence from assumed Gaussianity, then \nat the evidence from an additional assumption of constancy of variance in the presence of \nGaussianity and, finally, at the evidence from a further assumption of additivity in the \npresence of both other assumptions. So long as we are going to work with tight specifi- \ncations, where only a few parameters can be allowed to enter, it is hard to see how things \ncan be done in any other way than this. But from the point of view of the man with the"},{"page":38,"text":"247 \n19641 \nDiscussion on Paper by Professor Box and Professor Cox \nactual data, it would make much more sense to ask-possibly \nwhich one could examine first the evidence derived from assumed additivity in the absence \nof other assumptions, secondly (in those situations where this was appropriate) the evidence \nprovided by an additional assumption of constant variance in the presence of additivity, \nand thirdly (in perhaps a few cases) the additional evidence provided by assumed \nGaussianity, in the presence of both additivity and constancy of variance. (If additivity \n-or, more generally, parsimony-is at issue, considerations of constancy of variance and \nGaussianity of distribution are usually negligible, at least so far as the choice of a mode \nof expression is concerned. If additivity is not at issue, constancy of variance usually \ndominates Gaussianity of distribution.) If all of us can have enough good ideas over a long \nenough period of time, perhaps we can come, eventually, to a theory which corresponds \nmore directly to what we desire. It may well be that, with the exception of very rare \ninstances, the differences in practice associated with such an approach would be in- \nappreciably different from those suggested by the present approach. The widespread \ntendency for additivity, constancy of variance and Gaussianity of distribution to come \nand go as a group offers us such a hope. It would be nice to know whether or not this hope \nis justified. \nWe are all used to having maximum-likelihood estimation combine different bits of \nevidence with quite appropriate weights. Accordingly, we may hope that this is still the \ncase in the present situation, but I must report that the relative weighting of the evidence \nprovided by interaction sums of squares and error sums of squares does not feel as if it \nwere being quite fairly weighted when one merely looks, as in Table 3, at the total of these \ntwo sums of squares. Perhaps the decomposition into the three parts mentioned above, and \nconcentration upon the part associated with the additivity assumption, might produce a \nmuch heavier weighting of the interaction sums of squares. Again it would be interesting \nto know whether or not this is true. \nIn most circumstances one is going to be more interested in reaching additivity than \nin maximizing the formal sensitivity of the main effects. There will be, however, a few \ninstances where the reverse is true. I am not clear, from the discussion of Table 6, to what \nextent the results of applying the proposed approach rigorously and without thought \nwill differ from the results obtained by seeking maximum sensitivity. If there should be \ndifferences which persist as the amount of data is increased without limit, I think one \nwill have, in the long run, to look more carefully into the choice of criterion, where a \ndecision to look need not imply an ultimate decision to adopt a different criterion. \nClearly Box and Cox have made a major step forward in the succession of approxi- \nmations which give us better and better answers to an important problem of practice. \nin vain-for  an analysis in \nThe following written contribution was read by the Honorary Secretary. \nProfessor R. L. PLACKETT: The authors have come up with the interesting ideas we \nwould have expected from them, and deserve our congratulations for a paper which will \nbe widely appreciated. They have made full use of modern computational facilities and \nthe two systems of inference which are currently competing for our attention. An \nimpression ltft by reading their paper is that the data should be fed into a large and \npowerful machine which will very quickly draw all the necessary graphs and print out the \nbest analysis of variance available in the circumstances. Those accustomed to the blissful \nease of the standard analysis of variance calculations will need to be convinced that such \nhard work is really necessary, and will ask for assuran'ce that too much responsibility has \nnot been delegated. \nSo much has recently been said on Bayesian procedures that it is a relief to find that \nthe authors are not really Bayesians at all, but have been very ingenious in using Bayesian \narguments without ever becoming fully committed to them. Thus they call for uniform \ndistributions, but only over the region where the likelihood is appreciable, and they justify \ntheir preference for a Bayesian procedure on the grounds that the confidence coefficients"},{"page":39,"text":"248 \nDiscussion on Paper by Professor Box and Professor Cox \n[No. 2, \nfrom asymptotic distribution theory are closer to their nominal values if Lb is used instead \nof L,,,. \nIt is true that in the further analysis separating out A and H they suggest that \nthe two procedures may lead to appreciably different conclusions, but the circumstances \nin which this might occur are not closely defined. Surely it is not the magnitude of either \nS,,(h; z) or F(h; z) which is relevant, but that of the derivatives of these quantities with \nrespect to A. In any case, the authors do not tell us what they would do if the conclusions \ndiffer markedly; but it accords with the spirit of this long-awaited collaboration that we \nshould be left in doubt as to which method of inference to follow. \nLikelihood procedures have also been well publicized and discussed, but there is a \npractical point which seems not to have been emphasized in the midst of a good deal of \nmathematical and logical argument. It arises because the likelihood function contains \nmuch that is taken for granted in the way of distributional forms, and is no substitute \nfor an inspection of the data. As a simple illustration, consider a large sample of measure- \nments in which half are clustered round the value a and half round the value b (a# b). \nThe assumption that this constitutes a sample from a normal distribution with mean ,u \nand standard deviation a leads to an exactly parabolic log likelihood function for ,u, but \nthe inferences that this would suggest conflict with those obtained directly from the data. \nIt is. tempting to contrast the smooth and deceptive character of a likelihood function \nwith the spotty but straightforward nature of Anscombe and Tukey's procedures. They \nfit a full linear model to the original data and plot residuals against fitted values. Residuals \nare something which the authors have not calculated, but it would have been interesting \nto see other methods at work on the same examples. One might consider a modification \nof the Anscombe-Tukey procedure in which the predicted value Y is plotted against the \nobserved value y. This will lead to a linearizing transformation Y=f(y) (e.g. by Dolby's, \n1963, analysis of the simple family); the procedure can be iterated if necessary and should \nconverge under reasonable conditions. It may be objected that the possibility of differing \nvariances isnot taken into account, but the usual argument is that the same transformation \ndoes for both. If a greatly differing transformation is necessary to equalize the variances, \nthen the experiment is unlikely to be very successful. \nIn the second part of their paper, the authors separate the contributions of linearity, \nconstant variance and normality, but the place of normality in their analysis is logically \ndifferent from that occupied by the other two, since normality is not a constraint which \nthey either apply or relax. For that, they would presumably need to carry through the \nentire analysis with some other distribution. \nProfessor M. S. BARTLETT: \na major step forward in this paper on the theory of transformations. I think also, like \nProfessor Plackett, I was a little uneasy about the extent to which complicated analysis \nmight seem necessary. \nAgain, like Mr Nelder, I found myself wondering about the Box and Cox nature of the \npaper and in particular whether this kind of oscillatory character between likelihood and \nBayes analysis had any relevance to the Box and Cox aspect! Perhaps Professor Cox \nmay wish to comment on this; on this point of Bayes versus likelihood I would especially \nwelcome his views on whether he is advocating them as equally useful or whether he has \nreached any conclusions as to whether one is better than the other. In particular I would \ncertainly draw attention to the point made in the paper, and I think Professor Plackett \nmade this point also, that whichever analysis you make, the inference is very conditional \non your set of assumptions from which you start. \nNow to come to other minor points, I think I have only two to make. One was in the \napproximation used for the log likelihood, the max log likelihood and the use of x2with \nthis, and I wondered whether Professor Cox, or for that matter, Professor Box, could make \nany comment on the accuracy in this in other than very large samples. One knows that \nthe distribution is valid up to but not including order lln, and one knows, for example, \nfrom Professor Box's work, that if you want to go to order lln you have to bring in a \nLike Professor Tukey, I think that the authors have made"},{"page":40,"text":"249 \n19641 \nDiscussion on Paper by Professor Box and Professor Cox \ndifferent multiplying factor to your x2approximation. And it would help to know whether \nthere is any possibility of getting the sort of confidence limits based on the x2 analysis a \nbit more exact, and if not, how misleading they might occasionally be. \nI think my last point is one that was raised by Professor Tukey and that is, I did wonder \nabout the uniqueness of this order of taking the various factors, normality, additivity and \nhomogeneity of variances, and whether you Would reach anything like the same sort of \nconclusion if you tried to take them in a different order. \nDr M. R. SAMPFORD:\nassumed normality of the transformed variable on the additivity, in particular, and to a \nlesser extent on the homogeneity of variance, when in fact no single transformation will \nachieve all three properties. The relatively small amount of information about h obtained \nfrom the normality assumption in the example (Table 8) seems to be reassuring on this \npoint, but the possible effects when the transformed distribution is rather far from normal \nmight still be serious. Of course, one can sometimes advance a more plausible distributional \nmodel, and in this context it may be worth suggesting that, though the title of this paper \nshould more properly be \"An Analysis of Transformations to Normality\", the ingenious \napproach on which it is based could perfectly well be applied to other distributions. For \nexample, I have several times encountered response-time distributions-in \ndistributions of time to death-that appear log-normal at the lower end of the scale, but \nhave a secondary mode in the upper tail. This might suggest that some animals die as a \ndirect result of damage caused by the treatment, but that others, having a high tolerance \nor being, by chance, little damaged, may survive the initial shock, only to die later \nas a result of physiological disturbance caused by the damage. One might, by making \nsome assumptions about distributions of damage and tolerances, derive a more or less \nplausible class of distributions for transformed times that might be expected to be con- \nsistent with variance homogeneity and at least approximate additivity. The method of \nthis paper could then be applied to determine the most satisfactory transformation leading \nto a distribution in this class. This is perhaps a rather extreme example, but I hope suggests \nthe potential value of the authors' approach in situations where additivity need not be \nexpected to involve, as it often does, near-normality. \nLike Professor Tukey, I am rather nervous about the effect of the \nparticular, \nDr C. A. B. SMITH: I merely wish to draw attention to a recent paper by A. F. Naylor \n(1964). He applied the arcsine, logit, log-log and normal equivalent deviate transformations \nto four sets of biological data. He concluded that for all practical purposes they could be \nconsidered as equivalent. For example, in most of the entries the expected numbers \ncalculated from the four transformations differ only slightly in the first decimal place. \nMr D. KERRIDGE: \ngeneral comment is that it is very pleasant to have a paper in which the idea is obvious. \nI am not saying this in any derogatory sense. I think all the great ideas were obvious \nones. Nothing could be more obvious than the idea of taking a parametric family and \nestimating the parameter. It is strange that such an obvious idea should take such a long \ntime to be seen, but in many ways, the simpler the idea, the greater the discovery. There is, \nfor example, much more chance that a simple idea will be used in practice. The particular \ncomment concerns the rather strange prior distribution which has the interesting property \nthat it contains the observations. We cannot let the night go without saying something \nabout that. Clearly this is not an expression of belief, so some people would not call it a \nprobability. It is not prior, because it is determined a posteriori, and so it is a pseudo-prior \npseudo-probability. Now I am not against it because of its strangeness, since obviously \nthe authors have extremely good reasons for using it. They use it because it works. It is \nvery interesting indeed to find a practical example in which you have to use something \nwhich clearly is a pseudo-probability. I believe that as we get to use Bayes's theorem \ninstead of talking about it, as I hope we are going to do in the future, we are going to come \nTheI have two comments to make, one general and one particular."},{"page":41,"text":"250 \nDiscussion on Paper by Professor Box and Professor Cox \n[No. 2, \nup against many more of these peculiar things. For example, I think that to get sensible \nsignificance tests in Bayesian theory we are going to have to use prior probabilities which \ndepend on the number of observations. These again will be pseudo-probabilities, in a \nsense pseudo-prior too. So this is a very interesting first example of something which will \neventually, I think, shed some light on what probabilities really are. My view is that \nthey do not express beliefs. They are a convenient figment introduced to do something \nwe do not really understand yet, but by examining examples of this sort I hope that one \nday we will achieve understanding. \nMr E. M. L. BEALE: I should like to add my thanks to Professors Box and Cox for a \nmost valuable paper, and to ask one question. Would the authors ever consider using a \ntransformation of the type (1) when some y's are negative, or one of type (2) where some \ny,+ h, is negative? Such a transformation obviously has strange arithmetic properties. \nIt gives a real answer if A, is integral, and I think one can always overcome any problems \ncreated by the fact that y may not be uniquely determined by the value of y(\". But would \nthe transformation ever make sense statistically? \nThe following written contribution was received after the meeting: ?\nProfessor F. J. ANSCOMBE:?The authors are to be congratulated on a most remarkable \npaper. The basic idea is highly original, and the tackling of horrendous difficulties is \nbreath-taking. The examples are illuminating, and the preliminary \"rather informal\" \nanalysis of the textile example is statistry in the grand manner-but, \npaper is that. \nBecause of my own efforts with residuals, I have been particularly interested by \nSection 6. In my 1961 paper I gave a formula for roughly estimating the power transforma- \ntion that would remove Tukey's type of removable non-additivity, and also one for \nestimating the power transformation that would remove an exponential dependence of \nerror variance on the mean. The formulas were based essentially on the statistics denoted \nby TI,and T,,, respectively, in this paper. I did not also give a formula aimed at removing \nskewness of the error distribution, based on the statistic here denoted by T,,, though I \nhave since used such a formula; in the notation of my 1961 paper the formula goes \nindeed, the whole \n(My p is Box and Cox's h, 8 is the overall sample mean, s the residual root mean square, \nand g, and g, are analogues of Fisher's g-statistics.) It was my thought that one would \ncalculate one or more of these expressions, and (if more than one) hope they would \nsomewhat agree. No doubt, with factorial data showing pronounced effects for at least \ntwo factors, one would attach primary importance to additivity. With only one effective \nfactor, there would be no question of additivity, and one would attach primary importance \nto constancy of variance. With no effective factors, and in particular with a simple homo- \ngeneous sample, there would be nothing to worry about except skewness. \nNow, Professors Box and Cox have shown that these three separate estimates should \n(very nearly) be averaged in a certain proportion to yield a best estimate of the power. \nThis result, for the relatively simple calculations based on residuals from a least-squares \nanalysis on one scale, parallels the subtle decomposition of the likelihood function into \nthree parts in Section 5. \nProfessor Cox replied briefly at the meeting and the authors subsequently replied \nmore fully in writing as follows: \nWe are very grateful to the speakers for their encouraging and helpful remarks. \nOne important general issue raised by Professors Tukey, Plackett, Bartlett and \nDr Sampford cbncerns priorities for the criteria of simplicity of the model and specifically \nof additivity, A, homogeneity of variance, H, and normality, N. We certainly agree on"},{"page":42,"text":"19641 \nDiscussion on Paper by Professor Box and Professor Cox \n251 \nthe importance of the first of these, as indeed we indicate in our remarks at the end of \nSection 2. In the formal analysis of Section 5 we have considered N, HN, AHN as three \nmodels in that order. If one is to employ a parametric approach one must, it seems, start \nfrom some distributional assumption although, of course, if desired this could be broader \nthan that adopted here. Furthermore, there is no reason in principle why A should not \nhave been taken before H in discussing the biological example. We would then have to \nfit an additive model with separate within-cell variances. The rough justification for \nthinking that the procedure given in the paper genuinely separates out the effects of \nN,H and A is that M(h; z), on which (47) and (51) depend, is a valid descriptive \nmeasure of heterogeneity of variance independently of N. Likewise P(h; z) is a descriptive \nmeasure of non-additivity independently of H and N. If we started from a non-normal \nmodel, we would get a different measure of heterogeneity of variance, but except in \nextreme circumstances it is unlikely that it would be minimized by a value of h very \ndifferent from that minimizing M(h; z). An analogous remark applies to F(h; z). Under \nnon-normality the weighting of the different requirements will be different, but it is hard \nto see how a radically different value of h could emerge from the final analysis. \nConcerning Professor Tukey's point about the appropriateness of the weighting given \nby the likelihood in the biological example, the truth seems to be that in this example \nnon-additivity is not in fact the major contribution in determining h. The sizes of the \nmean squares in Table 3 seem rather to bear this out than to contradict it. Concerning \nTables 3 and 6 a striking thing is not only the removal of non-additivity, or correspondingly \nin Table 6 the simplification of the model, but also the large increase in sensitivity of the \nexperiment. The result achieved by transformation is in fact equivalent to threefold \nincrease in experimental effort. \nIn the. paper we were at pains to stress that, where the procedures do seem relevant, \nwe recommend using them in a flexible way, and that the assumptions on which they are \nbased are a tentative working basis for the analysis rather than anything to be adopted \nirrevocably. In particular, in the discussion of the textile example we deliberately gave \nfirst the \"common-sense\" analysis before the more elaborate one. As Mr Kerridge has \nvery rightly stressed, the basic idea is an extremely simple one; in particular, the absence \nof iterative calculations is a considerable practical advantage. We hope that this will \nreassure Professor Plackett that we are not advocating unnecessary elaboration. Mr Nelder \nhas stated extremely clearly the need for a more searching examination of \"assumptions\". \nWe have not specifically investigated the point raised by Professor Bartlett concerning \nthe adequacy of the chi-squared approximation for confidence intervals for h. However, \nthe line we have followed in finding a closer approximation to the posterior density of h \nleads to posterior intervals based on the Fdistribution and a similar approximation might \nbe found for confidence intervals. The use of L,(h) instead of L,,,(h) \nanalogy with Bartlett's (1937) procedure of applying the likelihood-ratio procedure after \nsuitable contrasts have been removed by transformation. The difficulty when h is unknown \nis that the transformations to remove the parameters 8 depend on A ,  so that the argument \nis at best approximate. We were most interested in Mr Nelder's remarks on this point \nand hope that he will develop his ideas further. \nThe maximum-likelihood approach and the Bayesian approach have deliberately been \ngiven as entirely separate but parallel developments. Professor Plackett suggests that we \njustify the Bayesian approach only because it leads to \"better\" confidence intervals; this \nis not so. Several speakers have commented on the special prior distribution (19) which \ninvolves the observations. As we remarked in the paper, it is possible that there is an \nalternative and better approach to this; one way may be to make the prior distributions \nfor the contrasts depend on the general population mean. However, the observations \nenter (19) only in a mild way in establishing the overall level of the observations, usually \nthe overall geometric mean in our special cases. It is essential that some allowance should \nbe made for the fact that the prior distribution for the magnitude of the contrasts depends \non the overall magnitude of the observations. \nwas suggested by"},{"page":43,"text":"Discussion on Paper by Professor Box and Professor Cox \n[No. 2, \nIn answer to Mr Beale's question, we feel that, while it is probably possible to develop \nthe theory for non-monotonic transformations of the dependent variable, we cannot think \nof any situations where such transformations would be physically allowable. \nWe are grateful to Dr Smith for his reference to Naylor's work. However, Naylor \nseems to be considering situations where the transformations are, over the relevant range, \npractically linear functions of one another. In our examples the relative range of variation \nof the observations is high, the transformations are very non-linear and this is of course \nwhy we are able to obtain fairly sharp discrimination between the different values of A. \nIn the quanta1 response case, the transformations in question become essentially different \nonly in the tails of the response curve, and observations there would be required for the \ndifferences to be detectable and of practical importance. \nWe are very interested in Professor Anscombe's remarks on residuals. Further \ncomparisons of the analysis of residuals with the methods of our paper would be of value. \nWe are interested in Dr Hartigan's problem and formulation. However, this seems \nessentially different from ours, partly because in our applications we are primarily interested \nin changes in response, rather than in absolute responses, and partly because one of our \nprimary objectives is to find a scale on which the factor effects are succinctly characterized \nby a few parameters. Even if the distributional assumptions were to be phrased non- \nparametrically (which we would in any case not wish to do), we must have parameters \nin order to describe at all concisely the changes in response in a complex system. \nREFERENCES\nIN THE DISCUSSION \nDOLBY, J. L. (1963), \"A quick method for choosing a transformation\", Technometrics, 5, 317-326. \nNAYLOR, A. F. (1964), \"Comparisons of regression constants fitted by maximum likelihood to \nfour common transformations of binomial data\", Ann. hum. Genet., Lond., 27, 241-246."}],"widgetId":"rgw26_56aba196a5cf3"},"id":"rgw26_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=200806059&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56aba196a5cf3"},"id":"rgw27_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=200806059&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":200806059,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/200806059_An_Analysis_of_Transformation","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba196a5cf3"},"id":"rgw2_56aba196a5cf3","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":200806059},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=200806059&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba196a5cf3"},"id":"rgw1_56aba196a5cf3","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"DCu7DFSDWF3uHz\/uN\/Ab9bg6JbIQR8eywtC9Ti67Cf5gHXG+w4CmQuxWtmS9ChcwWu2DSrxhh\/IkmKesFerm7WURm6FBKi\/pvKWO8X5C9XU5fbiaICL4xUl5uz7oKyWCp6ofLSL43Flanu+xHesVIkIo4ajqfkveBObqaBWVOvcil69X8Ram+UAIq+STmFiDSvG8Dp3jBw8Vw9qYq5Iwp3VDLzobw5\/thDFzGdSIPgL+\/JS\/puBugRtDbODFPY7c8rT4PvVyXfaKnNO0jpWrFA3sz2pds0g9xYfTiQPjEQE=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/200806059_An_Analysis_of_Transformation\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"An Analysis of Transformation\" \/>\n<meta property=\"og:description\" content=\"\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/200806059_An_Analysis_of_Transformation\/links\/00afa1950cf245659d00146a\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/200806059_An_Analysis_of_Transformation\" \/>\n<meta property=\"rg:id\" content=\"PB:200806059\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"An Analysis of Transformation\" \/>\n<meta name=\"citation_author\" content=\"George E. P. Box\" \/>\n<meta name=\"citation_author\" content=\"David R. Cox\" \/>\n<meta name=\"citation_publication_date\" content=\"1964\/01\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Stat\" \/>\n<meta name=\"citation_issn\" content=\"0038-9986\" \/>\n<meta name=\"citation_volume\" content=\"Series B\" \/>\n<meta name=\"citation_issue\" content=\"2\" \/>\n<meta name=\"citation_firstpage\" content=\"211\" \/>\n<meta name=\"citation_lastpage\" content=\"-246\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/200806059_An_Analysis_of_Transformation\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/200806059_An_Analysis_of_Transformation\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-737da285-0220-4653-a418-168e7988304d","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":546,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw28_56aba196a5cf3"},"id":"rgw28_56aba196a5cf3","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-737da285-0220-4653-a418-168e7988304d", "5b003cc5f009284583d3f40b0e5c28b5750003db");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-737da285-0220-4653-a418-168e7988304d", "5b003cc5f009284583d3f40b0e5c28b5750003db");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw29_56aba196a5cf3"},"id":"rgw29_56aba196a5cf3","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/200806059_An_Analysis_of_Transformation","requestToken":"OKYEUN\/3JCTqz5+Sca9zMsA7QKRFh2kg01nXC6UVOue8aQnN7uJMOF1blaSTKzxERj8TTbnJI9zC0YS55HZOAJonLLN0wx+lCg9pnrlMsRc9xFWMFEmyM+0o6JTw691YXopcvUesHoo7EA7qfpBVmFnbpiTBqWXAMbnMI2xvFhI0Wk5NG0yahgEiw36++eibGR\/pIlMzK1ktQ95\/ewSeZJDSt0hJgVDP4OO8g7uz1MKhcL\/Xmuw1yD5N+beDIBNjdHvyTq47IQmxBxhEuQCEXNou+T1fpX8jhvlbOe2BBpc=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=HAg26Ef5p6G0IDiUS927PzE6gdrT9S5CiobKmIhOR07kAml8jD-YHmEEhIt1MYfg","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjAwODA2MDU5X0FuX0FuYWx5c2lzX29mX1RyYW5zZm9ybWF0aW9u","signupCallToAction":"Join for free","widgetId":"rgw31_56aba196a5cf3"},"id":"rgw31_56aba196a5cf3","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw30_56aba196a5cf3"},"id":"rgw30_56aba196a5cf3","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw32_56aba196a5cf3"},"id":"rgw32_56aba196a5cf3","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
