<!DOCTYPE html> <html lang="en" class="" id="rgw42_56ab1ec708a09"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="rEI2C6ptCDP3jCLu+YXYgmMGcZ5X7nxoNJMsMRyjuqFFSAYJ2lSpZK6fB75SVmoYXAfOjJOUuTGF+0FZ4Xcat6gR08nrM7NXWE4HgNz2dugedL8tXGe/3h7SIOGNDZIbfKrwth4XVl8/u0SE+UXkCxQ08TrbBfdFoZu18EkSuLfGVhTxdKmpjHClb+xrS/kp55v8b1wc1NLRsyRCjdHnCg3PKCFO1Ij2uanZMCL60J0+OnOfEtliO781H4u5tuMg8wuHIQDDYB6f+D6t4+L3D53wFT6X/xMESV7HBsgrBC8="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-66a25bf3-5110-4b94-927f-373339038382",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data" />
<meta property="og:description" content="Multivariate categorical data occur in many applications of machine learning.
One of the main difficulties with these vectors of categorical variables is
sparsity. The number of possible..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data/links/565d728808aefe619b25b735/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data" />
<meta property="rg:id" content="PB:273388187" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data" />
<meta name="citation_author" content="Yarin Gal" />
<meta name="citation_author" content="Yutian Chen" />
<meta name="citation_author" content="Zoubin Ghahramani" />
<meta name="citation_publication_date" content="2015/03/07" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Yutian_Chen3/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data/links/565d728808aefe619b25b735.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1ec708a09" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1ec708a09" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab1ec708a09">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Latent%20Gaussian%20Processes%20for%20Distribution%20Estimation%20of%20Multivariate%20Categorical%20Data&rft.date=2015&rft.au=Yarin%20Gal%2CYutian%20Chen%2CZoubin%20Ghahramani&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data</h1> <meta itemprop="headline" content="Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data/links/565d728808aefe619b25b735/smallpreview.png">  <div id="rgw7_56ab1ec708a09" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab1ec708a09"> <a href="researcher/2069013556_Yarin_Gal" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Yarin Gal" alt="Yarin Gal" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Yarin Gal</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56ab1ec708a09">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2069013556_Yarin_Gal"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Yarin Gal" alt="Yarin Gal" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2069013556_Yarin_Gal" class="display-name">Yarin Gal</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab1ec708a09" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Yutian_Chen3" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A278543817822209%401443421429383_m" title="Yutian Chen" alt="Yutian Chen" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Yutian Chen</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56ab1ec708a09" data-account-key="Yutian_Chen3">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Yutian_Chen3"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A278543817822209%401443421429383_l" title="Yutian Chen" alt="Yutian Chen" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Yutian_Chen3" class="display-name">Yutian Chen</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/University_of_Cambridge" title="University of Cambridge">University of Cambridge</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab1ec708a09"> <a href="researcher/8159937_Zoubin_Ghahramani" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Zoubin Ghahramani</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab1ec708a09">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/8159937_Zoubin_Ghahramani"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Zoubin Ghahramani" alt="Zoubin Ghahramani" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/8159937_Zoubin_Ghahramani" class="display-name">Zoubin Ghahramani</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2015-03">  03/2015;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1503.02182" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw14_56ab1ec708a09" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Multivariate categorical data occur in many applications of machine learning.<br />
One of the main difficulties with these vectors of categorical variables is<br />
sparsity. The number of possible observations grows exponentially with vector<br />
length, but dataset diversity might be poor in comparison. Recent models have<br />
gained significant improvement in supervised tasks with this data. These models<br />
embed observations in a continuous space to capture similarities between them.<br />
Building on these ideas we propose a Bayesian model for the unsupervised task<br />
of distribution estimation of multivariate categorical data. We model vectors<br />
of categorical variables as generated from a non-linear transformation of a<br />
continuous latent space. Non-linearity captures multi-modality in the<br />
distribution. The continuous representation addresses sparsity. Our model ties<br />
together many existing models, linking the linear categorical latent Gaussian<br />
model, the Gaussian process latent variable model, and Gaussian process<br />
classification. We derive inference for our model based on recent developments<br />
in sampling based variational inference. We show empirically that the model<br />
outperforms its linear and discrete counterparts in imputation tasks of sparse<br />
data.</div> </p>  </div>   </div>      <div class="action-container"> <div id="rgw15_56ab1ec708a09" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56ab1ec708a09">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw34_56ab1ec708a09">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Yutian_Chen3/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data/links/565d728808aefe619b25b735.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Yutian_Chen3">Yutian Chen</a>, <span class="js-publication-date"> Dec 01, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw36_56ab1ec708a09" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw37_56ab1ec708a09" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw38_56ab1ec708a09" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw39_56ab1ec708a09" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw40_56ab1ec708a09" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw41_56ab1ec708a09" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw35_56ab1ec708a09" src="https://www.researchgate.net/c/o1o9o3/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FYutian_Chen3%2Fpublication%2F273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data%2Flinks%2F565d728808aefe619b25b735.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56ab1ec708a09"  itemprop="articleBody">  <p>Page 1</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate<br />Categorical Data<br />Yarin Gal<br />Yutian Chen<br />Zoubin Ghahramani<br />University of Cambridge<br />YG279@CAM.AC.UK<br />YC373@CAM.AC.UK<br />ZOUBIN@CAM.AC.UK<br />Abstract<br />Multivariate categorical data occur in many ap-<br />plications of machine learning. One of the main<br />difficulties with these vectors of categorical vari-<br />ables is sparsity. The number of possible obser-<br />vations grows exponentially with vector length,<br />but dataset diversity might be poor in compari-<br />son. Recent models have gained significant im-<br />provement in supervised tasks with this data.<br />These models embed observations in a continu-<br />ous space to capture similarities between them.<br />Building on these ideas we propose a Bayesian<br />model for the unsupervised task of distribution<br />estimation of multivariate categorical data.<br />We model vectors of categorical variables as<br />generated from a non-linear transformation of<br />a continuous latent space.<br />tures multi-modality in the distribution. The con-<br />tinuous representation addresses sparsity. Our<br />model ties together many existing models, link-<br />ing the linear categorical latent Gaussian model,<br />the Gaussian process latent variable model, and<br />Gaussian process classification. We derive in-<br />ference for our model based on recent develop-<br />ments in sampling based variational inference.<br />We show empirically that the model outperforms<br />its linear and discrete counterparts in imputation<br />tasks of sparse data.<br />Introduction<br />Multivariate categorical data is common in fields ranging<br />from language processing to medical diagnosis. Recently<br />proposed supervised models have gained significant im-<br />provement in tasks involving big labelled data of this form<br />(see for example Bengio et al. (2006); Collobert &amp; Weston<br />(2008)). These models rely on information that had been<br />largely ignored before: similarity between vectors of cate-<br />gorical variables. But what should we do in the unsuper-<br />vised setting, when we face small unlabelled data of this<br />form?<br />Non-linearity cap-<br />1<br />Medical diagnosis provides good examples of small unla-<br />belled data. Consider a dataset composed of test results of<br />a relatively small number of patients. Each patient has a<br />medical record composed often of dozens of examinations,<br />taking various categorical test results. We might be faced<br />with the task of deciding which tests are necessary for a<br />patient under examination to take, and which examination<br />results could be deduced from the existing tests. This can<br />be achieved with distribution estimation.<br />Several tools in the Bayesian framework could be used<br />for this task of distribution estimation of unlabelled small<br />datasets. Tools such as the Dirichlet-Multinomial distribu-<br />tion and its extensions are an example of such. These rely<br />on relative frequencies of categorical variables appearing<br />with others, with the addition of various smoothing tech-<br />niques. But when faced with long multivariate sequences,<br />these models run into problems of sparsity. This occurs<br />when the data consists of vectors of categorical variables<br />with most configurations of categorical values not in the<br />dataset. In medical diagnosis this happens when there is a<br />large number of possible examinations compared to a small<br />number of patients.<br />Building on ideas used for big labelled discrete data, we<br />propose a Bayesian model for distribution estimation of<br />small unlabelled data. Existing supervised models for dis-<br />crete labelled data embed the observations in a continuous<br />space. This is used to find the similarity between vectors of<br />categorical variables. We extend this idea to the small unla-<br />belled domain by modelling the continuous embedding as<br />a latent variable. A generative model is used to find a dis-<br />tribution over the discrete observations by modelling them<br />as dependent on the continuous latent variables.<br />Following the medical diagnosis example, patient n would<br />be modelled by a continuous latent variable xn∈ X. For<br />each examination d, the latent xninduces a vector of prob-<br />abilitiesf = (fnd1,...,fndK), oneprobabilityforeachpos-<br />sible test result k. A categorical distribution returns test re-<br />sult yndbased on these probabilities, resulting in a patient’s<br />medical assessment yn= (yn1,...,ynD). We need to de-<br />cide how to model the distribution over the latent space X<br />and vectors of probabilities f.<br />We would like to capture sparse multi-modal categorical<br />distributions. A possible approach would be to model the<br />arXiv:1503.02182v1  [stat.ML]  7 Mar 2015</p>  <p>Page 2</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />continuous representation with a simple latent space and a<br />non-linear transformation of points in the space to obtain<br />probabilities. In this approach we place a standard normal<br />distribution prior on a latent space, and feed the output of a<br />non-linear transformation of the latent space into a Softmax<br />to obtain probabilities. We use sparse Gaussian processes<br />(GPs) to transform the latent space non-linearly. Sparse<br />GPs form a distribution over functions supported on a small<br />number of points with linear time complexity (Qui˜ nonero-<br />Candela &amp; Rasmussen, 2005; Titsias, 2009). We use a co-<br />variance function that is able to transform the latent space<br />non-linearly. We name this model the Categorical Latent<br />Gaussian Process (CLGP). Using a Gaussian process with<br />a linear covariance function recovers the linear Gaussian<br />model (LGM, Marlin et al., 2011). This model linearly<br />transform a continuous latent space resulting in discrete ob-<br />servations.<br />The Softmax likelihood is not conjugate to the our Gaus-<br />sian prior, and integrating the latent variables with a Soft-<br />max distribution is intractable. A similar problem exists<br />with LGMs. Marlin et al. (2011) solved this by using vari-<br />ational inference and various bounds for the likelihood in<br />the binary case, or alternative likelihoods to the Softmax in<br />the categorical case (Khan et al., 2012). Many bounds have<br />been studied in the literature for the binary case: Jaakkola<br />and Jordan’s bound (Jaakkola &amp; Jordan, 1997), the tilted<br />bound (Knowles &amp; Minka, 2011), piecewise linear and<br />quadratic bounds (Marlin et al., 2011), and others. But for<br />categorical data fewer bounds exist, since the multivariate<br />Softmax is hard to approximate in high-dimensions. The<br />Bohning bound (B¨ ohning, 1992) and Blei and Lafferty’s<br />bound (Blei &amp; Lafferty, 2006) give poor approximation<br />(Khan et al., 2012).<br />Instead we use recent developments in sampling-based<br />variational inference (Blei et al., 2012) to avoid integrat-<br />ing the latent variables with the Softmax analytically. Our<br />approach takes advantage of this tool to obtain simple yet<br />powerful model and inference. We use Monte Carlo in-<br />tegration to approximate the non-conjugate likelihood ob-<br />taining noisy gradients (Kingma &amp; Welling, 2013; Rezende<br />et al., 2014; Titsias &amp; L´ azaro-Gredilla, 2014). We then use<br />learning-ratefreestochasticoptimisation(Tieleman&amp;Hin-<br />ton, 2012) to optimise the noisy objective. We leverage<br />symbolic differentiation (Theano, Bergstra et al., 2010) to<br />obtain simple and modular code1.<br />Weexperimentallyshowtheadvantagesofusingnon-linear<br />transformations for the latent space. We follow the ideas<br />brought in Paccanaro &amp; Hinton (2001) and evaluate the<br />models on relation embedding and relation learning. We<br />1Python code for the model and inference is given in the ap-<br />pendix, and available at http://github.com/yaringal/<br />CLGP<br />then demonstrate the utility of the model in the real-world<br />sparse data domain. We use a medical diagnosis dataset<br />where data is scarce, comparing our model to discrete fre-<br />quency based models. We use the estimated distribution<br />for a task similar to the above, where we attempt to in-<br />fer which test results can be deduced from the others. We<br />compare the models on the task of imputation of raw data<br />studying the effects of government terror warnings on po-<br />litical attitudes. We then evaluate the continuous models on<br />a binary Alphadigits dataset composed of binary images of<br />handwritten digits and letters, where each class contains a<br />small number of images. We inspect the latent space em-<br />beddings and separation of the classes. Lastly, we evaluate<br />the robustness of our inference, inspecting the Monte Carlo<br />estimate variance over time.<br />2<br />Our model (CLGP) relates to some key probabilistic mod-<br />els (fig. 1). It can be seen as a non-linear version of the<br />latent Gaussian model (LGM, Khan et al. (2012)) as dis-<br />cussed above. In the LGM we have a standard normal prior<br />placed on a latent space, which is transformed linearly and<br />fed into a Softmax likelihood function. The probability<br />vector output is then used to sample a single categorical<br />value for each categorical variable (e.g. medical test re-<br />sults) in a list of categorical variables (e.g. medical assess-<br />ment). These categorical variables correspond to elements<br />in a multivariate categorical vector. The parameters of the<br />linear transformation are optimised directly within an EM<br />framework. Khan et al. (2012) avoid the hard task of ap-<br />proximating the Softmax likelihood by using an alternative<br />function (product of sigmoids) which is approximated us-<br />ing numerical techniques. Our approach avoids this cum-<br />bersome inference.<br />Related Work<br />Gaussian process <br />regression<br />Gaussian process <br />classification<br />Logistic regression<br />Gaussian process latent <br />variable model<br />Factor analysis<br />Categorical Latent <br />Gaussian Process<br />Latent Gaussian models<br />Linear regression<br />Linear Non-linear<br />Continuous<br />Discrete<br />Observed input<br />Latent input<br />Figure 1. Relations between existing models and the model pro-<br />posed in this paper (Categorical Latent Gaussian Process); the<br />model can be seen as a non-linear version of the latent Gaussian<br />model (left to right, Khan et al. (2012)), it can be seen as a latent<br />counterpart to the Gaussian process classification model (back<br />to front, Williams &amp; Rasmussen (2006)), or alternatively as a dis-<br />crete extension of the Gaussian process latent variable model (top<br />to bottom, Lawrence (2005)).</p>  <p>Page 3</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />Ourproposed modelcan alsobeseen asa latentcounterpart<br />to the Gaussian process classification model (Williams &amp;<br />Rasmussen, 2006), in which a Softmax function is again<br />used to discretise the continuous values.<br />ous valued outputs are obtained from a Gaussian process,<br />which non-linearly transforms the inputs to the classifica-<br />tion problem. Compared to GP classification where the<br />inputs are fully observed, in our case the inputs are la-<br />tent. Lastly, our model can be seen as a discrete extension<br />of the Gaussian process latent variable model (GPLVM,<br />Lawrence, 2005). This model has been proposed recently<br />as means of performing non-linear dimensionality reduc-<br />tion (counterpart to the linear principal component analysis<br />(Tipping &amp; Bishop, 1999)) and density estimation in con-<br />tinuous space.<br />The continu-<br />3 A Latent Gaussian Process Model for Mul-<br />tivariate Categorical Data<br />We consider a generative model for a dataset Y with N<br />observations (patients for example) and D categorical vari-<br />ables (different possible examinations). The d-th categor-<br />ical variable in the n-th observation, ynd, is a categorical<br />variable that can take an integer value from 0 to Kd. For<br />ease of notation, we assume all the categorical variables<br />have the same cardinality, i.e. Kd≡ K, ∀d = 1,...,D.<br />In our generative model, each categorical variable yndfol-<br />lows a categorical distribution with probability given by a<br />Softmax with weights fnd = (0,fnd1,...,fndK). Each<br />weight fndkis the output of a nonlinear function of a Q<br />dimensional latent variable xn∈ RQ: Fdk(xn). To com-<br />plete the generative model, we assign an isotropic Gaussian<br />distribution prior with standard deviation σ2<br />variables, and a Gaussian process prior for each of the non-<br />linear functions. We also consider a set of M auxiliary<br />variables which are often called inducing inputs. These<br />inputs Z ∈ RM×Qlie in the latent space with their cor-<br />responding outputs U ∈ RM×D×Klying in the weight<br />space (together with fndk). The inducing points are used<br />as “support” for the function. Evaluating the covariance<br />function of the GP on these instead of the entire dataset<br />allows us to perform approximate inference in O(M2N)<br />time complexity instead of O(N3) (where M is the num-<br />ber of inducing points and N is the number of data points<br />(Qui˜ nonero-Candela &amp; Rasmussen, 2005)).<br />xfor the latent<br />The model is expressed as:<br />xnq<br />iid<br />∼ N(0,σ2<br />iid<br />∼ GP(Kd)<br />fndk= Fdk(xn),<br />ynd∼ Softmax(fnd),<br />for n ∈ [N] (the set of naturals between 1 and N), q ∈ [Q],<br />d ∈ [D], k ∈ [K], m ∈ [M], and where the Softmax<br />x)<br />(1)<br />Fdk<br />umdk= Fdk(zm)<br />distribution is defined as,<br />Softmax(y = k;f) = Categorical<br />?<br />exp(fk)<br />exp(lse(f))<br />?<br />,<br />lse(f) = log<br />?<br />1 +<br />K<br />?<br />k?=1<br />exp(fk?)<br />?<br />,<br />(2)<br />for k = 0,...,K and with f0:= 0.<br />Following our medical diagnosis example from the in-<br />troduction, each patient is modelled by latent xn; for<br />each examination d, xn has a sequence of weights<br />(fnd1,...,fndK), one weight for each possible test result k,<br />that follows a Gaussian process; Softmax returns test result<br />yndbased on these weights, resulting in a patient’s medical<br />assessment yn= (yn1,...,ynD).<br />Define fdk<br />(u1dk,...,uMdk).<br />with the latent nonlinear function, Fdk, marginalized un-<br />der the GP assumption, is a multi-variate Gaussian dis-<br />tribution N(0,Kd([X,Z],[X,Z])). It is easy to verify<br />that when we further marginalize the inducing outputs,<br />we end up with a joint distribution of the form fdk ∼<br />N(0,Kd(X,X)),∀d,k. Therefore, the introduction of in-<br />ducing outputs does not change the marginal likelihood of<br />the data Y. These are used in the variational inference<br />method in the next section and the inducing inputs Z are<br />considered as variational parameters.<br />=(f1dk,...,fNdk) and define udk<br />The joint distribution of (fdk,udk)<br />=<br />We use the automatic relevance determination (ARD) ra-<br />dial basis function (RBF) covariance function for our<br />model. ARD RBF is able to select the dimensionality of the<br />latent space automatically and transform it non-linearly.<br />4<br />The marginal log-likelihood, also known as log-evidence,<br />is intractable for our model due to the non-linearity of<br />the covariance function of the GP and the Softmax like-<br />lihood function. We first describe a lower bound of the<br />log-evidence (ELBO) by applying Jensen’s inequality with<br />a variational distribution of the latent variables following<br />Titsias &amp; Lawrence (2010).<br />Inference<br />Consider a variational approximation to the posterior dis-<br />tribution of X, F and U factorized as follows:<br />q(X,F,U) = q(X)q(U)p(F|X,U).<br />We can obtain the ELBO by applying Jensen’s inequality<br />?<br />≥<br />· logp(X)p(U)p(F|X,U)p(Y|F)<br />q(X)q(U)p(F|X,U)<br />= −KL(q(X)?p(X)) − KL(q(U)?p(U))<br />(3)<br />logp(Y) = logp(X)p(U)p(F|X,U)p(Y|F)X.F.U.<br />?<br />q(X)q(U)p(F|X,U)<br />X.F.U.</p>  <p>Page 4</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />+<br />N<br />?<br />n=1<br />D<br />?<br />d=1<br />?<br />q(xn)q(Ud)p(fnd|xn,Ud)<br />· logp(ynd|fnd)x.nf.ndU. d<br />:= L<br />(4)<br />where<br />p(fnd|xn,Ud) =<br />K<br />?<br />k=1<br />N(fndk|aT<br />ndudk,bnd)<br />(5)<br />with<br />and= K−1<br />bnd= Kd,nn− Kd,nMK−1<br />Notice however that the integration of logp(ynd|fnd) in eq.<br />4 involves a nonlinear function (lse(f) from eq. 2) and is<br />still intractable. Consequently, we donot have an analytical<br />form for the optimal variational distribution of q(U) unlike<br />in Titsias &amp; Lawrence (2010). Instead of applying a further<br />approximation/lower bound on L, we want to obtain bet-<br />ter accuracy by following a sampling-based approach (Blei<br />etal.,2012;Kingma&amp;Welling,2013;Rezendeetal.,2014;<br />Titsias &amp; L´ azaro-Gredilla, 2014) to compute the lower<br />bound L and its derivatives with the Monte Carlo method.<br />Specifically, we draw samples of xn, Ud and fnd from<br />q(xn), q(Ud), and p(fnd|xn,Ud) respectively and esti-<br />mate L with the sample average. Another advantage of us-<br />ing the Monte Carlo method is that we are not constrained<br />to a limited choice of covariance functions for the GP that<br />is otherwise required for an analytical solution in stan-<br />dard approaches to GPLVM for continuous data (Titsias &amp;<br />Lawrence, 2010; Hensman et al., 2013).<br />d,MMKd,Mn,<br />d,MMKd,Mn.<br />(6)<br />We consider a mean field approximation for the latent<br />points q(X) as in (Titsias &amp; Lawrence, 2010) and a joint<br />Gaussian distribution with the following factorisation for<br />q(U):<br />D<br />?<br />N<br />?<br />where the covariance matrix Σdis shared for the same cat-<br />egorical variable d (remember that K is the number of val-<br />ues this categorical variable can take). The KL divergence<br />in L can be computed analytically with the given varia-<br />tional distributions. The parameters we need to optimise<br />over2include the hyper-parameters for the GP θd, varia-<br />tional parameters for the inducing points Z, µdk, Σd, and<br />the mean and standard deviation of the latent points mni,<br />sni.<br />q(U) =<br />d=1<br />K<br />?<br />Q<br />?<br />k=1<br />N(udk|µdk,Σd),<br />q(X) =<br />n=1<br />i=1<br />N(xni|mni,s2<br />ni)<br />(7)<br />2Note that the number of parameters to optimise over can be<br />reduced by transforming the latent space non-linearly to a second<br />low-dimensional latent space, which is then transformed linearly<br />to the weight space containing points fndk.<br />4.1Transforming the Random Variables<br />InordertoobtainaMonteCarloestimatetothegradientsof<br />L with low variance, a useful trick introduced in (Kingma<br />&amp; Welling, 2013) is to transform the random variables to<br />be sampled so that the randomness does not depend on the<br />parameters with which the gradients will be computed. We<br />present the transformation of each variable to be sampled<br />as follows:<br />Transforming X.<br />transformation for X is straightforward as<br />xni= mni+ sniε(x)<br />For the mean field approximation, the<br />ni,ε(x)<br />ni∼ N(0,1)<br />(8)<br />Transforming udk.<br />a joint Gaussian distribution. Denote the Cholesky decom-<br />position of Σdas LdLT<br />udk= µdk+ Ldε(u)<br />We optimize the lower triangular matrix Ldinstead of Σd.<br />The variational distribution of udkis<br />d= Σd. We can rewrite udkas<br />dk,ε(u)<br />dk∼ N(0,IM)<br />(9)<br />Transforming fnd.<br />p(fnd|xn,Ud) in Eq. 5 is factorized over k we can define a<br />new random variable for every fndk:<br />fndk= aT<br />bndε(f)<br />Since the conditional distribution<br />ndudk+<br />?<br />ndk,ε(f)<br />ndk∼ N(0,1)<br />(10)<br />Notice that the transformation of the variables does not<br />change the distribution of the original variables and there-<br />fore does not change the KL divergence in Eq. 5.<br />4.2Lower Bound with Transformed Variables<br />Given the transformation we just defined, we can represent<br />the lower bound as<br />N<br />?<br />D<br />?<br />N<br />?<br />logSoftmaxynd<br />L = −<br />n=1<br />Q<br />?<br />i=1<br />KL(q(xni)?p(xni))<br />−<br />d=1<br />K<br />?<br />D<br />?<br />k=1<br />KL(q(udk)?p(udk))<br />+<br />n=1<br />d=1<br />Eε(x)<br />n ,ε(u)<br />d<br />,ε(f)<br />nd<br />???fnd<br />?<br />??<br />ε(f)<br />nd,Ud(ε(u)<br />d),xn(ε(x)<br />n)<br />???<br />(11)<br />where the expectation in the last line is with respect to the<br />fixed distribution defined in Eqs. 8, 9 and 10. Each expec-<br />tation term that involves the Softmax likelihood, denoted<br />as Lnd<br />Lnd<br />1<br />T<br />i=1<br />s, can be estimated using Monte Carlo integration as<br />s ≈<br />T<br />?<br />logSoftmax<br />?<br />ynd<br />???fnd<br />?<br />ε(f)<br />nd,i,Ud(ε(u)<br />d,i),xn(ε(x)<br />n,i)<br />??<br />(12)<br />with ε(x)<br />tributions. Since these distributions do not depend on the<br />n,i,ε(u)<br />d,i,ε(f)<br />nd,idrawn from their corresponding dis-</p>  <p>Page 5</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />parameters to be optimized, the derivatives of the objective<br />function L are now straight-forward to compute with the<br />same set of samples using the chain rule.<br />4.3 Stochastic Gradient Descent<br />We use gradient descent to find an optimal variational dis-<br />tribution. Gradient descent with noisy gradients is guaran-<br />teed to converge to a local optimum given decreasing learn-<br />ing rate with some conditions, but is hard to work with in<br />practice. Initial values set for the learning rate influence<br />the rate at which the algorithm converges, and misspeci-<br />fied values can cause it to diverge. For this reason new<br />techniques have been proposed that handle noisy gradients<br />well. Optimisers such as AdaGrad (Duchi et al., 2011),<br />AdaDelta(Zeiler,2012), andRMSPROP(Tieleman&amp;Hin-<br />ton, 2012) have been proposed, each handling the gradi-<br />ents slightly differently, all averaging over past gradients.<br />Schaul et al. (2013) have studied empirically the different<br />techniques, comparing them to one another on a variety<br />of unit tests. They found that RMSPROP works better on<br />many test sets compared to other optimisers evaluated. We<br />thus chose to use RMSPROP for our experiments.<br />A major advantage of our inference is that it is extremely<br />easy to implement and adapt. The straight-forward com-<br />putation of derivatives through the expectation makes it<br />possible to use symbolic differentiation. We use Theano<br />(Bergstra et al., 2010) for the inference implementation,<br />where the generative model is implemented as in Eqs. 8,<br />9 and 10, and the optimisation objective, evaluated on sam-<br />ples from the generative model, is given by Eq. 11.<br />5<br />We evaluate the categorical latent Gaussian process<br />(CLGP), comparing it to existing models for multivariate<br />categorical distribution estimation. These include mod-<br />els based on a discrete latent representation (such as the<br />Dirichlet-Multinomial), and continuous latent representa-<br />tion with a linear transformation of the latent space (latent<br />Gaussian model (LGM, Khan et al., 2012)). We demon-<br />strate over-fitting problems with the LGM, and inspect the<br />robustness of our inference.<br />Experimental Results<br />For the following experiments, both the linear and non-<br />linear models were initialised with a 2D latent space. The<br />meanvaluesofthelatentpoints, mn, wereinitialisedatran-<br />dom following a standard normal distribution. The mean<br />values of the inducing outputs (µdk) were initialised with<br />a normal distribution with standard deviation 10−2. This<br />is equivalent to using a uniform initial distribution for all<br />values. We initialise the standard deviation of each latent<br />point(sn)to0.1, andinitialisethelength-scalesoftheARD<br />RBF covariance function to 0.1. We then optimise the vari-<br />ational distribution for 500 iterations. At every iteration we<br />optimise the various quantities while holding udk’s varia-<br />tional parameters fixed, and then optimise udk’s variational<br />parameters holding the other quantities fixed.<br />Our setting supports semi-supervised learning with par-<br />tially observed data. The latents of partially observed<br />points are optimised with the training set, and then used<br />to predict the missing values. We assess the performance<br />of the models using test-set perplexity (a measure of how<br />much the model is surprised by the missing test values).<br />This is defined as the exponent of the negative average log<br />predicted probability.<br />5.1 Linear Models Have Difficulty with Multi-modal<br />Distributions<br />We show the advantages of using a non-linear categorical<br />distribution estimation compared to a linear one, evaluating<br />the CLGP against the linear LGM. We implement the latent<br />Gaussian model using a linear covariance function in our<br />model; we remove the KL divergence term in u following<br />the model specification in (Khan et al., 2012), and use our<br />inference scheme described above. Empirically, the Monte<br />Carlo inference scheme with the linear model results in the<br />same test error on (Inoguchi, 2008) as the piece-wise bound<br />based inference scheme developed in (Khan et al., 2012).<br />5.1.1Relation Embedding – Exclusive Or<br />A simple example of relational learning (following Pac-<br />canaro &amp; Hinton (2001)) can be used to demonstrate<br />when linear latent space models fail. In this task we are<br />given a dataset with example relations and the model is<br />(a) LGM<br />(b) CLGP<br />Figure 2. Density over the latent space for XOR as predicted<br />by the linear model (top, LGM), and non-linear model (bot-<br />tom, CLGP). Each figure shows the probability p(yd = 1|x)<br />as a function of x, for d = 0,1,2 (first, second, and third dig-<br />its in the triplet left to right). The darker the shade of green, the<br />higher the probability. In blue are the latents corresponding to the<br />training points, in yellow are the latents corresponding to the four<br />partially observed test points, and in red are the inducing points<br />used to support the function.</p>  <p>Page 6</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />(a) LGM(b) CLGP (c) Ground truth<br />Figure 3. Histogram of categorical values for XOR (encoded<br />in binary for the 8 possible values) for samples drawn from the<br />posterior of the latent space of the linear model (left, LGM), the<br />non-linear model (middle, CLGP), and the data used for training<br />(right).<br />to capture the distribution that generated them. A non-<br />linear dataset is constructed using the XOR (exclusive<br />or) relation.We collect 25 positive examples of each<br />assignment of the binary relation (triplets of the form<br />(0,0,0), (0,1,1), (1,0,1), (1,1,0), corresponding to<br />0 XOR 1 = 1 and so on). We then maximise the vari-<br />ational lower bound using RMSPROP for both the lin-<br />ear and non-linear models with 20 samples for the Monte<br />Carlo integration. We add four more triplets to the dataset:<br />(0,0,?), (0,1,?), (1,0,?), (1,1,?).<br />probabilities the models assign to each of the missing val-<br />ues (also known as imputation) and report the results.<br />We evaluate the<br />We assessed the test-set perplexity repeating the experi-<br />ment 3 times and averaging the results. The linear model<br />obtained a test-set perplexity (with standard deviation) of<br />75.785 ± 13.221, whereas the non-linear model obtained<br />a test-set perplexity of 1.027 ± 0.016. A perplexity of 1<br />corresponds to always predicting correctly.<br />Duringoptimisationthelinearmodeljumpsbetweendiffer-<br />ent local modes, trying to capture all four possible triplets<br />(fig. 2). The linear model assigns probabilities to the miss-<br />ing values by capturing some of the triplets well, but cannot<br />assign high probability to the others. In contrast, the CLGP<br />model is able to capture all possible values of the relation.<br />Sampling from probability vectors from the latent varia-<br />tional posterior for both models, we get a histogram of the<br />posterior distribution (fig. 3). As can be seen, the CLGP<br />model is able to fully capture the distribution whereas the<br />linear model is incapable of it.<br />5.1.2 Relation Learning – Complex Example<br />We repeat the previous experiment with a more com-<br />plex relation.We generated 1000 strings from a<br />simple probabilistic context free grammar with non-<br />terminals {α,β,A,B,C}, start symbol α, terminals<br />{a,b,c,d,e,f,g}, and derivation rules:<br />α → Aβ [1.0]<br />β → BA [0.5] | C [0.5]<br />A → a [0.5] | b [0.3] | c [0.2]<br />B → d [0.7] | e [0.3]<br />C → f [0.7] | g [0.3]<br />where we give the probability of following a derivation rule<br />inside square brackets. Following the start symbol and the<br />derivation rules we generate strings of the form ada, af,<br />ceb, and so on. We add two “start” and “end” symbols<br />s,s to each string obtaining strings of the form sscebss.<br />We then extract triplets of consecutive letters from these<br />strings resulting in training examples of the form ssc, sce,<br />..., bss. This is common practice in text preprocessing in<br />the natural language community. It allows us to generate<br />relations from scratch by conditioning on the prefix ss to<br />generate the first non-terminal (say a), then condition on<br />the prefix sa, and continue in this vein iteratively. Note<br />that a simple histogram based approach will do well on this<br />dataset (and the previous dataset) as it is not sparse.<br />Split<br />1<br />2<br />3<br />LGM<br />CLGP<br />7.898 ± 3.220<br />26.477 ± 23.457<br />6.748 ± 3.028<br />2.769 ± 0.070<br />2.958 ± 0.265<br />2.797 ± 0.081<br />Table 1. Test-set perplexity on relational data. Compared are<br />the linear LGM and the non-linear CLGP.<br />We compared the test-set perplexity of the non-linear<br />CLGP (with 50 inducing points) to that of the linear LGM<br />on these training inputs, repeating the same experiment set-<br />up as before. We impute one missing value at random in<br />each test string, using 20% of the strings as a test set with<br />3 different splits. The results are presented in table 1. The<br />linear model cannot capture this data well, and seems to get<br />very confident about misclassified values (resulting in very<br />high test-set perplexity).<br />5.2Sparse Small Data<br />We assess the model in the real-world domain of small<br />sparse data.We compare the CLGP model to a his-<br />togram based approach, demonstrating the difficulty with<br />frequency models for sparse data. We further compare our<br />model to the linear LGM.<br />5.2.1 Medical Diagnosis<br />We use the Wisconsin breast cancer dataset for which ob-<br />taining samples is a long and expensive process (Zwitter &amp;<br />Soklic, 1988). The dataset is composed of 683 data points,<br />with 9 categorical variables taking values between 1 and<br />10, and an additional categorical variable taking 0,1 values<br />– 2 × 109possible configurations. We use three quarters<br />of the dataset for training and leave the rest for testing, av-<br />eraging the test-set perplexity on three repetitions of the<br />experiment. We use three different random splits of the<br />dataset as the distribution of the data can be fairly different<br />among different splits. In the test set we randomly remove</p>  <p>Page 7</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />Split<br />1<br />2<br />3<br />Baseline<br />8.68<br />8.68<br />8.85<br />Multinomial<br />4.41<br />∞<br />4.64<br />Uni-Dir-Mult<br />4.41<br />4.42<br />4.64<br />Bi-Dir-Mult<br />3.41<br />3.49<br />3.67<br />LGM<br />CLGP<br />3.57 ± 0.208<br />3.47 ± 0.252<br />12.13 ± 9.705<br />2.86 ± 0.119<br />3.36 ± 0.186<br />3.34 ± 0.096<br />Table 2. Test-set perplexity on Breast cancer dataset, predicting randomly missing categorical test results. The models compared<br />are: Baseline predicting uniform probability for all values, Multinomial – predicting the probability for a missing value based on its<br />frequency, Uni-Dir-Mult – Unigram Dirichlet Multinomial with concentration parameter α = 0.01, Bi-Dir-Mult – Bigram Dirichlet<br />Multinomial with concentration parameter α = 1, LGM, and the proposed model (CLGP).<br />one of the 10 categorical values, and test the models’ abil-<br />ity to recover that value. Note that this is a harder task than<br />the usual use of this dataset for binary classification. We<br />use the same model set-up as in the first experiment.<br />We compare our model (CLGP) to a baseline predicting<br />uniform probability for all values (Baseline), a multino-<br />mial model predicting the probability for a missing value<br />based on its frequency (Multinomial), a unigram Dirichlet<br />Multinomial model with concentration parameter α = 0.01<br />(Uni-DirMult), a bigram Dirichlet Multinomial with con-<br />centration parameter α = 1 (Bi-Dir-Mult), and the lin-<br />ear continuous space model (LGM). More complicated fre-<br />quency based approaches are possible, performing variable<br />selection and then looking at frequencies of triplets of vari-<br />ables. These will be very difficult to work with in this<br />sparse small data problem.<br />Weevaluatethemodels’performanceusingthetest-setper-<br />plexity metric as before (table 2). As can be seen, the<br />frequency based approaches obtain worse results than the<br />continuous latent space approaches. The frequency model<br />with no smoothing obtains perplexity ∞ for split 2 because<br />one of the test points has a value not observed before in<br />the training set. Using smoothing solves this. The baseline<br />(predicting uniformly) obtains the highest perplexity on av-<br />erage. The linear model exhibits high variance for the last<br />split, and in general has higher perplexity standard devia-<br />tion than the non-linear model.<br />5.2.2 Terror Warning Effects on Political Attitude<br />We further compare the models on raw data collected in<br />an experimental study of the effects of government terror<br />warnings on political attitudes (from the START Terrorism<br />Data Archive Dataverse3). This dataset consists of 1282<br />cases and 49 variables. We used 1000 cases for training<br />and 282 for test. From the 49 variables, we used the 17 cor-<br />responding to categorical answers to study questions (the<br />other variables are subjects’ geographic info and an answer<br />to an open question). The 17 variables take 5 or 6 possible<br />values for each variable with some of the values missing.<br />We repeat the same experiment set-up as before, with a 6<br />3Obtained<br />thedata.harvard.edu/dvn/dv/start/faces/<br />study/StudyPage.xhtml?studyId=71190<br />fromtheHarvardDataverse Network<br />Baseline<br />2.50<br />Multinomial<br />2.20<br />LGM<br />3.07<br />CLGP<br />2.11<br />Table 3. Test-set perplexity for terror warning effects on polit-<br />ical attitude. Compared are the linear LGM and the non-linear<br />CLGP.<br />dimensional latent dimensions, 100 inducing points, 5 sam-<br />ples to estimate the lower bound, and running the optimiser<br />for 1000 iterations. We compare a Baseline using a uni-<br />form distribution for all values, a Multinomial model using<br />the frequencies of the individual variables, the linear LGM,<br />and the CLGP. The results are given in table 3. On the<br />training set, the CLGP obtained a training error of 1.56,<br />and the LGM obtained a training error of 1.34. The linear<br />model seems to over-fit to the data.<br />5.2.3Handwritten Binary Alphadigits<br />We evaluate the performance of our model on a sparse<br />dataset with a large number of dimensions. The dataset,<br />Binary Alphadigits, is composed of 20 × 16 binary images<br />of 10 handwritten digits and 26 handwritten capital letters,<br />each class with 39 images4. We resize each image to 10×8,<br />and obtain a dataset of 1404 data points each with 80 binary<br />variables. We repeat the same experiment set-up as before<br />with 2 latent dimensions for ease of visual comparison of<br />the latent embeddings. Fig. 4a shows an example of each<br />alphadigit class. Each class is then randomly split to 30<br />training and 9 test examples. In the test set, we randomly<br />remove 20% of the pixels and evaluate the prediction error.<br />Fig. 4b shows the training and test error in log perplex-<br />ity for both models. LGM converges faster than CLGP<br />but ends up with a much higher prediction error due to its<br />limited modeling capacity with 2 dimensional latent vari-<br />ables. This is validated with the latent embedding shown<br />in fig. 4c and 4d. Each color-marker combination denotes<br />one alphadigit class. As can be seen, CLGP has a better<br />separation of the different classes than LGM even though<br />the class labels are not used in the training.<br />4Obtained from http://www.cs.nyu.edu/˜roweis/<br />data.html</p>  <p>Page 8</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />(a) Example alphadigits<br />0200 4006008001000<br />0.7<br />0.8<br />0.9<br />1<br />1.1<br />Iteration<br />Training / Test Error<br /> <br /> <br />LGM, Train<br />LGM, Test<br />CLGP, Train<br />CLGP, Test<br />(b) Train &amp; Test log perplexity<br />−2−1012<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />Latent Dim 1<br />Latent Dim 2<br />(c) LGM latent space<br />−2 −1012<br />−2<br />−1.5<br />−1<br />−0.5<br />0<br />0.5<br />1<br />1.5<br />2<br />Latent Dim 1<br />Latent Dim 2<br />(d) CLGP latent space<br />Figure 4. Binary alphadigit dataset. (a) example of each class (b) Train and test log perplexity for 1000 iterations. (c,d) 2-D latend<br />embedding of the 36 alphadigit classes with LGM (left) and CLGP model (right). Each color-marker combination denotes one class.<br />5.3Latent Gaussian Model Over-fitting<br />It is interesting to note that the latent Gaussian model<br />(LGM) over-fits on the different datasets. It is possible<br />to contribute this to the lack of regularisation over the lin-<br />ear transformation – the weight matrix used to transform<br />the latent space to the Softmax weights is optimised with-<br />out a prior. In all medical diagnosis experiment repetitions,<br />for example, the model’s training log perplexity decreases<br />while the test log perplexity starts increasing (see fig. 5 for<br />the train and test log perplexities of split 1 of the breast<br />cancer dataset). Note that even though the test log perplex-<br />ity starts increasing, at its lowest point it is still higher than<br />the end test log perplexity of the CLGP model. This is ob-<br />served for all splits and all repetitions.<br />It is worth noting that we compared the CLGP to the LGM<br />on the ASES survey dataset (Inoguchi, 2008) used to assess<br />the LGM in (Khan et al., 2012). We obtained a test perplex-<br />ity of 1.98, compared to LGM’s test perplexity of 1.97. We<br />concluded that the dataset was fairly linearly separable.<br />5.4 Inference Robustness<br />Lastly, we inspect the robustness of our inference, evaluat-<br />ing the Monte Carlo (MC) estimate standard deviation as<br />optimisation progresses. Fig. 6 shows the ELBO and MC<br />standard deviation per iteration (on log scale) for the Al-<br />(a) LGM(b) CLGP<br />Figure 5. Train and test log perplexities for LGM (left) and the<br />CLGP model (right) for one of the splits of the breast cancer<br />dataset. The train log perplexity of LGM decreases while the test<br />log perplexity starts increasing at iteration 50.<br />phadigit dataset. It seems that the estimator standard devia-<br />tion decreases as the approximating variational distribution<br />fits to the posterior. This makes the stochastic optimisa-<br />tion easier, speeding up convergence. A further theoretical<br />study of this behaviour in future research will be of interest.<br />6<br />We have presented the first Bayesian model capable of cap-<br />turing sparse multi-modal categorical distributions based<br />on a continuous representation. This model ties together<br />many existing models in the field, linking the linear and<br />discrete latent Gaussian models to the non-linear continu-<br />ous space Gaussian process latent variable model and to<br />the fully observed discrete Gaussian process classification.<br />Discussion and Conclusions<br />In future work we aim to answer short-comings in the cur-<br />rent model such as scalability and robustness. We scale the<br />model following research on GP scalability done in (Hens-<br />man et al., 2013; Gal et al., 2014). The robustness of the<br />model depends on the sample variance in the Monte Carlo<br />integration. As discussed in Blei et al. (2012), variance<br />reduction techniques can help in the estimation of the inte-<br />gral, and methods such as the one developed in Wang et al.<br />(2013) can effectively increase inference robustness.<br />10<br />0<br />10<br />1<br />10<br />2<br />10<br />3<br />−2.2<br />−2<br />−1.8<br />−1.6<br />−1.4<br />−1.2<br />−1<br />−0.8<br />x 10<br />5<br />log iter<br />ELBO<br />Figure 6. ELBO and standard deviation per iteration (on log<br />scale) for the Alphadigits dataset.</p>  <p>Page 9</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />References<br />Bengio,<br />S´ ebastien, Morin, Fr´ ederic, and Gauvain, Jean-Luc.<br />Neural probabilistic language models. In Innovations in<br />Machine Learning, pp. 137–186. Springer, 2006.<br />Yoshua,Schwenk,Holger,Sen´ ecal,Jean-<br />Bergstra, James, Breuleux, Olivier, Bastien, Fr´ ed´ eric,<br />Lamblin, Pascal, Pascanu, Razvan, Desjardins, Guil-<br />laume, Turian, Joseph, Warde-Farley, David, and Ben-<br />gio, Yoshua. Theano: a CPU and GPU math expression<br />compiler. In Proceedings of the Python for Scientific<br />Computing Conference (SciPy), June 2010. Oral Pre-<br />sentation.<br />Blei, David and Lafferty, John. Correlated topic models.<br />Advances in neural information processing systems, 18:<br />147, 2006.<br />Blei, David M, Jordan, Michael I, and Paisley, John W.<br />Variational Bayesian inference with stochastic search.<br />In Proceedings of the 29th International Conference on<br />Machine Learning (ICML-12), pp. 1367–1374, 2012.<br />B¨ ohning, Dankmar. Multinomial logistic regression algo-<br />rithm. Annals of the Institute of Statistical Mathematics,<br />44(1):197–200, 1992.<br />Collobert, Ronan and Weston, Jason. A unified architecture<br />for natural language processing: Deep neural networks<br />with multitask learning. In Proceedings of the 25th inter-<br />national conference on Machine learning, pp. 160–167.<br />ACM, 2008.<br />Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive<br />subgradient methods for online learning and stochastic<br />optimization.The Journal of Machine Learning Re-<br />search, 12:2121–2159, 2011.<br />Gal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E.<br />Distributed variational inference in sparse Gaussian pro-<br />cess regression and latent variable models. In Advances<br />in Neural Information Processing Systems 27. 2014.<br />Hensman, James, Fusi, Nicolo, and Lawrence, Neil D.<br />Gaussian processes for big data.<br />arXiv:1309.6835, 2013.<br />arXiv preprint<br />Inoguchi, Takashi. Asia Europe survey (ASES): A multina-<br />tional comparative study in 18 countries, 2001. In Inter-<br />university Consortium for Political and Social Research<br />(ICPSR), 2008.<br />Jaakkola, Tommi S. and Jordan, Michael I. A variational<br />approach to Bayesian logistic regression models and<br />their extensions. In Proceedings of the Sixth Interna-<br />tional Workshop on Artificial Intelligence and Statistics,<br />1997.<br />Khan, Mohammad E, Mohamed, Shakir, Marlin, Ben-<br />jamin M, and Murphy, Kevin P. A stick-breaking like-<br />lihood for categorical data analysis with latent Gaussian<br />models. In International conference on Artificial Intelli-<br />gence and Statistics, pp. 610–618, 2012.<br />Kingma, Diederik P and Welling, Max. Auto-encoding<br />variational Bayes. arXiv preprint arXiv:1312.6114,<br />2013.<br />Knowles, David A and Minka, Tom. Non-conjugate vari-<br />ational message passing for multinomial and binary re-<br />gression. In Advances in Neural Information Processing<br />Systems, pp. 1701–1709, 2011.<br />Lawrence, Neil. Probabilistic non-linear principal compo-<br />nent analysis with Gaussian process latent variable mod-<br />els. TheJournalofMachineLearningResearch, 6:1783–<br />1816, 2005.<br />Marlin, Benjamin M, Khan, Mohammad Emtiyaz, and<br />Murphy, Kevin P.Piecewise bounds for estimating<br />bernoulli-logistic latent Gaussian models. In ICML, pp.<br />633–640, 2011.<br />Paccanaro, Alberto and Hinton, Geoffrey E. Learning dis-<br />tributed representations of concepts using linear rela-<br />tional embedding. Knowledge and Data Engineering,<br />IEEE Transactions on, 13(2):232–244, 2001.<br />Qui˜ nonero-Candela, Joaquin and Rasmussen, Carl Ed-<br />ward. A unifying view of sparse approximate Gaussian<br />process regression. The Journal of Machine Learning<br />Research, 6:1939–1959, 2005.<br />Rezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan.<br />Stochastic backpropagation and approximate inference<br />in deep generative models. In Proceedings of the 31st<br />International Conference on Machine Learning (ICML-<br />14), pp. 1278–1286, 2014.<br />Schaul, Tom, Antonoglou, Ioannis, and Silver, David. Unit<br />tests for stochastic optimization. abs/1312.6055, 2013.<br />URL http://arxiv.org/abs/1312.6055.<br />Tieleman, T. and Hinton, G.<br />COURSERA: Neural networks for machine learning,<br />2012.<br />Lecture 6.5 - rmsprop,<br />Tipping, MichaelEandBishop, ChristopherM. Probabilis-<br />tic principal component analysis. Journal of the Royal<br />Statistical Society: Series B (Statistical Methodology),<br />61(3):611–622, 1999.<br />Titsias, Michalis and Lawrence, Neil. Bayesian Gaussian<br />process latent variable model.<br />tional Conference on Artificial Intelligence and Statistics<br />(AISTATS), 2010.<br />In Thirteenth Interna-</p>  <p>Page 10</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />Titsias, Michalis and L´ azaro-Gredilla, Miguel.<br />stochasticvariationalBayesfor non-conjugateinference.<br />In Proceedings of the 31st International Conference on<br />Machine Learning (ICML-14), pp. 1971–1979, 2014.<br />Doubly<br />Titsias, Michalis K. Variational learning of inducing vari-<br />ables in sparse Gaussian processes. In Artificial Intelli-<br />gence and Statistics 12, pp. 567–574, 2009.<br />Wang, Chong, Chen, Xi, Smola, Alex, and Xing, Eric.<br />Variance reduction for stochastic gradient optimization.<br />In Advances in Neural Information Processing Systems,<br />pp. 181–189, 2013.<br />Williams, Christopher KI and Rasmussen, Carl Edward.<br />Gaussian processes for machine learning. the MIT Press,<br />2(3):4, 2006.<br />Zeiler, Matthew D. Adadelta: An adaptive learning rate<br />method. arXiv preprint arXiv:1212.5701, 2012.<br />Zwitter, M. and Soklic, M. Breast cancer dataset. In Uni-<br />versity Medical Centre, Institute of Oncology, Ljubljana,<br />Yugoslavia, 1988.</p>  <p>Page 11</p> <p>Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data<br />A Appendix<br />A.1Code<br />The basic model and inference (without covariance matrix<br />caching) can be implemented in 20 lines of Python and<br />Theano for each categorical variable:<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />8<br />9<br />import theano.tensor as T<br />m = T.dmatrix(’m’) # ..and other variables<br />X = m + s * randn(N, Q)<br />U = mu + L.dot(randn(M, K))<br />Kmm = RBF(sf2, l, Z)<br />Kmn = RBF(sf2, l, Z, X)<br />Knn = RBFnn(sf2, l, X)<br />KmmInv = T.matrix_inverse(Kmm)<br />A = KmmInv.dot(Kmn)<br />B = Knn - T.sum(Kmn * KmmInv.dot(Kmn), 0)<br />F = A.T.dot(U)+B[:,None]**0.5 * randn(N,K)<br />S = T.nnet.softmax(F)<br />KL_U, KL_X = get_KL_U(), get_KL_X()<br />LS = T.sum(T.log(T.sum(Y * S, 1)))<br />- KL_U - KL_X<br />LS_func = theano.function([’’’inputs’’’],<br />LS)<br />dLS_dm = theano.function([’’’inputs’’’],<br />T.grad(LS, m)) # and others<br /># ... and optimise LS with RMS-PROP<br />10<br />11<br />12<br />13<br />14<br />15<br />16<br />17<br />18<br />19<br />20</p>  <a href="https://www.researchgate.net/profile/Yutian_Chen3/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data/links/565d728808aefe619b25b735.pdf">Download full-text</a> </div> <div id="rgw20_56ab1ec708a09" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56ab1ec708a09">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56ab1ec708a09"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Yutian_Chen3/publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data/links/565d728808aefe619b25b735.pdf" class="publication-viewer" title="565d728808aefe619b25b735.pdf">565d728808aefe619b25b735.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Yutian_Chen3">Yutian Chen</a> &middot; Dec 1, 2015 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56ab1ec708a09"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1503.02182" target="_blank" rel="nofollow" class="publication-viewer" title="Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data">Latent Gaussian Processes for Distribution Estimat...</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1503.02182" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw30_56ab1ec708a09" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (1) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw31_56ab1ec708a09" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   "  id="rgw32_56ab1ec708a09" >  <div class="indent-left">  <div id="rgw33_56ab1ec708a09" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/277959103_Dropout_as_a_Bayesian_Approximation_Appendix">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="deref/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1506.02157" target="_blank" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: de.arxiv.org </div> </div>   </div>  </div>  <div class="indent-right">      </div>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/277959103_Dropout_as_a_Bayesian_Approximation_Appendix"> <span class="publication-title js-publication-title">Dropout as a Bayesian Approximation: Appendix</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2069013556_Yarin_Gal" class="authors js-author-name ga-publications-authors">Yarin Gal</a> &middot;     <a href="researcher/8159937_Zoubin_Ghahramani" class="authors js-author-name ga-publications-authors">Zoubin Ghahramani</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We show that a multilayer perceptron (MLP) with arbitrary depth and
nonlinearities, with dropout applied after every weight layer, is
mathematically equivalent to an approximation to a well known Bayesian model.
This interpretation offers an explanation to some of dropout&#39;s key properties,
such as its robustness to over-fitting. Our interpretation allows us to reason
about uncertainty in deep learning, and allows the introduction of the Bayesian
machinery into existing deep learning frameworks in a principled way.
This document is an appendix for the main paper &quot;Dropout as a Bayesian
Approximation: Representing Model Uncertainty in Deep Learning&quot; by Gal and
Ghahramani, 2015. </span> </div>    <div class="publication-meta publication-meta">  <span class="ico-publication-preview reset-background"></span> Preview    &middot; Article &middot; Jun 2015  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw25_56ab1ec708a09" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56ab1ec708a09">  </ul> </div> </div>   <div id="rgw16_56ab1ec708a09" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1ec708a09"> <div> <h5> <a href="publication/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">State Space representation of non-stationary Gaussian Processes</span></a>  </h5>  <div class="authors"> <a href="researcher/33382619_Alessio_Benavoli" class="authors ga-similar-publication-author">Alessio Benavoli</a>, <a href="researcher/10654903_Marco_Zaffalon" class="authors ga-similar-publication-author">Marco Zaffalon</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab1ec708a09"> <div> <h5> <a href="publication/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization" class="color-inherit ga-similar-publication-title"><span class="publication-title">Probabilistic Programming with Gaussian Process Memoization</span></a>  </h5>  <div class="authors"> <a href="researcher/2034167984_Ulrich_Schaechtle" class="authors ga-similar-publication-author">Ulrich Schaechtle</a>, <a href="researcher/2089456342_Ben_Zinberg" class="authors ga-similar-publication-author">Ben Zinberg</a>, <a href="researcher/2089399536_Alexey_Radul" class="authors ga-similar-publication-author">Alexey Radul</a>, <a href="researcher/8074903_Kostas_Stathis" class="authors ga-similar-publication-author">Kostas Stathis</a>, <a href="researcher/2089392273_Vikash_K_Mansinghka" class="authors ga-similar-publication-author">Vikash K. Mansinghka</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56ab1ec708a09"> <div> <h5> <a href="publication/287250902_Quantum_assisted_Gaussian_process_regression" class="color-inherit ga-similar-publication-title"><span class="publication-title">Quantum assisted Gaussian process regression</span></a>  </h5>  <div class="authors"> <a href="researcher/2089556720_Zhikuan_Zhao" class="authors ga-similar-publication-author">Zhikuan Zhao</a>, <a href="researcher/2089802894_Jack_K_Fitzsimons" class="authors ga-similar-publication-author">Jack K. Fitzsimons</a>, <a href="researcher/2089493348_Joseph_F_Fitzsimons" class="authors ga-similar-publication-author">Joseph F. Fitzsimons</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw43_56ab1ec708a09" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw44_56ab1ec708a09">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw45_56ab1ec708a09" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=Q4Lq3s6uAfP1hh-tq6JWTk0vD_YIDANdBZptsdQHOPuR9jIUBpPzQvhiLtTmNDGv" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="jjKDqEDVZQnKupvChqI2Y7M73dq4edNH2ntXXRMjyeUJkwL5m239xmi8BD+A2QYLcxFDA5U65/sGkaGhiWC4JOoeujcXYeizX3OhYDgMlefH1TLzSXwTQIoDY+Wg/xtqV7m2s2h7SMN0BzQtcA2EEx5wUR6uAyH913Z75XzeLm9ukR0Tf/fqaPfv/76SKRUKOtoNtLWXhSRCoTsFkGhFlQiP0Zt/NjE+q4Qr4YAhevwOY4PuR+FRFJ+/qjilByPL2fN+wsMajfSOLdfEmYfGHkwX5O2sng7uF2TS6kcQKDA="/> <input type="hidden" name="urlAfterLogin" value="publication/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjczMzg4MTg3X0xhdGVudF9HYXVzc2lhbl9Qcm9jZXNzZXNfZm9yX0Rpc3RyaWJ1dGlvbl9Fc3RpbWF0aW9uX29mX011bHRpdmFyaWF0ZV9DYXRlZ29yaWNhbF9EYXRh"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjczMzg4MTg3X0xhdGVudF9HYXVzc2lhbl9Qcm9jZXNzZXNfZm9yX0Rpc3RyaWJ1dGlvbl9Fc3RpbWF0aW9uX29mX011bHRpdmFyaWF0ZV9DYXRlZ29yaWNhbF9EYXRh"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjczMzg4MTg3X0xhdGVudF9HYXVzc2lhbl9Qcm9jZXNzZXNfZm9yX0Rpc3RyaWJ1dGlvbl9Fc3RpbWF0aW9uX29mX011bHRpdmFyaWF0ZV9DYXRlZ29yaWNhbF9EYXRh"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw46_56ab1ec708a09"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 490;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Yutian Chen","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Yutian_Chen3","institution":"University of Cambridge","institutionUrl":false,"widgetId":"rgw4_56ab1ec708a09"},"id":"rgw4_56ab1ec708a09","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4885109","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab1ec708a09"},"id":"rgw3_56ab1ec708a09","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=273388187","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":273388187,"title":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"03\/2015;","publicationDateRobot":"2015-03","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1503.02182","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data"},{"key":"rft.date","value":"2015"},{"key":"rft.au","value":"Yarin Gal,Yutian Chen,Zoubin Ghahramani"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56ab1ec708a09"},"id":"rgw6_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=273388187","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":273388187,"peopleItems":[{"data":{"authorUrl":"researcher\/2069013556_Yarin_Gal","authorNameOnPublication":"Yarin Gal","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Yarin Gal","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2069013556_Yarin_Gal","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56ab1ec708a09"},"id":"rgw9_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2069013556&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56ab1ec708a09"},"id":"rgw8_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2069013556&authorNameOnPublication=Yarin%20Gal","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Yutian Chen","accountUrl":"profile\/Yutian_Chen3","accountKey":"Yutian_Chen3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Yutian Chen","profile":{"professionalInstitution":{"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge"}},"professionalInstitutionName":"University of Cambridge","professionalInstitutionUrl":"institution\/University_of_Cambridge","url":"profile\/Yutian_Chen3","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A278543817822209%401443421429383_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Yutian_Chen3","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56ab1ec708a09"},"id":"rgw11_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4885109&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"University of Cambridge","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":273388187,"widgetId":"rgw10_56ab1ec708a09"},"id":"rgw10_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4885109&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=273388187","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/8159937_Zoubin_Ghahramani","authorNameOnPublication":"Zoubin Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Zoubin Ghahramani","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/8159937_Zoubin_Ghahramani","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab1ec708a09"},"id":"rgw13_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=8159937&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab1ec708a09"},"id":"rgw12_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=8159937&authorNameOnPublication=Zoubin%20Ghahramani","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab1ec708a09"},"id":"rgw7_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=273388187&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":273388187,"abstract":"<noscript><\/noscript><div>Multivariate categorical data occur in many applications of machine learning.<br \/>\nOne of the main difficulties with these vectors of categorical variables is<br \/>\nsparsity. The number of possible observations grows exponentially with vector<br \/>\nlength, but dataset diversity might be poor in comparison. Recent models have<br \/>\ngained significant improvement in supervised tasks with this data. These models<br \/>\nembed observations in a continuous space to capture similarities between them.<br \/>\nBuilding on these ideas we propose a Bayesian model for the unsupervised task<br \/>\nof distribution estimation of multivariate categorical data. We model vectors<br \/>\nof categorical variables as generated from a non-linear transformation of a<br \/>\ncontinuous latent space. Non-linearity captures multi-modality in the<br \/>\ndistribution. The continuous representation addresses sparsity. Our model ties<br \/>\ntogether many existing models, linking the linear categorical latent Gaussian<br \/>\nmodel, the Gaussian process latent variable model, and Gaussian process<br \/>\nclassification. We derive inference for our model based on recent developments<br \/>\nin sampling based variational inference. We show empirically that the model<br \/>\noutperforms its linear and discrete counterparts in imputation tasks of sparse<br \/>\ndata.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw14_56ab1ec708a09"},"id":"rgw14_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=273388187","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw15_56ab1ec708a09"},"id":"rgw15_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56ab1ec708a09"},"id":"rgw5_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=273388187&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":33382619,"url":"researcher\/33382619_Alessio_Benavoli","fullname":"Alessio Benavoli","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10654903,"url":"researcher\/10654903_Marco_Zaffalon","fullname":"Marco Zaffalon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes","usePlainButton":true,"publicationUid":289587204,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes","title":"State Space representation of non-stationary Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":33382619,"url":"researcher\/33382619_Alessio_Benavoli","fullname":"Alessio Benavoli","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10654903,"url":"researcher\/10654903_Marco_Zaffalon","fullname":"Marco Zaffalon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/289587204_State_Space_representation_of_non-stationary_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1ec708a09"},"id":"rgw17_56ab1ec708a09","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=289587204","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","usePlainButton":true,"publicationUid":287249271,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","title":"Probabilistic Programming with Gaussian Process Memoization","displayTitleAsLink":true,"authors":[{"id":2034167984,"url":"researcher\/2034167984_Ulrich_Schaechtle","fullname":"Ulrich Schaechtle","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089456342,"url":"researcher\/2089456342_Ben_Zinberg","fullname":"Ben Zinberg","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089399536,"url":"researcher\/2089399536_Alexey_Radul","fullname":"Alexey Radul","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8074903,"url":"researcher\/8074903_Kostas_Stathis","fullname":"Kostas Stathis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089392273,"url":"researcher\/2089392273_Vikash_K_Mansinghka","fullname":"Vikash K. Mansinghka","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287249271_Probabilistic_Programming_with_Gaussian_Process_Memoization\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab1ec708a09"},"id":"rgw18_56ab1ec708a09","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=287249271","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2089556720,"url":"researcher\/2089556720_Zhikuan_Zhao","fullname":"Zhikuan Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089802894,"url":"researcher\/2089802894_Jack_K_Fitzsimons","fullname":"Jack K. Fitzsimons","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089493348,"url":"researcher\/2089493348_Joseph_F_Fitzsimons","fullname":"Joseph F. Fitzsimons","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/287250902_Quantum_assisted_Gaussian_process_regression","usePlainButton":true,"publicationUid":287250902,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/287250902_Quantum_assisted_Gaussian_process_regression","title":"Quantum assisted Gaussian process regression","displayTitleAsLink":true,"authors":[{"id":2089556720,"url":"researcher\/2089556720_Zhikuan_Zhao","fullname":"Zhikuan Zhao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089802894,"url":"researcher\/2089802894_Jack_K_Fitzsimons","fullname":"Jack K. Fitzsimons","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2089493348,"url":"researcher\/2089493348_Joseph_F_Fitzsimons","fullname":"Joseph F. Fitzsimons","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/287250902_Quantum_assisted_Gaussian_process_regression","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/287250902_Quantum_assisted_Gaussian_process_regression\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56ab1ec708a09"},"id":"rgw19_56ab1ec708a09","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=287250902","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56ab1ec708a09"},"id":"rgw16_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=273388187&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":273388187,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":273388187,"publicationType":"article","linkId":"565d728808aefe619b25b735","fileName":"565d728808aefe619b25b735.pdf","fileUrl":"profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf","name":"Yutian Chen","nameUrl":"profile\/Yutian_Chen3","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Dec 1, 2015","fileSize":"1.18 MB","widgetId":"rgw22_56ab1ec708a09"},"id":"rgw22_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=273388187&linkId=565d728808aefe619b25b735&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":273388187,"publicationType":"article","linkId":"5507a5980cf27e990e07cfbd","fileName":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1503.02182","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1503.02182","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw23_56ab1ec708a09"},"id":"rgw23_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=273388187&linkId=5507a5980cf27e990e07cfbd&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56ab1ec708a09"},"id":"rgw21_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=273388187&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":2,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":7,"valueFormatted":"7","widgetId":"rgw24_56ab1ec708a09"},"id":"rgw24_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=273388187","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56ab1ec708a09"},"id":"rgw20_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=273388187&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":273388187,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56ab1ec708a09"},"id":"rgw26_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=273388187&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":7,"valueFormatted":"7","widgetId":"rgw27_56ab1ec708a09"},"id":"rgw27_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=273388187","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56ab1ec708a09"},"id":"rgw25_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=273388187&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate\nCategorical Data\nYarin Gal\nYutian Chen\nZoubin Ghahramani\nUniversity of Cambridge\nYG279@CAM.AC.UK\nYC373@CAM.AC.UK\nZOUBIN@CAM.AC.UK\nAbstract\nMultivariate categorical data occur in many ap-\nplications of machine learning. One of the main\ndifficulties with these vectors of categorical vari-\nables is sparsity. The number of possible obser-\nvations grows exponentially with vector length,\nbut dataset diversity might be poor in compari-\nson. Recent models have gained significant im-\nprovement in supervised tasks with this data.\nThese models embed observations in a continu-\nous space to capture similarities between them.\nBuilding on these ideas we propose a Bayesian\nmodel for the unsupervised task of distribution\nestimation of multivariate categorical data.\nWe model vectors of categorical variables as\ngenerated from a non-linear transformation of\na continuous latent space.\ntures multi-modality in the distribution. The con-\ntinuous representation addresses sparsity. Our\nmodel ties together many existing models, link-\ning the linear categorical latent Gaussian model,\nthe Gaussian process latent variable model, and\nGaussian process classification. We derive in-\nference for our model based on recent develop-\nments in sampling based variational inference.\nWe show empirically that the model outperforms\nits linear and discrete counterparts in imputation\ntasks of sparse data.\nIntroduction\nMultivariate categorical data is common in fields ranging\nfrom language processing to medical diagnosis. Recently\nproposed supervised models have gained significant im-\nprovement in tasks involving big labelled data of this form\n(see for example Bengio et al. (2006); Collobert & Weston\n(2008)). These models rely on information that had been\nlargely ignored before: similarity between vectors of cate-\ngorical variables. But what should we do in the unsuper-\nvised setting, when we face small unlabelled data of this\nform?\nNon-linearity cap-\n1\nMedical diagnosis provides good examples of small unla-\nbelled data. Consider a dataset composed of test results of\na relatively small number of patients. Each patient has a\nmedical record composed often of dozens of examinations,\ntaking various categorical test results. We might be faced\nwith the task of deciding which tests are necessary for a\npatient under examination to take, and which examination\nresults could be deduced from the existing tests. This can\nbe achieved with distribution estimation.\nSeveral tools in the Bayesian framework could be used\nfor this task of distribution estimation of unlabelled small\ndatasets. Tools such as the Dirichlet-Multinomial distribu-\ntion and its extensions are an example of such. These rely\non relative frequencies of categorical variables appearing\nwith others, with the addition of various smoothing tech-\nniques. But when faced with long multivariate sequences,\nthese models run into problems of sparsity. This occurs\nwhen the data consists of vectors of categorical variables\nwith most configurations of categorical values not in the\ndataset. In medical diagnosis this happens when there is a\nlarge number of possible examinations compared to a small\nnumber of patients.\nBuilding on ideas used for big labelled discrete data, we\npropose a Bayesian model for distribution estimation of\nsmall unlabelled data. Existing supervised models for dis-\ncrete labelled data embed the observations in a continuous\nspace. This is used to find the similarity between vectors of\ncategorical variables. We extend this idea to the small unla-\nbelled domain by modelling the continuous embedding as\na latent variable. A generative model is used to find a dis-\ntribution over the discrete observations by modelling them\nas dependent on the continuous latent variables.\nFollowing the medical diagnosis example, patient n would\nbe modelled by a continuous latent variable xn\u2208 X. For\neach examination d, the latent xninduces a vector of prob-\nabilitiesf = (fnd1,...,fndK), oneprobabilityforeachpos-\nsible test result k. A categorical distribution returns test re-\nsult yndbased on these probabilities, resulting in a patient\u2019s\nmedical assessment yn= (yn1,...,ynD). We need to de-\ncide how to model the distribution over the latent space X\nand vectors of probabilities f.\nWe would like to capture sparse multi-modal categorical\ndistributions. A possible approach would be to model the\narXiv:1503.02182v1  [stat.ML]  7 Mar 2015"},{"page":2,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\ncontinuous representation with a simple latent space and a\nnon-linear transformation of points in the space to obtain\nprobabilities. In this approach we place a standard normal\ndistribution prior on a latent space, and feed the output of a\nnon-linear transformation of the latent space into a Softmax\nto obtain probabilities. We use sparse Gaussian processes\n(GPs) to transform the latent space non-linearly. Sparse\nGPs form a distribution over functions supported on a small\nnumber of points with linear time complexity (Qui\u02dc nonero-\nCandela & Rasmussen, 2005; Titsias, 2009). We use a co-\nvariance function that is able to transform the latent space\nnon-linearly. We name this model the Categorical Latent\nGaussian Process (CLGP). Using a Gaussian process with\na linear covariance function recovers the linear Gaussian\nmodel (LGM, Marlin et al., 2011). This model linearly\ntransform a continuous latent space resulting in discrete ob-\nservations.\nThe Softmax likelihood is not conjugate to the our Gaus-\nsian prior, and integrating the latent variables with a Soft-\nmax distribution is intractable. A similar problem exists\nwith LGMs. Marlin et al. (2011) solved this by using vari-\national inference and various bounds for the likelihood in\nthe binary case, or alternative likelihoods to the Softmax in\nthe categorical case (Khan et al., 2012). Many bounds have\nbeen studied in the literature for the binary case: Jaakkola\nand Jordan\u2019s bound (Jaakkola & Jordan, 1997), the tilted\nbound (Knowles & Minka, 2011), piecewise linear and\nquadratic bounds (Marlin et al., 2011), and others. But for\ncategorical data fewer bounds exist, since the multivariate\nSoftmax is hard to approximate in high-dimensions. The\nBohning bound (B\u00a8 ohning, 1992) and Blei and Lafferty\u2019s\nbound (Blei & Lafferty, 2006) give poor approximation\n(Khan et al., 2012).\nInstead we use recent developments in sampling-based\nvariational inference (Blei et al., 2012) to avoid integrat-\ning the latent variables with the Softmax analytically. Our\napproach takes advantage of this tool to obtain simple yet\npowerful model and inference. We use Monte Carlo in-\ntegration to approximate the non-conjugate likelihood ob-\ntaining noisy gradients (Kingma & Welling, 2013; Rezende\net al., 2014; Titsias & L\u00b4 azaro-Gredilla, 2014). We then use\nlearning-ratefreestochasticoptimisation(Tieleman&Hin-\nton, 2012) to optimise the noisy objective. We leverage\nsymbolic differentiation (Theano, Bergstra et al., 2010) to\nobtain simple and modular code1.\nWeexperimentallyshowtheadvantagesofusingnon-linear\ntransformations for the latent space. We follow the ideas\nbrought in Paccanaro & Hinton (2001) and evaluate the\nmodels on relation embedding and relation learning. We\n1Python code for the model and inference is given in the ap-\npendix, and available at http:\/\/github.com\/yaringal\/\nCLGP\nthen demonstrate the utility of the model in the real-world\nsparse data domain. We use a medical diagnosis dataset\nwhere data is scarce, comparing our model to discrete fre-\nquency based models. We use the estimated distribution\nfor a task similar to the above, where we attempt to in-\nfer which test results can be deduced from the others. We\ncompare the models on the task of imputation of raw data\nstudying the effects of government terror warnings on po-\nlitical attitudes. We then evaluate the continuous models on\na binary Alphadigits dataset composed of binary images of\nhandwritten digits and letters, where each class contains a\nsmall number of images. We inspect the latent space em-\nbeddings and separation of the classes. Lastly, we evaluate\nthe robustness of our inference, inspecting the Monte Carlo\nestimate variance over time.\n2\nOur model (CLGP) relates to some key probabilistic mod-\nels (fig. 1). It can be seen as a non-linear version of the\nlatent Gaussian model (LGM, Khan et al. (2012)) as dis-\ncussed above. In the LGM we have a standard normal prior\nplaced on a latent space, which is transformed linearly and\nfed into a Softmax likelihood function. The probability\nvector output is then used to sample a single categorical\nvalue for each categorical variable (e.g. medical test re-\nsults) in a list of categorical variables (e.g. medical assess-\nment). These categorical variables correspond to elements\nin a multivariate categorical vector. The parameters of the\nlinear transformation are optimised directly within an EM\nframework. Khan et al. (2012) avoid the hard task of ap-\nproximating the Softmax likelihood by using an alternative\nfunction (product of sigmoids) which is approximated us-\ning numerical techniques. Our approach avoids this cum-\nbersome inference.\nRelated Work\nGaussian process \nregression\nGaussian process \nclassification\nLogistic regression\nGaussian process latent \nvariable model\nFactor analysis\nCategorical Latent \nGaussian Process\nLatent Gaussian models\nLinear regression\nLinear Non-linear\nContinuous\nDiscrete\nObserved input\nLatent input\nFigure 1. Relations between existing models and the model pro-\nposed in this paper (Categorical Latent Gaussian Process); the\nmodel can be seen as a non-linear version of the latent Gaussian\nmodel (left to right, Khan et al. (2012)), it can be seen as a latent\ncounterpart to the Gaussian process classification model (back\nto front, Williams & Rasmussen (2006)), or alternatively as a dis-\ncrete extension of the Gaussian process latent variable model (top\nto bottom, Lawrence (2005))."},{"page":3,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\nOurproposed modelcan alsobeseen asa latentcounterpart\nto the Gaussian process classification model (Williams &\nRasmussen, 2006), in which a Softmax function is again\nused to discretise the continuous values.\nous valued outputs are obtained from a Gaussian process,\nwhich non-linearly transforms the inputs to the classifica-\ntion problem. Compared to GP classification where the\ninputs are fully observed, in our case the inputs are la-\ntent. Lastly, our model can be seen as a discrete extension\nof the Gaussian process latent variable model (GPLVM,\nLawrence, 2005). This model has been proposed recently\nas means of performing non-linear dimensionality reduc-\ntion (counterpart to the linear principal component analysis\n(Tipping & Bishop, 1999)) and density estimation in con-\ntinuous space.\nThe continu-\n3 A Latent Gaussian Process Model for Mul-\ntivariate Categorical Data\nWe consider a generative model for a dataset Y with N\nobservations (patients for example) and D categorical vari-\nables (different possible examinations). The d-th categor-\nical variable in the n-th observation, ynd, is a categorical\nvariable that can take an integer value from 0 to Kd. For\nease of notation, we assume all the categorical variables\nhave the same cardinality, i.e. Kd\u2261 K, \u2200d = 1,...,D.\nIn our generative model, each categorical variable yndfol-\nlows a categorical distribution with probability given by a\nSoftmax with weights fnd = (0,fnd1,...,fndK). Each\nweight fndkis the output of a nonlinear function of a Q\ndimensional latent variable xn\u2208 RQ: Fdk(xn). To com-\nplete the generative model, we assign an isotropic Gaussian\ndistribution prior with standard deviation \u03c32\nvariables, and a Gaussian process prior for each of the non-\nlinear functions. We also consider a set of M auxiliary\nvariables which are often called inducing inputs. These\ninputs Z \u2208 RM\u00d7Qlie in the latent space with their cor-\nresponding outputs U \u2208 RM\u00d7D\u00d7Klying in the weight\nspace (together with fndk). The inducing points are used\nas \u201csupport\u201d for the function. Evaluating the covariance\nfunction of the GP on these instead of the entire dataset\nallows us to perform approximate inference in O(M2N)\ntime complexity instead of O(N3) (where M is the num-\nber of inducing points and N is the number of data points\n(Qui\u02dc nonero-Candela & Rasmussen, 2005)).\nxfor the latent\nThe model is expressed as:\nxnq\niid\n\u223c N(0,\u03c32\niid\n\u223c GP(Kd)\nfndk= Fdk(xn),\nynd\u223c Softmax(fnd),\nfor n \u2208 [N] (the set of naturals between 1 and N), q \u2208 [Q],\nd \u2208 [D], k \u2208 [K], m \u2208 [M], and where the Softmax\nx)\n(1)\nFdk\numdk= Fdk(zm)\ndistribution is defined as,\nSoftmax(y = k;f) = Categorical\n?\nexp(fk)\nexp(lse(f))\n?\n,\nlse(f) = log\n?\n1 +\nK\n?\nk?=1\nexp(fk?)\n?\n,\n(2)\nfor k = 0,...,K and with f0:= 0.\nFollowing our medical diagnosis example from the in-\ntroduction, each patient is modelled by latent xn; for\neach examination d, xn has a sequence of weights\n(fnd1,...,fndK), one weight for each possible test result k,\nthat follows a Gaussian process; Softmax returns test result\nyndbased on these weights, resulting in a patient\u2019s medical\nassessment yn= (yn1,...,ynD).\nDefine fdk\n(u1dk,...,uMdk).\nwith the latent nonlinear function, Fdk, marginalized un-\nder the GP assumption, is a multi-variate Gaussian dis-\ntribution N(0,Kd([X,Z],[X,Z])). It is easy to verify\nthat when we further marginalize the inducing outputs,\nwe end up with a joint distribution of the form fdk \u223c\nN(0,Kd(X,X)),\u2200d,k. Therefore, the introduction of in-\nducing outputs does not change the marginal likelihood of\nthe data Y. These are used in the variational inference\nmethod in the next section and the inducing inputs Z are\nconsidered as variational parameters.\n=(f1dk,...,fNdk) and define udk\nThe joint distribution of (fdk,udk)\n=\nWe use the automatic relevance determination (ARD) ra-\ndial basis function (RBF) covariance function for our\nmodel. ARD RBF is able to select the dimensionality of the\nlatent space automatically and transform it non-linearly.\n4\nThe marginal log-likelihood, also known as log-evidence,\nis intractable for our model due to the non-linearity of\nthe covariance function of the GP and the Softmax like-\nlihood function. We first describe a lower bound of the\nlog-evidence (ELBO) by applying Jensen\u2019s inequality with\na variational distribution of the latent variables following\nTitsias & Lawrence (2010).\nInference\nConsider a variational approximation to the posterior dis-\ntribution of X, F and U factorized as follows:\nq(X,F,U) = q(X)q(U)p(F|X,U).\nWe can obtain the ELBO by applying Jensen\u2019s inequality\n?\n\u2265\n\u00b7 logp(X)p(U)p(F|X,U)p(Y|F)\nq(X)q(U)p(F|X,U)\n= \u2212KL(q(X)?p(X)) \u2212 KL(q(U)?p(U))\n(3)\nlogp(Y) = logp(X)p(U)p(F|X,U)p(Y|F)X.F.U.\n?\nq(X)q(U)p(F|X,U)\nX.F.U."},{"page":4,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\n+\nN\n?\nn=1\nD\n?\nd=1\n?\nq(xn)q(Ud)p(fnd|xn,Ud)\n\u00b7 logp(ynd|fnd)x.nf.ndU. d\n:= L\n(4)\nwhere\np(fnd|xn,Ud) =\nK\n?\nk=1\nN(fndk|aT\nndudk,bnd)\n(5)\nwith\nand= K\u22121\nbnd= Kd,nn\u2212 Kd,nMK\u22121\nNotice however that the integration of logp(ynd|fnd) in eq.\n4 involves a nonlinear function (lse(f) from eq. 2) and is\nstill intractable. Consequently, we donot have an analytical\nform for the optimal variational distribution of q(U) unlike\nin Titsias & Lawrence (2010). Instead of applying a further\napproximation\/lower bound on L, we want to obtain bet-\nter accuracy by following a sampling-based approach (Blei\netal.,2012;Kingma&Welling,2013;Rezendeetal.,2014;\nTitsias & L\u00b4 azaro-Gredilla, 2014) to compute the lower\nbound L and its derivatives with the Monte Carlo method.\nSpecifically, we draw samples of xn, Ud and fnd from\nq(xn), q(Ud), and p(fnd|xn,Ud) respectively and esti-\nmate L with the sample average. Another advantage of us-\ning the Monte Carlo method is that we are not constrained\nto a limited choice of covariance functions for the GP that\nis otherwise required for an analytical solution in stan-\ndard approaches to GPLVM for continuous data (Titsias &\nLawrence, 2010; Hensman et al., 2013).\nd,MMKd,Mn,\nd,MMKd,Mn.\n(6)\nWe consider a mean field approximation for the latent\npoints q(X) as in (Titsias & Lawrence, 2010) and a joint\nGaussian distribution with the following factorisation for\nq(U):\nD\n?\nN\n?\nwhere the covariance matrix \u03a3dis shared for the same cat-\negorical variable d (remember that K is the number of val-\nues this categorical variable can take). The KL divergence\nin L can be computed analytically with the given varia-\ntional distributions. The parameters we need to optimise\nover2include the hyper-parameters for the GP \u03b8d, varia-\ntional parameters for the inducing points Z, \u00b5dk, \u03a3d, and\nthe mean and standard deviation of the latent points mni,\nsni.\nq(U) =\nd=1\nK\n?\nQ\n?\nk=1\nN(udk|\u00b5dk,\u03a3d),\nq(X) =\nn=1\ni=1\nN(xni|mni,s2\nni)\n(7)\n2Note that the number of parameters to optimise over can be\nreduced by transforming the latent space non-linearly to a second\nlow-dimensional latent space, which is then transformed linearly\nto the weight space containing points fndk.\n4.1Transforming the Random Variables\nInordertoobtainaMonteCarloestimatetothegradientsof\nL with low variance, a useful trick introduced in (Kingma\n& Welling, 2013) is to transform the random variables to\nbe sampled so that the randomness does not depend on the\nparameters with which the gradients will be computed. We\npresent the transformation of each variable to be sampled\nas follows:\nTransforming X.\ntransformation for X is straightforward as\nxni= mni+ sni\u03b5(x)\nFor the mean field approximation, the\nni,\u03b5(x)\nni\u223c N(0,1)\n(8)\nTransforming udk.\na joint Gaussian distribution. Denote the Cholesky decom-\nposition of \u03a3das LdLT\nudk= \u00b5dk+ Ld\u03b5(u)\nWe optimize the lower triangular matrix Ldinstead of \u03a3d.\nThe variational distribution of udkis\nd= \u03a3d. We can rewrite udkas\ndk,\u03b5(u)\ndk\u223c N(0,IM)\n(9)\nTransforming fnd.\np(fnd|xn,Ud) in Eq. 5 is factorized over k we can define a\nnew random variable for every fndk:\nfndk= aT\nbnd\u03b5(f)\nSince the conditional distribution\nndudk+\n?\nndk,\u03b5(f)\nndk\u223c N(0,1)\n(10)\nNotice that the transformation of the variables does not\nchange the distribution of the original variables and there-\nfore does not change the KL divergence in Eq. 5.\n4.2Lower Bound with Transformed Variables\nGiven the transformation we just defined, we can represent\nthe lower bound as\nN\n?\nD\n?\nN\n?\nlogSoftmaxynd\nL = \u2212\nn=1\nQ\n?\ni=1\nKL(q(xni)?p(xni))\n\u2212\nd=1\nK\n?\nD\n?\nk=1\nKL(q(udk)?p(udk))\n+\nn=1\nd=1\nE\u03b5(x)\nn ,\u03b5(u)\nd\n,\u03b5(f)\nnd\n???fnd\n?\n??\n\u03b5(f)\nnd,Ud(\u03b5(u)\nd),xn(\u03b5(x)\nn)\n???\n(11)\nwhere the expectation in the last line is with respect to the\nfixed distribution defined in Eqs. 8, 9 and 10. Each expec-\ntation term that involves the Softmax likelihood, denoted\nas Lnd\nLnd\n1\nT\ni=1\ns, can be estimated using Monte Carlo integration as\ns \u2248\nT\n?\nlogSoftmax\n?\nynd\n???fnd\n?\n\u03b5(f)\nnd,i,Ud(\u03b5(u)\nd,i),xn(\u03b5(x)\nn,i)\n??\n(12)\nwith \u03b5(x)\ntributions. Since these distributions do not depend on the\nn,i,\u03b5(u)\nd,i,\u03b5(f)\nnd,idrawn from their corresponding dis-"},{"page":5,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\nparameters to be optimized, the derivatives of the objective\nfunction L are now straight-forward to compute with the\nsame set of samples using the chain rule.\n4.3 Stochastic Gradient Descent\nWe use gradient descent to find an optimal variational dis-\ntribution. Gradient descent with noisy gradients is guaran-\nteed to converge to a local optimum given decreasing learn-\ning rate with some conditions, but is hard to work with in\npractice. Initial values set for the learning rate influence\nthe rate at which the algorithm converges, and misspeci-\nfied values can cause it to diverge. For this reason new\ntechniques have been proposed that handle noisy gradients\nwell. Optimisers such as AdaGrad (Duchi et al., 2011),\nAdaDelta(Zeiler,2012), andRMSPROP(Tieleman&Hin-\nton, 2012) have been proposed, each handling the gradi-\nents slightly differently, all averaging over past gradients.\nSchaul et al. (2013) have studied empirically the different\ntechniques, comparing them to one another on a variety\nof unit tests. They found that RMSPROP works better on\nmany test sets compared to other optimisers evaluated. We\nthus chose to use RMSPROP for our experiments.\nA major advantage of our inference is that it is extremely\neasy to implement and adapt. The straight-forward com-\nputation of derivatives through the expectation makes it\npossible to use symbolic differentiation. We use Theano\n(Bergstra et al., 2010) for the inference implementation,\nwhere the generative model is implemented as in Eqs. 8,\n9 and 10, and the optimisation objective, evaluated on sam-\nples from the generative model, is given by Eq. 11.\n5\nWe evaluate the categorical latent Gaussian process\n(CLGP), comparing it to existing models for multivariate\ncategorical distribution estimation. These include mod-\nels based on a discrete latent representation (such as the\nDirichlet-Multinomial), and continuous latent representa-\ntion with a linear transformation of the latent space (latent\nGaussian model (LGM, Khan et al., 2012)). We demon-\nstrate over-fitting problems with the LGM, and inspect the\nrobustness of our inference.\nExperimental Results\nFor the following experiments, both the linear and non-\nlinear models were initialised with a 2D latent space. The\nmeanvaluesofthelatentpoints, mn, wereinitialisedatran-\ndom following a standard normal distribution. The mean\nvalues of the inducing outputs (\u00b5dk) were initialised with\na normal distribution with standard deviation 10\u22122. This\nis equivalent to using a uniform initial distribution for all\nvalues. We initialise the standard deviation of each latent\npoint(sn)to0.1, andinitialisethelength-scalesoftheARD\nRBF covariance function to 0.1. We then optimise the vari-\national distribution for 500 iterations. At every iteration we\noptimise the various quantities while holding udk\u2019s varia-\ntional parameters fixed, and then optimise udk\u2019s variational\nparameters holding the other quantities fixed.\nOur setting supports semi-supervised learning with par-\ntially observed data. The latents of partially observed\npoints are optimised with the training set, and then used\nto predict the missing values. We assess the performance\nof the models using test-set perplexity (a measure of how\nmuch the model is surprised by the missing test values).\nThis is defined as the exponent of the negative average log\npredicted probability.\n5.1 Linear Models Have Difficulty with Multi-modal\nDistributions\nWe show the advantages of using a non-linear categorical\ndistribution estimation compared to a linear one, evaluating\nthe CLGP against the linear LGM. We implement the latent\nGaussian model using a linear covariance function in our\nmodel; we remove the KL divergence term in u following\nthe model specification in (Khan et al., 2012), and use our\ninference scheme described above. Empirically, the Monte\nCarlo inference scheme with the linear model results in the\nsame test error on (Inoguchi, 2008) as the piece-wise bound\nbased inference scheme developed in (Khan et al., 2012).\n5.1.1Relation Embedding \u2013 Exclusive Or\nA simple example of relational learning (following Pac-\ncanaro & Hinton (2001)) can be used to demonstrate\nwhen linear latent space models fail. In this task we are\ngiven a dataset with example relations and the model is\n(a) LGM\n(b) CLGP\nFigure 2. Density over the latent space for XOR as predicted\nby the linear model (top, LGM), and non-linear model (bot-\ntom, CLGP). Each figure shows the probability p(yd = 1|x)\nas a function of x, for d = 0,1,2 (first, second, and third dig-\nits in the triplet left to right). The darker the shade of green, the\nhigher the probability. In blue are the latents corresponding to the\ntraining points, in yellow are the latents corresponding to the four\npartially observed test points, and in red are the inducing points\nused to support the function."},{"page":6,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\n(a) LGM(b) CLGP (c) Ground truth\nFigure 3. Histogram of categorical values for XOR (encoded\nin binary for the 8 possible values) for samples drawn from the\nposterior of the latent space of the linear model (left, LGM), the\nnon-linear model (middle, CLGP), and the data used for training\n(right).\nto capture the distribution that generated them. A non-\nlinear dataset is constructed using the XOR (exclusive\nor) relation.We collect 25 positive examples of each\nassignment of the binary relation (triplets of the form\n(0,0,0), (0,1,1), (1,0,1), (1,1,0), corresponding to\n0 XOR 1 = 1 and so on). We then maximise the vari-\national lower bound using RMSPROP for both the lin-\near and non-linear models with 20 samples for the Monte\nCarlo integration. We add four more triplets to the dataset:\n(0,0,?), (0,1,?), (1,0,?), (1,1,?).\nprobabilities the models assign to each of the missing val-\nues (also known as imputation) and report the results.\nWe evaluate the\nWe assessed the test-set perplexity repeating the experi-\nment 3 times and averaging the results. The linear model\nobtained a test-set perplexity (with standard deviation) of\n75.785 \u00b1 13.221, whereas the non-linear model obtained\na test-set perplexity of 1.027 \u00b1 0.016. A perplexity of 1\ncorresponds to always predicting correctly.\nDuringoptimisationthelinearmodeljumpsbetweendiffer-\nent local modes, trying to capture all four possible triplets\n(fig. 2). The linear model assigns probabilities to the miss-\ning values by capturing some of the triplets well, but cannot\nassign high probability to the others. In contrast, the CLGP\nmodel is able to capture all possible values of the relation.\nSampling from probability vectors from the latent varia-\ntional posterior for both models, we get a histogram of the\nposterior distribution (fig. 3). As can be seen, the CLGP\nmodel is able to fully capture the distribution whereas the\nlinear model is incapable of it.\n5.1.2 Relation Learning \u2013 Complex Example\nWe repeat the previous experiment with a more com-\nplex relation.We generated 1000 strings from a\nsimple probabilistic context free grammar with non-\nterminals {\u03b1,\u03b2,A,B,C}, start symbol \u03b1, terminals\n{a,b,c,d,e,f,g}, and derivation rules:\n\u03b1 \u2192 A\u03b2 [1.0]\n\u03b2 \u2192 BA [0.5] | C [0.5]\nA \u2192 a [0.5] | b [0.3] | c [0.2]\nB \u2192 d [0.7] | e [0.3]\nC \u2192 f [0.7] | g [0.3]\nwhere we give the probability of following a derivation rule\ninside square brackets. Following the start symbol and the\nderivation rules we generate strings of the form ada, af,\nceb, and so on. We add two \u201cstart\u201d and \u201cend\u201d symbols\ns,s to each string obtaining strings of the form sscebss.\nWe then extract triplets of consecutive letters from these\nstrings resulting in training examples of the form ssc, sce,\n..., bss. This is common practice in text preprocessing in\nthe natural language community. It allows us to generate\nrelations from scratch by conditioning on the prefix ss to\ngenerate the first non-terminal (say a), then condition on\nthe prefix sa, and continue in this vein iteratively. Note\nthat a simple histogram based approach will do well on this\ndataset (and the previous dataset) as it is not sparse.\nSplit\n1\n2\n3\nLGM\nCLGP\n7.898 \u00b1 3.220\n26.477 \u00b1 23.457\n6.748 \u00b1 3.028\n2.769 \u00b1 0.070\n2.958 \u00b1 0.265\n2.797 \u00b1 0.081\nTable 1. Test-set perplexity on relational data. Compared are\nthe linear LGM and the non-linear CLGP.\nWe compared the test-set perplexity of the non-linear\nCLGP (with 50 inducing points) to that of the linear LGM\non these training inputs, repeating the same experiment set-\nup as before. We impute one missing value at random in\neach test string, using 20% of the strings as a test set with\n3 different splits. The results are presented in table 1. The\nlinear model cannot capture this data well, and seems to get\nvery confident about misclassified values (resulting in very\nhigh test-set perplexity).\n5.2Sparse Small Data\nWe assess the model in the real-world domain of small\nsparse data.We compare the CLGP model to a his-\ntogram based approach, demonstrating the difficulty with\nfrequency models for sparse data. We further compare our\nmodel to the linear LGM.\n5.2.1 Medical Diagnosis\nWe use the Wisconsin breast cancer dataset for which ob-\ntaining samples is a long and expensive process (Zwitter &\nSoklic, 1988). The dataset is composed of 683 data points,\nwith 9 categorical variables taking values between 1 and\n10, and an additional categorical variable taking 0,1 values\n\u2013 2 \u00d7 109possible configurations. We use three quarters\nof the dataset for training and leave the rest for testing, av-\neraging the test-set perplexity on three repetitions of the\nexperiment. We use three different random splits of the\ndataset as the distribution of the data can be fairly different\namong different splits. In the test set we randomly remove"},{"page":7,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\nSplit\n1\n2\n3\nBaseline\n8.68\n8.68\n8.85\nMultinomial\n4.41\n\u221e\n4.64\nUni-Dir-Mult\n4.41\n4.42\n4.64\nBi-Dir-Mult\n3.41\n3.49\n3.67\nLGM\nCLGP\n3.57 \u00b1 0.208\n3.47 \u00b1 0.252\n12.13 \u00b1 9.705\n2.86 \u00b1 0.119\n3.36 \u00b1 0.186\n3.34 \u00b1 0.096\nTable 2. Test-set perplexity on Breast cancer dataset, predicting randomly missing categorical test results. The models compared\nare: Baseline predicting uniform probability for all values, Multinomial \u2013 predicting the probability for a missing value based on its\nfrequency, Uni-Dir-Mult \u2013 Unigram Dirichlet Multinomial with concentration parameter \u03b1 = 0.01, Bi-Dir-Mult \u2013 Bigram Dirichlet\nMultinomial with concentration parameter \u03b1 = 1, LGM, and the proposed model (CLGP).\none of the 10 categorical values, and test the models\u2019 abil-\nity to recover that value. Note that this is a harder task than\nthe usual use of this dataset for binary classification. We\nuse the same model set-up as in the first experiment.\nWe compare our model (CLGP) to a baseline predicting\nuniform probability for all values (Baseline), a multino-\nmial model predicting the probability for a missing value\nbased on its frequency (Multinomial), a unigram Dirichlet\nMultinomial model with concentration parameter \u03b1 = 0.01\n(Uni-DirMult), a bigram Dirichlet Multinomial with con-\ncentration parameter \u03b1 = 1 (Bi-Dir-Mult), and the lin-\near continuous space model (LGM). More complicated fre-\nquency based approaches are possible, performing variable\nselection and then looking at frequencies of triplets of vari-\nables. These will be very difficult to work with in this\nsparse small data problem.\nWeevaluatethemodels\u2019performanceusingthetest-setper-\nplexity metric as before (table 2). As can be seen, the\nfrequency based approaches obtain worse results than the\ncontinuous latent space approaches. The frequency model\nwith no smoothing obtains perplexity \u221e for split 2 because\none of the test points has a value not observed before in\nthe training set. Using smoothing solves this. The baseline\n(predicting uniformly) obtains the highest perplexity on av-\nerage. The linear model exhibits high variance for the last\nsplit, and in general has higher perplexity standard devia-\ntion than the non-linear model.\n5.2.2 Terror Warning Effects on Political Attitude\nWe further compare the models on raw data collected in\nan experimental study of the effects of government terror\nwarnings on political attitudes (from the START Terrorism\nData Archive Dataverse3). This dataset consists of 1282\ncases and 49 variables. We used 1000 cases for training\nand 282 for test. From the 49 variables, we used the 17 cor-\nresponding to categorical answers to study questions (the\nother variables are subjects\u2019 geographic info and an answer\nto an open question). The 17 variables take 5 or 6 possible\nvalues for each variable with some of the values missing.\nWe repeat the same experiment set-up as before, with a 6\n3Obtained\nthedata.harvard.edu\/dvn\/dv\/start\/faces\/\nstudy\/StudyPage.xhtml?studyId=71190\nfromtheHarvardDataverse Network\nBaseline\n2.50\nMultinomial\n2.20\nLGM\n3.07\nCLGP\n2.11\nTable 3. Test-set perplexity for terror warning effects on polit-\nical attitude. Compared are the linear LGM and the non-linear\nCLGP.\ndimensional latent dimensions, 100 inducing points, 5 sam-\nples to estimate the lower bound, and running the optimiser\nfor 1000 iterations. We compare a Baseline using a uni-\nform distribution for all values, a Multinomial model using\nthe frequencies of the individual variables, the linear LGM,\nand the CLGP. The results are given in table 3. On the\ntraining set, the CLGP obtained a training error of 1.56,\nand the LGM obtained a training error of 1.34. The linear\nmodel seems to over-fit to the data.\n5.2.3Handwritten Binary Alphadigits\nWe evaluate the performance of our model on a sparse\ndataset with a large number of dimensions. The dataset,\nBinary Alphadigits, is composed of 20 \u00d7 16 binary images\nof 10 handwritten digits and 26 handwritten capital letters,\neach class with 39 images4. We resize each image to 10\u00d78,\nand obtain a dataset of 1404 data points each with 80 binary\nvariables. We repeat the same experiment set-up as before\nwith 2 latent dimensions for ease of visual comparison of\nthe latent embeddings. Fig. 4a shows an example of each\nalphadigit class. Each class is then randomly split to 30\ntraining and 9 test examples. In the test set, we randomly\nremove 20% of the pixels and evaluate the prediction error.\nFig. 4b shows the training and test error in log perplex-\nity for both models. LGM converges faster than CLGP\nbut ends up with a much higher prediction error due to its\nlimited modeling capacity with 2 dimensional latent vari-\nables. This is validated with the latent embedding shown\nin fig. 4c and 4d. Each color-marker combination denotes\none alphadigit class. As can be seen, CLGP has a better\nseparation of the different classes than LGM even though\nthe class labels are not used in the training.\n4Obtained from http:\/\/www.cs.nyu.edu\/\u02dcroweis\/\ndata.html"},{"page":8,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\n(a) Example alphadigits\n0200 4006008001000\n0.7\n0.8\n0.9\n1\n1.1\nIteration\nTraining \/ Test Error\n \n \nLGM, Train\nLGM, Test\nCLGP, Train\nCLGP, Test\n(b) Train & Test log perplexity\n\u22122\u22121012\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\nLatent Dim 1\nLatent Dim 2\n(c) LGM latent space\n\u22122 \u22121012\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\n1.5\n2\nLatent Dim 1\nLatent Dim 2\n(d) CLGP latent space\nFigure 4. Binary alphadigit dataset. (a) example of each class (b) Train and test log perplexity for 1000 iterations. (c,d) 2-D latend\nembedding of the 36 alphadigit classes with LGM (left) and CLGP model (right). Each color-marker combination denotes one class.\n5.3Latent Gaussian Model Over-fitting\nIt is interesting to note that the latent Gaussian model\n(LGM) over-fits on the different datasets. It is possible\nto contribute this to the lack of regularisation over the lin-\near transformation \u2013 the weight matrix used to transform\nthe latent space to the Softmax weights is optimised with-\nout a prior. In all medical diagnosis experiment repetitions,\nfor example, the model\u2019s training log perplexity decreases\nwhile the test log perplexity starts increasing (see fig. 5 for\nthe train and test log perplexities of split 1 of the breast\ncancer dataset). Note that even though the test log perplex-\nity starts increasing, at its lowest point it is still higher than\nthe end test log perplexity of the CLGP model. This is ob-\nserved for all splits and all repetitions.\nIt is worth noting that we compared the CLGP to the LGM\non the ASES survey dataset (Inoguchi, 2008) used to assess\nthe LGM in (Khan et al., 2012). We obtained a test perplex-\nity of 1.98, compared to LGM\u2019s test perplexity of 1.97. We\nconcluded that the dataset was fairly linearly separable.\n5.4 Inference Robustness\nLastly, we inspect the robustness of our inference, evaluat-\ning the Monte Carlo (MC) estimate standard deviation as\noptimisation progresses. Fig. 6 shows the ELBO and MC\nstandard deviation per iteration (on log scale) for the Al-\n(a) LGM(b) CLGP\nFigure 5. Train and test log perplexities for LGM (left) and the\nCLGP model (right) for one of the splits of the breast cancer\ndataset. The train log perplexity of LGM decreases while the test\nlog perplexity starts increasing at iteration 50.\nphadigit dataset. It seems that the estimator standard devia-\ntion decreases as the approximating variational distribution\nfits to the posterior. This makes the stochastic optimisa-\ntion easier, speeding up convergence. A further theoretical\nstudy of this behaviour in future research will be of interest.\n6\nWe have presented the first Bayesian model capable of cap-\nturing sparse multi-modal categorical distributions based\non a continuous representation. This model ties together\nmany existing models in the field, linking the linear and\ndiscrete latent Gaussian models to the non-linear continu-\nous space Gaussian process latent variable model and to\nthe fully observed discrete Gaussian process classification.\nDiscussion and Conclusions\nIn future work we aim to answer short-comings in the cur-\nrent model such as scalability and robustness. We scale the\nmodel following research on GP scalability done in (Hens-\nman et al., 2013; Gal et al., 2014). The robustness of the\nmodel depends on the sample variance in the Monte Carlo\nintegration. As discussed in Blei et al. (2012), variance\nreduction techniques can help in the estimation of the inte-\ngral, and methods such as the one developed in Wang et al.\n(2013) can effectively increase inference robustness.\n10\n0\n10\n1\n10\n2\n10\n3\n\u22122.2\n\u22122\n\u22121.8\n\u22121.6\n\u22121.4\n\u22121.2\n\u22121\n\u22120.8\nx 10\n5\nlog iter\nELBO\nFigure 6. ELBO and standard deviation per iteration (on log\nscale) for the Alphadigits dataset."},{"page":9,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\nReferences\nBengio,\nS\u00b4 ebastien, Morin, Fr\u00b4 ederic, and Gauvain, Jean-Luc.\nNeural probabilistic language models. In Innovations in\nMachine Learning, pp. 137\u2013186. Springer, 2006.\nYoshua,Schwenk,Holger,Sen\u00b4 ecal,Jean-\nBergstra, James, Breuleux, Olivier, Bastien, Fr\u00b4 ed\u00b4 eric,\nLamblin, Pascal, Pascanu, Razvan, Desjardins, Guil-\nlaume, Turian, Joseph, Warde-Farley, David, and Ben-\ngio, Yoshua. Theano: a CPU and GPU math expression\ncompiler. In Proceedings of the Python for Scientific\nComputing Conference (SciPy), June 2010. Oral Pre-\nsentation.\nBlei, David and Lafferty, John. Correlated topic models.\nAdvances in neural information processing systems, 18:\n147, 2006.\nBlei, David M, Jordan, Michael I, and Paisley, John W.\nVariational Bayesian inference with stochastic search.\nIn Proceedings of the 29th International Conference on\nMachine Learning (ICML-12), pp. 1367\u20131374, 2012.\nB\u00a8 ohning, Dankmar. Multinomial logistic regression algo-\nrithm. Annals of the Institute of Statistical Mathematics,\n44(1):197\u2013200, 1992.\nCollobert, Ronan and Weston, Jason. A unified architecture\nfor natural language processing: Deep neural networks\nwith multitask learning. In Proceedings of the 25th inter-\nnational conference on Machine learning, pp. 160\u2013167.\nACM, 2008.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive\nsubgradient methods for online learning and stochastic\noptimization.The Journal of Machine Learning Re-\nsearch, 12:2121\u20132159, 2011.\nGal, Yarin, van der Wilk, Mark, and Rasmussen, Carl E.\nDistributed variational inference in sparse Gaussian pro-\ncess regression and latent variable models. In Advances\nin Neural Information Processing Systems 27. 2014.\nHensman, James, Fusi, Nicolo, and Lawrence, Neil D.\nGaussian processes for big data.\narXiv:1309.6835, 2013.\narXiv preprint\nInoguchi, Takashi. Asia Europe survey (ASES): A multina-\ntional comparative study in 18 countries, 2001. In Inter-\nuniversity Consortium for Political and Social Research\n(ICPSR), 2008.\nJaakkola, Tommi S. and Jordan, Michael I. A variational\napproach to Bayesian logistic regression models and\ntheir extensions. In Proceedings of the Sixth Interna-\ntional Workshop on Artificial Intelligence and Statistics,\n1997.\nKhan, Mohammad E, Mohamed, Shakir, Marlin, Ben-\njamin M, and Murphy, Kevin P. A stick-breaking like-\nlihood for categorical data analysis with latent Gaussian\nmodels. In International conference on Artificial Intelli-\ngence and Statistics, pp. 610\u2013618, 2012.\nKingma, Diederik P and Welling, Max. Auto-encoding\nvariational Bayes. arXiv preprint arXiv:1312.6114,\n2013.\nKnowles, David A and Minka, Tom. Non-conjugate vari-\national message passing for multinomial and binary re-\ngression. In Advances in Neural Information Processing\nSystems, pp. 1701\u20131709, 2011.\nLawrence, Neil. Probabilistic non-linear principal compo-\nnent analysis with Gaussian process latent variable mod-\nels. TheJournalofMachineLearningResearch, 6:1783\u2013\n1816, 2005.\nMarlin, Benjamin M, Khan, Mohammad Emtiyaz, and\nMurphy, Kevin P.Piecewise bounds for estimating\nbernoulli-logistic latent Gaussian models. In ICML, pp.\n633\u2013640, 2011.\nPaccanaro, Alberto and Hinton, Geoffrey E. Learning dis-\ntributed representations of concepts using linear rela-\ntional embedding. Knowledge and Data Engineering,\nIEEE Transactions on, 13(2):232\u2013244, 2001.\nQui\u02dc nonero-Candela, Joaquin and Rasmussen, Carl Ed-\nward. A unifying view of sparse approximate Gaussian\nprocess regression. The Journal of Machine Learning\nResearch, 6:1939\u20131959, 2005.\nRezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan.\nStochastic backpropagation and approximate inference\nin deep generative models. In Proceedings of the 31st\nInternational Conference on Machine Learning (ICML-\n14), pp. 1278\u20131286, 2014.\nSchaul, Tom, Antonoglou, Ioannis, and Silver, David. Unit\ntests for stochastic optimization. abs\/1312.6055, 2013.\nURL http:\/\/arxiv.org\/abs\/1312.6055.\nTieleman, T. and Hinton, G.\nCOURSERA: Neural networks for machine learning,\n2012.\nLecture 6.5 - rmsprop,\nTipping, MichaelEandBishop, ChristopherM. Probabilis-\ntic principal component analysis. Journal of the Royal\nStatistical Society: Series B (Statistical Methodology),\n61(3):611\u2013622, 1999.\nTitsias, Michalis and Lawrence, Neil. Bayesian Gaussian\nprocess latent variable model.\ntional Conference on Artificial Intelligence and Statistics\n(AISTATS), 2010.\nIn Thirteenth Interna-"},{"page":10,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\nTitsias, Michalis and L\u00b4 azaro-Gredilla, Miguel.\nstochasticvariationalBayesfor non-conjugateinference.\nIn Proceedings of the 31st International Conference on\nMachine Learning (ICML-14), pp. 1971\u20131979, 2014.\nDoubly\nTitsias, Michalis K. Variational learning of inducing vari-\nables in sparse Gaussian processes. In Artificial Intelli-\ngence and Statistics 12, pp. 567\u2013574, 2009.\nWang, Chong, Chen, Xi, Smola, Alex, and Xing, Eric.\nVariance reduction for stochastic gradient optimization.\nIn Advances in Neural Information Processing Systems,\npp. 181\u2013189, 2013.\nWilliams, Christopher KI and Rasmussen, Carl Edward.\nGaussian processes for machine learning. the MIT Press,\n2(3):4, 2006.\nZeiler, Matthew D. Adadelta: An adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701, 2012.\nZwitter, M. and Soklic, M. Breast cancer dataset. In Uni-\nversity Medical Centre, Institute of Oncology, Ljubljana,\nYugoslavia, 1988."},{"page":11,"text":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\nA Appendix\nA.1Code\nThe basic model and inference (without covariance matrix\ncaching) can be implemented in 20 lines of Python and\nTheano for each categorical variable:\n1\n2\n3\n4\n5\n6\n7\n8\n9\nimport theano.tensor as T\nm = T.dmatrix(\u2019m\u2019) # ..and other variables\nX = m + s * randn(N, Q)\nU = mu + L.dot(randn(M, K))\nKmm = RBF(sf2, l, Z)\nKmn = RBF(sf2, l, Z, X)\nKnn = RBFnn(sf2, l, X)\nKmmInv = T.matrix_inverse(Kmm)\nA = KmmInv.dot(Kmn)\nB = Knn - T.sum(Kmn * KmmInv.dot(Kmn), 0)\nF = A.T.dot(U)+B[:,None]**0.5 * randn(N,K)\nS = T.nnet.softmax(F)\nKL_U, KL_X = get_KL_U(), get_KL_X()\nLS = T.sum(T.log(T.sum(Y * S, 1)))\n- KL_U - KL_X\nLS_func = theano.function([\u2019\u2019\u2019inputs\u2019\u2019\u2019],\nLS)\ndLS_dm = theano.function([\u2019\u2019\u2019inputs\u2019\u2019\u2019],\nT.grad(LS, m)) # and others\n# ... and optimise LS with RMS-PROP\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf","widgetId":"rgw28_56ab1ec708a09"},"id":"rgw28_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=273388187&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56ab1ec708a09"},"id":"rgw29_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=273388187&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":273388187,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":273388187,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithSlurp","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[[]],"isFulltext":false,"isSlurp":true,"isNoText":false,"publicationType":"Article","publicationDate":"Jun 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/277959103_Dropout_as_a_Bayesian_Approximation_Appendix","usePlainButton":true,"publicationUid":277959103,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/277959103_Dropout_as_a_Bayesian_Approximation_Appendix","title":"Dropout as a Bayesian Approximation: Appendix","displayTitleAsLink":true,"authors":[{"id":2069013556,"url":"researcher\/2069013556_Yarin_Gal","fullname":"Yarin Gal","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8159937,"url":"researcher\/8159937_Zoubin_Ghahramani","fullname":"Zoubin Ghahramani","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":"We show that a multilayer perceptron (MLP) with arbitrary depth and\nnonlinearities, with dropout applied after every weight layer, is\nmathematically equivalent to an approximation to a well known Bayesian model.\nThis interpretation offers an explanation to some of dropout's key properties,\nsuch as its robustness to over-fitting. Our interpretation allows us to reason\nabout uncertainty in deep learning, and allows the introduction of the Bayesian\nmachinery into existing deep learning frameworks in a principled way.\nThis document is an appendix for the main paper \"Dropout as a Bayesian\nApproximation: Representing Model Uncertainty in Deep Learning\" by Gal and\nGhahramani, 2015.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/277959103_Dropout_as_a_Bayesian_Approximation_Appendix","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":true,"sourceUrl":"deref\/http%3A%2F%2Fde.arxiv.org%2Fpdf%2F1506.02157","sourceName":"de.arxiv.org","hasSourceUrl":true},"publicationUid":277959103,"publicationUrl":"publication\/277959103_Dropout_as_a_Bayesian_Approximation_Appendix","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/277959103_Dropout_as_a_Bayesian_Approximation_Appendix\/links\/557f87e008aeb61eae261b76\/smallpreview.png","linkId":"557f87e008aeb61eae261b76","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=277959103&reference=557f87e008aeb61eae261b76&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1ec708a09"},"id":"rgw33_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=277959103&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":273388187,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/277959103_Dropout_as_a_Bayesian_Approximation_Appendix\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw32_56ab1ec708a09"},"id":"rgw32_56ab1ec708a09","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=277959103&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":273388187,"publicationLink":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","hasShowMore":false,"newOffset":3,"pageSize":10,"widgetId":"rgw31_56ab1ec708a09"},"id":"rgw31_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=273388187&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=1","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":1,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw30_56ab1ec708a09"},"id":"rgw30_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=273388187&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"565d728808aefe619b25b735","name":"Yutian Chen","date":"Dec 01, 2015 ","nameLink":"profile\/Yutian_Chen3","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"64eecd998cfd4e5dbbb93db6ef5def2f","showFileSizeNote":false,"fileSize":"1.18 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"565d728808aefe619b25b735","name":"Yutian Chen","date":"Dec 01, 2015 ","nameLink":"profile\/Yutian_Chen3","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"64eecd998cfd4e5dbbb93db6ef5def2f","showFileSizeNote":false,"fileSize":"1.18 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=xQIqdDQJIZDkxDO3d8Nqz-olPX1K83UQb8EI2mP-WtfoqUqD025WIwMbrGr0_AAKjm1PyUXVG03iuQ5lDa8R3Q.nvN6zfetsh0f17llx2oKD4VgWe_Wx_WHaccie01WAJ6hiqEOOcdtA0RN70ssU43gs6TQeC-YdgG572sDJhBZaw","clickOnPill":"publication.PublicationFigures.html?_sg=zCFToiNbXSxNbkymyJ20ICRbnurKl3W1I2D5Og87ZoHnUohls4VZrcBJYAoWfqbhTs-6AkqC_XTalaDNYq1Hsw.-ssb2Y3aY7kPT1u4owfjUJam5sO-xCJpgCGaF_LRhLCRCXs_Ka1sQS_X8a2U_97nPEIphZr00mPziGYf7SR2sQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1o9o3\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FYutian_Chen3%2Fpublication%2F273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data%2Flinks%2F565d728808aefe619b25b735.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1o9o3\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=xjizt3GxjPx8g9rrnha35Rlr9s-r_kmzWeF-HDOvWmVsMCfbgWCaZKmZqwYkDTIUjaDtT_K9hHNNtVcQ5scsBw","urlHash":"970dd46b4e83218fdaf806210ecdc6ad","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=U4dmbOhy8Rp6Q2ZcJI6ivc3XHZsvoxmPDW1Aj7bB44fFc9kXjkKhju6IM4kStOmBadsxuKYkj3ztg-nQ2cPDpxd7FAkRj2qSl5LN6LBrMig.rYEh7rroOwN2iHV5C_GdH_SRjvPoz_NIWuEpIZPUGoz_q01Yvhg5gSt6D-AWwQNbQbm5wUHC10p9fZo3VLQL9w.l8gDd3V6EAiRnBZ3IdEQxMVIfAL3FEQEmWm-awlp43wx9CadJaRDk6J27_tbmf10j5mBmbd8xOQ1lX12aDEfKg","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"565d728808aefe619b25b735","trackedDownloads":{"565d728808aefe619b25b735":{"v":false,"d":false}},"assetId":"AS:301794166099976@1448964744682","readerDocId":"4725141","assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":273388187,"commentCursorPromo":null,"widgetId":"rgw35_56ab1ec708a09"},"id":"rgw35_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FYutian_Chen3%2Fpublication%2F273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data%2Flinks%2F565d728808aefe619b25b735.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A301794166099976%401448964744682&publicationUid=273388187&linkId=565d728808aefe619b25b735&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=c2J1dGcKgVgZEVUPxAbkB-LKPpAOF34qQjqZlTB4qqXwL9pEyhENKyvcF3DY-4LPKf9xhjGLj3aBMuUfDvZ4uEwKLde7LyAzSmvXdYOQUhs._PZE9Q7u3qWkrvGnX3SpaUr5EvpFZYrae7WeOrqw_1gM9P0RbMyav15vCXzXwHIFiRMt60EiEqsZq0C4DqtGpQ.PT9208sTXlaqU9NSOcL8KrXOL4Qjv25EngE-OnJLC8PO0m3DrR5F2Y_4XllyP1OGpL7DThtD0NAbSirQvcW6hA","publicationUid":273388187,"trackedDownloads":{"565d728808aefe619b25b735":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw37_56ab1ec708a09"},"id":"rgw37_56ab1ec708a09","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw38_56ab1ec708a09"},"id":"rgw38_56ab1ec708a09","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw39_56ab1ec708a09"},"id":"rgw39_56ab1ec708a09","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw40_56ab1ec708a09"},"id":"rgw40_56ab1ec708a09","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw41_56ab1ec708a09"},"id":"rgw41_56ab1ec708a09","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw36_56ab1ec708a09"},"id":"rgw36_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw34_56ab1ec708a09"},"id":"rgw34_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1ec708a09"},"id":"rgw2_56ab1ec708a09","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":273388187},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=273388187&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1ec708a09"},"id":"rgw1_56ab1ec708a09","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"rEI2C6ptCDP3jCLu+YXYgmMGcZ5X7nxoNJMsMRyjuqFFSAYJ2lSpZK6fB75SVmoYXAfOjJOUuTGF+0FZ4Xcat6gR08nrM7NXWE4HgNz2dugedL8tXGe\/3h7SIOGNDZIbfKrwth4XVl8\/u0SE+UXkCxQ08TrbBfdFoZu18EkSuLfGVhTxdKmpjHClb+xrS\/kp55v8b1wc1NLRsyRCjdHnCg3PKCFO1Ij2uanZMCL60J0+OnOfEtliO781H4u5tuMg8wuHIQDDYB6f+D6t4+L3D53wFT6X\/xMESV7HBsgrBC8=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\" \/>\n<meta property=\"og:description\" content=\"Multivariate categorical data occur in many applications of machine learning.\nOne of the main difficulties with these vectors of categorical variables is\nsparsity. The number of possible...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\" \/>\n<meta property=\"rg:id\" content=\"PB:273388187\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Latent Gaussian Processes for Distribution Estimation of Multivariate Categorical Data\" \/>\n<meta name=\"citation_author\" content=\"Yarin Gal\" \/>\n<meta name=\"citation_author\" content=\"Yutian Chen\" \/>\n<meta name=\"citation_author\" content=\"Zoubin Ghahramani\" \/>\n<meta name=\"citation_publication_date\" content=\"2015\/03\/07\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Yutian_Chen3\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\/links\/565d728808aefe619b25b735.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-66a25bf3-5110-4b94-927f-373339038382","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":475,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw42_56ab1ec708a09"},"id":"rgw42_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-66a25bf3-5110-4b94-927f-373339038382", "2a3de2a72f216a4f77bd94f589716ecc4f377f35");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-66a25bf3-5110-4b94-927f-373339038382", "2a3de2a72f216a4f77bd94f589716ecc4f377f35");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw43_56ab1ec708a09"},"id":"rgw43_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/273388187_Latent_Gaussian_Processes_for_Distribution_Estimation_of_Multivariate_Categorical_Data","requestToken":"jjKDqEDVZQnKupvChqI2Y7M73dq4edNH2ntXXRMjyeUJkwL5m239xmi8BD+A2QYLcxFDA5U65\/sGkaGhiWC4JOoeujcXYeizX3OhYDgMlefH1TLzSXwTQIoDY+Wg\/xtqV7m2s2h7SMN0BzQtcA2EEx5wUR6uAyH913Z75XzeLm9ukR0Tf\/fqaPfv\/76SKRUKOtoNtLWXhSRCoTsFkGhFlQiP0Zt\/NjE+q4Qr4YAhevwOY4PuR+FRFJ+\/qjilByPL2fN+wsMajfSOLdfEmYfGHkwX5O2sng7uF2TS6kcQKDA=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=Q4Lq3s6uAfP1hh-tq6JWTk0vD_YIDANdBZptsdQHOPuR9jIUBpPzQvhiLtTmNDGv","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjczMzg4MTg3X0xhdGVudF9HYXVzc2lhbl9Qcm9jZXNzZXNfZm9yX0Rpc3RyaWJ1dGlvbl9Fc3RpbWF0aW9uX29mX011bHRpdmFyaWF0ZV9DYXRlZ29yaWNhbF9EYXRh","signupCallToAction":"Join for free","widgetId":"rgw45_56ab1ec708a09"},"id":"rgw45_56ab1ec708a09","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw44_56ab1ec708a09"},"id":"rgw44_56ab1ec708a09","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw46_56ab1ec708a09"},"id":"rgw46_56ab1ec708a09","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
