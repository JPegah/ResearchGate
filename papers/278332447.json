{"abstract":"Gaussian process (GP) models form a core part of probabilistic machine learning. Considerable research effort has been made into attacking three issues with GP models: how to compute efficiently when the number of data is large; how to approximate the posterior when the likelihood is not Gaussian and how to estimate covariance function parameter posteriors. This paper simultaneously addresses these, using a variational approximation to the posterior which is sparse in support of the function but otherwise free-form. The result is a Hybrid Monte-Carlo sampling scheme which allows for a non-Gaussian approximation over the function values and covariance parameters simultaneously, with efficient computations based on inducing-point sparse GPs. Code to replicate each experiment in this paper will be available shortly.","authors":["Alexander G. de G. Matthews","Zoubin Ghahramani"],"title":"MCMC for Variationally Sparse Gaussian Processes","pub_id":278332447,"citee":[11500673,259390620,257618460,267759656,268079368,41781406,2834582,227701452,260089482,41781429,6690142,220320094,23252767,266502061,41781800,45893043,259844876,220343914,275588150,220499817,267454466,45921723,234779817,235703008,220320048,3192933,262348245,221618434,239030086,2795225,247598156,271218362],"citedBy":[280773011],"URL":"https:\/\/www.researchgate.net\/publication\/278332447_MCMC_for_Variationally_Sparse_Gaussian_Processes"}