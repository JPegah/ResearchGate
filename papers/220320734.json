{"abstract":"ABSTRACT We describedistributedalgorithmsfortwowidely-usedtopicmodels,namelytheLatentDirich- let Allocation (LDA) model, and the Hierarchical Dirichet Process (HDP) model. In ourdistributed algorithmsthe data is partitioned across separate processors and inference is done in a parallel, dis- tributed fashion. We propose two distributed algorithms for LDA. The first algorithm is a straight- forward mapping of LDA to a distributed processor setting. In this algorithm processors concur- rently perform Gibbs sampling over local data followed by a global update of topic counts. The al- gorithmissimpletoimplementandcanbeviewedasanapproximationtoGibbs-sampledLDA.The second version is a model that uses a hierarchical Bayesian extension of LDA to directly account for distributed data. This model has a theoretical guarantee of convergence but is more complex to implement than the first algorithm. Our distributed algorithm for HDP takes the straightforward mapping approach, and merges newly-created topics either by matching or by topic-id. Using five real-world textcorporawe show that distributed learning works well in practice. For both LDA and HDP, we show that the convergedtest-data log probability for distributed learning is indistinguish- able from that obtained with single-processor learning. Our extensive experimental results include learning topic models for two multi-million document collections using a 1024-processor parallel computer.","authors":["Arthur U. Asuncion","Padhraic Smyth","Max Welling"],"title":"Distributed Algorithms for Topic Models","pub_id":220320734,"citee":[],"citedBy":[],"URL":"https:\/\/www.researchgate.net\/publication\/220320734_Distributed_Algorithms_for_Topic_Models"}