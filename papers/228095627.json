{"abstract":"We present a hybrid algorithm for Bayesian topic models that combines the efficiency of sparse Gibbs sampling with the scalability of online stochastic inference. We used our algorithm to analyze a corpus of 1.2 million books (33 billion words) with thousands of topics. Our approach reduces the bias of variational inference and generalizes to many Bayesian hidden-variable models.","authors":["Matt Hoffman","David Blei"],"title":"Sparse Stochastic Inference for Latent Dirichlet allocation","_id":228095627,"citee":[221619032],"citedBy":[279309917,268451692,236687792,232063267,268435721,268009492,220874072,230786935,256327429,259125814,257672541,262452147,270878916,270878346,261398124,262491329,267570378,278048621,280491523,281671331,284204722,291553679],"URL":"https:\/\/www.researchgate.net\/publication\/228095627_Sparse_Stochastic_Inference_for_Latent_Dirichlet_allocation"}