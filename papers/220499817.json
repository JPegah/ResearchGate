{"abstract":"Abstract It is well known in the statistics literature that augmenting binary and polychotomous response models with Gaussian latent variables enables exact Bayesian analysis via Gibbs sampling from the parameter posterior. By adopting such a data augmentation strategy, dispensing with priors over regression coecien ts in favour of Gaussian Process (GP) priors over functions, and employing variational approximations to the full posterior we obtain ecien t computational methods for Gaussian Process classication in the multi-class setting,. The model augmentation with additional latent variables ensures full a posteriori class coupling whilst retaining the simple a priori independent GP covariance structure from which sparse approximations, such as multi-class Informative Vector Machines (IVM), emerge in a very natural and straightforward manner. This is the rst time that a fully Variational Bayesian treatment for multi-class GP classication has been developed without having to resort to additional explicit approximations to the non-Gaussian likelihood term. Empirical comparisons with exact analysis via MCMC and Laplace approximations illustrate the utility of the variational approximation as a computationally economic alternative to full MCMC and it is shown to be more accurate than the Laplace approximation.","authors":[],"title":"Variational Bayesian Multinomial Probit Regression with Gaussian Process Priors","pub_id":220499817,"citee":[],"citedBy":[],"URL":"https:\/\/www.researchgate.net\/publication\/220499817_Variational_Bayesian_Multinomial_Probit_Regression_with_Gaussian_Process_Priors"}