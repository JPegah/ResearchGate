{"abstract":"Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural net-works. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically re-duce the number of network weights and lead to improved generalisation. Exper-imental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.","authors":["Alex Graves"],"title":"Practical Variational Inference for Neural Networks","_id":267706055,"citee":[2280218,3593275,265368057,3175691,222812561,221620610,220499717,3568712,221346365,3303419,220905856,23252767,3302208],"citedBy":[281768642,277959252,228095632,261309981,258818407,258818168,258201479,262991675,269932714,269932513,262030045,282005595,277959103,277959098],"URL":"https:\/\/www.researchgate.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks"}