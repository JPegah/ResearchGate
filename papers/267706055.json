{"abstract":"Variational methods have been previously explored as a tractable approximation to Bayesian inference for neural networks. However the approaches proposed so far have only been applicable to a few simple network architectures. This paper introduces an easy-to-implement stochastic variational method (or equivalently, minimum description length loss function) that can be applied to most neural net-works. Along the way it revisits several common regularisers from a variational perspective. It also provides a simple pruning heuristic that can both drastically re-duce the number of network weights and lead to improved generalisation. Exper-imental results are provided for a hierarchical multidimensional recurrent neural network applied to the TIMIT speech corpus.","authors":["Alex Graves"],"title":"Practical Variational Inference for Neural Networks","pub_id":267706055,"citee":[],"citedBy":[],"URL":"https:\/\/www.researchgate.net\/publication\/267706055_Practical_Variational_Inference_for_Neural_Networks"}