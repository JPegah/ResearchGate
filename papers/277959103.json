{"abstract":"We show that a multilayer perceptron (MLP) with arbitrary depth and nonlinearities, with dropout applied after every weight layer, is mathematically equivalent to an approximation to a well known Bayesian model. This interpretation offers an explanation to some of dropout's key properties, such as its robustness to over-fitting. Our interpretation allows us to reason about uncertainty in deep learning, and allows the introduction of the Bayesian machinery into existing deep learning frameworks in a principled way. This document is an appendix for the main paper \"Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\" by Gal and Ghahramani, 2015.","authors":["Yarin Gal","Zoubin Ghahramani"],"title":"Dropout as a Bayesian Approximation: Appendix","_id":277959103,"citee":[277022910,232805135,273388187,260089482,267706055,257069490,13853244,228092206,259400035,267960550,30771436,259367763,239666609,269996418,262991675,201976635,220320635,11216584,246546737,221619263],"citedBy":[],"URL":"https:\/\/www.researchgate.net\/publication\/277959103_Dropout_as_a_Bayesian_Approximation_Appendix?ev=auth_pub"}