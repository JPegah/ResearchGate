{"abstract":"ABSTRACT A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian \"evidence\" automatically embodies \"Occam's razor,\" penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.","authors":["David J. C. MacKay"],"title":"MacKay, D.J.C.: A Practical Bayesian Framework for Backprop Networks. Neural Computation 4(3), 448-472","pub_id":30771436,"citee":[],"citedBy":[],"URL":"https:\/\/www.researchgate.net\/publication\/30771436_MacKay_DJC_A_Practical_Bayesian_Framework_for_Backprop_Networks_Neural_Computation_43_448-472"}