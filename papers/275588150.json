{"abstract":"ABSTRACT The variational framework for learning inducing variables Titsias (2009) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximate and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. Many of the technical requirements for such a result were derived in the pioneering work of Seeger (2003,2003b). In this work we give a relatively gentle and largely self-contained explanation of the result. The result is important in understanding the variational inducing framework and could lead to principled novel generalizations.","authors":["Alexander G. de G. Matthews","Zoubin Ghahramani"],"title":"On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes","pub_id":275588150,"citee":[],"citedBy":[],"URL":"https:\/\/www.researchgate.net\/publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes"}