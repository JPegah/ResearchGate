{"abstract":"ABSTRACT The variational framework for learning inducing variables Titsias (2009) has had a large impact on the Gaussian process literature. The framework may be interpreted as minimizing a rigorously defined Kullback-Leibler divergence between the approximate and posterior processes. To our knowledge this connection has thus far gone unremarked in the literature. Many of the technical requirements for such a result were derived in the pioneering work of Seeger (2003,2003b). In this work we give a relatively gentle and largely self-contained explanation of the result. The result is important in understanding the variational inducing framework and could lead to principled novel generalizations.","authors":["Alexander G. de G. Matthews","Zoubin Ghahramani"],"title":"On Sparse variational methods and the Kullback-Leibler divergence between stochastic processes","_id":275588150,"citee":[266271350,232805135,256504640,265507008,2543633,220320048,235409940,265469282,257069490,228092206,220320635,262348245,268079368,40498273,11500673,2893638,269116839],"citedBy":[278332447],"URL":"https:\/\/www.researchgate.net\/publication\/275588150_On_Sparse_variational_methods_and_the_Kullback-Leibler_divergence_between_stochastic_processes"}