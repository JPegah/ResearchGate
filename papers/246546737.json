{"abstract":"Dropout and other feature noising schemes control overfitting by artificially corrupting the training data. For generalized linear models, dropout performs a form of adaptive regularization. Using this viewpoint, we show that the dropout regularizer is first-order equivalent to an L2 regularizer applied after scaling the features by an estimate of the inverse diagonal Fisher information matrix. We also establish a connection to AdaGrad, an online learner, and find that a close relative of AdaGrad operates by repeatedly solving linear dropout-regularized problems. By casting dropout as regularization, we develop a natural semi-supervised algorithm that uses unlabeled data to create a better adaptive regularizer. We apply this idea to document classification tasks, and show that it consistently boosts the performance of dropout training, improving on state-of-the-art results on the IMDB reviews dataset.","authors":["Percy Liang"],"title":"Dropout Training as Adaptive Regularization","_id":246546737,"citee":[221013030,51888056,40498065,2698556,226507886,228915801,46515750,230801973,2454184,221497515,222475504,228467601,221620124,220873867,228102719,221344761,260428601,245664723,262204398,237619703,3114053],"citedBy":[287249941,284219037,284220021,281227820,280911806,281054181,278048288,280936053,280082635,269998041,269935273,269722198,263930123,262604062,258247515,260231599,261922902,263471626,263544707,265554557,273394316,269935371,269935213,271592132,277959103,280105148,286446355,284219659,282183852],"URL":"https:\/\/www.researchgate.net\/publication\/246546737_Dropout_Training_as_Adaptive_Regularization"}