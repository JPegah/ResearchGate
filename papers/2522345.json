{"abstract":"TOR REGRESSION MODELS 1.1 SVD Regression Begin with the linear model y = X# + # where y is the n-vector of responses, X is the n p matrix of predictors, # is the p-vector regression parameter, and # , # I) is the n-vector error term. Of key interest are cases when p >> n, when X is \"long and skinny.\" The standard empirical factor (principal component) regression is best represented using the reduced singular-value decomposition (SVD) of X, namely X = FA where F is the nk factor matrix (columns are factors, rows are samples) and A is the k p SVD \"loadings\" matrix, subject to AA # = I and F # F = D where D is the diagonal matrix of k positive singular values, arranged in decreasing order. This reduced form assumes factors with zero singular values have been ignored without loss; k with equality only if all singular values are positive. Now the regression transforms via X# = F# where # = A# is the k-vector of regression parameters for the factor variables, representing","authors":["Mike West"],"title":"Bayesian Factor Regression Models in the \"Large p, Small n\" Paradigm","pub_id":2522345,"citee":[],"citedBy":[],"URL":"https:\/\/www.researchgate.net\/publication\/2522345_Bayesian_Factor_Regression_Models_in_the_Large_p_Small_n_Paradigm"}