<!DOCTYPE html> <html lang="en" class="" id="rgw38_56ab1e9180e1f"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="FOn3/McQ8bcCp45f7CLNCgE4uBJky3bnlIiAdIgNQRdsaOFBDth9x5UZMfLUzsLM7t/ZAcWpDCV1tmJ8EcpXvSLGqwkv78YnTH/aqu/clUiPP0n9n/XOGNgcqQz/3/38ciicYdjbH2gageZff58ziseP0CF2F0tH0pPIweXgUoGvJ/sq0dK/t/GjOXe4Kumf0R8xsU8D2cXR6IkA+0G4wE82OMpPJZ72nVZESe2DaId3piG8aqMeuAQW+npRVww1zksGN8mMCPv1Hp1ruurDWpZOQVJk6BYpvAoz5xKqAz0="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-10d010ea-1641-4526-ae2e-49eac12a0358",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/239030086_Differentiation_of_the_Cholesky_Algorithm" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Differentiation of the Cholesky Algorithm" />
<meta property="og:description" content="One way to estimate variance components is by restricted maximum likelihood. The log-likelihood function is fully defined by the Cholesky factor of a matrix that is usually large and sparse. In..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/239030086_Differentiation_of_the_Cholesky_Algorithm/links/02dd4f420cf23be0d317224f/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/239030086_Differentiation_of_the_Cholesky_Algorithm" />
<meta property="rg:id" content="PB:239030086" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1080/10618600.1995.10474671" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Differentiation of the Cholesky Algorithm" />
<meta name="citation_author" content="S. P. Smith" />
<meta name="citation_publication_date" content="1995/06/01" />
<meta name="citation_journal_title" content="Journal of Computational and Graphical Statistics" />
<meta name="citation_issn" content="1061-8600" />
<meta name="citation_volume" content="4" />
<meta name="citation_issue" content="2" />
<meta name="citation_firstpage" content="134" />
<meta name="citation_lastpage" content="147" />
<meta name="citation_doi" content="10.1080/10618600.1995.10474671" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/239030086_Differentiation_of_the_Cholesky_Algorithm" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/239030086_Differentiation_of_the_Cholesky_Algorithm" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Differentiation of the Cholesky Algorithm</title>
<meta name="description" content="Differentiation of the Cholesky Algorithm on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab1e9180e1f" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab1e9180e1f" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56ab1e9180e1f">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1080%2F10618600.1995.10474671&rft.atitle=Differentiation%20of%20the%20Cholesky%20Algorithm&rft.title=Journal%20of%20Computational%20and%20Graphical%20Statistics%20-%20J%20COMPUT%20GRAPH%20STAT&rft.jtitle=Journal%20of%20Computational%20and%20Graphical%20Statistics%20-%20J%20COMPUT%20GRAPH%20STAT&rft.volume=4&rft.issue=2&rft.date=1995&rft.pages=134-147&rft.issn=1061-8600&rft.au=S.%20P.%20Smith&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Differentiation of the Cholesky Algorithm</h1> <meta itemprop="headline" content="Differentiation of the Cholesky Algorithm">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/239030086_Differentiation_of_the_Cholesky_Algorithm/links/02dd4f420cf23be0d317224f/smallpreview.png">  <div id="rgw10_56ab1e9180e1f" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw11_56ab1e9180e1f"> <a href="researcher/2012005959_S_P_Smith" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="S. P. Smith" alt="S. P. Smith" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">S. P. Smith</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56ab1e9180e1f">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2012005959_S_P_Smith"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="S. P. Smith" alt="S. P. Smith" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2012005959_S_P_Smith" class="display-name">S. P. Smith</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/1061-8600_Journal_of_Computational_and_Graphical_Statistics"><span itemprop="name">Journal of Computational and Graphical Statistics</span></a> </span>    (Impact Factor: 1.22).     <meta itemprop="datePublished" content="1995-06">  06/1995;  4(2):134-147.    DOI:&nbsp;10.1080/10618600.1995.10474671           </div> <div id="rgw13_56ab1e9180e1f" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>One way to estimate variance components is by restricted maximum likelihood. The log-likelihood function is fully defined by the Cholesky factor of a matrix that is usually large and sparse. In this article forward and backward differentiation methods are developed for calculating the first and second derivatives of the Cholesky factor and its functions. These differentiation methods are general and can be applied to either a full or a sparse matrix. Moreover, these methods can be used to calculate the derivatives that are needed for restricted maximum likelihood, resulting in substantial savings in computation.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56ab1e9180e1f">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw25_56ab1e9180e1f"  itemprop="articleBody">  <p>Page 1</p> <p>Interface Foundation of America<br />Differentiation of the Cholesky Algorithm<br />Author(s): S. P. Smith<br />Source: Journal of Computational and Graphical Statistics, Vol. 4, No. 2 (Jun., 1995), pp. 134-<br />147<br />Published by: American Statistical Association, Institute of Mathematical Statistics, and<br />Interface Foundation of America<br />Stable URL: http://www.jstor.org/stable/1390762<br />Accessed: 22/05/2010 10:00<br />Your use of the JSTOR archive indicates your acceptance of JSTOR&#39;s Terms and Conditions of Use, available at<br />http://www.jstor.org/page/info/about/policies/terms.jsp. JSTOR&#39;s Terms and Conditions of Use provides, in part, that unless<br />you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you<br />may use content in the JSTOR archive only for your personal, non-commercial use.<br />Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at<br />http://www.jstor.org/action/showPublisher?publisherCode=astata.<br />Each copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed<br />page of such transmission.<br />JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of<br />content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms<br />of scholarship. For more information about JSTOR, please contact support@jstor.org.<br />American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of America are<br />collaborating with JSTOR to digitize, preserve and extend access to Journal of Computational and Graphical<br />Statistics.<br />http://www.jstor.org</p>  <p>Page 2</p> <p>Differentiation of the Cholesky Algorithm <br />S. P. SMITH* <br />One way to estimate variance components is by restricted maximum likelihood. <br />The log-likelihood function is fully defined by the Cholesky factor of a matrix that is <br />usually large and sparse. In this article forward and backward differentiation methods <br />are developed for calculating the first and second derivatives of the Cholesky factor and <br />its functions. These differentiation methods are general and can be applied to either a <br />full or a sparse matrix. Moreover, these methods can be used to calculate the derivatives <br />that are needed for restricted maximum likelihood, resulting in substantial savings in <br />computation. <br />Key Words: Backward differentiation; Determinant; Forward differentiation; Recursion; <br />Restricted maximum likelihood; Sparse matrix; Variance components. <br />1. INTRODUCTION <br />Variance estimation by restricted maximum likelihood or REML (Patterson and <br />Thompson 1971), is frequently computation intensive because of the need to calcu- <br />late derivatives of the log-likelihood. (See Searle, Casalla, and McCulloch [1992], pp. <br />252, 253, for an overview of some derivatives.) Calculating the likelihood itself is less <br />difficult when a decomposition algorithm for sparse matrices is feasible (Tier and Smith <br />1989). Some researchers (e.g., Smith and Graser 1986; Graser, Smith, and Tier 1987; <br />Meyer 1989, 1991) have designed derivative-free approaches to maximizing likelihoods. <br />It would seem to be a contradiction that derivatives of the log-likelihood are hard <br />to calculate although the log-likelihood itself is easy to compute. Most algorithms can <br />be differentiated, and efficient code for likelihood evaluation should lead to an efficient <br />approach for differentiation. For a review of the theory of automatic differentiation and <br />a survey of the available software packages see the conference proceedings in Griewank <br />and Corliss (1991). Smith and Lin (1989) used algorithmic differentiation to evaluate <br />derivatives of the log-determinant of a tridiagonal matrix in linear time. Although their <br />method is useful only for specific REML applications, it does demonstrate the potential <br />value of algorithmic differentiation in variance component estimation. <br />The log-likelihood for REML is completely determined by items computed during <br />the Cholesky decomposition, and this association is reviewed in Section 3. This method <br />*Statistician, EA Engineering, Science and Technology, 3468 Mt. Diablo Blvd., Suite B-100, Lafayette, CA <br />94549 <br />(1995 American Statistical Association, Institute of Mathematical Statistics, <br />and Interface Foundation of North America <br />Journal of Computational and Graphical Statistics, Volume 4, Number 2, Pages 134-147 <br />134</p>  <p>Page 3</p> <p>DIFFERENTIATION  OF THE CHOLESKY ALGORITHM <br />of likelihood evaluation has proven to be very efficient, suggesting that algorithmic <br />differentiation may also be efficient. This has led to investigations of the connection <br />between the Cholesky decomposition, REML, and algorithmic differentiation. <br />There are two main ways to differentiate algorithms, called forward and backward <br />differentiation (Iri 1991). It turns out that forward differentiation is a direct extension of <br />the Cholesky decomposition. The calculations for backward differentiation show less sim- <br />ilarity to the Cholesky decomposition and are said to be adjoint to the forward equations. <br />Both approaches are feasible, but backward differentiation is preferable when several or <br />numerous partial derivatives are wanted. More importantly, derivatives can be calculated <br />in sparse matrix mode, using amounts of computing time similar to that needed for likeli- <br />hood evaluation. This means that REML is not prohibitively difficult in situations where <br />it is easy to calculate the likelihood by factorization. <br />The purpose of this article is to show how to use recursion to evaluate first and second <br />derivatives of both the Cholesky decomposition and its functions. The Cholesky decom- <br />position is reviewed in Section 2.1. Section 2.2 is devoted to forward differentiation, and <br />Section 2.3 presents backward differentiation. Sparse matrix implementation is treated <br />in Section 2.4. The application of algorithmic differentiation to REML is presented in <br />Section 3. Conditions under which the proposed techniques provide improvements over <br />traditional methodologies are described in Section 4. <br />2. CHOLESKY DECOMPOSITION <br />2.1 BASIC RECURSIONS <br />The Cholesky decomposition takes a symmetric and nonnegative definite matrix M <br />and factorizes it into a lower triangular matrix L, where LL&#39; = M. (The requirement <br />of nonnegativity can be relaxed if complex arithmetic is allowed.) An elegant way to <br />compute the decomposition is to put M in a half-stored array and overwrite the array with <br />the lower triangular elements of L (Golub and Van Loan 1983, p. 88; Press, Teukolsky, <br />Vetterling, and Flannery 1992, p. 89). <br />Let Lij be the ijth element of the half-stored array (i &gt; j). The following recursions <br />and definitions describe the Cholesky algorithm. <br />1. Set Lij to the ijth element of M (half-stored). <br />2. For k = 1 to N, apply the following recursions (a through c) if \Lkkl is larger <br />than the operational zero: <br />(a) define pivot <br />Lkk= L 2 <br />(b) adjust lead column <br />Ljk:= <br />(c) row operations <br />Lij:= Lij - LkLjk, for j = k + 1,... N, and i = j,...N. <br />At the end of the process, Lij is returned as the ijth element of the lower triangular <br />matrix L. <br />This description is readily turned into computer code, where := indicates that the <br />Ljk/Lkk, for j = k + 1,... N. <br />135</p>  <p>Page 4</p> <p>S. P. SMITH <br />calculated quantity on the right recursively replaces the item on the left. <br />The previous algorithm is intentionally represented as an ordered list of simple <br />recursions. (For nonsparse matrices there are approximately N3/6 entries in this list.) <br />Various elements of the work array are accessed at different points on the list, but the list <br />of recursions is simple, and each item is one of only three possible calculations (square <br />root, division, or multiplication and subtraction). The list is ordered, so rearranging the <br />order of the recursions may corrupt the calculations. However, there are many acceptable <br />ways to rearrange the calculations. The outer product form is followed in this article, but <br />there are two other arrangements-the bordering method and the inner product form- <br />that are also popular (George and Liu 1981, pp. 17-20). <br />The algorithm is presented as a list of simple recursions because this removes the <br />complication that disguises the chain rule of calculus. Differentiation is direct and easy, <br />not just for the Cholesky decomposition, but for any algorithm that can be represented <br />as an ordered list of simple recursions. <br />2.2  FORWARD DIFFERENTIATION <br />To describe the forward differentiation, let Lij{)- <br />02Lij/&amp;xAy. (This notation allows for the case where x = y.) <br />Differentiation is directly applied to Steps 1 and 2 described in Section 2.1, and leads <br />to the recursions listed in the following. Only first and second derivatives are evaluated <br />here, but higher-order derivatives are possible by further differentiation at each step. <br />= OLij/Ox and Lij{x,y <br />1. Set Lij to the ijth element of M, Lij{x} =OMij/Ox, <br />(i j). <br />2. For k = 1 to N, apply recursions a through c if Lkk] is larger than the operational <br />zero: <br />(a) define pivot <br />Cholesky factor: Lkk: = Lkk <br />first derivatives: Lkk{x} := .5Lkk{x}/Lkk <br />second derivatives: Lkk{x,y} := [.5Lkk{x,y} - Lkk{x}Lkk{y}]/Lkk <br />(b) adjust lead column <br />Cholesky factor: Ljk := Ljk/Lkk <br />first derivatives: Ljk{x} := [Ljk{x} - LjkLkk{x}]/Lkk <br />second derivatives: Ljk{x,y} := [Ljk{x,y -LjkLkk{x,y)-Ljk{x)Lkk{y}- <br />Ljk{y}Lkk{x}]/Lkk for j = k + 1,.., <br />(c) row operations <br />Cholesky factor: Li := L <br />first derivatives: Lij{x} := Lij{x} - Lik{}Ljk <br />second derivatives: Lij{x,y} := ij,y <br />Lik{x}Ljk{y} <br />This returns Lij, the ijth element of the lower triangular matrix L. The derivatives <br />are given by Lij{x} and Lijx,y}. <br />The previous recursions form an ordered list, and rearranging the order may destroy <br />and Lij{x,y} =-02Mij/lxOy <br />N. <br />- LikLjk <br />- LkLj{x} <br />- Lik{x,y}Ljk <br />- LikLjk{x,y} <br />N, and i = j,..., <br />- <br />- Lk{y}Ljk{x} <br />for j = k + 1,..., <br />N. <br />136</p>  <p>Page 5</p> <p>DIFFERENTIATION <br />OF THE CHOLESKY ALGORITHM <br />the calculations. However, there are different arrangements that work and may offer some <br />advantage. For example, it is not necessary to evaluate the Cholesky decomposition and <br />all of the first and second derivatives simultaneously. Items can be parsed to permit <br />tighter control over memory requirements. Moreover, changing the order of calculation <br />can also lead to improved numerical stability by allowing the accumulation of inner <br />products in double precision (George and Liu 1981, p. 20). <br />The previous instructions can also be modified to allow selection of the kth pivot by <br />some rule. One such rule is to exchange rows and columns to bring the largest diagonal <br />element to the top of the list. This leads to a more numerically stable algorithm, but it <br />does not permit the exploitation of sparsity. The use of pivoting to reduce fill-in precludes <br />other uses. <br />Each first derivative requires no more than twice the work needed to evaluate L, <br />say 2&#39; (where i is the number of basic recursions listed in Section 2.1). Each second <br />derivative requires at most 4I additional operations. Because the strategy can be imple- <br />mented in sparse matrix mode, the efficiency is great when T is small. This development <br />has far-reaching implications, but there are overhead costs for sparse matrix storage. The <br />amount of work required to decompose banded matrices (including tridiagonal structures) <br />and evaluate the derivatives is linear in N. <br />One disadvantage of forward differentiation is the dependence of the amount of work <br />required on n, where n is the number of parameters that determine M; calculating all n <br />first derivatives uses 2nr operations, and calculating all second derivatives uses an extra <br />2n24 operations. A referee (personal communication 1994) suggested backward differ- <br />entiation (also called reverse-mode differentiation) as an attractive alternative to forward <br />differentiation. This very good advice reduces the dependence of the work requirement <br />on n. <br />2.3 <br />BACKWARD <br />DECOMPOSITION <br />DIFFERENTIATION  OF  FUNCTIONS OF <br />THE  CHOLESKY <br />In this section backward differentiation of f(L) is described, where f() is a scalar- <br />valued function of L. A typical example of f() is f(L) = <br />Note that differentiating f(L) is a task that already shows less similarity to forward <br />differentiations. Derivatives of all elements of L are evaluated jointly in the forward <br />calculations of Section 2.2, whereas here attentions is given only to f(L). <br />One approach for implementing backward differentiation is to apply a computer <br />program to enumerate the recursions of Section 2.1, and then differentiates the recursions <br />automatically. Unfortunately, the amount of storage required to represent the recursions <br />can be very much larger than the storage needed for L. Moreover, properties of the <br />Cholesky decomposition related to speed and sparse matrix storage can be exploited to <br />achieve a significant savings over the use of general-purpose packages. As with forward <br />differentiation, the objective here is to show enough of the detail to help programmers <br />in the design of software particular to the Cholesky decomposition. <br />log |M|, which is Ei log(Lii). <br />137</p>  <p>Page 6</p> <p>S. P. SMITH <br />2.3.1 First Derivatives <br />Reverse-mode differentiation is a process that starts at the bottom of the list of <br />recursions and works backwards. The last recursion is the function f = f(L), and this <br />is followed by the list of basic recursions (Section 2.1) in reverse order. Rather than <br />taking derivatives with respect to a parameter like x, backward differentiation proceeds <br />by evaluating derivatives with respect to the intermediate quantities. For example, the <br />recursion h = h(u, v) is differentiated as hx = h,ux + h,vx, and the partials hu and <br />h, are accumulated in the work array F, using the rules F[u] = F[u] + F[h]hu and <br />F[v] = F[v] + F[h]h,, where F[s] is the item in F representing intermediate s = v, w, <br />or h. When backward differentiation is complete, the derivative of f(L) with respect to <br />the ijth element of M is a member of F. These can be applied to OM/Ox to evaluate <br />af(L)/Ox. <br />The work space F need not be any bigger than L, because, like the Cholesky de- <br />composition, the algorithm can be organized to overwrite intermediate calculations. The <br />basic rules for differentiation still apply, but there are subtleties that accompany the al- <br />gorithm which result from the fact that any single position in F can represent different <br />intermediates. <br />Backward differentiation is described in the following, where Fij is the ijth element <br />of array F. <br />1. Initialize the algorithm by setting Fij = <br />element of the Cholesky decomposition, already calculated. <br />2. For k = N to 1, apply the following recursions (a through c) if ILkkl is larger <br />than the operational zero: <br />(a) row operations <br />Fik := F <br />- FijLjk, <br />Fjk :=Fjk - FijLik, for j = k + 1,...,N, <br />(b) lead column <br />Fjk := Fjk/Lkk, <br />Fkk = Fkk - LjkFjk, for j = k + 1,..., <br />(c) pivot <br />Fkk := .5Fkk/Lkk <br />3. Evaluate af(L)/Ox as EijFijaMij/lx, <br />With F evaluated, Step 3 can be applied for several parameters x. Therefore, if <br />Step 3 is negligible, backward differentiation requires only 24 operations to evaluate <br />all first derivatives. This compares favorably to the 2nQ operations needed for forward <br />differentiation. <br />It is convenient to describe Step 2 as a transformation that maps an array Q into <br />an array S. That is, initially set F = Q, and after the process, F is returned as S. The <br />transformation is explicitly defined when L is provided. Step 2 is briefly denoted as <br />S = F(QIL). This operator is also used in the evaluations of second derivatives. <br />f (L)/OLij (i <br />j). Let Lij be the ijth <br />and i = j,...,N. <br />N. <br />(i &gt; j). <br />138</p>  <p>Page 7</p> <p>DIFFERENTIATION <br />OF THE CHOLESKY ALGORITHM <br />2.3.2 Second Derivatives <br />Rules for backward differentiation were applied twice to derive the following al- <br />gorithm for second derivatives. It is also possible to apply forward differentiation to <br />the backward equations, or backward differentiation to the forward equations, to obtain <br />alternative algorithms. <br />1. Let the Cholesky decomposition L, and F = F({Of(L)/OLij}IL) be provided. <br />Define a half-stored array Q initialized to AM/0x, and let S be an array of equal <br />size. <br />2. For k = 1 to N, apply the following recursions (a through c) if ILkkl is larger <br />than the operational zero: <br />(a) pivot <br />Qkk := .Qkk/Lkk <br />Skk := -2QkkFkk <br />(b) lead column <br />Qjk = [Qjk - QkkLjk]/Lkk, <br />Sjk := -QkkFjk, <br />Skk := Skk - QjkFjk, for j = k + 1,..., <br />(c) row operations <br />Qij = Qij - QikLj - LikQjk, <br />= Sjk - <br />QikFij, <br />Sik = Sik - QjkFij, for j = k + 1,..., <br />3. For all i &gt; j, evaluate Sij := Sj + ZtrQtrO2f(L)/OLtrOLij, <br />4. Evaluate S := F(SIL) <br />5. Evaluate 02f(L)/9Oxy as EijSij9Mij /y + EijFij2Mij/xy, <br />N <br />Sjk <br />N, and i = j,...,N. <br />(t &gt; r). <br />(i &gt; j). <br />Step 5 can be repeated for different parameters y. If the work required for Steps 3 <br />and 5 is negligible, the extra work required to evaluate all second derivatives is no more <br />than 6n, operations, the previous algorithm being repeated n times. <br />Note that Q is turned into AL/Ox, and indeed is calculated using forward differenti- <br />ation. However, the algorithm can be organized to overwrite Q with S, but only if f() is <br />a simple function; for example, 02f(L)/LLtrOLij = 0 if r : j. After Step 2c the lead <br />column of Q can be applied to operations in Step 3, and then the lead column can be <br />overwritten with Skk, ... SNk. Hence, only one extra work array is needed to evaluate <br />all second derivatives. <br />2.4 SPARSE-MATRIX IMPLEMENTATION <br />It is beyond the scope of this aticle to describe in much detail the devices needed <br />to implement sparse matrix factorization. Techniques currently exist for sparse matrices <br />(e.g., George and Liu 1981), and modifying them for forward and backward differentia- <br />tion is a feasible adjustment. In particular, sparse-matrix tools for forward differentiation <br />can be developed with minor modifications, because the calculations are just parallel <br />operations that are embedded in the Cholesky decomposition. Moreover, sparse-matrix <br />tools are just as easily applied to backward differentiation, because the columns of L <br />139</p>  <p>Page 8</p> <p>S. P. SMITH <br />define the nature of all derivative operations. The reverse-mode calculations for first and <br />second derivatives are appended as follows: adjustments involving j for the kth lead <br />column occur only if Ljk is nonzero; adjustments for the kth set of row operations in- <br />volving i and j occur only if Lik and Ljk are nonzero. Consequently, the arrays F and <br />S of Sections 2.3.1 and 2.3.2 have the same pattern of sparsity as L. <br />Algorithms that exploit sparsity can be organized in various ways, and additional <br />research is needed to find the variants that are most efficient. However, a good start is <br />made by software accompanying this article provided on StatLib (see Section 3.3). <br />3. USING &#39;THE TECHNIQUES TO ESTIMATE <br />VARIANCES AND COVARIANCES BY REML <br />3.1  METHODOLOGY <br />Our focus is on general mixed models of the form <br />y = Xp + Zu + e, <br />where y, 3, u, and e are vectors of observations, fixed effects, random effects, and random <br />residuals; X and Z are incidence matrices; and <br />E{u} <br />= <br />0, <br />G, var{e} = <br />E{e} <br />= <br />0, <br />R, cov{u,e&#39;} = 0. <br />var{u} = <br />Array 0 is understood to be a null vector or matrix, depending on the context. <br />Matrices R and G are functions of parameters (say 0) that are to be estimated by <br />maximum likelihood. Under conditions of multivariate normality, the log-likelihood that <br />is to be maximized in REML is the following: <br />logL = - {loglRI + loglGI + loglCI + y&#39;Py}, <br />(3.1) <br />where ICI is the determinant of <br />X&#39;R-&#39;X  X&#39;R-&#39;Z <br />C= <br />Z&#39;R- X <br />Z&#39;R-Z <br />+ G-1 <br />and P = V-1 - V-1X(X&#39;V-&#39;X)-X&#39;V-1, <br />Matrix C is assumed to be nonsingular, and this is usually the case when X is chosen <br />to have full column rank. Terms |RI and IGI are usually easy to evaluate, as are their <br />derivatives. As noted by Tier and Smith (1989), ICI and y&#39;Py can be computed from <br />items created during linked-list absorption-that is, by Gaussian elimination. This is just <br />as easily accomplished with the Cholesky decomposition (Boldman and Van Vleck 1991). <br />The likelihood is a function of determinants. Even the quadratic y&#39;Py is a function <br />of determinants, MI/|CI, where the mixed model matrix is given as: <br />&quot; <br />X&#39;R- <br />1 <br />X&#39;R-1Z X&#39;R-Z <br />M= <br />Z&#39;R-&#39;X <br />Z&#39;R-Z + G-1 <br />where V = ZGZ&#39; + R (Harville 1977). <br />-y <br />Z&#39;R-1y <br />y&#39;R-ly <br />y&#39;R-&#39;X <br />y&#39;R-&#39;Z <br />140</p>  <p>Page 9</p> <p>DIFFERENTIATION OF THE CHOLESKY ALGORITHM <br />However, y&#39;Py is better computed as the square of the last diagonal element of the <br />Cholesky factor of M. Boldman and Van Vleck (1991) provided yet another calculation <br />for y&#39;Py. The quantity &#39;logICI is the sum of the logarithms of all the nonzero diagonal <br />elements of the Cholesky factor except the last one. <br />Matrix M is usually very large and very sparse. Sparseness is a direct consequence <br />of R-1 and G-1 being sparse, even if R and G are not. Furthermore, these inverse <br />matrices are frequently known directly from simple rules for inverting: diagonal or block <br />diagonal matrices; matrix sums (Henderson and Searle 1981); direct products (Searle et <br />al. 1992, p. 444); additive genetic variance-covariance matrices for animal populations <br />(e.g., Quaas 1988); and variance matrices for autoregressive time series (e.g., Fuller <br />1976, p. 235; Robinson 1991). General tricks for inverting variance matrices that occur <br />for spatial fields (e.g., Cressie 1991, p. 85) are less well known, but approximations <br />or simplifications for particular models do exist. For example, sparse inverses can be <br />derived from nearest-neighbor analysis (Jones and Vecchia 1993). Similarly, a sparse <br />inverse can be built directly in a way that accommodates spatial dependence among <br />adjacent elements in a lattice field (Cressie 1991, p. 406). If the model is such that <br />sparse R-1 and G-1 are unavailable, sparse-matrix techniques are not useful, and in <br />this case alternative representations of the log-likelihood (e.g., see equation (14) of Jones <br />and Vecchia 1993) may offer advantages in designing numerical strategies. <br />Newton-Raphson is a suitable method to maximize the likelihood, and this method <br />requires the evaluation of first and second derivatives (Lindstrom and Bates 1988; Searle <br />et al. 1992, p. 293). Derivatives of the log-likelihood are obtained by differentiating <br />IRl, IGI, and the diagonal elements of the Cholesky factor of M with respect to the <br />parameters 0. The strategies described in Sections 2.2 and 2.3 apply directly. To apply <br />the procedures, derivatives of M are needed, and these are easy to find when G and <br />R have simple structures. Some simple variance structures are given by Meyer (1989, <br />1991). <br />To set up the reverse-mode calculations for REML, note that the last function in the <br />list of recursions is f(L) = - {loglCI + y&#39;Py}. Therefore, the initializations for the <br />first derivative calculations (Step 1, Sec. 2.3.1) are Fii <br />FNN =-LNN. <br />For second derivatives, the interpretation of Step 3 in Section 2.3.2 is <br />SI ii <br />+ <br />Si <br />i <br />= <br />ii, <br />i <br />Blind application of Newton-Raphson is not advised-starting values affect con- <br />vergence, and it may be necessary to try several starting values (Harville and Callanan <br />1990). Moreover, REML is a constrained maximization problem, and algorithms should <br />avoid producing estimates outside the parameter space (Searle et al. 1992, p. 291). <br />-/Lii, <br />i = 1,... N - 1, and <br />1, ... N - 1, and SNN = SNN - QNN. <br />3.2 <br />ALTERNATIVE <br />METHODS <br />When data are balanced, REML estimates are easily obtained by equating ANOVA <br />sums of squares to their expectations (Searle 1989). The quantities involved in estimating <br />variance components are known directly from simple formulas, and there is no need for <br />general algorithms to calculate derivatives of determinants. Calvin and Dykstra (1991) <br />described a procedure for balanced multivariate models that guarantees that estimates will <br />141</p>  <p>Page 10</p> <p>S. P. SMITH <br />o <br />I <br />Q <br />O <br />&#39;0 <br />|C <br />||^ <br />6 <br />6 <br />i60 <br />20- <br />0-, <br />06/01 <br />07/01 <br />08/01 <br />Date <br />09/01 10/01 <br />Figure 1. Time Series of Percent Relative Humidity for the San Joaquin Valley During the Summer of 1993. <br />be in the parameter space, and Calvin (1993) extended this approach for some unbalanced <br />models, using an EM algorithm. <br />In the unbalanced case, numerical simplifications exist for applying REML in groups <br />of models typical to animal breeding (Dempster, Selwyn, Patel, and Roth 1984; Graser <br />et al. 1987; Smith and Graser 1986; Taylor, Bean, Marshall, and Sullivan 1985) and in <br />repeated-measures models (Jennrich and Schluchter 1986; Laird, Lange, and Stram 1987; <br />Lindstrom and Bates 1988). These methods are very good when they can be used. <br />But in general, REML can be very difficult to implement. Some have resorted to <br />sparse-matrix inversion on supercomputers (Misztal 1990). Although matrix inversion is <br />still a very difficult proposition, Misztal and Perez-Enciso (1993) rediscovered an efficient <br />strategy (Takahashi, Fagan, and Chen 1973) for calculating some of the inverse elements <br />and all of the elements necessary for performing an EM algorithm. <br />Experience is being gained with the new strategies, and Meyer (1994) found that <br />Newton-Raphson with forward differentiation works faster than derivative-free REML <br />for multivariate problems involving several variance-covariance parameters. However, <br />the derivative-free approach uses less memory and is more robust in the face of poor <br />starting values than Newton-Raphson. Comparisons with backward differentiation were <br />not made, but work is continuing. <br />3.3 <br />REML WITH UNEQUALLY <br />SPACED TIME-SERIES DATA <br />In this section, time-series data are used to illustrate the Newton-Raphson algorithm <br />for REML. The analysis can be reproduced from FORTRAN software and data provided <br />in the StatLib newsgroup (statlib@lib.stat.cmu.edu). The subroutines are very general, <br />and can be borrowed by diverse applications where sparse algorithms are needed. <br />142</p>  <p>Page 11</p> <p>DIFFERENTIATION OF THE CHOLESKY ALGORITHM <br />The data (Fig. 1) are composed of 115 measurements of mean-daily relative humidity. <br />These readings were taken during the summer of 1993 near the Tuolumne River in the <br />San Joaquin Valley, California. Humidity measurements were made almost daily, but the <br />series contains seven missing values because of errors in record-keeping. Although it <br />is entirely reasonable to treat the seven values as missing, the present approach uses a <br />continuous time-series model that describes the series at irregular time steps. The theory <br />can be very complicated. For a discussion involving a state-space model and the Kalman <br />filter see Jones and Ackerson (1990). In what follows, however, only a simple time-series <br />model is considered: <br />Yt <br />= <br />t + et <br />Zt = <br />u + ph [zt-h - U] +t, <br />(3.2) <br />where Yt depicts the relative humidity at day or time t. This series is represented as a <br />composite time series involving a first-order process (zt) and white noise (et). The resid- <br />uals, et and Et, are assumed to be uncorrelated and normally distributed with variances <br />2 and (1 - p2h)2. <br />The purpose of this illustration is to estimate the autocorrelation <br />p, and the two variances from the observed time series yt. The REML estimates are <br />invariant to the location parameter u. <br />The mixed model matrix for (3.2) is <br />I+ aT- <br />1 <br />y <br />M=(-2 <br />1&#39; <br />s <br />l&#39;y <br />y&#39; <br />y&#39;l  y&#39;y <br />where I is the identity matrix, a = a2/o2, aU2T is the variance matrix for zt (in ascending <br />order of t), 1 is a vector of l&#39;s, y is a vector of all Yt, and s = 115 is the sample size. To <br />accommodate missing values, it is feasible to leave gaps in I, 1, and y, but this approach <br />was not followed. <br />To simplify the calculations, write <br />M = <br />-2Mc, <br />y&#39;Py - <br />e ,2y&#39;Py, <br />and <br />C = a2Cc. <br />This allows a direct estimate of cr2 as y&#39;Pcy/(s- <br />to retrieve a well-known function, the concentrated likelihood <br />1) that can be plugged back into (3.1) <br />logL = -2 {logla-lT <br />+ log|lC| + (s - <br />)log(y&#39;P,y)}, <br />(3.3) <br />where ?logjCc| is the sum of the logarithms of the first N - 1 diagonal elements of <br />L(N = s + 2, L&#39;L = Me), and 1log(y&#39;Pcy) is the logarithm of the last diagonal of L. <br />The advantage of (3.3) is that it only depends on two of the parameters, a and p, and it <br />serves as a starting point for Newton-Raphson. <br />If rows and columns of Mc are ordered as presented, L can be computed in time <br />linear in s, because T-1 has a very simple tridiagonal structure for both equally (Thomas <br />and Wallis 1971) and unequally (Wade and Quaas 1993) spaced time steps. Moreover, <br />computing and differentiating loglTI is trivial. <br />143</p>  <p>Page 12</p> <p>S. P. SMITH <br />Table 1. Newton-Raphson Iteration <br />Iteration <br />a <br />p <br />2 <br />a2 <br />Log-likelihood (3.3) <br />Start <br />1 <br />2 <br />3 <br />4 <br />5 <br />.3333 <br />.2386 <br />.2101 <br />.2028 <br />.2022 <br />.2022 <br />.7500 <br />.8624 <br />.8405 <br />.8333 <br />.8327 <br />.8327 <br />-494.9849 <br />-493.9282 <br />-493.8421 <br />-493.8371 <br />-493.8370 <br />-493.8370 <br />78.44 <br />92.51 <br />84.38 <br />81.44 <br />81.18 <br />18.71 <br />19.44 <br />17.11 <br />16.46 <br />16.41 <br />The provided software employs the backwards calculations of Sections 2.3.1 and <br />2.3.2 to find the needed derivatives at each step in Newton-Raphson. There are, however, <br />minor modifications to the protocol. When backwards differentiation is applied to <br />f(L) =-  {log|Ccj + (s - 1)log(y&#39;Pcy)}, <br />2 <br />the initializations for first derivatives (Step 1 in Section 2.3.1) are Fii = -1/Li, <br />1,... N-1, and FNN = -(S-1 )/LNN. For second derivatives, (Step 3 in Section 2.3.2) <br />they are Sii = Sii <br />Qii/L <br />i = 1,... N- <br />Inclusion of white noise does complicate the analysis. For example, when p is zero, <br />the variances a2 and a2 cannot be separated by the likelihood. Moreover, the algorithm <br />may not converge, although this depends on starting values as well as the data. Aside <br />from these problems, the Newton-Raphson algorithm is a real time calculation. With <br />suitable starting values, the software produced estimates that converged. These results <br />are listed in Table 1. <br />i = <br />1, and SNN = SNN + (S - 1)QNN/L2 N <br />4. DISCUSSION <br />This article describes inexpensive ways to calculate derivatives that are used for <br />REML. These can be calculated whenever it is easy to calculate the log-likelihood by <br />factorization. This increases the number of problems where it is realistic to perform <br />REML. <br />Although the strategies of Sections 2.2 and 2.3 show tremendous utility in variance <br />estimation, other applications are also possible. The methods could aid in the search for <br />D-optimal experimental designs by providing a way to calculate derivatives of IX&#39;XI, <br />where X is the design matrix (Bates 1983). However, other known algorithms deserve <br />careful consideration. For the case in point, Bates developed a different technique to <br />compute first derivatives of IX&#39;XI, using the QR algorithm to decompose X directly. <br />The need to evaluate derivatives of the log-determinant is a recurring theme. There- <br />fore, it is useful to consider how log-determinants are usually differentiated, to advance <br />speculation regarding the merits of algorithmic differentiation. The formulas given by <br />Searle et al. (1992, pp. 456, 457) are listed in the following for a nonsingular matrix M, <br />and these suggest standard approaches. <br />OloglMI/Ox = tr[M-&#39;Mx] <br />144 <br />(4.1)</p>  <p>Page 13</p> <p>DIFFERENTIATION <br />OF THE CHOLESKY ALGORITHM <br />and <br />02loglMl/0xOy <br />= tr[M-&#39;Mxy <br />- M-lmxM-&#39;My] <br />(4.2) <br />where Mx = AM/Ox, My = OM/9y, and Mxy = 02M/&amp;xay. <br />The sparse inverse in Takahashi et al. (1973) can be used to evaluate (4.1), and it turns <br />out that this approach is as efficient as backward differentiation. Although Takahashi&#39;s <br />algorithm was not derived from properties of reverse-mode differentiation, there are <br />striking similarities. <br />More common approaches for differentiating loglMI can be derived through the <br />algebraic manipulation of (4.1) and (4.2). Simplifications occur because Mx and My <br />can usually be represented as Mx = HH&#39; and My = TT&#39; for some matrices H and <br />T. Furthermore, when M is linear in x and y, Mxy is null and H and T are constant. <br />The commutative law applies for the trace operator, so (4.1) and (4.2) are reduced to <br />tr[H&#39;M-1H] and tr[T&#39;M-IHH&#39;M-&#39;T]. <br />A standard way to calculate (4.1) via tr[H&#39;M-1H] is the following: <br />1. Evaluate L, where LL&#39; = M, using the Cholesky decomposition. <br />2. Solve gi in Lgi = hi, where hi is the ith column of H. <br />3. Accumulate the inner products, gigi for all i, to calculate the trace. <br />The standard approach uses little memory other than that needed for L. Moreover, <br />the matrix L can be evaluated using sparse-matrix techniques, and thus the traditional <br />scheme appears to be very competitive. However, for sparse matrices, both forward and <br />backward differentiation can show significant advantages. For example, differentiating <br />the log-determinant of a tridiagonal matrix requires only linear time, but the traditional <br />method may require quadratic time. Alternatively, the traditional approach is preferable <br />when H has few columns, even for sparse M. <br />Calculating (4.2) or tr[T&#39;M-&#39;HH&#39;M-&#39;T] <br />because matrices like H&#39;M-1T must be evaluated in full, if we interpret the algebra as <br />written. The W transformation (Goodnight and Hemmerle 1979) can be applied to help <br />evaluate these matrices, as in the case of variance component estimation. Programs can <br />take full advantage of the linked-list tool for sparse matrices (Tier and Smith 1989) in <br />performing the required forward Doolittle operations (Goodnight 1979). Nevertheless, for <br />sparse matrices the traditional approach performs badly for second derivatives, because <br />matrices like L-1T or L-1H tend to lose their sparsity, and this implies that forming <br />H&#39;M-&#39;T = (L-&#39;H)&#39;L-lT <br />will involve cubic work (assuming that T and H are almost <br />full row-rank). Note that L-1 is not sparse for tridiagonal matrix M, and for this case <br />the traditional approach requires cubic time to evaluate (4.2), whereas both forward and <br />backward differentiation require only linear time. <br />The conclusion is that algorithmic differentiation is useful when M is big but sparse, <br />and easy to factorize. Traditional methods are best when Mx, My, and Mxy are low-rank <br />matrices with trivial factorizations. <br />Forward differentiation is less attractive than backward differentiation when deriva- <br />tives are needed for a single function, such as log|MI in the previous discussion, and <br />when M is a function of several parameters. The forward calculations are most useful <br />by traditional means is more difficult, <br />145</p>  <p>Page 14</p> <p>S. P. SMITH <br />when derivatives for numerous functions of L, say the vector f(L), are needed. An exam- <br />ple involves a function of the solution vector b, where Mb = r and there are numerous <br />vectors to use as right-hand sides-that is, r. A combination approach provides one way <br />to compute Of(b)/Ox: evaluate OL/Ox by the forward calculations, and for each r find <br />Of(b)/OLij(i &gt; j) by the backward calculations. A combination algorithm has already <br />been described in Section 2.3.2, where Q was found by the forward approach. <br />ACKNOWLEDGMENTS <br />The author thanks colleagues at EA Engineering, Science, and Technology and at the Animal Genetics <br />and Breeding Unit (Australia) for providing support. The referees were very helpful. <br />[Received October 1993. Revised January 1995.] <br />REFERENCES <br />Bates, D. M. (1983), &quot;The Derivative of IX&#39;XI and Its Uses,&quot; Technometrics, 25, 373-376. <br />Boldman, K. G., and Van Vleck, L. D. (1991), &quot;Derivative-Free Restricted Maximum Likelihood Estimation <br />in Animal Models with a Sparse Matrix Solver,&quot; Journal of Dairy Science, 74, 4337-4343. <br />Calvin, J. A. (1993), &quot;REML Estimation in Unbalanced Multivariate Variance Components Models using an <br />EM Algorithm,&quot; Biometrics, 49, 691-701. <br />Calvin, J. A., and Dykstra, R. L. (1991), &quot;Maximum Likelihood Estimation of a Set of Covariance Matri- <br />ces Under Lower Order Restrictions with Applications to Balanced Multivariate Variance Components <br />Models,&quot; The Annals of Statistics, 19, 850-869. <br />Cressie, N. (1991), Statistics for Spatial Data, New York: John Wiley. <br />Dempster, A. P., Selwyn, M. R., Patel, C. M., and Roth, A. J. (1984), &quot;Statistical and Computational Aspects <br />of Mixed Model Analysis,&quot; Applied Statistics, 33, 203-214. <br />Fuller, W. A. (1976), Introduction to Statistical Time Series, New York: John Wiley. <br />George, A., and Liu, J. W-H. (1981), Computer Solution of Large Sparse Positive Definite Systems, Englewood <br />Cliffs, NJ: Prentice-Hall. <br />Golub, G. H., and Van Loan, C. F. (1983), Matrix Computation, Baltimore, MD: The John Hopkins University <br />Press. <br />Goodnight, J. H. (1979), &quot;A Tutorial on the Sweep Operator,&quot; The American Statistician, 33, 149-158. <br />Goodnight, J. H., and Hemmerle, W. J. (1979), &quot;A Simplified Algorithm for the W-Transformation in Variance <br />Component Estimation,&quot; Technometrics, 21, 265-268. <br />Graser, H.-U., Smith, S. P., and Tier, B. (1987), &quot;A Derivative Free Approach for Estimating Variance Com- <br />ponents in Animal Models by REML,&quot; Journal of Animal Science, 64, 1362-1370. <br />Griewank, A., and Corliss, G. F., (eds.) (1991), Automatic Differentiation of Algorithms: Theory, Implementa- <br />tion, and Application, Philadelphia: SIAM. <br />Harville, D. A. (1977), &quot;Maximum Likelihood Approaches to Variance Component Estimation and to Related <br />Problems,&quot; Journal of the American Statistical Association, 72, 320-340. <br />Harville, D. A., and Callanan, T. P. (1990), &quot;Computational Aspects of Likelihood-Based Inference for Vari- <br />ance Components,&quot; in Statistical Methods for Genetic Improvement of Livestock, eds. D. Gianola and K. <br />Hammond, New York: Springer-Verlag, pp. 136-176. <br />Henderson, H. V., and Searle, S. R. (1981), &quot;On Deriving the Inverse of a Sum of Matrices,&quot; SIAM Review, <br />23, 53-60. <br />Iri, M. (1991), &quot;History of Automatic Differentiation and Rounding Error Estimation,&quot; in Automatic Differ- <br />entiation of Algorithms: Theory, Implementation, and Application, eds. A. Griewank and G. F. Corliss, <br />Philadelphia: SIAM, pp. 3-24. <br />146</p>  <p>Page 15</p> <p>DIFFERENTIATION  OF THE CHOLESKY ALGORITHM <br />Jennrich, R. I., and Schluchter, M. D. (1986), &quot;Unbalanced Repeated-Measures Models with Structured Co- <br />variance Matrices,&quot; Biometrics, 42, 805-820. <br />Jones, R. H., and Ackerson, L. M. (1990), &quot;Serial Correlation in Unequally Spaced Longitudinal Data,&quot; <br />Biometrika, 77, 721-731. <br />Jones, R. H., and Vecchia, A. V. (1993), &quot;Fitting Continuous ARMA Models to Unequally Spaced Spatial <br />Data,&quot; Journal of the American Statistical Association, 88, 947-954. <br />Laird, N. M., Lange, N., and Stram, D. (1987), &quot;Maximum Likelihood Computations with Repeated Measures: <br />Application of the EM Algorithm,&quot; Journal of the American Statistical Association, 82, 97-105. <br />Lindstrom, M. J., and Bates, D. M. (1988), &quot;Newton-Raphson and EM Algorithms for Linear Mixed-Effects <br />Models for Repeated Measures Data,&quot; Journal of the American Statistical Association, 83, 1014-1022. <br />Meyer, K. (1989), &quot;Restricted Maximum Likelihood to Estimate Variance Components for Animal Models <br />with Several Random Effects Using a Derivative-Free Algorithm,&quot; Genetique Selection Evolution, 21, <br />317-340. <br />- <br />(1991), &quot;Estimating Variances and Covariances for Multivariate Animal Models by Restricted Maximum <br />Likelihood,&quot; Genetique Selection Evolution, 23, 67-83. <br />(1994), &quot;Derivative-Intense Restricted Maximum Likelihood Estimation of Covariance Components for <br />Animal Models,&quot; in Proceedings of the 5th World Congress on Genetics Applied to Livestock Production, <br />pp. 365-369. <br />Misztal, I. (1990), &quot;Restricted Maximum Likelihood Estimation of Variance Components in Animal Models <br />Using Sparse Matrix Inversion and a Supercomputer,&quot; Journal of Dairy Science, 73, 163-172. <br />Misztal, I., and Perez-Enciso, M. (1993), &quot;Sparse Matrix Inversion for Restricted Maximum Likelihood Estima- <br />tion of Variance Components by Expectation-Maximization,&quot; Journal of Dairy Science, 76, 1479-1483. <br />Patterson, H. D., and Thompson, R. (1971), &quot;Recovery of Inter-Block Information When Block Sizes are <br />Unequal,&quot; Biometrika, 58, 545-554. <br />Press, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. (1992), Numerical Recipes in Fortran <br />(2nd ed.), Cambridge: Cambridge University Press. <br />Quaas, R. L. (1988), &quot;Additive Genetic Model with Groups and Relationships,&quot; Journal of Dairy Science, 71, <br />1338-1347. <br />Robinson, G. K. (1991), &quot;That BLUP is a Good Thing: The Estimation of Random Effects,&quot; Statistical Science, <br />6, 15-51. <br />Searle, S. R. (1989), &quot;Variance Components-Some History and a Summary Account of Estimation Methods,&quot; <br />Journal of Animal Breeding and Genetics, 106, 1-29. <br />Searle, S. R., Casalla, G., McCulloch, C. E. (1992), Variance Components, New York: John Wiley. <br />Smith, S. P., and Graser, H.-U. (1986), &quot;Estimating Variance Components in a Class of Mixed Models by <br />Restricted Maximum Likelihood,&quot; Journal of Dairy Science, 69, 1156-1165. <br />Smith, S. P., and Lin, C. Y. (1989), &quot;Efficient Implementation of the New REML Algorithms,&quot; Journal of <br />Dairy Science, 72, 3336-3341. <br />Takahashi, K., Fagan, J., and Chen, M. (1973), &quot;Formation of a Sparse Bus Impedance Matrix and its Ap- <br />plication to Short Circuit Study,&quot; in 8th Power Industry Computer Applications Conference, New York: <br />IEEE, 63-69. <br />Taylor, J. F., Bean, B., Marshall, C. E., and Sullivan, J. J. (1985), &quot;Genetic and Environmental Components <br />of Semen Production Traits of Artificial Insemination Holstein Bulls,&quot; Journal of Dairy Science, 68, <br />2703-2722. <br />Thomas, J. J., Wallis, K. F. (1971), &quot;Season Variation in Regression Analysis,&quot; Journal of the Royal Statistical <br />Society, Ser. A, 134, 57-72. <br />Tier, B., and Smith, S. P. (1989), &quot;Use of Sparse Matrix Absorption in Animal Breeding,&quot; Genetique Selection <br />Evolution, 21, 457-466. <br />Wade, K. M., and Quaas, R. L. (1993), &quot;Solutions to a System of Equations Involving a First-Order Autore- <br />gressive Process,&quot; Journal of Dairy Science, 76, 3026-3032. <br />147</p>   </div> <div id="rgw18_56ab1e9180e1f" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56ab1e9180e1f">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56ab1e9180e1f"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://cs.hi.is/blasdiff/smith-1995-cholesky-diff.PDF" target="_blank" rel="nofollow" class="publication-viewer" title="Differentiation of the Cholesky Algorithm">Differentiation of the Cholesky Algorithm</a> </div>  <div class="details">   Available from <a href="http://cs.hi.is/blasdiff/smith-1995-cholesky-diff.PDF" target="_blank" rel="nofollow">cs.hi.is</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw27_56ab1e9180e1f" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (38) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw28_56ab1e9180e1f" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw29_56ab1e9180e1f" >  <div class="indent-left">  <div id="rgw30_56ab1e9180e1f" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Karin_Meyer2" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Karin Meyer </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw31_56ab1e9180e1f">  <li class="citation-context-item"> "The default algorithm used by WOMBAT to locate the maximum of the likelihood function is the so-called &#39;average information&#39; algorithm, developed by Thompson and co-workers (Thompson et al., 2005). This is implemented using an automatic differentiation (Smith, 1995) rather than sparse matrix inversion of the coefficient matrix in the mixed model equations. To ensure an increase in the likelihood in each iterate, step sizes are scaled if necessary, using the backtracking line search of Dennis and Schnabel (1996) to determine the optimum scale factor. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML"> <span class="publication-title js-publication-title">WOMBAT - A tool for mixed model analyses in quantitative genetics by REML</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/5611721_Karin_Meyer" class="authors js-author-name ga-publications-authors">Karin Meyer</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> WOMBAT is a software package for quantitative genetic analyses of continuous traits, fitting a linear, mixed model; estimates of covariance components and the resulting genetic parameters are obtained by restricted maximum likelihood. A wide range of models, comprising numerous traits, multiple fixed and random effects, selected genetic covariance structures, random regression models and reduced rank estimation are accommodated. WOMBAT employs up-to-date numerical and computational methods. Together with the use of efficient compilers, this generates fast executable programs, suitable for large scale analyses. Use of WOMBAT is illustrated for a bivariate analysis. The package consists of the executable program, available for LINUX and WINDOWS environments, manual and a set of worked example, and can be downloaded free of charge from (http://agbu. une.edu.au/~kmeyer/wombat.html). </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Dec 2007  &middot; Journal of Zhejiang University SCIENCE B  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Karin_Meyer2/publication/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML/links/02e7e522258ceb1362000000.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw32_56ab1e9180e1f" >  <div class="indent-left">  <div id="rgw33_56ab1e9180e1f" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/7696504_Estimation_of_quantitative_genetic_parameters">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Robin_Thompson5" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Robin Thompson </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw34_56ab1e9180e1f">  <li class="citation-context-item"> "These were an improvement on derivative-free methods, but could still be slow to converge. It is possible to calculate second differentials using automatic differentiation (Smith 1995), but the computation of each second differential requires six times as many multiplications as those involved in a single likelihood calculation (Smith 1995), and this becomes more costly as the number of parameters increases. There are various suggestions on approximating the second differential. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/7696504_Estimation_of_quantitative_genetic_parameters"> <span class="publication-title js-publication-title">Estimation of quantitative genetic parameters</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/10041375_Robin_Thompson" class="authors js-author-name ga-publications-authors">Robin Thompson</a> &middot;     <a href="researcher/2071908322_Sue_Brotherstone" class="authors js-author-name ga-publications-authors">Sue Brotherstone</a> &middot;     <a href="researcher/15784675_Ian_M_S_White" class="authors js-author-name ga-publications-authors">Ian M S White</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> This paper gives a short review of the development of genetic parameter estimation over the last 40 years. This shows the development of more statistically and computationally efficient methods that allow the fitting of more biologically appropriate models. Methods have evolved from direct methods based on covariances between relatives to methods based on individual animal models. Maximum-likelihood methods have a natural interpretation in terms of best linear unbiased predictors. Improvements in iterative schemes to give estimates are discussed. As an example, a recent estimation of genetic parameters for a British population of dairy cattle is discussed. The development makes a connection to relevant work by Bill Hill. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Aug 2005  &middot; Philosophical Transactions of The Royal Society B Biological Sciences  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Robin_Thompson5/publication/7696504_Estimation_of_quantitative_genetic_parameters/links/0c96052265e58bbd9c000000.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw35_56ab1e9180e1f" >  <div class="indent-left">  <div id="rgw36_56ab1e9180e1f" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/265943768_Prospects_for_statistical_methods_in_animal_breeding">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Esa_Maentysaari2" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Esa Mäntysaari </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw37_56ab1e9180e1f">  <li class="citation-context-item"> "These were an improvement on derivative free methods but could still be slow to converge. It is possible to calculate second differentials using the automatic differentation ideas of Smith [34] but the calculation of each second differential requires the computation of the order of six likelihood calculations (Smith [34]) and this becomes Illore costly as the number of parameters increase. There are various suggestions on approxmating the second differential. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/265943768_Prospects_for_statistical_methods_in_animal_breeding"> <span class="publication-title js-publication-title">Prospects for statistical methods in animal breeding</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2054581511_Robin_Thompson" class="authors js-author-name ga-publications-authors">Robin Thompson</a> &middot;     <a href="researcher/2054592430_Esa_Mantysaari" class="authors js-author-name ga-publications-authors">Esa Mantysaari</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Accurate prediction of breeding values is of great importance for animal improvement programmes. The prediction of breeding values requires knowledge of the magnitude of the variances and covariances of random effects. This paper gives a short review of methods of estimation of genetic variance parameters, contrasting analytical estimates with iterative and sampling based methods. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Jan 2004  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Esa_Maentysaari2/publication/265943768_Prospects_for_statistical_methods_in_animal_breeding/links/557ac30408ae8d04819315d5.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw22_56ab1e9180e1f" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56ab1e9180e1f">  </ul> </div> </div>   <div id="rgw14_56ab1e9180e1f" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56ab1e9180e1f"> <div> <h5> <a href="publication/262404149_The_conditions_for_the_applicability_of_the_generalized_Cholesky_algorithm" class="color-inherit ga-similar-publication-title"><span class="publication-title">The conditions for the applicability of the generalized Cholesky algorithm</span></a>  </h5>  <div class="authors"> <a href="researcher/2048299508_L_V_Maslovskaya" class="authors ga-similar-publication-author">L. V. Maslovskaya</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56ab1e9180e1f"> <div> <h5> <a href="publication/221994663_Programming_a_generalized_cholesky_algorithm_for_mixed_discrete_analogues_of_elliptic_boundary-value_problems" class="color-inherit ga-similar-publication-title"><span class="publication-title">Programming a generalized cholesky algorithm for mixed discrete analogues of elliptic boundary-value problems</span></a>  </h5>  <div class="authors"> <a href="researcher/2026803741_AA_Kobozeva" class="authors ga-similar-publication-author">A.A. Kobozeva</a>, <a href="researcher/2047882586_LV_Maslovskaya" class="authors ga-similar-publication-author">L.V. Maslovskaya</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab1e9180e1f"> <div> <h5> <a href="publication/237703144_The_block_interval_Cholesky_algorithm" class="color-inherit ga-similar-publication-title"><span class="publication-title">The block interval Cholesky algorithm</span></a>  </h5>  <div class="authors"> <a href="researcher/2009383049_Uwe_Sch" class="authors ga-similar-publication-author">Uwe Sch</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56ab1e9180e1f" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56ab1e9180e1f">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56ab1e9180e1f" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=ADESLmQ-fWV0zp5jMT0fC18hxd6bOCEPt-CPMaePyJ0AmrRZVjKwcBQUZ0jNUDTB" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="xHBtyzdY4T9bGi5pYSzA0cvSr/3IrsUQnGFDJniunfdXahIaZPsaGprGEuwtuVuEFmBBUlQK2Sk9XBEKtsmlaa69VwA2J8PEvCX1hiyHeUaBbkteh483uY6lBIpDdKfyTkR7IuaKJyU5Gqq8QUuc2RobGnSrvQCIOy48VP3b3c0fYXQrD3ZzuTHkRHxZIcZ+U3mm0Vraxm3GdT7DBxT5KWD6i61F7/dbiRXZKGM3QFbzIsy5EXpZ4/MqyVTg/hBtE4sKdGHLDw3CUIxSayglEOSO7gJMAQyUwYvIcS/kAJ0="/> <input type="hidden" name="urlAfterLogin" value="publication/239030086_Differentiation_of_the_Cholesky_Algorithm"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjM5MDMwMDg2X0RpZmZlcmVudGlhdGlvbl9vZl90aGVfQ2hvbGVza3lfQWxnb3JpdGht"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjM5MDMwMDg2X0RpZmZlcmVudGlhdGlvbl9vZl90aGVfQ2hvbGVza3lfQWxnb3JpdGht"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjM5MDMwMDg2X0RpZmZlcmVudGlhdGlvbl9vZl90aGVfQ2hvbGVza3lfQWxnb3JpdGht"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56ab1e9180e1f"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 1033;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/2012005959_S_P_Smith","fullname":"S. P. Smith","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"1.22","widgetId":"rgw5_56ab1e9180e1f"},"id":"rgw5_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=2012005959","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":1,"widgetId":"rgw6_56ab1e9180e1f"},"id":"rgw6_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=2012005959","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56ab1e9180e1f"},"id":"rgw4_56ab1e9180e1f","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=2012005959","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56ab1e9180e1f"},"id":"rgw3_56ab1e9180e1f","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=239030086","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":239030086,"title":"Differentiation of the Cholesky Algorithm","journalTitle":"Journal of Computational and Graphical Statistics","journalDetailsTooltip":{"data":{"journalTitle":"Journal of Computational and Graphical Statistics","journalAbbrev":"J COMPUT GRAPH STAT","publisher":"American Statistical Association; Institute of Mathematical Statistics; Interface Foundation of North America, Taylor & Francis","issn":"1061-8600","impactFactor":"1.22","fiveYearImpactFactor":"1.81","citedHalfLife":">10.0","immediacyIndex":"0.27","eigenFactor":"0.01","articleInfluence":"1.88","widgetId":"rgw8_56ab1e9180e1f"},"id":"rgw8_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=1061-8600","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1080\/10618600.1995.10474671","journalInfos":{"journal":"","publicationDate":"06\/1995;","publicationDateRobot":"1995-06","article":"4(2):134-147.","journalTitle":"Journal of Computational and Graphical Statistics","journalUrl":"journal\/1061-8600_Journal_of_Computational_and_Graphical_Statistics","impactFactor":1.22}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1080\/10618600.1995.10474671"},{"key":"rft.atitle","value":"Differentiation of the Cholesky Algorithm"},{"key":"rft.title","value":"Journal of Computational and Graphical Statistics - J COMPUT GRAPH STAT"},{"key":"rft.jtitle","value":"Journal of Computational and Graphical Statistics - J COMPUT GRAPH STAT"},{"key":"rft.volume","value":"4"},{"key":"rft.issue","value":"2"},{"key":"rft.date","value":"1995"},{"key":"rft.pages","value":"134-147"},{"key":"rft.issn","value":"1061-8600"},{"key":"rft.au","value":"S. P. Smith"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw9_56ab1e9180e1f"},"id":"rgw9_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=239030086","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":239030086,"peopleItems":[{"data":{"authorUrl":"researcher\/2012005959_S_P_Smith","authorNameOnPublication":"S. P. Smith","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"S. P. Smith","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2012005959_S_P_Smith","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56ab1e9180e1f"},"id":"rgw12_56ab1e9180e1f","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2012005959&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56ab1e9180e1f"},"id":"rgw11_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2012005959&authorNameOnPublication=S.%20P.%20Smith","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab1e9180e1f"},"id":"rgw10_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=239030086&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":239030086,"abstract":"<noscript><\/noscript><div>One way to estimate variance components is by restricted maximum likelihood. The log-likelihood function is fully defined by the Cholesky factor of a matrix that is usually large and sparse. In this article forward and backward differentiation methods are developed for calculating the first and second derivatives of the Cholesky factor and its functions. These differentiation methods are general and can be applied to either a full or a sparse matrix. Moreover, these methods can be used to calculate the derivatives that are needed for restricted maximum likelihood, resulting in substantial savings in computation.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw13_56ab1e9180e1f"},"id":"rgw13_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=239030086","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/239030086_Differentiation_of_the_Cholesky_Algorithm\/links\/02dd4f420cf23be0d317224f\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56ab1e9180e1f"},"id":"rgw7_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=239030086&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2048299508,"url":"researcher\/2048299508_L_V_Maslovskaya","fullname":"L. V. Maslovskaya","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 1992","journal":"Computational Mathematics and Mathematical Physics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/262404149_The_conditions_for_the_applicability_of_the_generalized_Cholesky_algorithm","usePlainButton":true,"publicationUid":262404149,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.79","url":"publication\/262404149_The_conditions_for_the_applicability_of_the_generalized_Cholesky_algorithm","title":"The conditions for the applicability of the generalized Cholesky algorithm","displayTitleAsLink":true,"authors":[{"id":2048299508,"url":"researcher\/2048299508_L_V_Maslovskaya","fullname":"L. V. Maslovskaya","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Computational Mathematics and Mathematical Physics 03\/1992; 32(3):279-286."],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/262404149_The_conditions_for_the_applicability_of_the_generalized_Cholesky_algorithm","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/262404149_The_conditions_for_the_applicability_of_the_generalized_Cholesky_algorithm\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56ab1e9180e1f"},"id":"rgw15_56ab1e9180e1f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=262404149","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2026803741,"url":"researcher\/2026803741_AA_Kobozeva","fullname":"A.A. Kobozeva","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2047882586,"url":"researcher\/2047882586_LV_Maslovskaya","fullname":"L.V. Maslovskaya","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Mar 1990","journal":"USSR Computational Mathematics and Mathematical Physics","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/221994663_Programming_a_generalized_cholesky_algorithm_for_mixed_discrete_analogues_of_elliptic_boundary-value_problems","usePlainButton":true,"publicationUid":221994663,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/221994663_Programming_a_generalized_cholesky_algorithm_for_mixed_discrete_analogues_of_elliptic_boundary-value_problems","title":"Programming a generalized cholesky algorithm for mixed discrete analogues of elliptic boundary-value problems","displayTitleAsLink":true,"authors":[{"id":2026803741,"url":"researcher\/2026803741_AA_Kobozeva","fullname":"A.A. Kobozeva","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2047882586,"url":"researcher\/2047882586_LV_Maslovskaya","fullname":"L.V. Maslovskaya","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["USSR Computational Mathematics and Mathematical Physics 03\/1990; 30(2-30):56-62. DOI:10.1016\/0041-5553(90)90076-5"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/221994663_Programming_a_generalized_cholesky_algorithm_for_mixed_discrete_analogues_of_elliptic_boundary-value_problems","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/221994663_Programming_a_generalized_cholesky_algorithm_for_mixed_discrete_analogues_of_elliptic_boundary-value_problems\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab1e9180e1f"},"id":"rgw16_56ab1e9180e1f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=221994663","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2009383049,"url":"researcher\/2009383049_Uwe_Sch","fullname":"Uwe Sch","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/237703144_The_block_interval_Cholesky_algorithm","usePlainButton":true,"publicationUid":237703144,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/237703144_The_block_interval_Cholesky_algorithm","title":"The block interval Cholesky algorithm","displayTitleAsLink":true,"authors":[{"id":2009383049,"url":"researcher\/2009383049_Uwe_Sch","fullname":"Uwe Sch","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/237703144_The_block_interval_Cholesky_algorithm","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/237703144_The_block_interval_Cholesky_algorithm\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab1e9180e1f"},"id":"rgw17_56ab1e9180e1f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=237703144","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56ab1e9180e1f"},"id":"rgw14_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=239030086&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":239030086,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":239030086,"publicationType":"article","linkId":"02dd4f420cf23be0d317224f","fileName":"Differentiation of the Cholesky Algorithm","fileUrl":"http:\/\/cs.hi.is\/blasdiff\/smith-1995-cholesky-diff.PDF","name":"cs.hi.is","nameUrl":"http:\/\/cs.hi.is\/blasdiff\/smith-1995-cholesky-diff.PDF","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw20_56ab1e9180e1f"},"id":"rgw20_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=239030086&linkId=02dd4f420cf23be0d317224f&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56ab1e9180e1f"},"id":"rgw19_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=239030086&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":8,"valueFormatted":"8","widgetId":"rgw21_56ab1e9180e1f"},"id":"rgw21_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=239030086","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56ab1e9180e1f"},"id":"rgw18_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=239030086&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":239030086,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56ab1e9180e1f"},"id":"rgw23_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=239030086&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":8,"valueFormatted":"8","widgetId":"rgw24_56ab1e9180e1f"},"id":"rgw24_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=239030086","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56ab1e9180e1f"},"id":"rgw22_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=239030086&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Interface Foundation of America\nDifferentiation of the Cholesky Algorithm\nAuthor(s): S. P. Smith\nSource: Journal of Computational and Graphical Statistics, Vol. 4, No. 2 (Jun., 1995), pp. 134-\n147\nPublished by: American Statistical Association, Institute of Mathematical Statistics, and\nInterface Foundation of America\nStable URL: http:\/\/www.jstor.org\/stable\/1390762\nAccessed: 22\/05\/2010 10:00\nYour use of the JSTOR archive indicates your acceptance of JSTOR's Terms and Conditions of Use, available at\nhttp:\/\/www.jstor.org\/page\/info\/about\/policies\/terms.jsp. JSTOR's Terms and Conditions of Use provides, in part, that unless\nyou have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you\nmay use content in the JSTOR archive only for your personal, non-commercial use.\nPlease contact the publisher regarding any further use of this work. Publisher contact information may be obtained at\nhttp:\/\/www.jstor.org\/action\/showPublisher?publisherCode=astata.\nEach copy of any part of a JSTOR transmission must contain the same copyright notice that appears on the screen or printed\npage of such transmission.\nJSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of\ncontent in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms\nof scholarship. For more information about JSTOR, please contact support@jstor.org.\nAmerican Statistical Association, Institute of Mathematical Statistics, Interface Foundation of America are\ncollaborating with JSTOR to digitize, preserve and extend access to Journal of Computational and Graphical\nStatistics.\nhttp:\/\/www.jstor.org"},{"page":2,"text":"Differentiation of the Cholesky Algorithm \nS. P. SMITH* \nOne way to estimate variance components is by restricted maximum likelihood. \nThe log-likelihood function is fully defined by the Cholesky factor of a matrix that is \nusually large and sparse. In this article forward and backward differentiation methods \nare developed for calculating the first and second derivatives of the Cholesky factor and \nits functions. These differentiation methods are general and can be applied to either a \nfull or a sparse matrix. Moreover, these methods can be used to calculate the derivatives \nthat are needed for restricted maximum likelihood, resulting in substantial savings in \ncomputation. \nKey Words: Backward differentiation; Determinant; Forward differentiation; Recursion; \nRestricted maximum likelihood; Sparse matrix; Variance components. \n1. INTRODUCTION \nVariance estimation by restricted maximum likelihood or REML (Patterson and \nThompson 1971), is frequently computation intensive because of the need to calcu- \nlate derivatives of the log-likelihood. (See Searle, Casalla, and McCulloch [1992], pp. \n252, 253, for an overview of some derivatives.) Calculating the likelihood itself is less \ndifficult when a decomposition algorithm for sparse matrices is feasible (Tier and Smith \n1989). Some researchers (e.g., Smith and Graser 1986; Graser, Smith, and Tier 1987; \nMeyer 1989, 1991) have designed derivative-free approaches to maximizing likelihoods. \nIt would seem to be a contradiction that derivatives of the log-likelihood are hard \nto calculate although the log-likelihood itself is easy to compute. Most algorithms can \nbe differentiated, and efficient code for likelihood evaluation should lead to an efficient \napproach for differentiation. For a review of the theory of automatic differentiation and \na survey of the available software packages see the conference proceedings in Griewank \nand Corliss (1991). Smith and Lin (1989) used algorithmic differentiation to evaluate \nderivatives of the log-determinant of a tridiagonal matrix in linear time. Although their \nmethod is useful only for specific REML applications, it does demonstrate the potential \nvalue of algorithmic differentiation in variance component estimation. \nThe log-likelihood for REML is completely determined by items computed during \nthe Cholesky decomposition, and this association is reviewed in Section 3. This method \n*Statistician, EA Engineering, Science and Technology, 3468 Mt. Diablo Blvd., Suite B-100, Lafayette, CA \n94549 \n(1995 American Statistical Association, Institute of Mathematical Statistics, \nand Interface Foundation of North America \nJournal of Computational and Graphical Statistics, Volume 4, Number 2, Pages 134-147 \n134"},{"page":3,"text":"DIFFERENTIATION  OF THE CHOLESKY ALGORITHM \nof likelihood evaluation has proven to be very efficient, suggesting that algorithmic \ndifferentiation may also be efficient. This has led to investigations of the connection \nbetween the Cholesky decomposition, REML, and algorithmic differentiation. \nThere are two main ways to differentiate algorithms, called forward and backward \ndifferentiation (Iri 1991). It turns out that forward differentiation is a direct extension of \nthe Cholesky decomposition. The calculations for backward differentiation show less sim- \nilarity to the Cholesky decomposition and are said to be adjoint to the forward equations. \nBoth approaches are feasible, but backward differentiation is preferable when several or \nnumerous partial derivatives are wanted. More importantly, derivatives can be calculated \nin sparse matrix mode, using amounts of computing time similar to that needed for likeli- \nhood evaluation. This means that REML is not prohibitively difficult in situations where \nit is easy to calculate the likelihood by factorization. \nThe purpose of this article is to show how to use recursion to evaluate first and second \nderivatives of both the Cholesky decomposition and its functions. The Cholesky decom- \nposition is reviewed in Section 2.1. Section 2.2 is devoted to forward differentiation, and \nSection 2.3 presents backward differentiation. Sparse matrix implementation is treated \nin Section 2.4. The application of algorithmic differentiation to REML is presented in \nSection 3. Conditions under which the proposed techniques provide improvements over \ntraditional methodologies are described in Section 4. \n2. CHOLESKY DECOMPOSITION \n2.1 BASIC RECURSIONS \nThe Cholesky decomposition takes a symmetric and nonnegative definite matrix M \nand factorizes it into a lower triangular matrix L, where LL' = M. (The requirement \nof nonnegativity can be relaxed if complex arithmetic is allowed.) An elegant way to \ncompute the decomposition is to put M in a half-stored array and overwrite the array with \nthe lower triangular elements of L (Golub and Van Loan 1983, p. 88; Press, Teukolsky, \nVetterling, and Flannery 1992, p. 89). \nLet Lij be the ijth element of the half-stored array (i > j). The following recursions \nand definitions describe the Cholesky algorithm. \n1. Set Lij to the ijth element of M (half-stored). \n2. For k = 1 to N, apply the following recursions (a through c) if \\Lkkl is larger \nthan the operational zero: \n(a) define pivot \nLkk= L 2 \n(b) adjust lead column \nLjk:= \n(c) row operations \nLij:= Lij - LkLjk, for j = k + 1,... N, and i = j,...N. \nAt the end of the process, Lij is returned as the ijth element of the lower triangular \nmatrix L. \nThis description is readily turned into computer code, where := indicates that the \nLjk\/Lkk, for j = k + 1,... N. \n135"},{"page":4,"text":"S. P. SMITH \ncalculated quantity on the right recursively replaces the item on the left. \nThe previous algorithm is intentionally represented as an ordered list of simple \nrecursions. (For nonsparse matrices there are approximately N3\/6 entries in this list.) \nVarious elements of the work array are accessed at different points on the list, but the list \nof recursions is simple, and each item is one of only three possible calculations (square \nroot, division, or multiplication and subtraction). The list is ordered, so rearranging the \norder of the recursions may corrupt the calculations. However, there are many acceptable \nways to rearrange the calculations. The outer product form is followed in this article, but \nthere are two other arrangements-the bordering method and the inner product form- \nthat are also popular (George and Liu 1981, pp. 17-20). \nThe algorithm is presented as a list of simple recursions because this removes the \ncomplication that disguises the chain rule of calculus. Differentiation is direct and easy, \nnot just for the Cholesky decomposition, but for any algorithm that can be represented \nas an ordered list of simple recursions. \n2.2  FORWARD DIFFERENTIATION \nTo describe the forward differentiation, let Lij{)- \n02Lij\/&xAy. (This notation allows for the case where x = y.) \nDifferentiation is directly applied to Steps 1 and 2 described in Section 2.1, and leads \nto the recursions listed in the following. Only first and second derivatives are evaluated \nhere, but higher-order derivatives are possible by further differentiation at each step. \n= OLij\/Ox and Lij{x,y \n1. Set Lij to the ijth element of M, Lij{x} =OMij\/Ox, \n(i j). \n2. For k = 1 to N, apply recursions a through c if Lkk] is larger than the operational \nzero: \n(a) define pivot \nCholesky factor: Lkk: = Lkk \nfirst derivatives: Lkk{x} := .5Lkk{x}\/Lkk \nsecond derivatives: Lkk{x,y} := [.5Lkk{x,y} - Lkk{x}Lkk{y}]\/Lkk \n(b) adjust lead column \nCholesky factor: Ljk := Ljk\/Lkk \nfirst derivatives: Ljk{x} := [Ljk{x} - LjkLkk{x}]\/Lkk \nsecond derivatives: Ljk{x,y} := [Ljk{x,y -LjkLkk{x,y)-Ljk{x)Lkk{y}- \nLjk{y}Lkk{x}]\/Lkk for j = k + 1,.., \n(c) row operations \nCholesky factor: Li := L \nfirst derivatives: Lij{x} := Lij{x} - Lik{}Ljk \nsecond derivatives: Lij{x,y} := ij,y \nLik{x}Ljk{y} \nThis returns Lij, the ijth element of the lower triangular matrix L. The derivatives \nare given by Lij{x} and Lijx,y}. \nThe previous recursions form an ordered list, and rearranging the order may destroy \nand Lij{x,y} =-02Mij\/lxOy \nN. \n- LikLjk \n- LkLj{x} \n- Lik{x,y}Ljk \n- LikLjk{x,y} \nN, and i = j,..., \n- \n- Lk{y}Ljk{x} \nfor j = k + 1,..., \nN. \n136"},{"page":5,"text":"DIFFERENTIATION \nOF THE CHOLESKY ALGORITHM \nthe calculations. However, there are different arrangements that work and may offer some \nadvantage. For example, it is not necessary to evaluate the Cholesky decomposition and \nall of the first and second derivatives simultaneously. Items can be parsed to permit \ntighter control over memory requirements. Moreover, changing the order of calculation \ncan also lead to improved numerical stability by allowing the accumulation of inner \nproducts in double precision (George and Liu 1981, p. 20). \nThe previous instructions can also be modified to allow selection of the kth pivot by \nsome rule. One such rule is to exchange rows and columns to bring the largest diagonal \nelement to the top of the list. This leads to a more numerically stable algorithm, but it \ndoes not permit the exploitation of sparsity. The use of pivoting to reduce fill-in precludes \nother uses. \nEach first derivative requires no more than twice the work needed to evaluate L, \nsay 2' (where i is the number of basic recursions listed in Section 2.1). Each second \nderivative requires at most 4I additional operations. Because the strategy can be imple- \nmented in sparse matrix mode, the efficiency is great when T is small. This development \nhas far-reaching implications, but there are overhead costs for sparse matrix storage. The \namount of work required to decompose banded matrices (including tridiagonal structures) \nand evaluate the derivatives is linear in N. \nOne disadvantage of forward differentiation is the dependence of the amount of work \nrequired on n, where n is the number of parameters that determine M; calculating all n \nfirst derivatives uses 2nr operations, and calculating all second derivatives uses an extra \n2n24 operations. A referee (personal communication 1994) suggested backward differ- \nentiation (also called reverse-mode differentiation) as an attractive alternative to forward \ndifferentiation. This very good advice reduces the dependence of the work requirement \non n. \n2.3 \nBACKWARD \nDECOMPOSITION \nDIFFERENTIATION  OF  FUNCTIONS OF \nTHE  CHOLESKY \nIn this section backward differentiation of f(L) is described, where f() is a scalar- \nvalued function of L. A typical example of f() is f(L) = \nNote that differentiating f(L) is a task that already shows less similarity to forward \ndifferentiations. Derivatives of all elements of L are evaluated jointly in the forward \ncalculations of Section 2.2, whereas here attentions is given only to f(L). \nOne approach for implementing backward differentiation is to apply a computer \nprogram to enumerate the recursions of Section 2.1, and then differentiates the recursions \nautomatically. Unfortunately, the amount of storage required to represent the recursions \ncan be very much larger than the storage needed for L. Moreover, properties of the \nCholesky decomposition related to speed and sparse matrix storage can be exploited to \nachieve a significant savings over the use of general-purpose packages. As with forward \ndifferentiation, the objective here is to show enough of the detail to help programmers \nin the design of software particular to the Cholesky decomposition. \nlog |M|, which is Ei log(Lii). \n137"},{"page":6,"text":"S. P. SMITH \n2.3.1 First Derivatives \nReverse-mode differentiation is a process that starts at the bottom of the list of \nrecursions and works backwards. The last recursion is the function f = f(L), and this \nis followed by the list of basic recursions (Section 2.1) in reverse order. Rather than \ntaking derivatives with respect to a parameter like x, backward differentiation proceeds \nby evaluating derivatives with respect to the intermediate quantities. For example, the \nrecursion h = h(u, v) is differentiated as hx = h,ux + h,vx, and the partials hu and \nh, are accumulated in the work array F, using the rules F[u] = F[u] + F[h]hu and \nF[v] = F[v] + F[h]h,, where F[s] is the item in F representing intermediate s = v, w, \nor h. When backward differentiation is complete, the derivative of f(L) with respect to \nthe ijth element of M is a member of F. These can be applied to OM\/Ox to evaluate \naf(L)\/Ox. \nThe work space F need not be any bigger than L, because, like the Cholesky de- \ncomposition, the algorithm can be organized to overwrite intermediate calculations. The \nbasic rules for differentiation still apply, but there are subtleties that accompany the al- \ngorithm which result from the fact that any single position in F can represent different \nintermediates. \nBackward differentiation is described in the following, where Fij is the ijth element \nof array F. \n1. Initialize the algorithm by setting Fij = \nelement of the Cholesky decomposition, already calculated. \n2. For k = N to 1, apply the following recursions (a through c) if ILkkl is larger \nthan the operational zero: \n(a) row operations \nFik := F \n- FijLjk, \nFjk :=Fjk - FijLik, for j = k + 1,...,N, \n(b) lead column \nFjk := Fjk\/Lkk, \nFkk = Fkk - LjkFjk, for j = k + 1,..., \n(c) pivot \nFkk := .5Fkk\/Lkk \n3. Evaluate af(L)\/Ox as EijFijaMij\/lx, \nWith F evaluated, Step 3 can be applied for several parameters x. Therefore, if \nStep 3 is negligible, backward differentiation requires only 24 operations to evaluate \nall first derivatives. This compares favorably to the 2nQ operations needed for forward \ndifferentiation. \nIt is convenient to describe Step 2 as a transformation that maps an array Q into \nan array S. That is, initially set F = Q, and after the process, F is returned as S. The \ntransformation is explicitly defined when L is provided. Step 2 is briefly denoted as \nS = F(QIL). This operator is also used in the evaluations of second derivatives. \nf (L)\/OLij (i \nj). Let Lij be the ijth \nand i = j,...,N. \nN. \n(i > j). \n138"},{"page":7,"text":"DIFFERENTIATION \nOF THE CHOLESKY ALGORITHM \n2.3.2 Second Derivatives \nRules for backward differentiation were applied twice to derive the following al- \ngorithm for second derivatives. It is also possible to apply forward differentiation to \nthe backward equations, or backward differentiation to the forward equations, to obtain \nalternative algorithms. \n1. Let the Cholesky decomposition L, and F = F({Of(L)\/OLij}IL) be provided. \nDefine a half-stored array Q initialized to AM\/0x, and let S be an array of equal \nsize. \n2. For k = 1 to N, apply the following recursions (a through c) if ILkkl is larger \nthan the operational zero: \n(a) pivot \nQkk := .Qkk\/Lkk \nSkk := -2QkkFkk \n(b) lead column \nQjk = [Qjk - QkkLjk]\/Lkk, \nSjk := -QkkFjk, \nSkk := Skk - QjkFjk, for j = k + 1,..., \n(c) row operations \nQij = Qij - QikLj - LikQjk, \n= Sjk - \nQikFij, \nSik = Sik - QjkFij, for j = k + 1,..., \n3. For all i > j, evaluate Sij := Sj + ZtrQtrO2f(L)\/OLtrOLij, \n4. Evaluate S := F(SIL) \n5. Evaluate 02f(L)\/9Oxy as EijSij9Mij \/y + EijFij2Mij\/xy, \nN \nSjk \nN, and i = j,...,N. \n(t > r). \n(i > j). \nStep 5 can be repeated for different parameters y. If the work required for Steps 3 \nand 5 is negligible, the extra work required to evaluate all second derivatives is no more \nthan 6n, operations, the previous algorithm being repeated n times. \nNote that Q is turned into AL\/Ox, and indeed is calculated using forward differenti- \nation. However, the algorithm can be organized to overwrite Q with S, but only if f() is \na simple function; for example, 02f(L)\/LLtrOLij = 0 if r : j. After Step 2c the lead \ncolumn of Q can be applied to operations in Step 3, and then the lead column can be \noverwritten with Skk, ... SNk. Hence, only one extra work array is needed to evaluate \nall second derivatives. \n2.4 SPARSE-MATRIX IMPLEMENTATION \nIt is beyond the scope of this aticle to describe in much detail the devices needed \nto implement sparse matrix factorization. Techniques currently exist for sparse matrices \n(e.g., George and Liu 1981), and modifying them for forward and backward differentia- \ntion is a feasible adjustment. In particular, sparse-matrix tools for forward differentiation \ncan be developed with minor modifications, because the calculations are just parallel \noperations that are embedded in the Cholesky decomposition. Moreover, sparse-matrix \ntools are just as easily applied to backward differentiation, because the columns of L \n139"},{"page":8,"text":"S. P. SMITH \ndefine the nature of all derivative operations. The reverse-mode calculations for first and \nsecond derivatives are appended as follows: adjustments involving j for the kth lead \ncolumn occur only if Ljk is nonzero; adjustments for the kth set of row operations in- \nvolving i and j occur only if Lik and Ljk are nonzero. Consequently, the arrays F and \nS of Sections 2.3.1 and 2.3.2 have the same pattern of sparsity as L. \nAlgorithms that exploit sparsity can be organized in various ways, and additional \nresearch is needed to find the variants that are most efficient. However, a good start is \nmade by software accompanying this article provided on StatLib (see Section 3.3). \n3. USING 'THE TECHNIQUES TO ESTIMATE \nVARIANCES AND COVARIANCES BY REML \n3.1  METHODOLOGY \nOur focus is on general mixed models of the form \ny = Xp + Zu + e, \nwhere y, 3, u, and e are vectors of observations, fixed effects, random effects, and random \nresiduals; X and Z are incidence matrices; and \nE{u} \n= \n0, \nG, var{e} = \nE{e} \n= \n0, \nR, cov{u,e'} = 0. \nvar{u} = \nArray 0 is understood to be a null vector or matrix, depending on the context. \nMatrices R and G are functions of parameters (say 0) that are to be estimated by \nmaximum likelihood. Under conditions of multivariate normality, the log-likelihood that \nis to be maximized in REML is the following: \nlogL = - {loglRI + loglGI + loglCI + y'Py}, \n(3.1) \nwhere ICI is the determinant of \nX'R-'X  X'R-'Z \nC= \nZ'R- X \nZ'R-Z \n+ G-1 \nand P = V-1 - V-1X(X'V-'X)-X'V-1, \nMatrix C is assumed to be nonsingular, and this is usually the case when X is chosen \nto have full column rank. Terms |RI and IGI are usually easy to evaluate, as are their \nderivatives. As noted by Tier and Smith (1989), ICI and y'Py can be computed from \nitems created during linked-list absorption-that is, by Gaussian elimination. This is just \nas easily accomplished with the Cholesky decomposition (Boldman and Van Vleck 1991). \nThe likelihood is a function of determinants. Even the quadratic y'Py is a function \nof determinants, MI\/|CI, where the mixed model matrix is given as: \n\" \nX'R- \n1 \nX'R-1Z X'R-Z \nM= \nZ'R-'X \nZ'R-Z + G-1 \nwhere V = ZGZ' + R (Harville 1977). \n-y \nZ'R-1y \ny'R-ly \ny'R-'X \ny'R-'Z \n140"},{"page":9,"text":"DIFFERENTIATION OF THE CHOLESKY ALGORITHM \nHowever, y'Py is better computed as the square of the last diagonal element of the \nCholesky factor of M. Boldman and Van Vleck (1991) provided yet another calculation \nfor y'Py. The quantity 'logICI is the sum of the logarithms of all the nonzero diagonal \nelements of the Cholesky factor except the last one. \nMatrix M is usually very large and very sparse. Sparseness is a direct consequence \nof R-1 and G-1 being sparse, even if R and G are not. Furthermore, these inverse \nmatrices are frequently known directly from simple rules for inverting: diagonal or block \ndiagonal matrices; matrix sums (Henderson and Searle 1981); direct products (Searle et \nal. 1992, p. 444); additive genetic variance-covariance matrices for animal populations \n(e.g., Quaas 1988); and variance matrices for autoregressive time series (e.g., Fuller \n1976, p. 235; Robinson 1991). General tricks for inverting variance matrices that occur \nfor spatial fields (e.g., Cressie 1991, p. 85) are less well known, but approximations \nor simplifications for particular models do exist. For example, sparse inverses can be \nderived from nearest-neighbor analysis (Jones and Vecchia 1993). Similarly, a sparse \ninverse can be built directly in a way that accommodates spatial dependence among \nadjacent elements in a lattice field (Cressie 1991, p. 406). If the model is such that \nsparse R-1 and G-1 are unavailable, sparse-matrix techniques are not useful, and in \nthis case alternative representations of the log-likelihood (e.g., see equation (14) of Jones \nand Vecchia 1993) may offer advantages in designing numerical strategies. \nNewton-Raphson is a suitable method to maximize the likelihood, and this method \nrequires the evaluation of first and second derivatives (Lindstrom and Bates 1988; Searle \net al. 1992, p. 293). Derivatives of the log-likelihood are obtained by differentiating \nIRl, IGI, and the diagonal elements of the Cholesky factor of M with respect to the \nparameters 0. The strategies described in Sections 2.2 and 2.3 apply directly. To apply \nthe procedures, derivatives of M are needed, and these are easy to find when G and \nR have simple structures. Some simple variance structures are given by Meyer (1989, \n1991). \nTo set up the reverse-mode calculations for REML, note that the last function in the \nlist of recursions is f(L) = - {loglCI + y'Py}. Therefore, the initializations for the \nfirst derivative calculations (Step 1, Sec. 2.3.1) are Fii \nFNN =-LNN. \nFor second derivatives, the interpretation of Step 3 in Section 2.3.2 is \nSI ii \n+ \nSi \ni \n= \nii, \ni \nBlind application of Newton-Raphson is not advised-starting values affect con- \nvergence, and it may be necessary to try several starting values (Harville and Callanan \n1990). Moreover, REML is a constrained maximization problem, and algorithms should \navoid producing estimates outside the parameter space (Searle et al. 1992, p. 291). \n-\/Lii, \ni = 1,... N - 1, and \n1, ... N - 1, and SNN = SNN - QNN. \n3.2 \nALTERNATIVE \nMETHODS \nWhen data are balanced, REML estimates are easily obtained by equating ANOVA \nsums of squares to their expectations (Searle 1989). The quantities involved in estimating \nvariance components are known directly from simple formulas, and there is no need for \ngeneral algorithms to calculate derivatives of determinants. Calvin and Dykstra (1991) \ndescribed a procedure for balanced multivariate models that guarantees that estimates will \n141"},{"page":10,"text":"S. P. SMITH \no \nI \nQ \nO \n'0 \n|C \n||^ \n6 \n6 \ni60 \n20- \n0-, \n06\/01 \n07\/01 \n08\/01 \nDate \n09\/01 10\/01 \nFigure 1. Time Series of Percent Relative Humidity for the San Joaquin Valley During the Summer of 1993. \nbe in the parameter space, and Calvin (1993) extended this approach for some unbalanced \nmodels, using an EM algorithm. \nIn the unbalanced case, numerical simplifications exist for applying REML in groups \nof models typical to animal breeding (Dempster, Selwyn, Patel, and Roth 1984; Graser \net al. 1987; Smith and Graser 1986; Taylor, Bean, Marshall, and Sullivan 1985) and in \nrepeated-measures models (Jennrich and Schluchter 1986; Laird, Lange, and Stram 1987; \nLindstrom and Bates 1988). These methods are very good when they can be used. \nBut in general, REML can be very difficult to implement. Some have resorted to \nsparse-matrix inversion on supercomputers (Misztal 1990). Although matrix inversion is \nstill a very difficult proposition, Misztal and Perez-Enciso (1993) rediscovered an efficient \nstrategy (Takahashi, Fagan, and Chen 1973) for calculating some of the inverse elements \nand all of the elements necessary for performing an EM algorithm. \nExperience is being gained with the new strategies, and Meyer (1994) found that \nNewton-Raphson with forward differentiation works faster than derivative-free REML \nfor multivariate problems involving several variance-covariance parameters. However, \nthe derivative-free approach uses less memory and is more robust in the face of poor \nstarting values than Newton-Raphson. Comparisons with backward differentiation were \nnot made, but work is continuing. \n3.3 \nREML WITH UNEQUALLY \nSPACED TIME-SERIES DATA \nIn this section, time-series data are used to illustrate the Newton-Raphson algorithm \nfor REML. The analysis can be reproduced from FORTRAN software and data provided \nin the StatLib newsgroup (statlib@lib.stat.cmu.edu). The subroutines are very general, \nand can be borrowed by diverse applications where sparse algorithms are needed. \n142"},{"page":11,"text":"DIFFERENTIATION OF THE CHOLESKY ALGORITHM \nThe data (Fig. 1) are composed of 115 measurements of mean-daily relative humidity. \nThese readings were taken during the summer of 1993 near the Tuolumne River in the \nSan Joaquin Valley, California. Humidity measurements were made almost daily, but the \nseries contains seven missing values because of errors in record-keeping. Although it \nis entirely reasonable to treat the seven values as missing, the present approach uses a \ncontinuous time-series model that describes the series at irregular time steps. The theory \ncan be very complicated. For a discussion involving a state-space model and the Kalman \nfilter see Jones and Ackerson (1990). In what follows, however, only a simple time-series \nmodel is considered: \nYt \n= \nt + et \nZt = \nu + ph [zt-h - U] +t, \n(3.2) \nwhere Yt depicts the relative humidity at day or time t. This series is represented as a \ncomposite time series involving a first-order process (zt) and white noise (et). The resid- \nuals, et and Et, are assumed to be uncorrelated and normally distributed with variances \n2 and (1 - p2h)2. \nThe purpose of this illustration is to estimate the autocorrelation \np, and the two variances from the observed time series yt. The REML estimates are \ninvariant to the location parameter u. \nThe mixed model matrix for (3.2) is \nI+ aT- \n1 \ny \nM=(-2 \n1' \ns \nl'y \ny' \ny'l  y'y \nwhere I is the identity matrix, a = a2\/o2, aU2T is the variance matrix for zt (in ascending \norder of t), 1 is a vector of l's, y is a vector of all Yt, and s = 115 is the sample size. To \naccommodate missing values, it is feasible to leave gaps in I, 1, and y, but this approach \nwas not followed. \nTo simplify the calculations, write \nM = \n-2Mc, \ny'Py - \ne ,2y'Py, \nand \nC = a2Cc. \nThis allows a direct estimate of cr2 as y'Pcy\/(s- \nto retrieve a well-known function, the concentrated likelihood \n1) that can be plugged back into (3.1) \nlogL = -2 {logla-lT \n+ log|lC| + (s - \n)log(y'P,y)}, \n(3.3) \nwhere ?logjCc| is the sum of the logarithms of the first N - 1 diagonal elements of \nL(N = s + 2, L'L = Me), and 1log(y'Pcy) is the logarithm of the last diagonal of L. \nThe advantage of (3.3) is that it only depends on two of the parameters, a and p, and it \nserves as a starting point for Newton-Raphson. \nIf rows and columns of Mc are ordered as presented, L can be computed in time \nlinear in s, because T-1 has a very simple tridiagonal structure for both equally (Thomas \nand Wallis 1971) and unequally (Wade and Quaas 1993) spaced time steps. Moreover, \ncomputing and differentiating loglTI is trivial. \n143"},{"page":12,"text":"S. P. SMITH \nTable 1. Newton-Raphson Iteration \nIteration \na \np \n2 \na2 \nLog-likelihood (3.3) \nStart \n1 \n2 \n3 \n4 \n5 \n.3333 \n.2386 \n.2101 \n.2028 \n.2022 \n.2022 \n.7500 \n.8624 \n.8405 \n.8333 \n.8327 \n.8327 \n-494.9849 \n-493.9282 \n-493.8421 \n-493.8371 \n-493.8370 \n-493.8370 \n78.44 \n92.51 \n84.38 \n81.44 \n81.18 \n18.71 \n19.44 \n17.11 \n16.46 \n16.41 \nThe provided software employs the backwards calculations of Sections 2.3.1 and \n2.3.2 to find the needed derivatives at each step in Newton-Raphson. There are, however, \nminor modifications to the protocol. When backwards differentiation is applied to \nf(L) =-  {log|Ccj + (s - 1)log(y'Pcy)}, \n2 \nthe initializations for first derivatives (Step 1 in Section 2.3.1) are Fii = -1\/Li, \n1,... N-1, and FNN = -(S-1 )\/LNN. For second derivatives, (Step 3 in Section 2.3.2) \nthey are Sii = Sii \nQii\/L \ni = 1,... N- \nInclusion of white noise does complicate the analysis. For example, when p is zero, \nthe variances a2 and a2 cannot be separated by the likelihood. Moreover, the algorithm \nmay not converge, although this depends on starting values as well as the data. Aside \nfrom these problems, the Newton-Raphson algorithm is a real time calculation. With \nsuitable starting values, the software produced estimates that converged. These results \nare listed in Table 1. \ni = \n1, and SNN = SNN + (S - 1)QNN\/L2 N \n4. DISCUSSION \nThis article describes inexpensive ways to calculate derivatives that are used for \nREML. These can be calculated whenever it is easy to calculate the log-likelihood by \nfactorization. This increases the number of problems where it is realistic to perform \nREML. \nAlthough the strategies of Sections 2.2 and 2.3 show tremendous utility in variance \nestimation, other applications are also possible. The methods could aid in the search for \nD-optimal experimental designs by providing a way to calculate derivatives of IX'XI, \nwhere X is the design matrix (Bates 1983). However, other known algorithms deserve \ncareful consideration. For the case in point, Bates developed a different technique to \ncompute first derivatives of IX'XI, using the QR algorithm to decompose X directly. \nThe need to evaluate derivatives of the log-determinant is a recurring theme. There- \nfore, it is useful to consider how log-determinants are usually differentiated, to advance \nspeculation regarding the merits of algorithmic differentiation. The formulas given by \nSearle et al. (1992, pp. 456, 457) are listed in the following for a nonsingular matrix M, \nand these suggest standard approaches. \nOloglMI\/Ox = tr[M-'Mx] \n144 \n(4.1)"},{"page":13,"text":"DIFFERENTIATION \nOF THE CHOLESKY ALGORITHM \nand \n02loglMl\/0xOy \n= tr[M-'Mxy \n- M-lmxM-'My] \n(4.2) \nwhere Mx = AM\/Ox, My = OM\/9y, and Mxy = 02M\/&xay. \nThe sparse inverse in Takahashi et al. (1973) can be used to evaluate (4.1), and it turns \nout that this approach is as efficient as backward differentiation. Although Takahashi's \nalgorithm was not derived from properties of reverse-mode differentiation, there are \nstriking similarities. \nMore common approaches for differentiating loglMI can be derived through the \nalgebraic manipulation of (4.1) and (4.2). Simplifications occur because Mx and My \ncan usually be represented as Mx = HH' and My = TT' for some matrices H and \nT. Furthermore, when M is linear in x and y, Mxy is null and H and T are constant. \nThe commutative law applies for the trace operator, so (4.1) and (4.2) are reduced to \ntr[H'M-1H] and tr[T'M-IHH'M-'T]. \nA standard way to calculate (4.1) via tr[H'M-1H] is the following: \n1. Evaluate L, where LL' = M, using the Cholesky decomposition. \n2. Solve gi in Lgi = hi, where hi is the ith column of H. \n3. Accumulate the inner products, gigi for all i, to calculate the trace. \nThe standard approach uses little memory other than that needed for L. Moreover, \nthe matrix L can be evaluated using sparse-matrix techniques, and thus the traditional \nscheme appears to be very competitive. However, for sparse matrices, both forward and \nbackward differentiation can show significant advantages. For example, differentiating \nthe log-determinant of a tridiagonal matrix requires only linear time, but the traditional \nmethod may require quadratic time. Alternatively, the traditional approach is preferable \nwhen H has few columns, even for sparse M. \nCalculating (4.2) or tr[T'M-'HH'M-'T] \nbecause matrices like H'M-1T must be evaluated in full, if we interpret the algebra as \nwritten. The W transformation (Goodnight and Hemmerle 1979) can be applied to help \nevaluate these matrices, as in the case of variance component estimation. Programs can \ntake full advantage of the linked-list tool for sparse matrices (Tier and Smith 1989) in \nperforming the required forward Doolittle operations (Goodnight 1979). Nevertheless, for \nsparse matrices the traditional approach performs badly for second derivatives, because \nmatrices like L-1T or L-1H tend to lose their sparsity, and this implies that forming \nH'M-'T = (L-'H)'L-lT \nwill involve cubic work (assuming that T and H are almost \nfull row-rank). Note that L-1 is not sparse for tridiagonal matrix M, and for this case \nthe traditional approach requires cubic time to evaluate (4.2), whereas both forward and \nbackward differentiation require only linear time. \nThe conclusion is that algorithmic differentiation is useful when M is big but sparse, \nand easy to factorize. Traditional methods are best when Mx, My, and Mxy are low-rank \nmatrices with trivial factorizations. \nForward differentiation is less attractive than backward differentiation when deriva- \ntives are needed for a single function, such as log|MI in the previous discussion, and \nwhen M is a function of several parameters. The forward calculations are most useful \nby traditional means is more difficult, \n145"},{"page":14,"text":"S. P. SMITH \nwhen derivatives for numerous functions of L, say the vector f(L), are needed. An exam- \nple involves a function of the solution vector b, where Mb = r and there are numerous \nvectors to use as right-hand sides-that is, r. A combination approach provides one way \nto compute Of(b)\/Ox: evaluate OL\/Ox by the forward calculations, and for each r find \nOf(b)\/OLij(i > j) by the backward calculations. A combination algorithm has already \nbeen described in Section 2.3.2, where Q was found by the forward approach. \nACKNOWLEDGMENTS \nThe author thanks colleagues at EA Engineering, Science, and Technology and at the Animal Genetics \nand Breeding Unit (Australia) for providing support. The referees were very helpful. \n[Received October 1993. Revised January 1995.] \nREFERENCES \nBates, D. M. (1983), \"The Derivative of IX'XI and Its Uses,\" Technometrics, 25, 373-376. \nBoldman, K. G., and Van Vleck, L. D. (1991), \"Derivative-Free Restricted Maximum Likelihood Estimation \nin Animal Models with a Sparse Matrix Solver,\" Journal of Dairy Science, 74, 4337-4343. \nCalvin, J. A. (1993), \"REML Estimation in Unbalanced Multivariate Variance Components Models using an \nEM Algorithm,\" Biometrics, 49, 691-701. \nCalvin, J. A., and Dykstra, R. L. (1991), \"Maximum Likelihood Estimation of a Set of Covariance Matri- \nces Under Lower Order Restrictions with Applications to Balanced Multivariate Variance Components \nModels,\" The Annals of Statistics, 19, 850-869. \nCressie, N. (1991), Statistics for Spatial Data, New York: John Wiley. \nDempster, A. P., Selwyn, M. R., Patel, C. M., and Roth, A. J. (1984), \"Statistical and Computational Aspects \nof Mixed Model Analysis,\" Applied Statistics, 33, 203-214. \nFuller, W. A. (1976), Introduction to Statistical Time Series, New York: John Wiley. \nGeorge, A., and Liu, J. W-H. (1981), Computer Solution of Large Sparse Positive Definite Systems, Englewood \nCliffs, NJ: Prentice-Hall. \nGolub, G. H., and Van Loan, C. F. (1983), Matrix Computation, Baltimore, MD: The John Hopkins University \nPress. \nGoodnight, J. H. (1979), \"A Tutorial on the Sweep Operator,\" The American Statistician, 33, 149-158. \nGoodnight, J. H., and Hemmerle, W. J. (1979), \"A Simplified Algorithm for the W-Transformation in Variance \nComponent Estimation,\" Technometrics, 21, 265-268. \nGraser, H.-U., Smith, S. P., and Tier, B. (1987), \"A Derivative Free Approach for Estimating Variance Com- \nponents in Animal Models by REML,\" Journal of Animal Science, 64, 1362-1370. \nGriewank, A., and Corliss, G. F., (eds.) (1991), Automatic Differentiation of Algorithms: Theory, Implementa- \ntion, and Application, Philadelphia: SIAM. \nHarville, D. A. (1977), \"Maximum Likelihood Approaches to Variance Component Estimation and to Related \nProblems,\" Journal of the American Statistical Association, 72, 320-340. \nHarville, D. A., and Callanan, T. P. (1990), \"Computational Aspects of Likelihood-Based Inference for Vari- \nance Components,\" in Statistical Methods for Genetic Improvement of Livestock, eds. D. Gianola and K. \nHammond, New York: Springer-Verlag, pp. 136-176. \nHenderson, H. V., and Searle, S. R. (1981), \"On Deriving the Inverse of a Sum of Matrices,\" SIAM Review, \n23, 53-60. \nIri, M. (1991), \"History of Automatic Differentiation and Rounding Error Estimation,\" in Automatic Differ- \nentiation of Algorithms: Theory, Implementation, and Application, eds. A. Griewank and G. F. Corliss, \nPhiladelphia: SIAM, pp. 3-24. \n146"},{"page":15,"text":"DIFFERENTIATION  OF THE CHOLESKY ALGORITHM \nJennrich, R. I., and Schluchter, M. D. (1986), \"Unbalanced Repeated-Measures Models with Structured Co- \nvariance Matrices,\" Biometrics, 42, 805-820. \nJones, R. H., and Ackerson, L. M. (1990), \"Serial Correlation in Unequally Spaced Longitudinal Data,\" \nBiometrika, 77, 721-731. \nJones, R. H., and Vecchia, A. V. (1993), \"Fitting Continuous ARMA Models to Unequally Spaced Spatial \nData,\" Journal of the American Statistical Association, 88, 947-954. \nLaird, N. M., Lange, N., and Stram, D. (1987), \"Maximum Likelihood Computations with Repeated Measures: \nApplication of the EM Algorithm,\" Journal of the American Statistical Association, 82, 97-105. \nLindstrom, M. J., and Bates, D. M. (1988), \"Newton-Raphson and EM Algorithms for Linear Mixed-Effects \nModels for Repeated Measures Data,\" Journal of the American Statistical Association, 83, 1014-1022. \nMeyer, K. (1989), \"Restricted Maximum Likelihood to Estimate Variance Components for Animal Models \nwith Several Random Effects Using a Derivative-Free Algorithm,\" Genetique Selection Evolution, 21, \n317-340. \n- \n(1991), \"Estimating Variances and Covariances for Multivariate Animal Models by Restricted Maximum \nLikelihood,\" Genetique Selection Evolution, 23, 67-83. \n(1994), \"Derivative-Intense Restricted Maximum Likelihood Estimation of Covariance Components for \nAnimal Models,\" in Proceedings of the 5th World Congress on Genetics Applied to Livestock Production, \npp. 365-369. \nMisztal, I. (1990), \"Restricted Maximum Likelihood Estimation of Variance Components in Animal Models \nUsing Sparse Matrix Inversion and a Supercomputer,\" Journal of Dairy Science, 73, 163-172. \nMisztal, I., and Perez-Enciso, M. (1993), \"Sparse Matrix Inversion for Restricted Maximum Likelihood Estima- \ntion of Variance Components by Expectation-Maximization,\" Journal of Dairy Science, 76, 1479-1483. \nPatterson, H. D., and Thompson, R. (1971), \"Recovery of Inter-Block Information When Block Sizes are \nUnequal,\" Biometrika, 58, 545-554. \nPress, W. H., Teukolsky, S. A., Vetterling, W. T., and Flannery, B. P. (1992), Numerical Recipes in Fortran \n(2nd ed.), Cambridge: Cambridge University Press. \nQuaas, R. L. (1988), \"Additive Genetic Model with Groups and Relationships,\" Journal of Dairy Science, 71, \n1338-1347. \nRobinson, G. K. (1991), \"That BLUP is a Good Thing: The Estimation of Random Effects,\" Statistical Science, \n6, 15-51. \nSearle, S. R. (1989), \"Variance Components-Some History and a Summary Account of Estimation Methods,\" \nJournal of Animal Breeding and Genetics, 106, 1-29. \nSearle, S. R., Casalla, G., McCulloch, C. E. (1992), Variance Components, New York: John Wiley. \nSmith, S. P., and Graser, H.-U. (1986), \"Estimating Variance Components in a Class of Mixed Models by \nRestricted Maximum Likelihood,\" Journal of Dairy Science, 69, 1156-1165. \nSmith, S. P., and Lin, C. Y. (1989), \"Efficient Implementation of the New REML Algorithms,\" Journal of \nDairy Science, 72, 3336-3341. \nTakahashi, K., Fagan, J., and Chen, M. (1973), \"Formation of a Sparse Bus Impedance Matrix and its Ap- \nplication to Short Circuit Study,\" in 8th Power Industry Computer Applications Conference, New York: \nIEEE, 63-69. \nTaylor, J. F., Bean, B., Marshall, C. E., and Sullivan, J. J. (1985), \"Genetic and Environmental Components \nof Semen Production Traits of Artificial Insemination Holstein Bulls,\" Journal of Dairy Science, 68, \n2703-2722. \nThomas, J. J., Wallis, K. F. (1971), \"Season Variation in Regression Analysis,\" Journal of the Royal Statistical \nSociety, Ser. A, 134, 57-72. \nTier, B., and Smith, S. P. (1989), \"Use of Sparse Matrix Absorption in Animal Breeding,\" Genetique Selection \nEvolution, 21, 457-466. \nWade, K. M., and Quaas, R. L. (1993), \"Solutions to a System of Equations Involving a First-Order Autore- \ngressive Process,\" Journal of Dairy Science, 76, 3026-3032. \n147"}],"widgetId":"rgw25_56ab1e9180e1f"},"id":"rgw25_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=239030086&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56ab1e9180e1f"},"id":"rgw26_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=239030086&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":239030086,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":239030086,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":5611721,"url":"researcher\/5611721_Karin_Meyer","fullname":"Karin Meyer","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Dec 2007","journal":"Journal of Zhejiang University SCIENCE B","showEnrichedPublicationItem":false,"citationCount":193,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML","usePlainButton":true,"publicationUid":5873674,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.28","url":"publication\/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML","title":"WOMBAT - A tool for mixed model analyses in quantitative genetics by REML","displayTitleAsLink":true,"authors":[{"id":5611721,"url":"researcher\/5611721_Karin_Meyer","fullname":"Karin Meyer","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Zhejiang University SCIENCE B 12\/2007; 8(11):815-21. DOI:10.1631\/jzus.2007.B0815"],"abstract":"WOMBAT is a software package for quantitative genetic analyses of continuous traits, fitting a linear, mixed model; estimates of covariance components and the resulting genetic parameters are obtained by restricted maximum likelihood. A wide range of models, comprising numerous traits, multiple fixed and random effects, selected genetic covariance structures, random regression models and reduced rank estimation are accommodated. WOMBAT employs up-to-date numerical and computational methods. Together with the use of efficient compilers, this generates fast executable programs, suitable for large scale analyses. Use of WOMBAT is illustrated for a bivariate analysis. The package consists of the executable program, available for LINUX and WINDOWS environments, manual and a set of worked example, and can be downloaded free of charge from (http:\/\/agbu. une.edu.au\/~kmeyer\/wombat.html).","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Karin_Meyer2\/publication\/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML\/links\/02e7e522258ceb1362000000.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Karin_Meyer2","sourceName":"Karin Meyer","hasSourceUrl":true},"publicationUid":5873674,"publicationUrl":"publication\/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML\/links\/02e7e522258ceb1362000000\/smallpreview.png","linkId":"02e7e522258ceb1362000000","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=5873674&reference=02e7e522258ceb1362000000&eventCode=&origin=publication_list","widgetId":"rgw30_56ab1e9180e1f"},"id":"rgw30_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=5873674&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"02e7e522258ceb1362000000","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":239030086,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/5873674_WOMBAT_-_A_tool_for_mixed_model_analyses_in_quantitative_genetics_by_REML\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["The default algorithm used by WOMBAT to locate the maximum of the likelihood function is the so-called 'average information' algorithm, developed by Thompson and co-workers (Thompson et al., 2005). This is implemented using an automatic differentiation (Smith, 1995) rather than sparse matrix inversion of the coefficient matrix in the mixed model equations. To ensure an increase in the likelihood in each iterate, step sizes are scaled if necessary, using the backtracking line search of Dennis and Schnabel (1996) to determine the optimum scale factor. "],"widgetId":"rgw31_56ab1e9180e1f"},"id":"rgw31_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw29_56ab1e9180e1f"},"id":"rgw29_56ab1e9180e1f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=5873674&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextSlurp","nextPublicationViewId":null,"authorsPartOne":[{"id":10041375,"url":"researcher\/10041375_Robin_Thompson","fullname":"Robin Thompson","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A305539672674305%401449857742658_m"},{"id":2071908322,"url":"researcher\/2071908322_Sue_Brotherstone","fullname":"Sue Brotherstone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":15784675,"url":"researcher\/15784675_Ian_M_S_White","fullname":"Ian M S White","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Aug 2005","journal":"Philosophical Transactions of The Royal Society B Biological Sciences","showEnrichedPublicationItem":false,"citationCount":56,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/7696504_Estimation_of_quantitative_genetic_parameters","usePlainButton":true,"publicationUid":7696504,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"7.06","url":"publication\/7696504_Estimation_of_quantitative_genetic_parameters","title":"Estimation of quantitative genetic parameters","displayTitleAsLink":true,"authors":[{"id":10041375,"url":"researcher\/10041375_Robin_Thompson","fullname":"Robin Thompson","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A305539672674305%401449857742658_m"},{"id":2071908322,"url":"researcher\/2071908322_Sue_Brotherstone","fullname":"Sue Brotherstone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":15784675,"url":"researcher\/15784675_Ian_M_S_White","fullname":"Ian M S White","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Philosophical Transactions of The Royal Society B Biological Sciences 08\/2005; 360(1459):1469-77. DOI:10.1098\/rstb.2005.1676"],"abstract":"This paper gives a short review of the development of genetic parameter estimation over the last 40 years. This shows the development of more statistically and computationally efficient methods that allow the fitting of more biologically appropriate models. Methods have evolved from direct methods based on covariances between relatives to methods based on individual animal models. Maximum-likelihood methods have a natural interpretation in terms of best linear unbiased predictors. Improvements in iterative schemes to give estimates are discussed. As an example, a recent estimation of genetic parameters for a British population of dairy cattle is discussed. The development makes a connection to relevant work by Bill Hill.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/7696504_Estimation_of_quantitative_genetic_parameters","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Robin_Thompson5\/publication\/7696504_Estimation_of_quantitative_genetic_parameters\/links\/0c96052265e58bbd9c000000.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Robin_Thompson5","sourceName":"Robin Thompson","hasSourceUrl":true},"publicationUid":7696504,"publicationUrl":"publication\/7696504_Estimation_of_quantitative_genetic_parameters","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/7696504_Estimation_of_quantitative_genetic_parameters\/links\/0c96052265e58bbd9c000000\/smallpreview.png","linkId":"0c96052265e58bbd9c000000","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=7696504&reference=0c96052265e58bbd9c000000&eventCode=&origin=publication_list","widgetId":"rgw33_56ab1e9180e1f"},"id":"rgw33_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=7696504&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"0c96052265e58bbd9c000000","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":239030086,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/7696504_Estimation_of_quantitative_genetic_parameters\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["These were an improvement on derivative-free methods, but could still be slow to converge. It is possible to calculate second differentials using automatic differentiation (Smith 1995), but the computation of each second differential requires six times as many multiplications as those involved in a single likelihood calculation (Smith 1995), and this becomes more costly as the number of parameters increases. There are various suggestions on approximating the second differential. "],"widgetId":"rgw34_56ab1e9180e1f"},"id":"rgw34_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw32_56ab1e9180e1f"},"id":"rgw32_56ab1e9180e1f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=7696504&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2054581511,"url":"researcher\/2054581511_Robin_Thompson","fullname":"Robin Thompson","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A305539672674305%401449857742658_m"},{"id":2054592430,"url":"researcher\/2054592430_Esa_Mantysaari","fullname":"Esa Mantysaari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Jan 2004","journal":null,"showEnrichedPublicationItem":false,"citationCount":2,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/265943768_Prospects_for_statistical_methods_in_animal_breeding","usePlainButton":true,"publicationUid":265943768,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/265943768_Prospects_for_statistical_methods_in_animal_breeding","title":"Prospects for statistical methods in animal breeding","displayTitleAsLink":true,"authors":[{"id":2054581511,"url":"researcher\/2054581511_Robin_Thompson","fullname":"Robin Thompson","last":false,"imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A305539672674305%401449857742658_m"},{"id":2054592430,"url":"researcher\/2054592430_Esa_Mantysaari","fullname":"Esa Mantysaari","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["01\/2004; 57(4)."],"abstract":"Accurate prediction of breeding values is of great importance for animal improvement programmes. The prediction of breeding values requires knowledge of the magnitude of the variances and covariances of random effects. This paper gives a short review of methods of estimation of genetic variance parameters, contrasting analytical estimates with iterative and sampling based methods.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/265943768_Prospects_for_statistical_methods_in_animal_breeding","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Esa_Maentysaari2\/publication\/265943768_Prospects_for_statistical_methods_in_animal_breeding\/links\/557ac30408ae8d04819315d5.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Esa_Maentysaari2","sourceName":"Esa M\u00e4ntysaari","hasSourceUrl":true},"publicationUid":265943768,"publicationUrl":"publication\/265943768_Prospects_for_statistical_methods_in_animal_breeding","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/265943768_Prospects_for_statistical_methods_in_animal_breeding\/links\/557ac30408ae8d04819315d5\/smallpreview.png","linkId":"557ac30408ae8d04819315d5","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=265943768&reference=557ac30408ae8d04819315d5&eventCode=&origin=publication_list","widgetId":"rgw36_56ab1e9180e1f"},"id":"rgw36_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=265943768&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"557ac30408ae8d04819315d5","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":239030086,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/265943768_Prospects_for_statistical_methods_in_animal_breeding\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["These were an improvement on derivative free methods but could still be slow to converge. It is possible to calculate second differentials using the automatic differentation ideas of Smith [34] but the calculation of each second differential requires the computation of the order of six likelihood calculations (Smith [34]) and this becomes Illore costly as the number of parameters increase. There are various suggestions on approxmating the second differential. "],"widgetId":"rgw37_56ab1e9180e1f"},"id":"rgw37_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw35_56ab1e9180e1f"},"id":"rgw35_56ab1e9180e1f","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=265943768&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":239030086,"publicationLink":"publication\/239030086_Differentiation_of_the_Cholesky_Algorithm","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw28_56ab1e9180e1f"},"id":"rgw28_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=239030086&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=38","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":38,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw27_56ab1e9180e1f"},"id":"rgw27_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=239030086&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/239030086_Differentiation_of_the_Cholesky_Algorithm","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab1e9180e1f"},"id":"rgw2_56ab1e9180e1f","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":239030086},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=239030086&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab1e9180e1f"},"id":"rgw1_56ab1e9180e1f","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"FOn3\/McQ8bcCp45f7CLNCgE4uBJky3bnlIiAdIgNQRdsaOFBDth9x5UZMfLUzsLM7t\/ZAcWpDCV1tmJ8EcpXvSLGqwkv78YnTH\/aqu\/clUiPP0n9n\/XOGNgcqQz\/3\/38ciicYdjbH2gageZff58ziseP0CF2F0tH0pPIweXgUoGvJ\/sq0dK\/t\/GjOXe4Kumf0R8xsU8D2cXR6IkA+0G4wE82OMpPJZ72nVZESe2DaId3piG8aqMeuAQW+npRVww1zksGN8mMCPv1Hp1ruurDWpZOQVJk6BYpvAoz5xKqAz0=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/239030086_Differentiation_of_the_Cholesky_Algorithm\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Differentiation of the Cholesky Algorithm\" \/>\n<meta property=\"og:description\" content=\"One way to estimate variance components is by restricted maximum likelihood. The log-likelihood function is fully defined by the Cholesky factor of a matrix that is usually large and sparse. In...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/239030086_Differentiation_of_the_Cholesky_Algorithm\/links\/02dd4f420cf23be0d317224f\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/239030086_Differentiation_of_the_Cholesky_Algorithm\" \/>\n<meta property=\"rg:id\" content=\"PB:239030086\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1080\/10618600.1995.10474671\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Differentiation of the Cholesky Algorithm\" \/>\n<meta name=\"citation_author\" content=\"S. P. Smith\" \/>\n<meta name=\"citation_publication_date\" content=\"1995\/06\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"Journal of Computational and Graphical Statistics\" \/>\n<meta name=\"citation_issn\" content=\"1061-8600\" \/>\n<meta name=\"citation_volume\" content=\"4\" \/>\n<meta name=\"citation_issue\" content=\"2\" \/>\n<meta name=\"citation_firstpage\" content=\"134\" \/>\n<meta name=\"citation_lastpage\" content=\"147\" \/>\n<meta name=\"citation_doi\" content=\"10.1080\/10618600.1995.10474671\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/239030086_Differentiation_of_the_Cholesky_Algorithm\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/239030086_Differentiation_of_the_Cholesky_Algorithm\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-10d010ea-1641-4526-ae2e-49eac12a0358","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":1010,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56ab1e9180e1f"},"id":"rgw38_56ab1e9180e1f","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-10d010ea-1641-4526-ae2e-49eac12a0358", "4d4a2d3e7863aa1212f44badbc9a798364fd177a");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-10d010ea-1641-4526-ae2e-49eac12a0358", "4d4a2d3e7863aa1212f44badbc9a798364fd177a");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56ab1e9180e1f"},"id":"rgw39_56ab1e9180e1f","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/239030086_Differentiation_of_the_Cholesky_Algorithm","requestToken":"xHBtyzdY4T9bGi5pYSzA0cvSr\/3IrsUQnGFDJniunfdXahIaZPsaGprGEuwtuVuEFmBBUlQK2Sk9XBEKtsmlaa69VwA2J8PEvCX1hiyHeUaBbkteh483uY6lBIpDdKfyTkR7IuaKJyU5Gqq8QUuc2RobGnSrvQCIOy48VP3b3c0fYXQrD3ZzuTHkRHxZIcZ+U3mm0Vraxm3GdT7DBxT5KWD6i61F7\/dbiRXZKGM3QFbzIsy5EXpZ4\/MqyVTg\/hBtE4sKdGHLDw3CUIxSayglEOSO7gJMAQyUwYvIcS\/kAJ0=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=ADESLmQ-fWV0zp5jMT0fC18hxd6bOCEPt-CPMaePyJ0AmrRZVjKwcBQUZ0jNUDTB","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjM5MDMwMDg2X0RpZmZlcmVudGlhdGlvbl9vZl90aGVfQ2hvbGVza3lfQWxnb3JpdGht","signupCallToAction":"Join for free","widgetId":"rgw41_56ab1e9180e1f"},"id":"rgw41_56ab1e9180e1f","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56ab1e9180e1f"},"id":"rgw40_56ab1e9180e1f","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56ab1e9180e1f"},"id":"rgw42_56ab1e9180e1f","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
