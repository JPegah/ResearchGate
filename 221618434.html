<!DOCTYPE html> <html lang="en" class="" id="rgw40_56aba0965207a"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="SEd/vvRiwmGyQFcBMxFWkdSz1NaTM6PSzpKPeonBDIko/dlCGwVTeT92MiKeM+Bpaigrf2O+TLmwxD/JCXFs6NsR0lV4mxCOdM7onouB2WFneeQXUR4WTkCXjcRAI59Q16v7UUW44uw+R6iPuJW4s61kbSB5pjO09xnNon/BsIig1ipUtBwpax7BrOXPiUzX+mFTowuFTL/IzSy9ScB/3DAmGzbTCOhIQzvv9T6JRGd2HxmCxMJdFdjP9kKJpXzTPyoQPZNiWztw5b9tJzWQQ7In4n271owjWpZ5Deu04U0="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-2164d29b-fa8c-406a-91cf-3a7cb0060870",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Fast Sparse Gaussian Process Methods: The Informative Vector Machine" />
<meta property="og:description" content="Abstract We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine" />
<meta property="rg:id" content="PB:221618434" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Fast Sparse Gaussian Process Methods: The Informative Vector Machine" />
<meta name="citation_author" content="Neil D. Lawrence" />
<meta name="citation_author" content="Matthias Seeger" />
<meta name="citation_author" content="Ralf Herbrich" />
<meta name="citation_conference_title" content="Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]" />
<meta name="citation_publication_date" content="2002/01/01" />
<meta name="citation_volume" content="15" />
<meta name="citation_firstpage" content="609" />
<meta name="citation_lastpage" content="616" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Fast Sparse Gaussian Process Methods: The Informative Vector Machine (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Fast Sparse Gaussian Process Methods: The Informative Vector Machine on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba0965207a" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba0965207a" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba0965207a">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Fast%20Sparse%20Gaussian%20Process%20Methods%3A%20The%20Informative%20Vector%20Machine&rft.title=Adv%20NIPS&rft.jtitle=Adv%20NIPS&rft.volume=15&rft.date=2002&rft.pages=609-616&rft.au=Neil%20D.%20Lawrence%2CMatthias%20Seeger%2CRalf%20Herbrich&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">Fast Sparse Gaussian Process Methods: The Informative Vector Machine</h1> <meta itemprop="headline" content="Fast Sparse Gaussian Process Methods: The Informative Vector Machine">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000/smallpreview.png">  <div id="rgw7_56aba0965207a" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba0965207a"> <a href="researcher/39663468_Neil_D_Lawrence" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Neil D. Lawrence" alt="Neil D. Lawrence" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Neil D. Lawrence</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw9_56aba0965207a">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/39663468_Neil_D_Lawrence"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Neil D. Lawrence" alt="Neil D. Lawrence" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/39663468_Neil_D_Lawrence" class="display-name">Neil D. Lawrence</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56aba0965207a" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Matthias_Seeger" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272524646809639%401441986347920_m" title="Matthias Seeger" alt="Matthias Seeger" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Matthias Seeger</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw11_56aba0965207a" data-account-key="Matthias_Seeger">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Matthias_Seeger"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272524646809639%401441986347920_l" title="Matthias Seeger" alt="Matthias Seeger" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Matthias_Seeger" class="display-name">Matthias Seeger</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Ecole_Polytechnique_Federale_de_Lausanne" title="École Polytechnique Fédérale de Lausanne">École Polytechnique Fédérale de Lausanne</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56aba0965207a"> <a href="researcher/3234005_Ralf_Herbrich" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Ralf Herbrich" alt="Ralf Herbrich" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ralf Herbrich</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56aba0965207a">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/3234005_Ralf_Herbrich"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ralf Herbrich" alt="Ralf Herbrich" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/3234005_Ralf_Herbrich" class="display-name">Ralf Herbrich</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">         Conference: Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]      <div class="pub-source"> Source: <a href="http://dblp.uni-trier.de/db/conf/nips/nips2002.html#LawrenceSH02" rel="nofollow">DBLP</a> </div>  </div> <div id="rgw14_56aba0965207a" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>Abstract We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d{sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory,requirements. The scaling of our method is at most O(n d,), and in large real-world classication experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signican tly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (&lsquo;error bars&rsquo;), allows for Bayesian model selection and is less complex in implementation.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw15_56aba0965207a" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw31_56aba0965207a">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw32_56aba0965207a">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Matthias_Seeger">Matthias Seeger</a>, <span class="js-publication-date"> Nov 15, 2012 </span>   </span>  <select class="publication-version-select rf js-publication-version-select" data-placeholder="View other sources"> <option> </option>  <optgroup label="Matthias Seeger"> <option value="https://www.researchgate.net/profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail"  data-date="Nov 15, 2012 " data-name="Matthias Seeger" data-hash="dd2a32ee0f57e685bcc581f78b5f7b65" data-viewer-url="https://www.researchgate.net/profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000.pdf?inViewer=1&amp;pdfJsDownload=1&amp;origin=publication_detail" data-name-link="profile/Matthias_Seeger" data-link-id="0fcfd50a4b79c5968f000000"> download.pdf (131.41 KB) </option> </optgroup>  <optgroup label="Matthias Seeger"> <option value="https://www.researchgate.net/profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79987036000000.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail"  data-date="" data-name="Matthias Seeger" data-hash="b9c0f0b408b96d7cd927edaa72c8c6b2" data-viewer-url="https://www.researchgate.net/profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79987036000000.pdf?inViewer=1&amp;pdfJsDownload=1&amp;origin=publication_detail" data-name-link="profile/Matthias_Seeger" data-link-id="0fcfd50a4b79987036000000">  (131.41 KB) </option> </optgroup>  </select>  </div>  <div class="social-share-container"><div id="rgw34_56aba0965207a" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw35_56aba0965207a" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw36_56aba0965207a" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw37_56aba0965207a" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw38_56aba0965207a" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw39_56aba0965207a" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw33_56aba0965207a" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMatthias_Seeger%2Fpublication%2F221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine%2Flinks%2F0fcfd50a4b79c5968f000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw30_56aba0965207a"  itemprop="articleBody">  <p>Page 1</p> <p>Fast Sparse Gaussian Process Methods:<br />The Informative Vector Machine<br />Neil Lawrence<br />University of Sheffield<br />211 Portobello Street<br />Sheffield, S1 4DP<br />neil@dcs.shef.ac.uk<br />Matthias Seeger<br />University of Edinburgh<br />5 Forrest Hill<br />Edinburgh, EH1 2QL<br />seeger@dai.ed.ac.uk<br />Ralf Herbrich<br />Microsoft Research Ltd<br />7 J J Thomson Avenue<br />Cambridge, CB3 0FB<br />rherb@microsoft.com<br />Abstract<br />We present a framework for sparse Gaussian process (GP) methods<br />which uses forward selection with criteria based on information-<br />theoretic principles, previously suggested for active learning. Our<br />goal is not only to learn d–sparse predictors (which can be evalu-<br />ated in O(d) rather than O(n), d ? n, n the number of training<br />points), but also to perform training under strong restrictions on<br />time and memory requirements. The scaling of our method is at<br />most O(n · d2), and in large real-world classification experiments<br />we show that it can match prediction performance of the popular<br />support vector machine (SVM), yet can be significantly faster in<br />training. In contrast to the SVM, our approximation produces esti-<br />mates of predictive probabilities (‘error bars’), allows for Bayesian<br />model selection and is less complex in implementation.<br />1 Introduction<br />Gaussian process (GP) models are powerful non-parametric tools for approximate<br />Bayesian inference and learning. In comparison with other popular nonlinear ar-<br />chitectures, such as multi-layer perceptrons, their behavior is conceptually simpler<br />to understand and model fitting can be achieved without resorting to non-convex<br />optimization routines. However, their training time scaling of O(n3) and memory<br />scaling of O(n2), where n the number of training points, has hindered their more<br />widespread use. The related, yet non-probabilistic, support vector machine (SVM)<br />classifier often renders results that are comparable to GP classifiers w.r.t. prediction<br />error at a fraction of the training cost. This is possible because many tasks can<br />be solved satisfactorily using sparse representations of the data set. The SVM is<br />triggered towards finding such representations through the use of a particular loss<br />function1that encourages some degree of sparsity, i.e. the final predictor depends<br />only on a fraction of training points crucial for good discrimination on the task.<br />Here, we call these utilized points the active set of the sparse predictor. In case of<br />SVM classification, the active set contains the support vectors, the points closest to<br />1An SVM classifier is trained by minimizing a regularized loss functional, a process<br />which cannot be interpreted as approximation to Bayesian inference.</p>  <p>Page 2</p> <p>the decision boundary and the misclassified ones. If the active set size d is much<br />smaller than n, an SVM classifier can be trained in average case running time be-<br />tween O(n · d2) and O(n2· d) with memory requirements significantly less than n2.<br />Note, however, that without any restrictions on the data distribution, d can rise to<br />n.<br />In an effort to overcome scaling problems a range of sparse GP approximations have<br />been proposed [1, 8, 9, 10, 11]. However, none of these has fully achieved the goals of<br />being a nontrivial approximation to a non-sparse GP model and matching the SVM<br />w.r.t. both prediction performance and run time. The algorithm proposed here ac-<br />complishes these objectives and, as our experiments show, can even be significantly<br />faster in training than the SVM. Furthermore, time and memory requirements may<br />be restricted a priori. The potential benefits of retaining the probabilistic charac-<br />teristics of the method are numerous, since hard problems, e.g. feature and model<br />selection, can be dealt with using standard techniques from Bayesian learning.<br />Our approach builds on earlier work of Lawrence and Herbrich [2] which we extend<br />here by considering randomized greedy selections and focusing on an alternative<br />representation of the GP model which facilitates generalizations to settings such<br />as regression and multi-class classification. In the next section we introduce the<br />GP classification model and a method for approximate inference. Section 3 then<br />contains the derivation of our fast greedy approximation and a description of the as-<br />sociated algorithm. In Section 4, we present large-scale experiments on the MNIST<br />database, comparing our method directly against the SVM. Finally we close with a<br />discussion in Section 5.<br />We denote vectors g = (gi)i and matrices G = (gi,j)i,j in bold-face2.<br />are sets of row and column indices respectively, we denote the corresponding sub-<br />matrix of G ∈ Rp,qby GI,J, furthermore we abbreviate GI,· to GI,1...q, GI,j to<br />GI,{j}, GIto GI,I, etc. The density of the Gaussian distribution with mean µ and<br />covariance matrix Σ is denoted by N(·|µ,Σ). Finally, we use diag(·) to represent an<br />‘overloaded’ operator which extracts the diagonal elements of a matrix as a vector<br />or produces a square matrix with diagonal elements from a given vector, all other<br />elements 0.<br />If I,J<br />2 Gaussian Process Classification<br />Assume we are given a sample S := ((x1,y1),...,(xn,yn)), xi∈ X, yi∈ {−1,+1},<br />drawn independently and identically distributed (i.i.d.) from an unknown data dis-<br />tribution3P(x,y). Our goal is to estimate P(y|x) for typical x or, less ambitiously,<br />to learn a predictor x → y with small error on future data. To model this situation,<br />we introduce a latent variable u ∈ R separating x and y, and some classification<br />noise model P(y|u) := Φ(y·(u+b)), where Φ is the cumulative distribution function<br />of the standard Gaussian N(0,1), and b ∈ R is a bias parameter. From the Bayesian<br />viewpoint, the relationship x → u is a random process u(·), which, in a Gaussian<br />process (GP) model, is given a GP prior with mean function 0 and covariance kernel<br />k(·,·). This prior encodes the belief that (before observing any data) for any finite<br />set X = {˜ x1,..., ˜ xp} ⊂ X, the corresponding latent outputs (u(˜ x1),...,u(˜ xp))T<br />are jointly Gaussian with mean 0 ∈ Rpand covariance matrix (k(˜ xi, ˜ xj))i,j∈ Rp,p.<br />GP models are non-parametric, that is, there is in general no finite-dimensional<br />2Whenever we use a bold symbol g or G for a vector or matrix, we denote its compo-<br />nents by the corresponding normal symbols gi and gi,j.<br />3We focus on binary classification, but our framework can be applied straightforwardly<br />to regression estimation and multi-class classification.</p>  <p>Page 3</p> <p>parametric representation for u(·). It is possible to write u(·) as linear function<br />in some feature space F associated with k, i.e. u(x) = wTφ(x), w ∈ F, in the<br />sense that a Gaussian prior on w induces a GP distribution on the linear function<br />u(·). Here, φ is a feature map from X into F, and the covariance function can be<br />written k(x,x?) = φ(x)Tφ(x?). This linear function view, under which predictors<br />become separating hyper-planes in F, is frequently used in the SVM community.<br />However, F is, in general, infinite-dimensional and not uniquely determined by<br />the kernel function k. We denote the sequence of latent outputs at the training<br />points by u := (u(x1),...,u(xn))T∈ Rnand the covariance or kernel matrix by<br />K := (k(xi,xj))i,j∈ Rn,n.<br />The Bayesian posterior process for u(·) can be computed in principle using Bayes’<br />formula. However, if the noise model P(y|u) is non-Gaussian (as is the case for<br />binary classification), it cannot be handled tractably and is usually approximated<br />by another Gaussian process, which should ideally preserve mean and covariance<br />function of the former. It is easy to show that this is equivalent to fitting the mo-<br />ments between the finite-dimensional (marginal) posterior P(u|S) over the train-<br />ing points and a Gaussian approximation Q(u), because the conditional posterior<br />P(u(x∗)|u,S) for some non-training point x∗is identical to the conditional prior<br />P(u(x∗)|u). In general, computing Q is also infeasible, but several authors have<br />proposed to approximate the global moment matching by iterative schemes which<br />locally focus on one training pattern at a time [1, 4]. These schemes (at least in<br />their simplest forms) result in a parametric form for the approximating Gaussian<br />Q(u) ∝ P(u)<br />n<br />?<br />i=1<br />exp<br />?<br />−pi<br />2(ui− mi)2?<br />.<br />(1)<br />This may be compared with the form of the true posterior P(u|S)<br />P(u)?n<br />called sites. Initially, all pi,miare 0, thus Q(u) = P(u). In order to update the<br />parameters for a site i, we replace it in Q(u) by the corresponding true likelihood<br />factor P(yi|ui), resulting in a non-Gaussian distribution whose mean and covari-<br />ance matrix can still be computed. This allows us to approximate it by a Gaussian<br />Qnew(u) using moment matching. The site update is called the inclusion of i into<br />the active set I. The factorized form of the likelihood implies that the new and old<br />Q differ only in the parameters pi,miof site i. This is a useful locality property of<br />the scheme which is referred to as assumed density filtering (ADF) (e.g. [4]). The<br />special case of ADF4for GP models has been proposed in [5].<br />∝<br />i=1P(yi|ui) and shows that Q(u) is obtained from P(u|S) by a likelihood<br />approximation. Borrowing from graphical models vocabulary, the factors in (1) are<br />3 Sparse Gaussian Process Classification<br />The simplest way to obtain a sparse Gaussian process classification (GPC) approx-<br />imation from the ADF scheme is to leave most of the site parameters at 0, i.e.<br />pi= 0, mi= 0 for all i ?∈ I, where I ⊂ {1,...,n} is the active set, |I| =: d &lt; n. For<br />this to succeed, it is important to choose I so that the decision boundary between<br />classes is represented essentially as accurately as if we used the whole training set.<br />An exhaustive search over all possible subsets I is, of course, intractable. Here, we<br />follow a greedy approach suggested in [2], including new patterns one at a time into<br />I. The selection of a pattern to include is made by computing a score function for<br />4A generalization of ADF, expectation propagation (EP) [4], allows for several iterations<br />over the data. In the context of sparse approximations, it allows us to remove points from<br />I or exchange them against such outside I, although we do not consider such moves here.</p>  <p>Page 4</p> <p>Algorithm 1 Informative vector machine algorithm<br />Require: A desired sparsity d ? n.<br />I = ∅, m = 0, Π = diag(0), diag(A) = diag(K), h = 0, J = {1,...,n}.<br />repeat<br />for j ∈ J do<br />Compute ∆jaccording to (4).<br />end for<br />i = argmaxj∈J∆j<br />Do updates for piand miaccording to (2).<br />Update matrices L, M, diag(A) and h according to (3).<br />I ← I ∪ {i}, J ← J \ {i}.<br />until |I| = d<br />all points in J = {1,...,n} \ I (or a subset thereof) and then picking the winner.<br />The heuristic we implement has also been considered in the context of active learn-<br />ing (see chapter 5 of [3]): score an example (xi,yi) by the decrease in entropy of<br />Q(·) upon its inclusion. As a result of the locality property of ADF and the fact<br />that Q is Gaussian, it is easy to see that the entropy difference H[Qnew] − H[Q] is<br />proportional to the log ratio between the variances of the marginals Qnew(ui) and<br />Q(ui). Thus, our heuristic (referred to as the differential entropy score) favors points<br />whose inclusion leads to a large reduction in predictive (posterior) variance at the<br />corresponding site. Whilst other selection heuristics can be argued for and utilized,<br />it turns out that the differential entropy score together with the simple likelihood<br />approximation in (1) leads to an extremely efficient and competitive algorithm.<br />In the remainder of this section, we describe our method and give a schematic<br />algorithm. A detailed derivation and discussions of some extensions can be found<br />in [7]. From (1) we have Q(·) = N(·|h,A), A := (K−1+ Π)−1, h := AΠm and<br />Π := diag(p). If I is the current active set, then all components of p and m not in<br />I are zero, and some algebra using the Woodbury formula gives<br />A = K − MTM,<br />where L is the lower-triangular Cholesky factor of<br />B = I + Π1/2<br />I<br />In order to compute the differential entropy score for a point j ?∈ I, we have to<br />know aj,jand hj. Thus, when including i into the active set I, we need to update<br />diag(A) and h accordingly, which in turn requires the matrices L and M to be<br />kept up-to-date. The update equations for pi, miare<br />pi=<br />1 − ai,iνi,<br />zi=yi· (hi+ b)<br />?1 + ai,i<br />We then update L → Lnewby appending the row (lT,l) and M → Mnewby<br />appending the row µT, where<br />l =√piM·,i,<br />Finally, diag(Anew) ← diag(A)−(µ2<br />entropy score for j ?∈ I can be computed based on the variables in (2) (with i → j)<br />as<br />∆j=1<br />2log(1 − aj,jνj),<br />M = L−1Π1/2<br />I<br />KI,·∈ Rd,n,<br />KIΠ1/2<br />I<br />∈ Rd,d.<br />νi<br />mi= hi+αi<br />νi,<br />where<br />,αi=<br />yi· N(zi|0,1)<br />Φ(zi)?1 + ai,i<br />,νi= αi<br />?<br />αi+<br />hi+ b<br />1 + ai,i<br />?<br />.<br />(2)<br />l =<br />?<br />1 + piKi,i− lTl,<br />j)jand hnew← h+αilp−1/2<br />µ = l−1(√piK·,i− MTl).<br />(3)<br />i<br />µ. The differential<br />(4)</p>  <p>Page 5</p> <p>which can be computed in O(1), given hj and aj,j. In Algorithm 1 we give an<br />algorithmic version of this scheme.<br />Each inclusion costs O(n · d), dominated by the computation of µ, apart from the<br />computation of the kernel matrix column K·,i. Thus the total time complexity is<br />O(n·d2). The storage requirement is O(n·d), dominated by the buffer for M. Given<br />diag(A) and h, the error or the expected log likelihood of the current predictor on<br />the remaining points J can be computed in O(n). These scores can be used in order<br />to decide how many points to include into the final I. For kernel functions with<br />constant diagonal, our selection heuristic is constant over patterns if I = ∅, so the<br />first (or the first few) inclusion candidate is chosen at random. After training is<br />complete, we can predict on test points x∗by evaluating the approximate predictive<br />distribution Q(u∗|x∗,S) =?P(u∗|u)Q(u)du = N(u∗|µ(x∗),σ2(x∗)), where<br />µ(x∗) = βTk(x∗),<br />with β := Π1/2<br />II<br />mIand k(x∗) := (k(xi,x∗))i∈I. We may compute σ2(x∗)<br />using one back-substitution with the factor L. The approximate predictive distri-<br />bution over y∗ can be obtained by averaging the noise model over the Gaussian.<br />The optimal predictor for the approximation is sgn(µ(x∗)+b), which is independent<br />of the variance σ2(x∗).<br />The simple scheme above employs full greedy selection over all remaining points to<br />find the inclusion candidate. This is sensible during early inclusions, but computa-<br />tionally wasteful during later ones, and an important extension of the basic scheme<br />of [2] allows for randomized greedy selections. To this end, we maintain a selection<br />index J ⊂ {1,...,n} with J ∩ I = ∅ at all times. Having included i into I we<br />modify the selection index J. This means that only the components J of diag(A)<br />and h have to be updated, which requires only the columns M·,J. Hence, if J<br />exhibits some inertia while moving over {1,...,n} \ I, many of the columns of M<br />will not have to be kept up-to-date. In our implementation, we employ a simple<br />delayed updating scheme for the columns of M which avoids double computations<br />(see [7] for details). After a number of initial inclusions are done using full greedy<br />selection, we use a J of fixed size m together with the following modification rule:<br />for a fraction τ ∈ (0,1), retain the τ · m best-scoring points in J, then fill it up to<br />size m by drawing at random from {1,...,n} \ (I ∪ J).<br />σ2(x∗) = k(x∗,x∗) − k(x∗)TΠ1/2<br />I<br />B−1Π1/2<br />I<br />k(x∗),<br />(5)<br />B−1Π1/2<br />4 Experiments<br />We now present results of experiments on the MNIST handwritten digits database5,<br />comparing our method against the SVM algorithm. We considered binary tasks of<br />the form ‘c-against-rest’, c ∈ {0,...,9}. c is mapped to +1, all others to −1. We<br />down-sampled the bitmaps to size 13 × 13 and split the MNIST training set into<br />a (new) training set of size n = 59000 and a validation set of size 1000; the test<br />set size is 10000. A run consisted of model selection, training and testing, and<br />all results are averaged over 10 runs. We employed the RBF kernel k(x,x?) =<br />C exp(−(γ/(2 · 169))?x − x??2), x ∈ R169with hyper-parameters C &gt; 0 (process<br />variance) and γ &gt; 0 (inverse squared length-scale). Model selection was done by<br />minimizing validation set error, training on random training set subsets of size<br />5000.6<br />5Available online at http://www.research.att.com/∼yann/exdb/mnist/index.html.<br />6The model selection training set for a run i is the same across tested methods. The<br />list of kernel parameters considered for selection has the same size across methods.</p>  <p>Page 6</p> <p>SVM IVM<br />gen<br />0.18<br />0.26<br />0.40<br />0.39<br />0.33<br />0.32<br />0.29<br />0.51<br />0.53<br />0.55<br />c<br />0<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />8<br />9<br />d<br />gen<br />0.22<br />0.20<br />0.40<br />0.41<br />0.40<br />0.29<br />0.28<br />0.54<br />0.50<br />0.58<br />time<br />1281<br />864<br />2977<br />3687<br />2442<br />2771<br />1520<br />2251<br />3909<br />3469<br />c<br />0<br />1<br />2<br />3<br />4<br />5<br />6<br />7<br />8<br />9<br />d<br />time<br />627<br />427<br />1690<br />2191<br />1210<br />1758<br />765<br />1110<br />2024<br />2444<br />1247<br />798<br />2240<br />2610<br />1826<br />2306<br />1331<br />1759<br />2636<br />2731<br />1130<br />820<br />2150<br />2500<br />1740<br />2200<br />1270<br />1660<br />2470<br />2740<br />Table 1: Test error rates (gen, %) and training times (time, s) on binary MNIST<br />tasks. SVM: Support vector machine (SMO); d: average number of SVs. IVM:<br />Sparse GPC, randomized greedy selections; d: final active set size. Figures are<br />means over 10 runs.<br />Our goal was to compare the methods not only w.r.t. performance, but also running<br />time. For the SVM, we chose the SMO algorithm [6] together with a fast elaborate<br />kernel matrix cache (see [7] for details). For the IVM, we employed randomized<br />greedy selections with fairly conservative settings.7Since each binary digit classifi-<br />cation task is very unbalanced, the bias parameter b in the GPC model was chosen<br />to be non-zero. We simply fixed b = Φ−1(r), where r is the ratio between +1 and<br />−1 patterns in the training set, and added a constant vb = 1/10 to the kernel k<br />to account for the variance of the bias hyper-parameter. Ideally, both b and vb<br />should be chosen by model selection, but initial experiments with different values<br />for (b, vb) exhibited no significant fluctuations in validation errors. To ensure a fair<br />comparison, we did initial SVM runs and initialized the active set size d with the<br />average number (over 10 runs) of SVs found, independently for each c. We then<br />re-ran the SVM experiments, allowing for O(dn) cache space. Table 1 shows the<br />results.<br />Note that IVM shows comparable performance to the SVM, while achieving sig-<br />nificantly lower training times. For less conservative settings of the randomized<br />selection parameters, further speed-ups might be realizable. We also registered<br />(not shown here) significant fluctuations in training time for the SVM runs, while<br />this figure is stable and a-priori predictable for the IVM. Within the IVM, we can<br />obtain estimates of predictive probabilities for test points, quantifying prediction<br />uncertainties. In Figure 1, which was produced for the hardest task c = 9, we reject<br />fractions of test set examples based on the size of |P(y∗= +1)−1/2|. For the SVM,<br />the size of the discriminant output is often used to quantify predictive uncertainty<br />heuristically. For c = 9, the latter is clearly inferior (although the difference is less<br />pronounced for the simpler binary tasks).<br />In the SVM community it is common to combine the ‘c-against-rest’ classifiers to<br />obtain a multi-class discriminant8as follows: for a test point x∗, decide for the class<br />whose associated classifier has the highest real-valued output. For the IVM, the<br />7First 2 selections at random, then 198 using full greedy, after that a selection index of<br />size 500 and a retained fraction τ = 1/2.<br />8Although much recent work has looked into more powerful combination schemes, e.g.<br />based on error-correcting codes.</p>  <p>Page 7</p> <p>00.05 0.1 0.150.2<br />10<br />−4<br />10<br />−3<br />10<br />−2<br />rejected fraction<br />error rate<br />SVM<br />IVM<br />Figure 1: Plot of test error rate against increasing rejection rate for the SVM<br />(dashed) and IVM (solid), for the task c = 9 against the rest. For SVM, we reject<br />based on “distance” from separating plane, for IVM based on estimates of predictive<br />probabilities. The IVM line runs below the SVM line exhibiting lower classification<br />errors for identical rejection rates.<br />equivalent would be to compare the estimates logP(y∗= +1) from each c-predictor<br />and pick the maximizing c. This is suboptimal, because the different predictors<br />have not been trained jointly.9However, the estimates of logP(y∗= +1) do depend<br />on predictive variances, i.e. a measure of uncertainty about the predictive mean,<br />which cannot be properly obtained within the SVM framework. This combination<br />scheme results in test errors of 1.54%(±0.0417%) for IVM, 1.62%(±0.0316%) for<br />SVM. When comparing these results to others in the literature, recall that our<br />experiments were based on images sub-sampled to size 13 × 13 rather than the<br />usual 28 × 28.<br />5 Discussion<br />We have demonstrated that sparse Gaussian process classifiers can be constructed<br />efficiently using greedy selection with a simple fast selection criterion. Although we<br />focused on the change in differential entropy in our experiments here, the simple<br />likelihood approximation at the basis of our method allows for other equally efficient<br />criteria such as information gain [3]. Our method retains many of the benefits<br />of probabilistic GP models (error bars, model combination, interpretability, etc.)<br />while being much faster and more memory-efficient both in training and prediction.<br />In comparison with non-probabilistic SVM classification, our method enjoys the<br />further advantages of being simpler to implement and having strictly predictable<br />time requirements. Our method can also be significantly faster10than SVM with the<br />SMO algorithm. This is due to the fact that SMO’s active set typically fluctuates<br />heavily across the training set, thus a large fraction of the full kernel matrix must<br />be evaluated. In contrast, IVM requires only d/n of K.<br />9It is straightforward to obtain the IVM for a joint GP classification model, however<br />the training costs raise by a factor of c2. Whether this factor can be reduced to c using<br />further sensible approximations, is an open question.<br />10We would expect SVMs to catch up with IVMs on tasks which require fairly large<br />active sets, and for which very simple and fast covariance functions are appropriate (e.g.<br />sparse input patterns).</p>  <p>Page 8</p> <p>Among the many proposed sparse GP approximations [1, 8, 9, 10, 11], our method<br />is most closely related to [1]. The latter is a sparse Bayesian online scheme which<br />does not employ greedy selections and uses a more accurate likelihood approxima-<br />tion than we do, at the expense of slightly worse training time scaling, especially<br />when compared with our randomized version. It also requires the specification of a<br />rejection threshold and is dependent on the ordering in which the training points<br />are presented. It incorporates steps to remove points from I, which can also be<br />done straightforwardly in our scheme, however such moves are likely to create nu-<br />merical stability problems. Smola and Bartlett [8] use a likelihood approximation<br />different from both the IVM and the scheme of [1] for GP regression, together with<br />greedy selections, but in contrast to our work they use a very expensive selection<br />heuristic (O(n·d) per score computation) and are forced to use randomized greedy<br />selection over small selection indexes. The differential entropy score has previously<br />been suggested in the context of active learning (e.g. [3]), but applies more directly<br />to our problem. In active learning, the label yiis not known at the time xihas to<br />be scored, and expected rather than actual entropy changes have to be considered.<br />Furthermore, MacKay [3] applies the selection to multi-layer perceptron (MLP)<br />models for which Gaussian posterior approximations over the weights can be very<br />poor.<br />Acknowledgments<br />We thank Chris Williams, David MacKay, Manfred Opper and Lehel Csat´ o for help-<br />ful discussions. MS gratefully acknowledges support through a research studentship<br />from Microsoft Research Ltd.<br />References<br />[1] Lehel Csat´ o and Manfred Opper. Sparse online Gaussian processes. N. Comp., 14:641–<br />668, 2002.<br />[2] Neil D. Lawrence and Ralf Herbrich. A sparse Bayesian compression scheme - the<br />informative vector machine. Presented at NIPS 2001 Workshop on Kernel Methods,<br />2001.<br />[3] David MacKay. Bayesian Methods for Adaptive Models. PhD thesis, California Insti-<br />tute of Technology, 1991.<br />[4] Thomas Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD<br />thesis, MIT, January 2001.<br />[5] Manfred Opper and Ole Winther. Gaussian processes for classification: Mean field<br />algorithms. N. Comp., 12(11):2655–2684, 2000.<br />[6] John C. Platt. Fast training of support vector machines using sequential minimal<br />optimization. In Sch¨ olkopf et. al., editor, Advances in Kernel Methods, pages 185–<br />208. 1998.<br />[7] Matthias Seeger, Neil D. Lawrence, and Ralf Herbrich. Sparse Bayesian learning:<br />The informative vector machine. Technical report, Department of Computer Science,<br />Sheffield, UK, 2002. See www.dcs.shef.ac.uk/~neil/papers/.<br />[8] Alex Smola and Peter Bartlett. Sparse greedy Gaussian process regression. In Ad-<br />vances in NIPS 13, pages 619–625, 2001.<br />[9] Michael Tipping.<br />J. M. Learn. Res., 1:211–244, 2001.<br />Sparse Bayesian learning and the relevance vector machine.<br />[10] Volker Tresp. A Bayesian committee machine. N. Comp., 12(11):2719–2741, 2000.<br />[11] Christopher K. I. Williams and Matthias Seeger. Using the Nystr¨ om method to speed<br />up kernel machines. In Advances in NIPS 13, pages 682–688, 2001.</p>  <a href="https://www.researchgate.net/profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000.pdf">Download full-text</a> </div> <div id="rgw20_56aba0965207a" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw21_56aba0965207a">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw22_56aba0965207a"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79c5968f000000.pdf" class="publication-viewer" title="download.pdf">download.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Matthias_Seeger">Matthias Seeger</a> &middot; Jan 20, 2016 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba0965207a"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Matthias_Seeger/publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine/links/0fcfd50a4b79987036000000.pdf" class="publication-viewer" title="0fcfd50a4b79987036000000.pdf">0fcfd50a4b79987036000000.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Matthias_Seeger">Matthias Seeger</a> &middot; Jun 4, 2014 </span>   </div>    </div> </li>  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw24_56aba0965207a"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.4925&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Fast Sparse Gaussian Process Methods: The Informative Vector Machine">Fast Sparse Gaussian Process Methods: The Informat...</a> </div>  <div class="details">   Available from <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.73.4925&amp;amp;rep=rep1&amp;amp;type=pdf" target="_blank" rel="nofollow">psu.edu</a>  </div>    </div> </li>  <li class="c-list-item pub-resource-item" style="display: none;" data-type="fulltext" id="rgw25_56aba0965207a"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://books.nips.cc/papers/files/nips15/AA16.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Fast Sparse Gaussian Process Methods: The Informative Vector Machine">Fast Sparse Gaussian Process Methods: The Informat...</a> </div>  <div class="details">   Available from <a href="http://books.nips.cc/papers/files/nips15/AA16.pdf" target="_blank" rel="nofollow">nips.cc</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw27_56aba0965207a" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw28_56aba0965207a">  </ul> </div> </div>   <div id="rgw16_56aba0965207a" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw17_56aba0965207a"> <div> <h5> <a href="publication/284788395_Near-Optimal_Active_Learning_of_Multi-Output_Gaussian_Processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">Near-Optimal Active Learning of Multi-Output Gaussian Processes</span></a>  </h5>  <div class="authors"> <a href="researcher/2086088364_Yehong_Zhang" class="authors ga-similar-publication-author">Yehong Zhang</a>, <a href="researcher/69643420_Trong_Nghia_Hoang" class="authors ga-similar-publication-author">Trong Nghia Hoang</a>, <a href="researcher/9575263_Kian_Hsiang_Low" class="authors ga-similar-publication-author">Kian Hsiang Low</a>, <a href="researcher/9587262_Mohan_Kankanhalli" class="authors ga-similar-publication-author">Mohan Kankanhalli</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56aba0965207a"> <div> <h5> <a href="publication/283825059_Multimodal_Control_in_Uncertain_Environments_using_Reinforcement_Learning_and_Gaussian_Processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">Multimodal Control in Uncertain Environments using Reinforcement Learning and Gaussian Processes</span></a>  </h5>  <div class="authors"> <a href="researcher/73924265_Mariano_De_Paula" class="authors ga-similar-publication-author">Mariano De Paula</a>, <a href="researcher/2048545089_Luis_O_Avila" class="authors ga-similar-publication-author">Luis O. Ávila</a>, <a href="researcher/2088679326_Carlos_Sanchez_Reinoso" class="authors ga-similar-publication-author">Carlos Sanchez Reinoso</a>, <a href="researcher/70774982_Gerardo_G_Acosta" class="authors ga-similar-publication-author">Gerardo G. Acosta</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba0965207a"> <div> <h5> <a href="publication/282315218_Controlling_blood_glucose_variability_under_uncertainty_using_reinforcement_learning_and_Gaussian_processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes</span></a>  </h5>  <div class="authors"> <a href="researcher/73924265_Mariano_De_Paula" class="authors ga-similar-publication-author">Mariano De Paula</a>, <a href="researcher/2048545089_Luis_Omar_Avila" class="authors ga-similar-publication-author">Luis Omar Ávila</a>, <a href="researcher/81604969_Ernesto_C_Martinez" class="authors ga-similar-publication-author">Ernesto C. Martínez</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw41_56aba0965207a" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw42_56aba0965207a">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw43_56aba0965207a" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=UpZsion51NIZ89EP6WM4hM87bUoqrE16fBnylvLTGHi2rvPeXoO80pF_LXQte_W-" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="yTGxvqRodiSpgMHmHw6lxw1ytbXf4AEKCW3VZZkH0QoLMxFHHyXNNfq25lSLYVaYFtQCxJPoDfiIeDMJuF+BrKxa8tSH/3I0PLRn32zwwKRdduwFuiMnj1ImnuMZRiv4xnrYAPTJNZb/BTP6JZ8H+imhezDFQV1DVm5LHIPrZs0jgKIYZFiJ7ZjGuBXtnxPkwgVrGmRJOrZUdYZecbPZwKG/tN62dLHw5Zjx7u5OTgNc/rmASg3ASgdW81tSPJ3nv/oWybxSX6mv1TLFFAiSXrobvgaCyOMDjgUVJk5nURk="/> <input type="hidden" name="urlAfterLogin" value="publication/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjE4NDM0X0Zhc3RfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3NfTWV0aG9kc19UaGVfSW5mb3JtYXRpdmVfVmVjdG9yX01hY2hpbmU%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjE4NDM0X0Zhc3RfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3NfTWV0aG9kc19UaGVfSW5mb3JtYXRpdmVfVmVjdG9yX01hY2hpbmU%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjIxNjE4NDM0X0Zhc3RfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3NfTWV0aG9kc19UaGVfSW5mb3JtYXRpdmVfVmVjdG9yX01hY2hpbmU%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw44_56aba0965207a"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 565;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Matthias Seeger","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272524646809639%401441986347920_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Matthias_Seeger","institution":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","institutionUrl":false,"widgetId":"rgw4_56aba0965207a"},"id":"rgw4_56aba0965207a","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=2322637","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba0965207a"},"id":"rgw3_56aba0965207a","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=221618434","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":221618434,"title":"Fast Sparse Gaussian Process Methods: The Informative Vector Machine","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Conference Paper","details":{"conferenceInfos":"Conference: Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]"},"source":{"sourceUrl":"http:\/\/dblp.uni-trier.de\/db\/conf\/nips\/nips2002.html#LawrenceSH02","sourceName":"DBLP"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Fast Sparse Gaussian Process Methods: The Informative Vector Machine"},{"key":"rft.title","value":"Adv NIPS"},{"key":"rft.jtitle","value":"Adv NIPS"},{"key":"rft.volume","value":"15"},{"key":"rft.date","value":"2002"},{"key":"rft.pages","value":"609-616"},{"key":"rft.au","value":"Neil D. Lawrence,Matthias Seeger,Ralf Herbrich"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw6_56aba0965207a"},"id":"rgw6_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=221618434","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":221618434,"peopleItems":[{"data":{"authorUrl":"researcher\/39663468_Neil_D_Lawrence","authorNameOnPublication":"Neil D. Lawrence","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Neil D. Lawrence","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/39663468_Neil_D_Lawrence","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw9_56aba0965207a"},"id":"rgw9_56aba0965207a","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=39663468&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba0965207a"},"id":"rgw8_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=39663468&authorNameOnPublication=Neil%20D.%20Lawrence","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorNameOnPublication":"Matthias Seeger","accountUrl":"profile\/Matthias_Seeger","accountKey":"Matthias_Seeger","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272524646809639%401441986347920_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Matthias Seeger","profile":{"professionalInstitution":{"professionalInstitutionName":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","professionalInstitutionUrl":"institution\/Ecole_Polytechnique_Federale_de_Lausanne"}},"professionalInstitutionName":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","professionalInstitutionUrl":"institution\/Ecole_Polytechnique_Federale_de_Lausanne","url":"profile\/Matthias_Seeger","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272524646809639%401441986347920_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Matthias_Seeger","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw11_56aba0965207a"},"id":"rgw11_56aba0965207a","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=2322637&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":221618434,"widgetId":"rgw10_56aba0965207a"},"id":"rgw10_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=2322637&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=221618434","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/3234005_Ralf_Herbrich","authorNameOnPublication":"Ralf Herbrich","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ralf Herbrich","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/3234005_Ralf_Herbrich","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56aba0965207a"},"id":"rgw13_56aba0965207a","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=3234005&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56aba0965207a"},"id":"rgw12_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=3234005&authorNameOnPublication=Ralf%20Herbrich","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba0965207a"},"id":"rgw7_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=221618434&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":221618434,"abstract":"<noscript><\/noscript><div>Abstract We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active learning. Our goal is not only to learn d{sparse predictors (which can be evaluated in O(d) rather than O(n), d n, n the number of training points), but also to perform training under strong restrictions on time and memory,requirements. The scaling of our method is at most O(n d,), and in large real-world classication experiments we show that it can match prediction performance of the popular support vector machine (SVM), yet can be signican tly faster in training. In contrast to the SVM, our approximation produces estimates of predictive probabilities (&lsquo;error bars&rsquo;), allows for Bayesian model selection and is less complex in implementation.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw14_56aba0965207a"},"id":"rgw14_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=221618434","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw15_56aba0965207a"},"id":"rgw15_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba0965207a"},"id":"rgw5_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=221618434&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2086088364,"url":"researcher\/2086088364_Yehong_Zhang","fullname":"Yehong Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69643420,"url":"researcher\/69643420_Trong_Nghia_Hoang","fullname":"Trong Nghia Hoang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9575263,"url":"researcher\/9575263_Kian_Hsiang_Low","fullname":"Kian Hsiang Low","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9587262,"url":"researcher\/9587262_Mohan_Kankanhalli","fullname":"Mohan Kankanhalli","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2015","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/284788395_Near-Optimal_Active_Learning_of_Multi-Output_Gaussian_Processes","usePlainButton":true,"publicationUid":284788395,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/284788395_Near-Optimal_Active_Learning_of_Multi-Output_Gaussian_Processes","title":"Near-Optimal Active Learning of Multi-Output Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":2086088364,"url":"researcher\/2086088364_Yehong_Zhang","fullname":"Yehong Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69643420,"url":"researcher\/69643420_Trong_Nghia_Hoang","fullname":"Trong Nghia Hoang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9575263,"url":"researcher\/9575263_Kian_Hsiang_Low","fullname":"Kian Hsiang Low","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":9587262,"url":"researcher\/9587262_Mohan_Kankanhalli","fullname":"Mohan Kankanhalli","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/284788395_Near-Optimal_Active_Learning_of_Multi-Output_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/284788395_Near-Optimal_Active_Learning_of_Multi-Output_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba0965207a"},"id":"rgw17_56aba0965207a","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=284788395","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":73924265,"url":"researcher\/73924265_Mariano_De_Paula","fullname":"Mariano De Paula","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048545089,"url":"researcher\/2048545089_Luis_O_Avila","fullname":"Luis O. \u00c1vila","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088679326,"url":"researcher\/2088679326_Carlos_Sanchez_Reinoso","fullname":"Carlos Sanchez Reinoso","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70774982,"url":"researcher\/70774982_Gerardo_G_Acosta","fullname":"Gerardo G. Acosta","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Oct 2015","journal":"Revista iberoamericana de automa\u0301tica e informa\u0301tica industrial (RIAI)","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/283825059_Multimodal_Control_in_Uncertain_Environments_using_Reinforcement_Learning_and_Gaussian_Processes","usePlainButton":true,"publicationUid":283825059,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.12","url":"publication\/283825059_Multimodal_Control_in_Uncertain_Environments_using_Reinforcement_Learning_and_Gaussian_Processes","title":"Multimodal Control in Uncertain Environments using Reinforcement Learning and Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":73924265,"url":"researcher\/73924265_Mariano_De_Paula","fullname":"Mariano De Paula","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048545089,"url":"researcher\/2048545089_Luis_O_Avila","fullname":"Luis O. \u00c1vila","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2088679326,"url":"researcher\/2088679326_Carlos_Sanchez_Reinoso","fullname":"Carlos Sanchez Reinoso","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70774982,"url":"researcher\/70774982_Gerardo_G_Acosta","fullname":"Gerardo G. Acosta","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Revista iberoamericana de automa\u0301tica e informa\u0301tica industrial (RIAI) 10\/2015; 12(4):385-396. DOI:10.1016\/j.riai.2015.09.004"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/283825059_Multimodal_Control_in_Uncertain_Environments_using_Reinforcement_Learning_and_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/283825059_Multimodal_Control_in_Uncertain_Environments_using_Reinforcement_Learning_and_Gaussian_Processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba0965207a"},"id":"rgw18_56aba0965207a","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=283825059","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":73924265,"url":"researcher\/73924265_Mariano_De_Paula","fullname":"Mariano De Paula","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048545089,"url":"researcher\/2048545089_Luis_Omar_Avila","fullname":"Luis Omar \u00c1vila","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":81604969,"url":"researcher\/81604969_Ernesto_C_Martinez","fullname":"Ernesto C. Mart\u00ednez","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jul 2015","journal":"Applied Soft Computing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282315218_Controlling_blood_glucose_variability_under_uncertainty_using_reinforcement_learning_and_Gaussian_processes","usePlainButton":true,"publicationUid":282315218,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.81","url":"publication\/282315218_Controlling_blood_glucose_variability_under_uncertainty_using_reinforcement_learning_and_Gaussian_processes","title":"Controlling blood glucose variability under uncertainty using reinforcement learning and Gaussian processes","displayTitleAsLink":true,"authors":[{"id":73924265,"url":"researcher\/73924265_Mariano_De_Paula","fullname":"Mariano De Paula","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2048545089,"url":"researcher\/2048545089_Luis_Omar_Avila","fullname":"Luis Omar \u00c1vila","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":81604969,"url":"researcher\/81604969_Ernesto_C_Martinez","fullname":"Ernesto C. Mart\u00ednez","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Applied Soft Computing 07\/2015; 35:310-332. DOI:10.1016\/j.asoc.2015.06.041"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282315218_Controlling_blood_glucose_variability_under_uncertainty_using_reinforcement_learning_and_Gaussian_processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282315218_Controlling_blood_glucose_variability_under_uncertainty_using_reinforcement_learning_and_Gaussian_processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba0965207a"},"id":"rgw19_56aba0965207a","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282315218","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw16_56aba0965207a"},"id":"rgw16_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=221618434&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":221618434,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":221618434,"publicationType":"inProceedings","linkId":"0fcfd50a4b79c5968f000000","fileName":"download.pdf","fileUrl":"profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000.pdf","name":"Matthias Seeger","nameUrl":"profile\/Matthias_Seeger","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jan 20, 2016","fileSize":"131.41 KB","widgetId":"rgw22_56aba0965207a"},"id":"rgw22_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221618434&linkId=0fcfd50a4b79c5968f000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":221618434,"publicationType":"inProceedings","linkId":"0fcfd50a4b79987036000000","fileName":"0fcfd50a4b79987036000000.pdf","fileUrl":"profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79987036000000.pdf","name":"Matthias Seeger","nameUrl":"profile\/Matthias_Seeger","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":true,"uploadDate":"Jun 4, 2014","fileSize":"131.41 KB","widgetId":"rgw23_56aba0965207a"},"id":"rgw23_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221618434&linkId=0fcfd50a4b79987036000000&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":221618434,"publicationType":"inProceedings","linkId":"0e5fabfcf0c41c4932e331bb","fileName":"Fast Sparse Gaussian Process Methods: The Informative Vector Machine","fileUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.73.4925&amp;rep=rep1&amp;type=pdf","name":"psu.edu","nameUrl":"http:\/\/citeseerx.ist.psu.edu\/viewdoc\/download?doi=10.1.1.73.4925&amp;rep=rep1&amp;type=pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw24_56aba0965207a"},"id":"rgw24_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221618434&linkId=0e5fabfcf0c41c4932e331bb&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationUid":221618434,"publicationType":"inProceedings","linkId":"0fff104b0cf2b20ef07590a7","fileName":"Fast Sparse Gaussian Process Methods: The Informative Vector Machine","fileUrl":"http:\/\/books.nips.cc\/papers\/files\/nips15\/AA16.pdf","name":"nips.cc","nameUrl":"http:\/\/books.nips.cc\/papers\/files\/nips15\/AA16.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":true,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw25_56aba0965207a"},"id":"rgw25_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=221618434&linkId=0fff104b0cf2b20ef07590a7&hide=1&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw21_56aba0965207a"},"id":"rgw21_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221618434&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":4,"hidden":false,"showMore":true,"fulltext":true,"publicationDownloadCount":{"data":{"value":83,"valueFormatted":"83","widgetId":"rgw26_56aba0965207a"},"id":"rgw26_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221618434","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw20_56aba0965207a"},"id":"rgw20_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221618434&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":221618434,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw28_56aba0965207a"},"id":"rgw28_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=221618434&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":83,"valueFormatted":"83","widgetId":"rgw29_56aba0965207a"},"id":"rgw29_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=221618434","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw27_56aba0965207a"},"id":"rgw27_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=221618434&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Fast Sparse Gaussian Process Methods:\nThe Informative Vector Machine\nNeil Lawrence\nUniversity of Sheffield\n211 Portobello Street\nSheffield, S1 4DP\nneil@dcs.shef.ac.uk\nMatthias Seeger\nUniversity of Edinburgh\n5 Forrest Hill\nEdinburgh, EH1 2QL\nseeger@dai.ed.ac.uk\nRalf Herbrich\nMicrosoft Research Ltd\n7 J J Thomson Avenue\nCambridge, CB3 0FB\nrherb@microsoft.com\nAbstract\nWe present a framework for sparse Gaussian process (GP) methods\nwhich uses forward selection with criteria based on information-\ntheoretic principles, previously suggested for active learning. Our\ngoal is not only to learn d\u2013sparse predictors (which can be evalu-\nated in O(d) rather than O(n), d ? n, n the number of training\npoints), but also to perform training under strong restrictions on\ntime and memory requirements. The scaling of our method is at\nmost O(n \u00b7 d2), and in large real-world classification experiments\nwe show that it can match prediction performance of the popular\nsupport vector machine (SVM), yet can be significantly faster in\ntraining. In contrast to the SVM, our approximation produces esti-\nmates of predictive probabilities (\u2018error bars\u2019), allows for Bayesian\nmodel selection and is less complex in implementation.\n1 Introduction\nGaussian process (GP) models are powerful non-parametric tools for approximate\nBayesian inference and learning. In comparison with other popular nonlinear ar-\nchitectures, such as multi-layer perceptrons, their behavior is conceptually simpler\nto understand and model fitting can be achieved without resorting to non-convex\noptimization routines. However, their training time scaling of O(n3) and memory\nscaling of O(n2), where n the number of training points, has hindered their more\nwidespread use. The related, yet non-probabilistic, support vector machine (SVM)\nclassifier often renders results that are comparable to GP classifiers w.r.t. prediction\nerror at a fraction of the training cost. This is possible because many tasks can\nbe solved satisfactorily using sparse representations of the data set. The SVM is\ntriggered towards finding such representations through the use of a particular loss\nfunction1that encourages some degree of sparsity, i.e. the final predictor depends\nonly on a fraction of training points crucial for good discrimination on the task.\nHere, we call these utilized points the active set of the sparse predictor. In case of\nSVM classification, the active set contains the support vectors, the points closest to\n1An SVM classifier is trained by minimizing a regularized loss functional, a process\nwhich cannot be interpreted as approximation to Bayesian inference."},{"page":2,"text":"the decision boundary and the misclassified ones. If the active set size d is much\nsmaller than n, an SVM classifier can be trained in average case running time be-\ntween O(n \u00b7 d2) and O(n2\u00b7 d) with memory requirements significantly less than n2.\nNote, however, that without any restrictions on the data distribution, d can rise to\nn.\nIn an effort to overcome scaling problems a range of sparse GP approximations have\nbeen proposed [1, 8, 9, 10, 11]. However, none of these has fully achieved the goals of\nbeing a nontrivial approximation to a non-sparse GP model and matching the SVM\nw.r.t. both prediction performance and run time. The algorithm proposed here ac-\ncomplishes these objectives and, as our experiments show, can even be significantly\nfaster in training than the SVM. Furthermore, time and memory requirements may\nbe restricted a priori. The potential benefits of retaining the probabilistic charac-\nteristics of the method are numerous, since hard problems, e.g. feature and model\nselection, can be dealt with using standard techniques from Bayesian learning.\nOur approach builds on earlier work of Lawrence and Herbrich [2] which we extend\nhere by considering randomized greedy selections and focusing on an alternative\nrepresentation of the GP model which facilitates generalizations to settings such\nas regression and multi-class classification. In the next section we introduce the\nGP classification model and a method for approximate inference. Section 3 then\ncontains the derivation of our fast greedy approximation and a description of the as-\nsociated algorithm. In Section 4, we present large-scale experiments on the MNIST\ndatabase, comparing our method directly against the SVM. Finally we close with a\ndiscussion in Section 5.\nWe denote vectors g = (gi)i and matrices G = (gi,j)i,j in bold-face2.\nare sets of row and column indices respectively, we denote the corresponding sub-\nmatrix of G \u2208 Rp,qby GI,J, furthermore we abbreviate GI,\u00b7 to GI,1...q, GI,j to\nGI,{j}, GIto GI,I, etc. The density of the Gaussian distribution with mean \u00b5 and\ncovariance matrix \u03a3 is denoted by N(\u00b7|\u00b5,\u03a3). Finally, we use diag(\u00b7) to represent an\n\u2018overloaded\u2019 operator which extracts the diagonal elements of a matrix as a vector\nor produces a square matrix with diagonal elements from a given vector, all other\nelements 0.\nIf I,J\n2 Gaussian Process Classification\nAssume we are given a sample S := ((x1,y1),...,(xn,yn)), xi\u2208 X, yi\u2208 {\u22121,+1},\ndrawn independently and identically distributed (i.i.d.) from an unknown data dis-\ntribution3P(x,y). Our goal is to estimate P(y|x) for typical x or, less ambitiously,\nto learn a predictor x \u2192 y with small error on future data. To model this situation,\nwe introduce a latent variable u \u2208 R separating x and y, and some classification\nnoise model P(y|u) := \u03a6(y\u00b7(u+b)), where \u03a6 is the cumulative distribution function\nof the standard Gaussian N(0,1), and b \u2208 R is a bias parameter. From the Bayesian\nviewpoint, the relationship x \u2192 u is a random process u(\u00b7), which, in a Gaussian\nprocess (GP) model, is given a GP prior with mean function 0 and covariance kernel\nk(\u00b7,\u00b7). This prior encodes the belief that (before observing any data) for any finite\nset X = {\u02dc x1,..., \u02dc xp} \u2282 X, the corresponding latent outputs (u(\u02dc x1),...,u(\u02dc xp))T\nare jointly Gaussian with mean 0 \u2208 Rpand covariance matrix (k(\u02dc xi, \u02dc xj))i,j\u2208 Rp,p.\nGP models are non-parametric, that is, there is in general no finite-dimensional\n2Whenever we use a bold symbol g or G for a vector or matrix, we denote its compo-\nnents by the corresponding normal symbols gi and gi,j.\n3We focus on binary classification, but our framework can be applied straightforwardly\nto regression estimation and multi-class classification."},{"page":3,"text":"parametric representation for u(\u00b7). It is possible to write u(\u00b7) as linear function\nin some feature space F associated with k, i.e. u(x) = wT\u03c6(x), w \u2208 F, in the\nsense that a Gaussian prior on w induces a GP distribution on the linear function\nu(\u00b7). Here, \u03c6 is a feature map from X into F, and the covariance function can be\nwritten k(x,x?) = \u03c6(x)T\u03c6(x?). This linear function view, under which predictors\nbecome separating hyper-planes in F, is frequently used in the SVM community.\nHowever, F is, in general, infinite-dimensional and not uniquely determined by\nthe kernel function k. We denote the sequence of latent outputs at the training\npoints by u := (u(x1),...,u(xn))T\u2208 Rnand the covariance or kernel matrix by\nK := (k(xi,xj))i,j\u2208 Rn,n.\nThe Bayesian posterior process for u(\u00b7) can be computed in principle using Bayes\u2019\nformula. However, if the noise model P(y|u) is non-Gaussian (as is the case for\nbinary classification), it cannot be handled tractably and is usually approximated\nby another Gaussian process, which should ideally preserve mean and covariance\nfunction of the former. It is easy to show that this is equivalent to fitting the mo-\nments between the finite-dimensional (marginal) posterior P(u|S) over the train-\ning points and a Gaussian approximation Q(u), because the conditional posterior\nP(u(x\u2217)|u,S) for some non-training point x\u2217is identical to the conditional prior\nP(u(x\u2217)|u). In general, computing Q is also infeasible, but several authors have\nproposed to approximate the global moment matching by iterative schemes which\nlocally focus on one training pattern at a time [1, 4]. These schemes (at least in\ntheir simplest forms) result in a parametric form for the approximating Gaussian\nQ(u) \u221d P(u)\nn\n?\ni=1\nexp\n?\n\u2212pi\n2(ui\u2212 mi)2?\n.\n(1)\nThis may be compared with the form of the true posterior P(u|S)\nP(u)?n\ncalled sites. Initially, all pi,miare 0, thus Q(u) = P(u). In order to update the\nparameters for a site i, we replace it in Q(u) by the corresponding true likelihood\nfactor P(yi|ui), resulting in a non-Gaussian distribution whose mean and covari-\nance matrix can still be computed. This allows us to approximate it by a Gaussian\nQnew(u) using moment matching. The site update is called the inclusion of i into\nthe active set I. The factorized form of the likelihood implies that the new and old\nQ differ only in the parameters pi,miof site i. This is a useful locality property of\nthe scheme which is referred to as assumed density filtering (ADF) (e.g. [4]). The\nspecial case of ADF4for GP models has been proposed in [5].\n\u221d\ni=1P(yi|ui) and shows that Q(u) is obtained from P(u|S) by a likelihood\napproximation. Borrowing from graphical models vocabulary, the factors in (1) are\n3 Sparse Gaussian Process Classification\nThe simplest way to obtain a sparse Gaussian process classification (GPC) approx-\nimation from the ADF scheme is to leave most of the site parameters at 0, i.e.\npi= 0, mi= 0 for all i ?\u2208 I, where I \u2282 {1,...,n} is the active set, |I| =: d < n. For\nthis to succeed, it is important to choose I so that the decision boundary between\nclasses is represented essentially as accurately as if we used the whole training set.\nAn exhaustive search over all possible subsets I is, of course, intractable. Here, we\nfollow a greedy approach suggested in [2], including new patterns one at a time into\nI. The selection of a pattern to include is made by computing a score function for\n4A generalization of ADF, expectation propagation (EP) [4], allows for several iterations\nover the data. In the context of sparse approximations, it allows us to remove points from\nI or exchange them against such outside I, although we do not consider such moves here."},{"page":4,"text":"Algorithm 1 Informative vector machine algorithm\nRequire: A desired sparsity d ? n.\nI = \u2205, m = 0, \u03a0 = diag(0), diag(A) = diag(K), h = 0, J = {1,...,n}.\nrepeat\nfor j \u2208 J do\nCompute \u2206jaccording to (4).\nend for\ni = argmaxj\u2208J\u2206j\nDo updates for piand miaccording to (2).\nUpdate matrices L, M, diag(A) and h according to (3).\nI \u2190 I \u222a {i}, J \u2190 J \\ {i}.\nuntil |I| = d\nall points in J = {1,...,n} \\ I (or a subset thereof) and then picking the winner.\nThe heuristic we implement has also been considered in the context of active learn-\ning (see chapter 5 of [3]): score an example (xi,yi) by the decrease in entropy of\nQ(\u00b7) upon its inclusion. As a result of the locality property of ADF and the fact\nthat Q is Gaussian, it is easy to see that the entropy difference H[Qnew] \u2212 H[Q] is\nproportional to the log ratio between the variances of the marginals Qnew(ui) and\nQ(ui). Thus, our heuristic (referred to as the differential entropy score) favors points\nwhose inclusion leads to a large reduction in predictive (posterior) variance at the\ncorresponding site. Whilst other selection heuristics can be argued for and utilized,\nit turns out that the differential entropy score together with the simple likelihood\napproximation in (1) leads to an extremely efficient and competitive algorithm.\nIn the remainder of this section, we describe our method and give a schematic\nalgorithm. A detailed derivation and discussions of some extensions can be found\nin [7]. From (1) we have Q(\u00b7) = N(\u00b7|h,A), A := (K\u22121+ \u03a0)\u22121, h := A\u03a0m and\n\u03a0 := diag(p). If I is the current active set, then all components of p and m not in\nI are zero, and some algebra using the Woodbury formula gives\nA = K \u2212 MTM,\nwhere L is the lower-triangular Cholesky factor of\nB = I + \u03a01\/2\nI\nIn order to compute the differential entropy score for a point j ?\u2208 I, we have to\nknow aj,jand hj. Thus, when including i into the active set I, we need to update\ndiag(A) and h accordingly, which in turn requires the matrices L and M to be\nkept up-to-date. The update equations for pi, miare\npi=\n1 \u2212 ai,i\u03bdi,\nzi=yi\u00b7 (hi+ b)\n?1 + ai,i\nWe then update L \u2192 Lnewby appending the row (lT,l) and M \u2192 Mnewby\nappending the row \u00b5T, where\nl =\u221apiM\u00b7,i,\nFinally, diag(Anew) \u2190 diag(A)\u2212(\u00b52\nentropy score for j ?\u2208 I can be computed based on the variables in (2) (with i \u2192 j)\nas\n\u2206j=1\n2log(1 \u2212 aj,j\u03bdj),\nM = L\u22121\u03a01\/2\nI\nKI,\u00b7\u2208 Rd,n,\nKI\u03a01\/2\nI\n\u2208 Rd,d.\n\u03bdi\nmi= hi+\u03b1i\n\u03bdi,\nwhere\n,\u03b1i=\nyi\u00b7 N(zi|0,1)\n\u03a6(zi)?1 + ai,i\n,\u03bdi= \u03b1i\n?\n\u03b1i+\nhi+ b\n1 + ai,i\n?\n.\n(2)\nl =\n?\n1 + piKi,i\u2212 lTl,\nj)jand hnew\u2190 h+\u03b1ilp\u22121\/2\n\u00b5 = l\u22121(\u221apiK\u00b7,i\u2212 MTl).\n(3)\ni\n\u00b5. The differential\n(4)"},{"page":5,"text":"which can be computed in O(1), given hj and aj,j. In Algorithm 1 we give an\nalgorithmic version of this scheme.\nEach inclusion costs O(n \u00b7 d), dominated by the computation of \u00b5, apart from the\ncomputation of the kernel matrix column K\u00b7,i. Thus the total time complexity is\nO(n\u00b7d2). The storage requirement is O(n\u00b7d), dominated by the buffer for M. Given\ndiag(A) and h, the error or the expected log likelihood of the current predictor on\nthe remaining points J can be computed in O(n). These scores can be used in order\nto decide how many points to include into the final I. For kernel functions with\nconstant diagonal, our selection heuristic is constant over patterns if I = \u2205, so the\nfirst (or the first few) inclusion candidate is chosen at random. After training is\ncomplete, we can predict on test points x\u2217by evaluating the approximate predictive\ndistribution Q(u\u2217|x\u2217,S) =?P(u\u2217|u)Q(u)du = N(u\u2217|\u00b5(x\u2217),\u03c32(x\u2217)), where\n\u00b5(x\u2217) = \u03b2Tk(x\u2217),\nwith \u03b2 := \u03a01\/2\nII\nmIand k(x\u2217) := (k(xi,x\u2217))i\u2208I. We may compute \u03c32(x\u2217)\nusing one back-substitution with the factor L. The approximate predictive distri-\nbution over y\u2217 can be obtained by averaging the noise model over the Gaussian.\nThe optimal predictor for the approximation is sgn(\u00b5(x\u2217)+b), which is independent\nof the variance \u03c32(x\u2217).\nThe simple scheme above employs full greedy selection over all remaining points to\nfind the inclusion candidate. This is sensible during early inclusions, but computa-\ntionally wasteful during later ones, and an important extension of the basic scheme\nof [2] allows for randomized greedy selections. To this end, we maintain a selection\nindex J \u2282 {1,...,n} with J \u2229 I = \u2205 at all times. Having included i into I we\nmodify the selection index J. This means that only the components J of diag(A)\nand h have to be updated, which requires only the columns M\u00b7,J. Hence, if J\nexhibits some inertia while moving over {1,...,n} \\ I, many of the columns of M\nwill not have to be kept up-to-date. In our implementation, we employ a simple\ndelayed updating scheme for the columns of M which avoids double computations\n(see [7] for details). After a number of initial inclusions are done using full greedy\nselection, we use a J of fixed size m together with the following modification rule:\nfor a fraction \u03c4 \u2208 (0,1), retain the \u03c4 \u00b7 m best-scoring points in J, then fill it up to\nsize m by drawing at random from {1,...,n} \\ (I \u222a J).\n\u03c32(x\u2217) = k(x\u2217,x\u2217) \u2212 k(x\u2217)T\u03a01\/2\nI\nB\u22121\u03a01\/2\nI\nk(x\u2217),\n(5)\nB\u22121\u03a01\/2\n4 Experiments\nWe now present results of experiments on the MNIST handwritten digits database5,\ncomparing our method against the SVM algorithm. We considered binary tasks of\nthe form \u2018c-against-rest\u2019, c \u2208 {0,...,9}. c is mapped to +1, all others to \u22121. We\ndown-sampled the bitmaps to size 13 \u00d7 13 and split the MNIST training set into\na (new) training set of size n = 59000 and a validation set of size 1000; the test\nset size is 10000. A run consisted of model selection, training and testing, and\nall results are averaged over 10 runs. We employed the RBF kernel k(x,x?) =\nC exp(\u2212(\u03b3\/(2 \u00b7 169))?x \u2212 x??2), x \u2208 R169with hyper-parameters C > 0 (process\nvariance) and \u03b3 > 0 (inverse squared length-scale). Model selection was done by\nminimizing validation set error, training on random training set subsets of size\n5000.6\n5Available online at http:\/\/www.research.att.com\/\u223cyann\/exdb\/mnist\/index.html.\n6The model selection training set for a run i is the same across tested methods. The\nlist of kernel parameters considered for selection has the same size across methods."},{"page":6,"text":"SVM IVM\ngen\n0.18\n0.26\n0.40\n0.39\n0.33\n0.32\n0.29\n0.51\n0.53\n0.55\nc\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nd\ngen\n0.22\n0.20\n0.40\n0.41\n0.40\n0.29\n0.28\n0.54\n0.50\n0.58\ntime\n1281\n864\n2977\n3687\n2442\n2771\n1520\n2251\n3909\n3469\nc\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nd\ntime\n627\n427\n1690\n2191\n1210\n1758\n765\n1110\n2024\n2444\n1247\n798\n2240\n2610\n1826\n2306\n1331\n1759\n2636\n2731\n1130\n820\n2150\n2500\n1740\n2200\n1270\n1660\n2470\n2740\nTable 1: Test error rates (gen, %) and training times (time, s) on binary MNIST\ntasks. SVM: Support vector machine (SMO); d: average number of SVs. IVM:\nSparse GPC, randomized greedy selections; d: final active set size. Figures are\nmeans over 10 runs.\nOur goal was to compare the methods not only w.r.t. performance, but also running\ntime. For the SVM, we chose the SMO algorithm [6] together with a fast elaborate\nkernel matrix cache (see [7] for details). For the IVM, we employed randomized\ngreedy selections with fairly conservative settings.7Since each binary digit classifi-\ncation task is very unbalanced, the bias parameter b in the GPC model was chosen\nto be non-zero. We simply fixed b = \u03a6\u22121(r), where r is the ratio between +1 and\n\u22121 patterns in the training set, and added a constant vb = 1\/10 to the kernel k\nto account for the variance of the bias hyper-parameter. Ideally, both b and vb\nshould be chosen by model selection, but initial experiments with different values\nfor (b, vb) exhibited no significant fluctuations in validation errors. To ensure a fair\ncomparison, we did initial SVM runs and initialized the active set size d with the\naverage number (over 10 runs) of SVs found, independently for each c. We then\nre-ran the SVM experiments, allowing for O(dn) cache space. Table 1 shows the\nresults.\nNote that IVM shows comparable performance to the SVM, while achieving sig-\nnificantly lower training times. For less conservative settings of the randomized\nselection parameters, further speed-ups might be realizable. We also registered\n(not shown here) significant fluctuations in training time for the SVM runs, while\nthis figure is stable and a-priori predictable for the IVM. Within the IVM, we can\nobtain estimates of predictive probabilities for test points, quantifying prediction\nuncertainties. In Figure 1, which was produced for the hardest task c = 9, we reject\nfractions of test set examples based on the size of |P(y\u2217= +1)\u22121\/2|. For the SVM,\nthe size of the discriminant output is often used to quantify predictive uncertainty\nheuristically. For c = 9, the latter is clearly inferior (although the difference is less\npronounced for the simpler binary tasks).\nIn the SVM community it is common to combine the \u2018c-against-rest\u2019 classifiers to\nobtain a multi-class discriminant8as follows: for a test point x\u2217, decide for the class\nwhose associated classifier has the highest real-valued output. For the IVM, the\n7First 2 selections at random, then 198 using full greedy, after that a selection index of\nsize 500 and a retained fraction \u03c4 = 1\/2.\n8Although much recent work has looked into more powerful combination schemes, e.g.\nbased on error-correcting codes."},{"page":7,"text":"00.05 0.1 0.150.2\n10\n\u22124\n10\n\u22123\n10\n\u22122\nrejected fraction\nerror rate\nSVM\nIVM\nFigure 1: Plot of test error rate against increasing rejection rate for the SVM\n(dashed) and IVM (solid), for the task c = 9 against the rest. For SVM, we reject\nbased on \u201cdistance\u201d from separating plane, for IVM based on estimates of predictive\nprobabilities. The IVM line runs below the SVM line exhibiting lower classification\nerrors for identical rejection rates.\nequivalent would be to compare the estimates logP(y\u2217= +1) from each c-predictor\nand pick the maximizing c. This is suboptimal, because the different predictors\nhave not been trained jointly.9However, the estimates of logP(y\u2217= +1) do depend\non predictive variances, i.e. a measure of uncertainty about the predictive mean,\nwhich cannot be properly obtained within the SVM framework. This combination\nscheme results in test errors of 1.54%(\u00b10.0417%) for IVM, 1.62%(\u00b10.0316%) for\nSVM. When comparing these results to others in the literature, recall that our\nexperiments were based on images sub-sampled to size 13 \u00d7 13 rather than the\nusual 28 \u00d7 28.\n5 Discussion\nWe have demonstrated that sparse Gaussian process classifiers can be constructed\nefficiently using greedy selection with a simple fast selection criterion. Although we\nfocused on the change in differential entropy in our experiments here, the simple\nlikelihood approximation at the basis of our method allows for other equally efficient\ncriteria such as information gain [3]. Our method retains many of the benefits\nof probabilistic GP models (error bars, model combination, interpretability, etc.)\nwhile being much faster and more memory-efficient both in training and prediction.\nIn comparison with non-probabilistic SVM classification, our method enjoys the\nfurther advantages of being simpler to implement and having strictly predictable\ntime requirements. Our method can also be significantly faster10than SVM with the\nSMO algorithm. This is due to the fact that SMO\u2019s active set typically fluctuates\nheavily across the training set, thus a large fraction of the full kernel matrix must\nbe evaluated. In contrast, IVM requires only d\/n of K.\n9It is straightforward to obtain the IVM for a joint GP classification model, however\nthe training costs raise by a factor of c2. Whether this factor can be reduced to c using\nfurther sensible approximations, is an open question.\n10We would expect SVMs to catch up with IVMs on tasks which require fairly large\nactive sets, and for which very simple and fast covariance functions are appropriate (e.g.\nsparse input patterns)."},{"page":8,"text":"Among the many proposed sparse GP approximations [1, 8, 9, 10, 11], our method\nis most closely related to [1]. The latter is a sparse Bayesian online scheme which\ndoes not employ greedy selections and uses a more accurate likelihood approxima-\ntion than we do, at the expense of slightly worse training time scaling, especially\nwhen compared with our randomized version. It also requires the specification of a\nrejection threshold and is dependent on the ordering in which the training points\nare presented. It incorporates steps to remove points from I, which can also be\ndone straightforwardly in our scheme, however such moves are likely to create nu-\nmerical stability problems. Smola and Bartlett [8] use a likelihood approximation\ndifferent from both the IVM and the scheme of [1] for GP regression, together with\ngreedy selections, but in contrast to our work they use a very expensive selection\nheuristic (O(n\u00b7d) per score computation) and are forced to use randomized greedy\nselection over small selection indexes. The differential entropy score has previously\nbeen suggested in the context of active learning (e.g. [3]), but applies more directly\nto our problem. In active learning, the label yiis not known at the time xihas to\nbe scored, and expected rather than actual entropy changes have to be considered.\nFurthermore, MacKay [3] applies the selection to multi-layer perceptron (MLP)\nmodels for which Gaussian posterior approximations over the weights can be very\npoor.\nAcknowledgments\nWe thank Chris Williams, David MacKay, Manfred Opper and Lehel Csat\u00b4 o for help-\nful discussions. MS gratefully acknowledges support through a research studentship\nfrom Microsoft Research Ltd.\nReferences\n[1] Lehel Csat\u00b4 o and Manfred Opper. Sparse online Gaussian processes. N. Comp., 14:641\u2013\n668, 2002.\n[2] Neil D. Lawrence and Ralf Herbrich. A sparse Bayesian compression scheme - the\ninformative vector machine. Presented at NIPS 2001 Workshop on Kernel Methods,\n2001.\n[3] David MacKay. Bayesian Methods for Adaptive Models. PhD thesis, California Insti-\ntute of Technology, 1991.\n[4] Thomas Minka. A Family of Algorithms for Approximate Bayesian Inference. PhD\nthesis, MIT, January 2001.\n[5] Manfred Opper and Ole Winther. Gaussian processes for classification: Mean field\nalgorithms. N. Comp., 12(11):2655\u20132684, 2000.\n[6] John C. Platt. Fast training of support vector machines using sequential minimal\noptimization. In Sch\u00a8 olkopf et. al., editor, Advances in Kernel Methods, pages 185\u2013\n208. 1998.\n[7] Matthias Seeger, Neil D. Lawrence, and Ralf Herbrich. Sparse Bayesian learning:\nThe informative vector machine. Technical report, Department of Computer Science,\nSheffield, UK, 2002. See www.dcs.shef.ac.uk\/~neil\/papers\/.\n[8] Alex Smola and Peter Bartlett. Sparse greedy Gaussian process regression. In Ad-\nvances in NIPS 13, pages 619\u2013625, 2001.\n[9] Michael Tipping.\nJ. M. Learn. Res., 1:211\u2013244, 2001.\nSparse Bayesian learning and the relevance vector machine.\n[10] Volker Tresp. A Bayesian committee machine. N. Comp., 12(11):2719\u20132741, 2000.\n[11] Christopher K. I. Williams and Matthias Seeger. Using the Nystr\u00a8 om method to speed\nup kernel machines. In Advances in NIPS 13, pages 682\u2013688, 2001."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000.pdf","widgetId":"rgw30_56aba0965207a"},"id":"rgw30_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=221618434&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw31_56aba0965207a"},"id":"rgw31_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=221618434&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":221618434,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"0fcfd50a4b79c5968f000000","name":"Matthias Seeger","date":"Nov 15, 2012 ","nameLink":"profile\/Matthias_Seeger","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"dd2a32ee0f57e685bcc581f78b5f7b65","showFileSizeNote":false,"fileSize":"131.41 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"0fcfd50a4b79c5968f000000","name":"Matthias Seeger","date":"Nov 15, 2012 ","nameLink":"profile\/Matthias_Seeger","filename":"download.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"dd2a32ee0f57e685bcc581f78b5f7b65","showFileSizeNote":false,"fileSize":"131.41 KB","noFollow":false,"isDefault":true,"doi":null},{"linkId":"0fcfd50a4b79987036000000","name":"Matthias Seeger","date":null,"nameLink":"profile\/Matthias_Seeger","filename":"","downloadLink":"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79987036000000.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79987036000000.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"b9c0f0b408b96d7cd927edaa72c8c6b2","showFileSizeNote":false,"fileSize":"131.41 KB","noFollow":false,"isDefault":false,"doi":null}],"hasDisplayableLinks":true,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Conference Paper","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=HAUo-xlmmKr9XXsODnB9-1nM9w4rvMDdEY-pkD2BvXC-YaOR-mkMTvom55ODKGzBfzm1L8Bkvl3LxedoE_qAEg.BcTyCLkBh8uEx3Mh3qbjLUwuPLh6Pp5HFHAH5QdXfgC_lM24E5TO_zQ2Kn4h3SvfNYTCBD4zBq0PIpQbudE6UA","clickOnPill":"publication.PublicationFigures.html?_sg=6sBjbmg6L-5OCpARZYt7HKl8NI1UDSXPTIPpYxZEJtyouDecFKVPL3XmCI-wt3LshE1QYpdR_pn1XOO2ESlXMQ.CvCA7Xy-V2LozD2xid0EH-3fukv3XdrnmUth1V3Prq__jW1fOooGjSr4v8M8Zgvp6ruPYLxZdeXgL_OrzMv-Hw"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMatthias_Seeger%2Fpublication%2F221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine%2Flinks%2F0fcfd50a4b79c5968f000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=PCN7DwZ25QpcfnGwgBM1cY6qhb6YN3TglYaRa93bSi65opJ89vfSOZ-hh8itvH00UZpkEmkCb-notmQjufPBaQ","urlHash":"1792327b2f71e017dd8e8611eb518b7f","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=bXmI-3TDCGsKBuM9gRvJYJLfjNeSGpISz2hqN-i211ZrVQZOqYTYAjQnv8BGxv681JtzttIMMP5pQ2wteuRuyUXhHt-L9k0jG1AeYjX82K0.IKAzv2QQQ1QJzG3-b-6NvkMk3gc010gx8_otw5rJiZwKmSkrKcBZrmxaIVpmEI0Agdkw1ZFYW7IuwNUUnU1kBA.Uh36k66BCrENRamgO6K8_NTevKFl_RwCyGNzKs1syhdfSxcXCjQug-oMu5HIGieTdZ38PaJ2TCH8BnPjRG5xKQ","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"0fcfd50a4b79c5968f000000","trackedDownloads":{"0fcfd50a4b79c5968f000000":{"v":false,"d":false}},"assetId":"AS:99200494735367@1400662649203","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":221618434,"commentCursorPromo":null,"widgetId":"rgw33_56aba0965207a"},"id":"rgw33_56aba0965207a","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMatthias_Seeger%2Fpublication%2F221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine%2Flinks%2F0fcfd50a4b79c5968f000000.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A99200494735367%401400662649203&publicationUid=221618434&linkId=0fcfd50a4b79c5968f000000&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Fast Sparse Gaussian Process Methods: The Informative Vector Machine","publicationType":"Conference Paper","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=6grynkjGiddqaa126ZI93vJRypCoeNm5jCkkpkRG_raYZ_jlGTjEIuZxyyRXpR7dE0a74qFL4GqKFtbglM2JkI5zm_zyLVuwA4b4XbdZspY.WCg-eOslXjyjBJVJF7mFW2YZQGxDgpmOwqT93hqXUgJJZUNFoOr3QOGvZbAPkzcb3-VENmEa7LfxtQEeg9Sslg.dMCVGdWKJTdCXqxq8kK7fOjm6tx4kWxU_AO6fOaFTVbOKdjKFTVk-CRhvO4ecEIYwYLnbp8m90gMcqKYbcH0fg","publicationUid":221618434,"trackedDownloads":{"0fcfd50a4b79c5968f000000":{"v":false,"d":false},"0fcfd50a4b79987036000000":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba0965207a"},"id":"rgw35_56aba0965207a","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw36_56aba0965207a"},"id":"rgw36_56aba0965207a","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw37_56aba0965207a"},"id":"rgw37_56aba0965207a","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw38_56aba0965207a"},"id":"rgw38_56aba0965207a","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw39_56aba0965207a"},"id":"rgw39_56aba0965207a","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw34_56aba0965207a"},"id":"rgw34_56aba0965207a","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw32_56aba0965207a"},"id":"rgw32_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba0965207a"},"id":"rgw2_56aba0965207a","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":221618434},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=221618434&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba0965207a"},"id":"rgw1_56aba0965207a","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"SEd\/vvRiwmGyQFcBMxFWkdSz1NaTM6PSzpKPeonBDIko\/dlCGwVTeT92MiKeM+Bpaigrf2O+TLmwxD\/JCXFs6NsR0lV4mxCOdM7onouB2WFneeQXUR4WTkCXjcRAI59Q16v7UUW44uw+R6iPuJW4s61kbSB5pjO09xnNon\/BsIig1ipUtBwpax7BrOXPiUzX+mFTowuFTL\/IzSy9ScB\/3DAmGzbTCOhIQzvv9T6JRGd2HxmCxMJdFdjP9kKJpXzTPyoQPZNiWztw5b9tJzWQQ7In4n271owjWpZ5Deu04U0=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Fast Sparse Gaussian Process Methods: The Informative Vector Machine\" \/>\n<meta property=\"og:description\" content=\"Abstract We present a framework for sparse Gaussian process (GP) methods which uses forward selection with criteria based on informationtheoretic principles, previously suggested for active...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\" \/>\n<meta property=\"rg:id\" content=\"PB:221618434\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Fast Sparse Gaussian Process Methods: The Informative Vector Machine\" \/>\n<meta name=\"citation_author\" content=\"Neil D. Lawrence\" \/>\n<meta name=\"citation_author\" content=\"Matthias Seeger\" \/>\n<meta name=\"citation_author\" content=\"Ralf Herbrich\" \/>\n<meta name=\"citation_conference_title\" content=\"Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, NIPS 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]\" \/>\n<meta name=\"citation_publication_date\" content=\"2002\/01\/01\" \/>\n<meta name=\"citation_volume\" content=\"15\" \/>\n<meta name=\"citation_firstpage\" content=\"609\" \/>\n<meta name=\"citation_lastpage\" content=\"616\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Matthias_Seeger\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\/links\/0fcfd50a4b79c5968f000000.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-2164d29b-fa8c-406a-91cf-3a7cb0060870","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":536,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw40_56aba0965207a"},"id":"rgw40_56aba0965207a","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-2164d29b-fa8c-406a-91cf-3a7cb0060870", "88bb907ff1c64a4c38561bf90e06aff1e665fbc9");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-2164d29b-fa8c-406a-91cf-3a7cb0060870", "88bb907ff1c64a4c38561bf90e06aff1e665fbc9");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw41_56aba0965207a"},"id":"rgw41_56aba0965207a","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/221618434_Fast_Sparse_Gaussian_Process_Methods_The_Informative_Vector_Machine","requestToken":"yTGxvqRodiSpgMHmHw6lxw1ytbXf4AEKCW3VZZkH0QoLMxFHHyXNNfq25lSLYVaYFtQCxJPoDfiIeDMJuF+BrKxa8tSH\/3I0PLRn32zwwKRdduwFuiMnj1ImnuMZRiv4xnrYAPTJNZb\/BTP6JZ8H+imhezDFQV1DVm5LHIPrZs0jgKIYZFiJ7ZjGuBXtnxPkwgVrGmRJOrZUdYZecbPZwKG\/tN62dLHw5Zjx7u5OTgNc\/rmASg3ASgdW81tSPJ3nv\/oWybxSX6mv1TLFFAiSXrobvgaCyOMDjgUVJk5nURk=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=UpZsion51NIZ89EP6WM4hM87bUoqrE16fBnylvLTGHi2rvPeXoO80pF_LXQte_W-","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjIxNjE4NDM0X0Zhc3RfU3BhcnNlX0dhdXNzaWFuX1Byb2Nlc3NfTWV0aG9kc19UaGVfSW5mb3JtYXRpdmVfVmVjdG9yX01hY2hpbmU%3D","signupCallToAction":"Join for free","widgetId":"rgw43_56aba0965207a"},"id":"rgw43_56aba0965207a","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw42_56aba0965207a"},"id":"rgw42_56aba0965207a","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw44_56aba0965207a"},"id":"rgw44_56aba0965207a","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
