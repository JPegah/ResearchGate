<!DOCTYPE html> <html lang="en" class="" id="rgw38_56aba1bfdff61"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="Z/Ax4lu3tNsBjiuqDARuT0/6CpWqUiB/mVj56kujWoNboDdhl36Vwu8vfgK5KoxNR8Cs807HEM9VX+RpdqMPSaKT4egkSjNNYprQSybudzEfk456GD4iYwbYQJ8aab+8ckzfxFcgBNHg6FL/xn9CGtA2G3U0C5WH3/GosoI0/EKb+1PTyPDLSbEBy6izlSx2icB9A/Skg4EaazVH2+5vVpZvpn4mw28lEzktyalqvgFNu8+Zro96CGyBL/UfIWCbYcXHI+5Yd6YGdNWlRI+x1kku9hW6ITCUdivXC3yvKqQ="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-acf1f9bb-b938-4a3b-b1f2-c45a65a03891",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds" />
<meta property="og:description" content="The global and local convergence properties of a class of augmented Lagrangian methods for solving nonlinear programming problems are considered. In such methods, simple bound constraints are..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds/links/5452316c0cf2bf864cbb3140/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds" />
<meta property="rg:id" content="PB:265543262" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1137/0728030" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds" />
<meta name="citation_author" content="Andrew R. Conn" />
<meta name="citation_author" content="Nicholas I.M. Gould" />
<meta name="citation_author" content="Philippe L. Toint" />
<meta name="citation_publication_date" content="1991/04/01" />
<meta name="citation_journal_title" content="SIAM Journal on Numerical Analysis" />
<meta name="citation_issn" content="0036-1429" />
<meta name="citation_volume" content="28" />
<meta name="citation_issue" content="2" />
<meta name="citation_doi" content="10.1137/0728030" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Andrew_Conn/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds/links/5452316c0cf2bf864cbb3140.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba1bfdff61" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba1bfdff61" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba1bfdff61">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1137%2F0728030&rft.atitle=A%20Globally%20Convergent%20Augmented%20Lagrangian%20Algorithm%20for%20Optimization%20with%20General%20Constraints%20and%20Simple%20Bounds&rft.title=SIAM%20Journal%20on%20Numerical%20Analysis&rft.jtitle=SIAM%20Journal%20on%20Numerical%20Analysis&rft.volume=28&rft.issue=2&rft.date=1991&rft.issn=0036-1429&rft.au=Andrew%20R.%20Conn%2CNicholas%20I.M.%20Gould%2CPhilippe%20L.%20Toint&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds</h1> <meta itemprop="headline" content="A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds/links/5452316c0cf2bf864cbb3140/smallpreview.png">  <div id="rgw8_56aba1bfdff61" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw9_56aba1bfdff61" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Andrew_Conn" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A272408045158413%401441958547518_m" title="Andrew R. Conn" alt="Andrew R. Conn" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Andrew R. Conn</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw10_56aba1bfdff61" data-account-key="Andrew_Conn">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Andrew_Conn"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A272408045158413%401441958547518_l" title="Andrew R. Conn" alt="Andrew R. Conn" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Andrew_Conn" class="display-name">Andrew R. Conn</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/IBM2" title="IBM">IBM</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw11_56aba1bfdff61"> <a href="researcher/3213163_Nicholas_IM_Gould" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Nicholas I.M. Gould" alt="Nicholas I.M. Gould" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Nicholas I.M. Gould</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw12_56aba1bfdff61">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/3213163_Nicholas_IM_Gould"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Nicholas I.M. Gould" alt="Nicholas I.M. Gould" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/3213163_Nicholas_IM_Gould" class="display-name">Nicholas I.M. Gould</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw13_56aba1bfdff61"> <a href="researcher/69746694_Philippe_L_Toint" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Philippe L. Toint" alt="Philippe L. Toint" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Philippe L. Toint</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw14_56aba1bfdff61">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/69746694_Philippe_L_Toint"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Philippe L. Toint" alt="Philippe L. Toint" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/69746694_Philippe_L_Toint" class="display-name">Philippe L. Toint</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">      <span itemprop="isPartOf" itemscope itemtype="http://schema.org/Periodical"> <a itemprop="sameAs" href="journal/0036-1429_SIAM_Journal_on_Numerical_Analysis"><span itemprop="name">SIAM Journal on Numerical Analysis</span></a> </span>    (Impact Factor: 1.79).     <meta itemprop="datePublished" content="1991-04">  04/1991;  28(2).    DOI:&nbsp;10.1137/0728030           </div> <div id="rgw15_56aba1bfdff61" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>The global and local convergence properties of a class of augmented Lagrangian methods for solving nonlinear programming problems are considered. In such methods, simple bound constraints are treated separately from more general constraints and the stopping rules for the inner minimization algorithm have this in mind. Global convergence is proved, and it is established that a potentially troublesome penalty parameter is bounded away from zero.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw16_56aba1bfdff61" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56aba1bfdff61">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw30_56aba1bfdff61">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Andrew_Conn/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds/links/5452316c0cf2bf864cbb3140.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Andrew_Conn">Andrew R. Conn</a>, <span class="js-publication-date"> Oct 30, 2014 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw32_56aba1bfdff61" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw33_56aba1bfdff61" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw34_56aba1bfdff61" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw35_56aba1bfdff61" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw36_56aba1bfdff61" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw37_56aba1bfdff61" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw31_56aba1bfdff61" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndrew_Conn%2Fpublication%2F265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds%2Flinks%2F5452316c0cf2bf864cbb3140.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw28_56aba1bfdff61"  itemprop="articleBody">  <p>Page 1</p> <p>SIAM J. NUMER. ANAL.<br />Vol. 28, No. 2, pp. 545-572, April 1991<br />(C) 1991 Society for Industrial and Applied Mathematics<br />015<br />A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN ALGORITHM<br />FOR OPTIMIZATION WITH GENERAL CONSTRAINTS AND<br />SIMPLE BOUNDS*<br />ANDREW R. CONN?, NICHOLAS<br />I. M. GOULD$, AND PHILIPPE L. TOINT<br />Abstract. The global and local convergence properties of a class of augmented Lagrangian methods<br />for solving nonlinear programming problems are considered. In such methods, simple bound constraints<br />are treated separately from more general constraints and the stopping rules for the inner minimization<br />algorithm have this in mind. Global convergence is proved, and it is established that a potentially troublesome<br />penalty parameter is bounded away from zero.<br />Key words, constrained optimization, augmented Lagrangian, simple bounds, general constraints<br />AMS(MOS) subject classifications. 65K05, 90C30<br />1. Introduction. In this paper, we consider the problem offinding a local minimizer<br />of the function<br />(1.1)<br />f(x),<br />where x is required to satisfy the constraints<br />(1.2)<br />and the simple bounds<br />Ci(X<br />O,<br />&lt;--<br />&lt;--m,<br />(1.3)<br />Herefand ei map R<br />we shall assume that the region B<br />further assume that<br />(AS1)<br />The functions f(x) and ci(x) are twice continuously ditterentiable for all<br />xB.<br />We assume that any general inequality constraints ci(x)&gt;-0 have already been converted<br />into equations by the introduction of slack variables (see, e.g., Fletcher (1981, p. 8));<br />we wish the combinatorial side of the minimization problem to be represented purely<br />in terms of simple bound constraints. We shall attempt to solve our problem by means<br />of a sequential minimization of the augmented Lagrangianfunction<br />l&lt;=x&lt;=u.<br />into R and inequalities (1.3) are considered componentwise;<br />{xll&lt;-x&lt;-_u} is nonempty and may be infinite. We<br />1<br />(1.4)<br />(I)(x, A, S,/x)-f(x)/2 l\ici(x)/<br />SiiCi(X)<br />2<br />i=1<br />i=1<br />where the components Aiof the vector A are known as Lagrange multiplier estimates,<br />where the entries sii of the diagonal matrix S are positive scaling factors, and where<br />/z is known as the penalty parameter. Note that we do not include the simple bounds<br />(1.3) in the augmented Lagrangian function; rather the intention is that the sequential<br />minimization will automatically ensure that these constraints are always satisfied.<br />Received by the editors October 31, 1988; accepted for publication (in revised form) April 4, 1990.<br />? Department of Combinatorics and Optimization, University of Waterloo, Ontario, Canada. The<br />research of this author was supported in part by the Natural Sciences and Engineering Research Council<br />of Canada and by the Information Technology Research Centre, which is funded by the Province of Ontario.<br />$ Computer Science and Systems Division, United Kingdom Atomic Energy Authority, Harwell, Oxford,<br />United Kingdom.<br />Department of Mathematics, Facult6s Universitaires Notre Dame de la Paix, B-5000, Namur, Belgium.<br />545<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 2</p> <p>546<br />A. R. CONN, N.<br />I. M. GOULD, AND P. L. TOINT<br />Our principal interest is in solving large-scale problems. With a few notable<br />exceptions (see, for example, Murtagh and Saunders (1980), Lasdon (1982), Drud<br />(1985)), there has been little progress in constructing algorithms for such problems;<br />this is somewhat understandable in view of the lack of a consensus as to the &quot;best&quot;<br />algorithm for solving small nonlinear programs. Nevertheless, there are many large-<br />scale applications awaiting a suitable algorithm.<br />A similar situation existed for unconstrained optimization in the early 1970s.<br />However, during the past ten years, this deficiency has been redressed primarily through<br />the development ofthree important ideas. The first is the recognition that large problems<br />normally have considerable structure and that such structure usually manifests itself<br />as sparsity or low rank of the relevant matrices. This has lead to suitable ways of<br />storing and approximating problem data (function, gradient, and Hessian approxima-<br />tions), see, for example, Griewank and Toint (1982). The second development is the<br />realization that, although Newton’s method (or a good approximation to it) is necessary<br />for rapid asymptotic convergence of an algorithm, in early iterations only very crude<br />approximations to the solution ofthe Newton equations are needed to guarantee global<br />convergence. In particular, the steepest descent method often makes very good initial<br />progress towards a minimizer. This has led to a study of realistic conditions that suffice<br />to guarantee global convergence of an algorithm and also of methods which satisfy<br />such conditions, the truncated conjugate gradient method being a particularly successful<br />example. This work is described, for example, by Toint (1981), Dembo, Eisenstat, and<br />Steihaug (1982), and Steihaug (1983). Third, the development of trust-region methods<br />(see, e.g., Mor6 (1983)) has allowed a sensible handling of negative curvature in the<br />objective function; for large-scale problems whose second derivatives are available<br />(contrary to popular belief, an extremely common circumstance in many problem<br />areas), this enables meaningful steps towards the solution to be made when the Hessian<br />matrix is indefinite. Significantly, these ideas have had an important impact on the<br />design of algorithms not only for large problems but also for small ones (see, Toint<br />(1988), and Dixon, Dolan, and Price (1988)).<br />One issue that is not present in unconstrained minimization, but is in evidence<br />here, is the combinatorial problem of finding which of the variables lie at a bound at<br />the solution (such bound constraints are said to be active). In active-set algorithms,<br />the intention is to predict these variables and to minimize the function with respect<br />to the remaining variables. Obviously, an incorrect prediction is undesirable, and it is<br />then useful (indeed essential for large problems) to be able to make rapid changes in<br />the active set to correct for wrong initial choices. Unfortunately, many existing<br />algorithms for constrained optimization only allow very small changes in the active<br />set at each iteration, and consequently, for large problems, there is the possibility of<br />requiring a large number of iterations to find the solution. Fortunately, for simple<br />bound constraints, it is easy to allow for rapid changes in the active set in the design<br />of algorithms (see, e.g., Berksekas (1982b, pp. 76-92), and Conn, Gould, and Toint,<br />(1988a)).<br />Our intention here is to develop a fairly general algorithm which may benefit from<br />the above-mentioned advances. We have recently developed and tested (Conn et al.<br />(1987), Conn, Gould, and Toint (1988a), (1988b)) an algorithm for solving bound<br />constrained minimization problems (problems of the form minimize (1.1) subject to<br />(1.3)) which is appropriate in the large-scale case. Our basic idea is now to use this<br />algorithm within an augmented Lagrangian framework, that is to use the algorithm to<br />find an approximation to a minimizer of the augmented Lagrangian function (1.4)<br />subject to the bounds (1.3) for a sequence of different values of S, A, and/x.The novelty<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 3</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />547<br />comes from being able to solve the augmented Lagrangian problems approximately<br />and on being able to deal with the bounds in an efficient manner.<br />The augmented Lagrangian method was proposed independently by Hestenes<br />(1969) and Powell (1969), partly as a reaction to the unfortunate side-effects associated<br />with ill-conditioning ofthe simpler ditterentiable penalty and barrier functions (Murray<br />(1971)). Indeed, Powell showed, using a very simple device, how to ensure that the<br />penalty parameter does not converge to zero and hence that the resulting ill-conditioning<br />does not occur. A similar device is employed in the algorithms with which we are<br />concerned in this paper with the same consequence. A concise statement of the.salient<br />features of augmented Lagrangian methods, or multiplier methods as they are some-<br />times known, is given, for example, by Fletcher (1981). The most comprehensive<br />references on augmented Lagrangians are the paper by Tapia (1977) and the book by<br />Bertsekas (1982b). Globally convergent methods have been given by Powell (1969),<br />Rockafellar (1976), Bertsekas (1982b), Polak and Tits (1980), Yamashita (1982),<br />Bartholomew-Biggs (1987), and Hager (1987). Powell’s method requires that the<br />augmented Lagrangian be minimized exactly for fixed values of the multipliers and<br />parameters. The multiplier estimates are guaranteed to be bounded, but convergence<br />is only established in the case where the underlying nonlinear program has a unique<br />solution. Rockafellar, Bertsekas, and Polak and Tits allow inexact minimization of the<br />augmented Lagrangian function, but they require that the Lagrange multiplier estimates<br />remain bounded--the methods differ in the stopping criteria used. Hager is slightly<br />more restrictive in that he considers a particular multiplier update and specifies the<br />method used for approximately minimizing the augmented Lagrangian function. His<br />analysis also assumes that a subsequence of the Lagrange multiplier estimates con-<br />verges. Yamashita and Bartholomew-Biggs are more specific in the method used for<br />the inner minimization calculation--an appropriate quadratic program is solved--but<br />their methods allow for more frequent updating ofthe penalty parameter and multiplier<br />estimates. Yamashita establishes convergence under the assumption that the Lagrange<br />multipliers for the quadratic programming problem stay bounded; the possibility of<br />proving convergence for Bartholomew-Bigg’s method under similar circumstances is<br />only hinted at although encouraging numerical results are presented.<br />Interest in augmented Lagrangians declined with the introduction of successive<br />quadratic programming (SQP) techniques but recently has gained in popularity. (See<br />for example the papers of Schittkowski (1981) and Gill et al. (1986) which combine<br />SQP with an augmented Lagrangian merit function. Both these methods are not pure<br />augmented Lagrangian techniques since they perform a line search on the augmented<br />Lagrangian as a function of both the position x and the multipliers I in contrast to<br />the method described in this paper.)<br />One strong disadvantage ofSQP methods for large-scale problems is that, although<br />there is a theory of how to truncate the solution process in the early iterations (see,<br />Dembo and Tulowitzki (1984))mas is used so successfully in the unconstrained casemit<br />is not clear to us how to construct an efficient algorithm that conforms to this theory.<br />We feel that solving a quadratic programming to completion at each iteration is probably<br />too expensive a calculation for large-scale problems in the same way that solving the<br />Newton equations exactly is considered too expensive in large-scale unconstrained<br />minimization. We thus feel there are compelling reasons for trying to use an alternative<br />to the SQP approach.<br />Bertsekas<br />(1982a)<br />and<br />others, however,<br />Lagrangians are particularly attractive for large problems, where active set strategies<br />are inappropriate, and we tend to agree with this sentiment. In particular, simple<br />have<br />remarked that<br />augmented<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 4</p> <p>548<br />A. R. CONN, N.<br />I. M. GOULD, AND P. L. TOINT<br />multiplier estimates may be used. In this paper we explore some of the issues involved<br />in using an augmented Lagrangian approach for large-scale problems. We have deliber-<br />ately not included the results of numerical testing as, in our view, the construction of<br />appropriate software is by no means trivial and we wish to make a thorough job of<br />it. We will report on our numerical experience in due course. We should comment,<br />nonetheless, that at the heart of any method for solving nonlinear programming<br />problems, there is a need to find an approximate solution to a system oflinear equations.<br />Linear equation solvers may be broadly categorised as either direct or iterative methods.<br />In the former, a factorization of the relevant matrix is used, while in the latter,<br />matrix-vector products involving the relevant matrix are required. Here the coefficient<br />matrix for such systems will typically be symmetric submatrices of the Hessian of the<br />augmented Lagrangian function (1.4). For iterative methods, sparsity in the derivatives<br />ofthe objective function and constraints may be exploited in the matrix-vector products.<br />For direct methods, there is often a concern that the Hessian of an augmented<br />Lagrangian function may be less sparse than for the Lagrangian function because of<br />the last term in (1.4). While this is certainly true, it is worth noting that variables that<br />appear nonlinearly in the constraint functions give rise to nonzeros in the same positions<br />in the Hessians of both the Lagrangian and augmented Lagrangian function. It may<br />therefore be worth treating linear constraints in a different way from nonlinear ones;<br />we are currently pursuing this line of research.<br />Our exposition will be considerably simplified if we consider the special case<br />where li=0 and ui=oo for all 1-&lt;i- &lt;n<br />modification required to handle more general constraints will be indicated at the end<br />of the paper. Thus we consider the problem:<br />(1.5)<br />minimizef(x),<br />in (1.3). Although straightforward, the<br />subject to the constraints<br />(1.6)<br />Ci(X<br />O,<br />&lt;= &lt;= m,<br />and the nonnegativity restrictions<br />(1.7)<br />x B ={x R&quot;[x&gt;=O}.<br />The paper is organised as follows. In<br />and then state a pair of related algorithms for solving (1.5)-(1.7) in<br />convergence is established in<br />4, while issues of asymptotic convergence follow in<br />An example showing the importance of a certain assumption in<br />while in<br />7 the consequences of satisfying second-order conditions are given. We<br />conclude in<br />8 by indicating how this theory applies to the original problem (1.1)-(1.3).<br />2 we introduce concepts and definitions<br />3. Global<br />5.<br />6,<br />5 is given in<br />2. Notation. In this section we introduce the notation to be used throughout the<br />paper. We will use the projection operator defined componentwise by<br />0<br />if xg=&lt;0,<br />otherwise.<br />(2.1)<br />(P[x]),<br />xi<br />This operator projects the point x onto the region B. Furthermore, we will make use<br />of the &quot;projection&quot;<br />x-/’Ix- v].<br />Let g(x) denote the gradient Vxf(X) of f(x) and let H(x) denote its Hessian matrix<br />Vxxf(x). Let A(x) denote the m-by-n Jacobian of e(x), where<br />(2.3)<br />c(x)<br />[c,(x),<br />(2.2)<br />/’(x, v)<br />&quot;, c,,(x)]r,<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 5</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />549<br />and let Hi(x) denote the Hessian matrix VxxCi(X<br />HL(x,A) denote the gradient and Hessian matrix (taken with respect to its first<br />argument) of the Lagrangian function<br />of Ci(X). Finally, let gL(x, A) and<br />(2.4)<br />L(x, A) =f(x)+2 ici(x)<br />i=1<br />We note that L(x, A) is the Lagrangian function with respect to the C constraints only.<br />If we definefirst-orderLagrange multiplier estimates<br />(2.5)<br />g(x, h(x,,S, I)).<br />Now suppose that {x(k=&gt;0} and {A(k)} are infinite sequences of n-vectors and<br />m-vectors, respectively, that {S(k)} is an infinite sequence of positive-definite diagonal<br />matrices, and that {/(k)} is an infinite sequence of positive scalars. For any function<br />F, we shall use the notation that F(k)denotes F evaluated with arguments x(kA(k<br />S(k), or/(k) as appropriate. So, for instance, using the identity (2.6), we have<br />(2.7)<br />V,(k<br />Vx(x(k, k, Sk),/k))<br />where we have written<br />X(x,,S,/)= h + Sc(x)/I,<br />we shall make much use of the identity<br />(2.6)<br />Vx(X, A, S, )<br />gL(xk, (k)),<br />(2.8)<br />(2.9)<br />For any x(k, we have two possibilities for each component<br />(i)<br />Oxlk)(VxO(k))i<br />(Vx(I)(k))i &lt;Xl<br />or<br />(ii)<br />k)<br />namely,<br />In case (i) we then have<br />(2.10)<br />(P(x(k), Vxlff(k)))<br />Xl<br />k)<br />whereas in case (ii) we have<br />(2.11)<br />We shall refer to anxl<br />satisfies (ii) is known as a floating variable. The algorithms we are about to develop<br />construct iterates Which force P(x(k),VxtI)(k)) to zero as k increases. Thedomi0ated<br />variables are thus pushed to zero, while the floating variables are allowed to find their<br />own level.<br />If, in addition, there is a convergent subsequence {xk)}, k<br />x*, we wish to partition the set N<br />{1, 2,<br />are related to the two possibilities (i) and (ii) above and to the corresponding x*:<br />I={i[xlk)are floating for all k K sufficiently large and x/*&gt;0},<br />I2={i[r&lt;k)<br />--i<br />(2.12)<br />I3<br />14= N\(I, U12[,..JI).<br />From time to time we will slightly abuse notation by saying that a variablexi belongs<br />to (for instance) I, when strictly we should say that the index of the variable belongs<br />to I. We will also mention the components of a (given) vector in the set I when<br />strictly we mean the components of the vector whose indices lie in I.<br />(P(xk), V k))),<br />(Vxk))i&quot;<br />k)which satisfies (i) as a dominated variable; a variable which<br />K, with limit point<br />, n} into the following four subsets which<br />are dominated for all k K sufficiently large},<br />{ilxl<br />k)are floating for all k K sufficiently large but x/* =0}, and<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 6</p> <p>550<br />A. R. CONN, N.<br />I. M. GOULD, AND P. L. TOINT<br />If the iterates are chosen so that P(x(k), Vx(I.)(k)) approaches zero as k increases,<br />we have the following result.<br />LEMMA 2.1. Suppose that {x(k)}, k<br />K, is a convergent subsequence with limitpoint<br />x*, that h (k), s(k),i(k)I1, 12, I3, andI4are as above, and that P(x(k,Vx(k)) approaches<br />zero as k K increases. Then<br />(i) The variables in sets I2, 13, and14all converge to their bounds;<br />(ii) The components of(V,k))i in the setsIand13 converge to zero’, and<br />(iii) Ifa component of(V@k)i in the set14 converges to afinite limit, then the<br />limit is zero.<br />Proof (i) The result is true for variables in 12 from (2.10), for those in 13 by<br />definition and for those in 14 as, again from (2.10), there must be a subsequence of<br />the k K for whichXl<br />(ii) The result follows for iinIand13from (2.11).<br />(iii) This is true for<br />from (2.11), (Tx((k))i converges to zero.<br />It will sometimes be convenient to group the variables in sets I3 and/4 together<br />and call the resulting set<br />k)converges to zero.<br />in14as there must be a subsequence of the k K for which,<br />(2.13)<br />As we see from Lemma 2.1, I5 gives variables which are zero at the solution and which<br />may correspond to zero components of the gradient of the augmented Lagrangian<br />function. These variables are potentially (dual) degenerate at the solution of the<br />nonlinear programming problem.<br />We will let (x) denote the components of g(x) indexed by I. Similarily,(x)<br />denotes the corresponding columns ofthe Jacobian matrix; indeed any matrix M refers<br />to the columns of the generic matrix M indexed by I.In addition, we will define the<br />least-squares Lagrange multiplier estimates (corresponding to the set 11)<br />-(A(x)+)(x)<br />15 13 U/4.<br />(2.14)<br />A (x)<br />at all points where the right generalized inverse<br />(2.15)<br />of /i(x) is well defined. We note that ,(x) is dilterentiable; for completeness the<br />derivative is given in the following lemma.<br />LEMMA 2.2. Suppose that (AS1) holds. Ifi(x)(x)<br />differentiableand its derivative is given by<br />(2.16)<br />VxA(x)<br />where the ith rowofR(x) is<br />Proof The result follows by observing that (2.14) may be rewritten as<br />(2.17)<br />A(x)+=,(x)(A(x)ft(x))-’<br />is nonsingular, A(x)<br />is<br />(A(x)+)TIIL(x, t (X)) --(A(X)A(X)T)-IR(X),<br />and A(x)r(x)=O<br />for some vector r(x). Differentiating (2.17) and eliminating the derivative of r(x) from<br />the resulting equations gives the required result.<br />We stress that, as stated, the Lagrange multiplier estimate (2.14) is not a directly<br />calculable quantity as it requires an a priori knowledge of x*. It is merely introduced<br />as an analytical device but we shall show in due course that a variant of this estimate<br />may be calculated and used.<br />We are now in a position to describe more precisely the. algorithms we propose<br />to use.<br />[3<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 7</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />551<br />3. Statement of the algorithms. In order to solve problem (1.5)-(1.7), we consider<br />the following algorithmic models. Here ]t&quot;<br />matrix norm).<br />denotes any vector norm (or its subordinate<br />ALGORITHM 1.<br />Step 0 [Initialization]. An initial vector of Lagrange multiplier estimates A() is<br />given. The positive constants r/o,/o, too, z &lt; 1, /1(1, to,&lt;&lt; 1, r/,&lt;&lt; 1, a,/3, c%,and<br />too(a()).,,r/()= r/o(a())%, and k=0.<br />finare specified. The diagonal matrices S1 and S=, for which 0&lt;S_-&lt;S=&lt;, are<br />given (the inequalities are to be understood elementwise for the diagonal elements).<br />Set/x() =/Xo,a()=min (/x(), y), to()<br />Step 1 [Inner iteration]. Define a diagonal scaling matrix S()for whichS-<br />IlP(x<br />-&lt;_S()-&lt;_<br />$2. Find x()B such that<br />(3.1)<br />If<br />execute step 2. Otherwise, execute step 3.<br />(3.3)<br />Step 2<br />[Test for convergence and update Lagrange multiplier<br />IlP(x), x7())ll :&lt;o, and IIc(x(&quot;)ll_-&lt;n,, stop. Otherwise, set<br />estimates]. If<br />/(k+l)<br />(x(k),,(k), S(k), [.1,(k)),<br />]’(k+l)<br />]’(k),<br />a(k/)<br />min (/(k/) Yl)<br />tok(ag/)o<br />r/(k)(t(k+l))/3.,<br />to(k+<br />r/(k+l)<br />increment k by one and go to step 1.<br />(3.4)<br />Step 3 [Reduce the penalty parameter]. Set<br />h(k+)<br />h(k)<br />(k+l)<br />(k)<br />/x<br />’/x<br />1), ’)/1),<br />a(k+)<br />min (/x<br />to0(a(k+1)) a,,.,,<br />r/o(a(k+))%,<br />to(k+l)<br />r/(k+)<br />increment k by one and go to step 1.<br />ALGORITHM 2.<br />Step 0 [Initialization]. An initial vector of Lagrange multiplier estimates, h(, is<br />given. The nonnegative constantanand the positive constants r/o,/o, z &lt; 1, too, Y&lt; 1,<br />7, &lt; 1,to.&lt;&lt; 1, r/.&lt;&lt; 1, u, ao, flo, andfinare specified. The diagonal matrices Sand<br />$2, for which O&lt;S-(&lt;-S&lt;, are given. Set /z(=/o, a()=min(/(), 7), to(o)=<br />too(a(o))%, r/(o)<br />r/o(a(o))%, and k<br />0.<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 8</p> <p>552<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />Step 1 [Inner iteration]. Define a diagonal scaling matrix S(k)for whichS-<br />$2. Find x&lt;k)B such that<br />IlP(x(&quot;), X7x(&quot;)II-&lt;_ o&quot;.<br />Compute a new vector of Lagrange multiplier estimates &lt;k+l). If<br />IIc(x)ll_&lt;-<br />II(x,Vx))ll_-&lt;,o, and IIc(x))ll-&lt;_ n,, stop. Otherwise, set<br />(k+l)<br />(k)<br />=/<br />(3.5)<br />(3.6)<br />execute step 2. Otherwise, execute step 3.<br />Step 2<br />[Test for convergence and update Lagrange multiplier<br />estimates]. If<br />A(k+)<br />[A<br />k)<br />otherwise,<br />(3.7)<br />k+l)<br />k+)<br />(g)(a(k+)),<br />k)(ak+))&amp;<br />increment k by one and go to step 1.<br />Step 3 [Refluce the penalty parameter and up,ate Lagrange multiplier estimates], Set<br />(k+)<br />(k)<br />A(+)<br />[A)<br />min (+’ y)<br />Wo(+))&quot;o,<br />o(a+)-,,<br />otherwise,<br />(3.8)<br />+)<br />w+)<br />+<br />increment k by one and go to step 1.<br />The motivation for both algorithms is quite straightforward. Traditional augmented<br />Lagrangian methods are known to be locally convergent if the penalty parameter is<br />sufficiently small and if the augmented Lagrangian is approximately minimized at each<br />stage (see, for instance, Rockafellar (1976), Bertsekas (1982b,<br />that the method is globally convergent, as a last resort we must drive the penalty<br />parameter to zero and ensure that the Lagrange multiplier estimates do not behave<br />too badly. The convergence of such a scheme is guaranteed, since in this case, the<br />iteration is essentially that used in the quadratic penalty function method (see, for<br />example, Gould (1989)). We consider this further in<br />traditional multiplier iteration to take over, the test on the size of the constraints<br />(3.2)/(3.6) is based upon the size that might be expected if the multiplier iteration is<br />converging. This aspect is considered in<br />The algorithms differ in their use of multiplier updates. Algorithm 1 is designed<br />specifically for the first-order estimate (2.5); the multiplier estimates are encouraged<br />to behave well as a consequence of the test (3.2). For large-scale computations, it is<br />likely that first-order estimates will be used and thus Algorithm 1 is directly applicable.<br />Algorithm 2 allows any multiplier estimate to be used. This extra freedom means that<br />2.5)). In order to ensure<br />4. In order to try to allow the<br />5.<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 9</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />553<br />tighter control must be maintained on the acceptance of the estimates to make sure<br />that they do not grow unacceptably fast. In this algorithm, we have in mind using any<br />ofthe well-known Lagrange multiplier update formulae, including the first-order update<br />(2.5) (used in Algorithm 1), the least-squares update (2.14), and other updates summar-<br />ized, for instance, by Tapia (1977). We note, however, that some of these updates may<br />require a significant amount ofcomputation and this may prove prohibitively expensive<br />for large-scale problems. Algorithm 2 is identical to Algorithm<br />Lagrange multiplier updates, the fact that these updates may also occur in step 3 and<br />the presence of the scalars,and y.<br />Both algorithms use a number of free parameters. To give the reader some feel<br />for what might be typical values, we suggest that for well-scaled problems ao =/o<br />’<br />4. Gh)balconvergence analysis. In this section we shall make use of the following<br />assumptions&quot;<br />The iterates {x(k)} considered lie within a closed, bounded domain<br />(AS3)<br />x*, of the sequences {x(k)} considered in this paper.<br />Note that (AS3) excludes the possibility that11is empty unless there are no general<br />constraints. In view of Lemma 2.1, this seems reasonable as otherwise we are allowing<br />the possibility that all the constraints and bounds are satisfied as equations at x*. We<br />also observe that (AS3) is equivalent to assuming that the gradients of the general<br />constraints and active bounds at any limit point are linearly independent, an assumption<br />that is commonly made in the analysis of other methods (see Bertsekas (1982b), and<br />Fletcher (1981)).<br />We shall analyse the convergence of the algorithms of<br />convergence tolerancesto.and 7, are both zero. We require the following pair of<br />lemmas in the proof of global convergence of our algorithms. Essentially, the results<br />show that the Lagrange multiplier estimates generated by either algorithm cannot<br />behave too badly.<br />LEMMA 4.1. Suppose thatt.t<br />is executed. Then the producttz<br />h<br />converges to zero.<br />(k)<br />Proof If<br />converges to zero, step 3 ofthe algorithm must be executed infinitely<br />often. Let K<br />{ko, kl, km,&quot;<br />3 of the algorithm is executed and for which<br />/z(k)__&lt; min (()1/,<br />We consider how the Lagrange multiplier estimates change between two successive<br />iterations indexed in the set K. At iteration ki+j, fork&lt;k+j &lt;-ki+l, we have<br />except for the allowed<br />y<br />70<br />too<br />1, a, o<br />yl<br />0.1,/3,<br />0.9, and z<br />0.01 are appropriate.<br />(ASm)<br />The matrix /](x*) has column rank no smaller than rn at any limit point,<br />3 in the case where the<br />(k)converges to zero as k increases when Algorithm 1<br />k<br />k<br />&quot;} be the set of the indices of the iterations in which step<br />(4.1)<br />j--1<br />(4.2)<br />l(ki+J)<br />l(ki)-[<br />s(ki+l)c(x(ki+l))/[J,(ki+l)<br />/=1<br />and<br />(4.3)<br />tz(ki+’)<br />[J,(ki+J)<br />[.lb(ki+l)<br />&quot;/’/(ki)<br />where the summation in (4.2) is null ifj<br />of iterationski+ 1,<br />from (3.2), (4.3), and the recursive definition of7(g), we must also have<br />(4.4)<br />1. Now suppose that j&gt; 1. Then for the set<br />&lt;j, step 2 of the algorithm must have been executed and hence,<br />&lt;-<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 10</p> <p>554<br />A. R. CONN, N.<br />I. M. GOULD, AND P. L. TOINT<br />Combining equations (4.1) to (4.4) and using the imposed upper bound on S(k)<br />obtain the bound<br />j-1<br />IIh( ’+ &gt;ll<br />II ( ’&gt;ll +Z Ils<br />!=1<br />(k+l)<br />j-1<br />(4.5)<br />l=l<br />wheres2is the norm of $2. Thus we obtain that<br />Equation<br />(.+)<br />(4.6)<br />(k+)<br />is<br />T(.)<br />also<br />satisfied when j=l<br />()<br />as equations<br />(3.4) and<br />(4.3)<br />give<br />Hence from (4.6),<br />(4.7)<br />Equation (4.7) then gives that &lt;k’)llA&lt;k’)]] converges to zero as k increases. For, if we<br />define<br />(4.8)<br />and<br />equations (4.3), (4.7), and (4.8)ivethat<br />(4.9)<br />a+_-&lt;’c+’%fl<br />and<br />and hence that<br />i-1<br />(4.10)<br />0--&lt;_a,-&lt;r’Ceo+(r%)’ E<br />/=0<br />Ifre,&lt; 1, the sum in (4.10) can be bounded to give<br />(4.11<br />0<br />ol<br />’l’it)go -J1-<br />7&quot;<br />ijo/ 1<br />whereas ifc,&gt; 1, we obtain the alternative<br />(4.12)<br />0_-&lt;c_-&lt;&quot;(Co+ ’%-/3o/(1<br />’%-1)),<br />and ifa,<br />1,<br />(4.13)<br />0&lt;-_ai&lt;--’iao+fl’io.<br />But, both aoand /30 are finite. Thus, as<br />(4.9) implies thatfl converges to zero. Therefore, as the right-hand side of (4.6)<br />converges to zero, the truth of the lemma is established.<br />We note that Lemmas 4.1 may be proved under much weaker conditions on the<br />sequence {r/(k)} than those imposed in Algorithm 1. All that is needed is that, in the<br />proof just given,Zj-ll=l<br />positive power of/x(k’+).<br />Turning to Algorithm 2, we have the following easier-to-establish result.<br />LEMMA 4.2. Suppose thattx<br />is executed. Then the productI<br />l<br />converges to zero.<br />increases, c converges to zero; equation<br />c(x(k‘+/))]] in (4.5) should be bounded by some multiple of a<br />(k)converges to zero as k increases when Algorithm 2<br />k<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 11</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />555<br />Proof Let K<br />(4.14)<br />and consequently on which A(+<br />(4.15)<br />If K is finite, A(k)will be fixed for all k sufficiently large and the result is immediate.<br />IfK is infinite, for anyk&lt;k&lt;-ki+l,A(k)=A(ki+l), and/b/(k)/(k,+l). Hence, from (4.15)<br />(4.16)<br />/xll<br />By hypothesis, the right-hand side of (4.16) can be made arbitrarily small by choosing<br />ki large enough, and so /x(k)[[A(k)[[ converges to zero.<br />As a precursor to our main result, we have the following general convergence<br />lemma. The lemma and resulting theorem are in the spirit of Proposition 2.3 of Bertsekas<br />(1982b) but do not require that the Lagrange multiplier estimates stay bounded and<br />allow for our handling of simple bound constraints.<br />LEMMA 4.3. Suppose that (AS1) holds. Let {xk)}<br />(AS2) which converges to the point x*forwhich (AS3) holds and let A*<br />Asatisfies (2.14). Assume that {A k)}, k<br />is any sequenceofdiagonal matrices satisfying 0 &lt; $1<br />k<br />K,forma nonincreasing sequenceofpositive scalars. Supposefurtherthat<br />(4.17)<br />IIP(x{,v)ll =&lt; o<br />where the to(k) are positive scalar parameters which converge to zero as k K increases.<br />Then<br />(i) There are positive constants a, a, s and an integerkosuch that<br />(4.18)<br />IIX(x&lt;),<br />(4.19)<br />and<br />{ko, kl, k2,&quot; &quot;} be the iterations on which<br />iiX+lll<br />(+l.Then, from (4.14),<br />#,+,lla(,+,ll__&lt;(#,+,)l-,.<br />p(j(.$(k+l))-T<br />B, k<br />K, be a sequence satisfying<br />A (x*), where<br />K, is any sequenceofvectors, that {Sk)}, k<br />-1&lt;S(k&lt;S2&lt; e, and that {tx(k)},<br />K,<br />)s<br />A (x()<br />**11--&lt; a=llx&quot;-x*l,<br />(4.20)<br />forall k&gt;-ko,<br />Suppose, in addition, that c(x*)=0. Then<br />(ii) x* is a Kuhn- Tuckerpoint rst-orderstationary point)forthe problem (1.5)-<br />(1.7), A*<br />{,(x(k) /(k)s(k)<br />(iii) The gradients Vcb(k) converge to gL(X*, A*)fork<br />Proof As a consequence of (AS1)-(AS3), we have that for k K sufficiently large,<br />,(x(k))+exists, is bounded, and converges to ft.(x*)+. Thus we may write<br />(4.21)<br />(A(x()+)TII<br />for some constant a &gt; 0. As the variables in the set11are floating, equations (2.7),<br />(2.8), (2.11), and the inner iteration termination criterion (4.17) give that<br />(4.22)<br />?(x) +A(x)X(<br />By assumption, A(x) is bounded for all x in a neighbourhood of x*. Thus we may<br />deduce from (2.14), (4.21), and (4.22) that<br />IIX(-a(x()II<br />(4.23)<br />_&lt;((x()+) II,o__&lt; al,o(&gt;.<br />k K ).<br />is the corresponding vector of Lagrange multipliers, and the sequences<br />(k))} and {A (x(k))} converge to A*fork<br />K&quot;<br />K.<br />_--&lt;a<br />_-&lt; ,o(.<br />(A(x()+)?(x()+X(<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 12</p> <p>556<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />Moreover, from the integral mean value theorem and Lemma 2.2 we have that<br />(4.24)<br />1(x()-1(x*)<br />V1(x(s)) ds&quot; (x(-x*),<br />whereVI(x) is given by equation (2.16) and where x(s)=x(+s(x*-x(). Now the<br />terms within the integral sign are bounded for all x sufficiently close to x* and hence<br />(4.24) gives<br />I1(x())<br />for some constanta2&gt; 0, which is just the inequality (4.19). We then have that A (x(k)<br />converges to A*. Combining (4.23) and (4.25) we obtain<br />IlX)-*ll&lt;-IlX)-(x))ll/ll(x))-*ll&lt;-ao/a211x)-x*ll,<br />the required inequality (4.18). Then, since by construction w(ktends to zero as k<br />increases, (4.18) implies that )converges to A* and from (4.22) we have that<br />L(x*, A *)<br />Moreover, from the identity (2.6), X7x) converges to gL(x*, A*). Furthermore, multi-<br />plying (2.5) byx), we obtain<br />(4.28)<br />(X(k))<br />(4.25)<br />*11-&lt;a211x()-x*ll<br />(4.26)<br />(4.27)<br />if(x*) +/](x*)rA*<br />0.<br />idb(k)s(k)-l(((k)<br />1 :#)+ (I:<br />I (k))).<br />Taking norms of (4.28) and using (4.26) we derive (4.20), where S<br />Now suppose that<br />is the norm of $1.<br />(4.29)<br />(x*) =0<br />and consider the status of the variables in the sets 11, 12, and15.Lemma 2.1 and the<br />convergence ofVx)to &amp;(x*, A*) show that the complementary slackness condition<br />(4.30)<br />gL(x*, A *)rx*<br />O<br />is satisfied. The variables in the set11 are, by definition, positive at x*. The components<br />of g(x*, A*) indexed by 12 are all nonnegative from (2.9) as their corresponding<br />variables are dominated. This then gives the conditions<br />x*&gt;0<br />and<br />(gc(x*,A*))i=O<br />(4.31)<br />x*=O<br />and<br />(g(x*,a*))_-&gt;O<br />x*=O<br />and<br />(gc(x*,a*)),=O<br />Equations (4.29) and (4.31) thus show that x* is a Kuhn-Tucker point and A* are the<br />corresponding set of Lagrange multipliers. Moreover, (4.18) and (4.19) ensure the<br />for/e/l,<br />for ieI2,<br />and<br />for i/.<br />convergence of the sequences {(x(k), A(), S,x)} and {A(x)} to A* for k<br />Hence the lemma is proved.<br />We now show that both Algorithms 1 and 2 possess a powerful global convergence<br />property under relatively weak conditions.<br />THEOREM 4.4. Assume that (AS1) holds. Let x* be any limit pointofthe sequence<br />{X(k)} generated by Algorithm 1 or by Algorithm 2of<br />hold and let K be the setofindicesofaninfinitesubsequenceofthe x(whose limit is<br />x*. Then conclusions (i), (ii), and (iii) ofLemma 4.3 hold.<br />Proof The assumptions given are sufficient to reach the conclusions of part (i)<br />of Lemma 4.3. We now show that (4.29) holds for Algorithms 1 and 2. To see this, we<br />consider two separate cases:<br />K.<br />[3<br />3forwhich (AS2) and (AS3)<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 13</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />557<br />(i) If/x<br />(k)is bounded away from zero, step 2 must be executed every iteration<br />for k sufficiently large. But this implies that (3.2) is always satisfied (k large enough)<br />and r/(k converges to zero.<br />Hence c(x(k) converges to zero.<br />(ii) If/x(kconverges to zero, Lemma 4.1 for Algorithm 1 and Lemma 4.2 for<br />Algorithm 2 show that /x(&quot;ll<br /> *11converges to zero. But then, inequality (4.20)<br />gives the required result.<br />Hence (4.29)<br />holds.<br />Note that Theorem 4.4 would remain true regardless of the actual choices of<br />and {r/(k)} provided that both sequences converge to zero.<br />is satisfied and thus conclusions<br />(ii) and<br />(iii) of Lemma 4.3<br />5. Asymptotic convergence analysis. We now give our first rate-of-convergence<br />result. It is inconvenient that the estimates (4.18)-(4.20) depend uponIIx&lt;  -x*ll.The<br />next lemma removes this dependence and gives a result similar to the classical theory<br />in which the errors in x are bounded by the errors in the multiplier estimates<br />(see Bertsekas (1982b, Prop. 2.4)); however, as an inexact minimization of the aug-<br />mented Lagrangian function is made, in the spirit of Bertsekas (1982b, Prop. 2.14)),<br />a term reflecting this is also present in the bound. Once again, the result allows for<br />our handling of simple bound constraints. Before giving our result, we need to make<br />two additional assumptions.<br />We use the notation that, ifJ1andJ2are any subsets of N, Hl(X*, h)[ji,J2] is the<br />matrix formed by taking the rows and columns of Hl(X*, h*) indexed by J1 and J2,<br />respectively, andA(x*)[j,]is the matrix formed by taking the columns ofA(x*) indexed<br />by J. We use the following assumptions:<br />(AS4)<br />The second derivatives of the functions f(x) and the Ci(X<br />(AS5)<br />(5.1)<br />are Lipschitz<br />continuous at all points within.<br />Suppose that (x*, A*) is a Kuhn-Tucker point for problem (1.5)-(1.7) and<br />that<br />J-{il(gL(X*,h*))i-O<br />J2={i[(gL(x*,A*))i=O<br />Then we assume that the matrix<br />and<br />x/*&gt;0},<br />x*=0}.<br />and<br />A*)tj,jl<br />(A(x*)tjl)<br />A(x*)tj]<br />0<br />is nonsingular for all sets J, where J is any set made up from the union of<br />J and any subset ofJ.<br />We note that assumption (AS5) implies (AS3). Furthermore, ifJ2is empty, any point<br />satisfying the well-known second-order sufficiency condition for a minimizer of (1.5)-<br />(1.7) (see, e.g., Fletcher (1981, Thm. 9.3.2)) automatically satisfies (AS5) (see, e.g.,<br />Gould (1985)). When J2 is nonempty, the connection between (AS5) and Fletcher’s<br />condition is less clear, although (AS5) is certainly implied by the stronger second-order<br />sufficiency condition given by Luenberger (1973, pp. 234-235). We believe, however,<br />our assumption is reasonable in that small perturbations to the problem can cause<br />some elements ofJe to defect to J while others may drop entirely from J as their<br />gradient components become positive. As we do not know which might defect under<br />such perturbations, (AS5) is a form of &quot;insurance&quot; against all possible eventualities.<br />LEMMA 5.1. Suppose that (AS1) holds. Let {xk)}<br />converges to the Kuhn-Tucker point x*forwhich (AS2), (AS4), and (AS5) hold, and<br />B, k<br />K, be a subsequence which<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 14</p> <p>558<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />let A* be the corresponding vectorofLagrange multipliers. Assume that {A (k)}, k<br />any sequenceofvectors, that {S(k}, k<br />K, is any sequenceofdiagonal matrices satisfying<br />0&lt;S-<br />scalars, so that theproduct 11A)-A*<br />furtherthat<br />IIP(x(, vx&lt;))ll &lt;,<br />en there are positive constants,a3, a4, as, a6, ands and an integer valuekoso<br />thatif<br />K, is<br />&lt;-_S(k&lt;_$2 &lt; o, and that {txk}, k<br />K,forma nonincreasing sequenceofpositive<br />converges to zero as k increases. Now, suppose<br />(5.2)<br />where the w(are positive scalar parameters whichconveneto zero as k K increases.<br />(oN g then<br />and<br />IIc(x()ll s,(asoo((+((+ a6(/z(k))2) I(-- *11)<br />forall k&gt;-ko,<br />Proof. We will denote the gradient and Hessian of the Lagrangian function, taken<br />with respect to x, at the limit point (x*, A*) by g* and H*, respectively.<br />We first need to make some observations concerning the status of the variables<br />as the limit point is approached. We pick k sufficiently large that the setsI1and 12,<br />defined in (2.12), have been determined. Then, for k<br />float (variables in I3) or oscillate between floating and being dominated (variables in<br />I4). Now recall the definition (2.13) of15and pick an infinite subsequence, K of K<br />such that:<br />(i) 15<br />(ii) Variables in16are floating for all k<br />(iii) Variables inI7are dominated for all k<br />Note that the set13of (2.12) is contained within 16. Note, also, that there are only a<br />finite number (-&lt;2151) of such subsequences / and that for k sufficiently large, each<br />k K is in one such subsequence. It is thus sufficient to prove the lemma for k<br />Now, for k<br />K, define<br />k K ).<br />K, the remaining variables either<br />16[_J17with16[&quot;]17 ;<br />K; and<br />K.<br />K.<br />(5.6)<br />Iv=I,U16<br />and<br />I=12UI7.<br />So, the variables in I: are floating while those in Io are dominated. We may now<br />invoke Lemma 4.3(i) to obtain inequalities (4.18) and (4.20) for some A*. Furthermore,<br />using (4.20) and the current assumption that txk)llAk)--A*ll converges to zero as k<br />increases, we have that c(x*)= 0. Thus Lemma 4.3(ii) implies that A* is the vector of<br />Lagrange multipliers corresponding to x*. We thus have<br />(5.7)<br />and<br />for all sufficiently large k/ from inequalities (4.18) and (4.20). Moreover,<br />converges to A* and henceVx@k)converges to g*. Therefore, from Lemma 2.1,<br />(5.9)<br />x/*<br />0<br />for all<br />Ic,<br />and<br />(g*L)i<br />0<br />for all<br />I:.<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 15</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />559<br />Using Taylor’s theorem,<br />Vxf(k)<br />g(k)+<br />g(x*) + H(x*)(x(k)-x*)+ A(x*)<br />(5.10)<br />where<br />(5.11)<br />and<br />(5.12)<br />r2(x(k), x*, X(k), A*) ,(X}k-Af)t-I(x*)(x(g)-x*).<br />j=l<br />The boundedness and Lipschitz continuity of the Hessian matrices offand Cs in a<br />neighbourhood of x* along with the convergence of(k) to A* then give that<br />r,(x(), x*, X())II-&lt;avlx()-x*ll2,<br />rz(x(&quot;), x*, X(&quot;), A*)ll &lt;--a8llx(&quot;-x*ll Ilx(&quot;-A*11<br />for some positive constants a7andas.In addition, again using Taylor’s theorem and<br />that c(x*)<br />O,<br />c(x(k))<br />A(x*)(x(k)-x*) + r3(x(k), x*),<br />(5.13)<br />(5.14)<br />where<br />(5.15)<br />(r3(x(k),x*))i<br />S<br />(x(k--X*)rHi(x*+ tS(x(k--X*))(x(k--X*) dtds<br />o<br />(see Gruver and Sachs (1980, p. 11)). The boundedness of the Hessian matrices of the<br />c in a neighbourhood of x* then gives that<br />(5.16)<br />r(x(), x*)ll-&lt;a9llx(k)-x*ll<br />for some constant ag&gt; 0. Combining (5.10) and (5.14), we obtain<br />(517)(Ht(x*,A*) Ar(x*))(x(k)--x*) (Vx*(k)--gL(X*,A*)) (r,+r2)<br />A(x*)<br />where we have suppressed the arguments of rl, r2 and r3 for brevity. To proceed<br />further, we introduce the notation thatyjis the vectorformed by taking the components<br />of the vector y indexed by the set J. We may then rewrite (5.17) as<br />Hc(x*,<br />2<br />0<br />X(k)-A*<br />c(x(k))<br />r<br />A*)[IF,IF]<br />Hc(x*,,<br />H(x* A*)i,i Ar(x*)II<br />A(x*)tol<br />h*)[lv,l,)]<br />AT(x*)[lv]fl(x(k)--X*)[l.][<br />(k)(X(k))[ID]h,<br />H(x*,h*)i,,,,.<br />A(x*)t.j<br />0<br />(Vx()-g(x*,*)),<br />C(X(k))<br />(rl+r)o<br />r<br />]<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 16</p> <p>560<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />using (5.9). Then, rearranging (5.18) and removing the middle horizontal block we<br />obtain<br />(HL(X*<br />A(x*),,:<br />(5.19)<br />A*)[i_,,.]<br />AT(x*)[,:])((x(k)--X*)[,.])<br />0<br />C(X(k))<br />A(x*)[l](x(k))[l<br />r3<br />Roughly, the rest of the proof proceeds by showing that thet the right-hand side of<br />(5.19) is O(w(k))+O((k)llA(k)--A*]). This will then ensure that the vector on the<br />’left-hand side is of the same size, which is the result we require. First, observe that<br />(5.20<br />from (2.10) and (5.2) and<br />(5.21)<br />from (2.11). Consequently, again using (5.9),<br />(5.22)<br />Let x()=II(x(-x*)c,.ll. Combining (5.7) and (5.22), we obtain<br />(5.23)<br />a + a2. Furthermore, from (5.13), (5.16), (5.22), and (5.23),<br />((r,+<br />where a<br />a7+ a9+aaaz,<br />a2<br />2(a7+ a9)+ as(ao+ a2),<br />Moreover, from (5.8), (5.20), (5.21), and (5.22),<br />IIx&lt;-x*ll<br />IIX(-a*11<br />alo(+a2x,<br />where ao<br />all(X(k))2+al2X(k)w(k)+al3(W(k))2<br />and<br />a3<br />a7+ a9+ aaao.<br />(5.25)<br />k<br />c(x()-A(x*),,)(x(),<br />&lt;a4()+s(()[[A()-A*[[ +ao<br />()+a2()x(),<br />where<br />(5.26)<br />a,4<br />+<br />A(x*),,]<br />By assumption (AS5), the coefficient matrix on the left-hand side of (5.19) is non-<br />singular. Let its inverse have norm M. Multiplying both sides of the equation by this<br />inverse and taking norms, we obtain<br />x(k)--X*)[I]<br />&lt;= M[a 40)(k)<br />(k)<br />(k)<br />(k<br />(k)<br />(5.27)<br />+aow<br />+ a,,(ax()+aax((+ a(w()].<br />+a2)x<br />Now, suppose that k is sufficiently large that<br />(5.28)<br />w(Nmin (1, 1/(4Maz)).<br />Fuhermore, let<br />(5.29)<br />g =min (1, 1/(4Mazs,)).<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 17</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />561<br />Then, ifIx<br />(5.30)<br />where a-sao+ a3+ a4.As Ax()converges to zero, we have that<br />(5.34)<br />mx()ll-&lt; 1/(4Ma11)<br />for all k sufficiently large. Hence inequalities (5.30) and (5.31) give that<br />(5.32)<br />Writing a3=4Ma15+1 and a4=4Msl, we obtain the desired inequality (5.3) from<br />(5.22) and (5.32). Now, using (5.3) and (5.7), we obtain (5.4), wherea5<br />a6<br />multiplying the inequality byIX(k).<br />We can obtain the following simple corollary.<br />COROLLARY 5.2. Suppose that the conditionsofLemma 5.1 hold and that (k+) is<br />any Lagrange multiplier estimateforwhich<br />(5.33)<br />&lt;-a,6[[x(-x*ll/a7o<br />forsome positive constantsa16anda17and all k K sufficiently large. Then there are<br />positive constants12, a3, a4, a5, a6, s1and an integervaluekoso thatifix(k)&lt;--_12then (5.3),<br />(5.34)<br />and (5.5) holdforall k&gt;-ko, (k<br />K).<br />Proof Inequality (5.34) follows immediately from (5.33) and (5.3).<br />We now show that the penalty parameter will normally be bounded away from<br />zero in both Algorithms 1 and 2. This is important as many methods for solving the<br />inner iteration subproblem will encounter difficulties if the parameter converges to<br />zero since this causes the Hessian of the augmented Lagrangian to become increasingly<br />ill-conditioned.<br />THEOREM 5.3. Suppose that the iterates {x(k)}ofAlgorithm<br />to the single limit point x*, that (AS1), (AS2), (AS4), and (AS5) hold, thataand<br />satisfy<br />(5.35)<br />a,&lt; a-= min (1, a,o),<br />(5.36)<br />and that (5.33) holdsforall k sufficiently large when Algorithm 2 is used. Then there is<br />(k)&gt;<br />forall k.<br />--ix<br />Proof Suppose, otherwise, that Ix(k) tends to zero. Then, step 3 of the algorithm<br />must be executed infinitely often. We aim to obtain a contradiction to this statement<br />by showing that step 2 is always executed for k sufficiently large. We note that our<br />assumptions are sufficient for the conclusions of Theorem 4.4 to hold.<br />First, we show that the sequence of Lagrange multipliers {h(k)} converges to h*.<br />Consider Algorithm 1. The result is clear if step 2 is executed infinitely often as<br />each time the step is executed, A(k+)=(k and the inequality (4.18) guarantees that<br />(k)converges to A *. Suppose that step 2 is not executed infinitely often. Then ]IA(k)_A&quot;11<br />will remain fixed for all k&gt;-kfor some k, as step 3 is executed for each remaining<br />iteration. But then (4.20) implies that IIc(x   )ll_-&lt;a,7 <br />k&gt;k2&gt;k As Ix(k) converges to zero as k increases, al7ix(k)&lt;7/’lO(ix(k))an<br />k sufficiently large. But then inequality (3.2) must be satisfied for some k_-&gt;kl, which<br />is impossible, as this would imply that step 2 is again executed. Hence, step 2 must<br />be executed infinitely often.<br />(k)__&lt;/2, (5.27)-(5.29) give<br />AX(k)&lt;=4m(alw(k)+slix(k)l]A(k)--h*[[).<br />al+a2a3and<br />a2a4. Finally, (5.5) follows from (5.4) by substituting for (k), using (2.5), and<br />[3<br />or 2of<br />3 converges<br />/3,&lt; min<br />1,/3,0 ),<br />a constanttx&gt;Osuch thatIx<br />for some constanta17for all<br />(k) for all<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 18</p> <p>562<br />A. R. CONN, N.<br />I. M. GOULD, AND P. L. TOINT<br />Now consider Algorithm 2. The result is clear ifthe multiplier updates are accepted<br />infinitely often, as each time the update is performed Ak+l= (k+l and assumption<br />(5.33) guarantees that(+1)converges to A*. Suppose that the update is not accepted<br />infinitely often. Then for all k sufficiently large, I](k+l)ll &gt; P((k+l))-y which implies<br />that (k+)]] diverges. But this contradicts assumption (5.33) and henceA(kconverges<br />to A*.<br />ThereforekAk<br />Letkbe the smallest integer for which<br />(5.37)<br />for all kk. Now let wk)be as generated by either algorithm. Note that, by<br />construction and inequality (5.37),<br />(5.38)<br />o()<br />for all kk. (This follows by definition if step 2 of either algorithm occurs and<br />because the penalty parameter is unchanged whileWk)is reduced when step 3 occurs.)<br />We shall apply Lemma 5.1 or Corollary 5.2 to the iterates generated by the algorithm;<br />we identify the set K with the complete set of integers larger thankand the scalars<br />(k)with the set of penalty parameters computed in steps 2 and 3 of either algorithm.<br />Therefore we can ensure that ) is sufficiently small so that Lemma 5.1 applies to<br />step 1 of Algorithm 1 (or Corollary 5.2 to step 1 of Algorithm 2) and thus that there<br />is an integer k2and constants as, a6, andsso that (5.4)/(5.34) and (5.5) hold for all<br />k<br />kz. Letk3be the smallest integer such that<br />((k))-%<br />--WoS(a+2)’<br />min(<br />a8’ Wos(a+ 2a8)<br />and, if Algorithm 2 is used,<br />A*<br />tends to zero as k increases for both algorithms.<br />(k)<br />Yl&lt;<br />(5.39)<br />o<br />(5.40)<br />()’-,<br />o<br />)<br />(5.41)<br />())r<br />wherea8= a+a6.Note that (5.37) and (5.40) imply that<br />1<br />1<br />(5.42)<br />k())-,<br />18<br />a6<br />for all k<br />k3. Fuhermore, letk4be such that<br />for all k<br />at iteration k-1 and k<br />F has an infinite number of elements.<br />If Algorithm 2 is used, inequality (5.34) gives that<br />k4.Now definek<br />max (k, k, k3, k4), let F be the set {kl Step 3 is executed<br />k} and let kobe the smallest element of F. By assumption,<br />I1 *11+<br />+<br /> *11<br />(from<br />( rom<br />(5.44)<br />IA*I+woa,8())<br />+<br />(+)-<br />(from (5.35))<br /> from<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 19</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />563<br />for all k&gt;ks, the last inequality following from (5.41) and because /al,(k+l)/.(k).<br />Hence, the multiplier update A(k+l)<br />(k+l) in Algorithm 2 will always take place when<br />k&gt;=ko.<br />For iteration ko, wk=Wo(tzk),and k=o(k))%. Then (5.5) gives<br />c(x.o))i<br />((.o+a6())=)IIAo)_A*ll+ao)<br />(2ollo-*ll+aoo)<br />(5.45)<br />S(2Wok+aWo(k))l+)<br />WoS(a+2)k)<br />V0(k)%<br />Thus, from (5.45), Step 2 of Algorithm 1 or the same step of Algorithm 2 will be<br />executed<br />with<br />A(k+l)<br />(X(k), A(k), S(k),<br />Inequality (5.4)/(5.34) in conjunction with (5.35), (5.38), and (5.43) guarantee that<br />(5.46)<br />IlA(o+-A*lla5(+a6(llA(-A*lloa,8(()<br />We shall now suppose that step 2 is executed for iterations ko+ i, (0N Nj), and<br />that<br />I1(o+,+)<br />Inequalities (5.45) and (5.46) show that this is true for j =0. We aim to show that the<br />same is true for i=j+ 1. Under our supposition, we have, for iteration ko+j+ 1, that<br />(o++<br />(o),w(o++)<br />Wo((o))(+)+-, and<br />(5.5) gives<br />(rom (,42))<br />(from (5.43))<br />(from (5.37))<br />(from (5.39)).<br />k)<br />(k))<br />or<br />A(k+l)<br />(k+l)<br />respectively.<br />(5.47)<br />,11<br />oal8<br />(o++l<br />o((o)),(+)+%. Then<br />S,(2(+J+l)ll(++l--*[l+a(++l(++l)<br />N s(2woa8()(())+e#+ asWo(()&quot;+(++)<br />s(2woa((<br />(from (5.42))<br />(from 5.47))<br />(5.48)<br />(from (5.35)-(5.37))<br />(from (5.37))<br />(from (5.40)).<br />os(a+2a8)((o)-,((o),(++%<br />o((o),(++%<br />Thus, from (5.48), step 2 of Algorithm 1 or the same step of Algorithm 2 will be<br />executed with<br />respectively. Inequality (5.4)/(5.34) then guarantees that<br />Ila(++=)<br />*ll<br />woas(#))&quot;+j+)+woa6a8#)())&quot;+<br />woas(#))+.i+)+woa6a8#)(#))&quot;+<br />(5.49)<br />wo(a5+ a6a8(#))<br />wo(as+a6)(#o))&quot;+o,j+)<br />woa8(())&quot;+,(j+),<br />which establishes (5.47) for i=j+ 1. Hence, step 2 of the appropriate algorithm is<br />executed for all iterations k<br />ko. But this implies that F is finite, which contradicts<br />the assumption that step 3<br />proved.<br />n(o++l<br />a5w(+i+)+ 06(++’) x&lt;++’)<br />a*ll<br />(from (5.47))<br />(from (5.35)-(5.37))<br />(from (5.40))<br />is executed infinitely often. Hence the theorem<br />is<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 20</p> <p>564<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />Note, in particular, that if Algorithm 2 is used with t(k+l)chosen as either the<br />first-order or least-squares multiplier estimates, the penalty parameter /k) will stay<br />bounded away from zero. This follows directly from Theorem 5.3 because of the<br />inequalities (4.18) and (4.20).<br />Our definition of floating variables has a further desirable consequence ifwe make<br />the following additional assumption.<br />(AS6)<br />(Strict complementary slackness condition) If the iterates xk), k<br />verge to the limit point x* with corresponding Lagrange multipliers A*, we<br />assume that the set<br />K, con-<br />(5.50)<br />Jz--{il(gL(x*,A*))i=O<br />and<br />x*=0}<br />is empty.<br />Note that if inequality constraints ei(x)&gt;-0 have been converted to equations by<br />the subtraction of slack variables (i.e., rewritten as ei(x)-xn+i=O, xn+i&gt;-_O), this<br />statement of strict complementary slackness is equivalent to the more usual one which<br />says that no inequality constraint shall be both active (the constraint function vanishing)<br />and have a corresponding zero Lagrange parameter (see, e.g., Fletcher (1981, p. 51)).<br />For it is easy to show that the Lagrange parameter for such a constraint is precisely<br />the corresponding component of the gradient of the Lagrangian function. A constraint<br />being active and having a corresponding zero Lagrange parameter is thus the same as<br />the slack variable having the value zero, and its corresponding element in the gradient<br />of the Lagrangian function vanishing so the latter is excluded under (AS6).<br />THEOREM 5.4. Suppose that the iterates X(k), k<br />with corresponding Lagrange multipliers A*, and that (AS1)-(AS3) and (AS6) hold.<br />Thenfork sufficiently large, the setoffloating variables are precisely those which lie away<br />fromtheir bounds at x*.<br />Proof From Theorem 4.4,Vk)converges to gl(X*, A*) and from Lemma 2.1,<br />the variables in the set I5then converge to zero and the corresponding components<br />ofg(x*, A *) are zero. Hence, under (AS6),I5is null. Therefore, each variable ultimately<br />remains tied to one of the setsIorIfor all k sufficiently large; a variable inI is,<br />by definition, floating and converges to a value away from its bound. Conversely, a<br />variable in12is dominated and converges to its bound.<br />As a consequence of Theorem 5.4, the least-squares multiplier estimates (2.14)<br />are implementable. By this we mean that if/k) and k) are the columns of A(x)<br />K, converge to the limit point x*<br />and components of g(x) corresponding to the floating variables at x,respectively,<br />the estimates<br />(=-((k)+)()<br />(5.51)<br />are identical to those given by (2.14) for all k sufficiently large. The estimates (5.51),<br />unlike (2.14), are well defined when x* is unknown.<br />We conclude the section by giving a rate-of-convergence result for our algorithms.<br />For a comprehensive discussion of convergence, the reader is referred to Ortega and<br />Rheinboldt (1970).<br />THEOREM 5.5. Under the assumptionsofTheorem 5.3, the iteratesxk, theLagrange<br />multiplier estimates k)ofAlgorithm<br />are at least R-linearly convergent withR-factorat most16min(fl’fln), where/<br />and wheretzis the smallest valueofthe penalty parameter generated by the algorithm in<br />question.<br />and anyk)satisfying (5.33)forAlgorithm 2<br />min y<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 21</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />565<br />Proof The proofparallels that ofLemma 5.1. First, for k sufficiently large, Theorem<br />5.3 shows that the penalty parameter x(k) remains fixed at some value z, say, and, for<br />all subsequent iterations, inequalities (3.2)/(3.6) and<br />(5.52)<br />(5.25) may be replaced by a4w(k+rl,and consequently,<br />(5.53)<br />Axk)M(aawk)+k)+a(Axk)+aAxkw)+a3(wk))2).<br />Hence, if k is sufficiently large that<br />w(k)Nmin (1, 1/(2Maz))<br />(.O(k+ )-&quot;.)/3O)(k)<br />and<br />T<br />(k+ 1)<br />([)/3n&quot;i<br />(k)<br />hold. Then, from (3.2)/(3.6), (5.20), and (5.21), the bound on the right-hand side of<br />(5.54)<br />and<br />(5.55)<br />Ax(k)<br />1/(4Ma,),<br />inequalities (5.53)-(5.55) can be rearranged to give<br />(5.56)<br />x(k)<br />4M(a9w(k)+ (k))<br />whereal9<br />a3+ a4.But then (5.22) and (5.56) give<br />[Ix()-x’l[N azow()+ae<br />1 +4Ma9andaz<br />that x()converges at least R-linearly, with R-factor m(.,), to X*. That the same<br />is true for ) and ) follows directly from (5.7)/(5.33) and (5.57).<br />(5.57)<br />whereao<br />4M. As, by assumption,,&lt; 1, (5.52) and (5.57) show<br />6. An example. In Theorem 5.3, we showed that, if there is a unique limit point<br />for the iterates generated by the algorithms, the penalty parameter<br />bounded away from zero. We now show that, if there is more than a single limit point,<br />but all the other assumptions of Theorem 5.3 are satisfied, it is indeed possible for the<br />penalty parameter to become arbitrarily close to zero.<br />We consider the problem<br />()<br />is necessarily<br />(6.1)<br />minimize x<br />subject to the single constraint<br />(6.2)<br />x2-1<br />0,<br />for some r&gt; 0. This problem has two stationary points, namely,<br />(x*,*)=(-1,)<br />No bounds appear in the problem, and hence P(x(), V,,()) V()for all k. (Of<br />course, strictly we have not yet defined our algorithms for such a casethis case is<br />covered in<br />8; however, we might think of (6.1)-(6.2) as resulting from a transformation<br />of variables where the nonnegativity constraint has been shifted so as to play no role<br />here.) For simplicity, we choose S(k=I for all k, and it can be verified that<br />Vx(x,,,I,/x)<br />(6.3)<br />and(x*,*)=(1,-).<br />(6.4)<br />2x(x2-1) + 2xA + r.<br />We wish to show that Algorithm 1 can generate a sequence of points that oscillate<br />between neighbourhoods ofx*and x*, and such that the penalty parameter/x(tends<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 22</p> <p>566<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />to zero. The idea is to consider an infinite sequence of iteration cycles, each of length<br />j+ 1, where j is the smallest integer such that<br />(6.5)<br />r/o(min [/Zo, y])%+J, &lt;- min [/Zo, y].<br />2<br />For the first j iterations of each cycle, x)lies in a neighbourhood ofx2*and step 2<br />is executed; for the iteration that remains, x)has a value less thanx*and the penalty<br />parameter is reduced as step 3 is executed. The process is started with A)=A2*.<br />It remains to show that such a sequence can be constructed. For simplicity, we<br />shall consider a single cycle. We will denote the sequence of generated iterates and<br />corresponding Lagrange multiplier estimates by {x(k)} and {A (k)}, 1-&lt;k &lt;-j+ 1, respec-<br />(k)<br />tively, and will let<br />denote the constant penalty parameter throughout the cycle.<br />Now definefl=max(1, ao, flo). Note that, under conditions (5.35) and (5.36),<br />c% and/3, are smaller than/3. Therefore, because our cycle consists ofj iterations in<br />which step 2 is executed followed by a single iteration with step 3, the convergence<br />tolerances satisfy<br />m(k)<br />m0C%,+(k-1)&gt;_mO0<br />(6.6)<br />and<br />T/(k<br />+(k-1)fl,<br />kfl<br />(6.7)<br />oa =oa<br />where a<br />min (z, Yl)&lt; 1. Furthermore, pick<br />(6.8)<br />r_-&lt;4/o<br />and define<br />(6.9)<br />: min(<br />min (1’ 7)<br />-)<br />,o-, r/o<br />The cycle involves two types of iterate&quot;<br />(i) For the first j iterations,<br />(6.10)<br />for k=l,<br />x(k)lies in a neighbourhood ofx*and step 2 is executed. (Strictly, for this demonstra-<br />tion, the power of a in (6.10) need only be kfl; however, the extra power of/3 will<br />be important when we discuss Algorithm 2.)<br />(ii) For the last iteration, h(+=-r/2, x(+l)&lt;x*and step 3 is executed.<br />Turning to details, consider case (i). We first show that equation (6.10) determines<br />x(k). For (6.10) gives that<br />t<br />(6.11)<br />h(k+l=h(k+{--(1--at)a<br />{--a(+)t<br />But then equations (2.5), (3.3), and (6.11) imply that<br />3/3<br />for k<br />for 1<br />for k=j.<br />1,<br />(k+l<br />(6.12)<br />(X(k)) -:(1 a/)t(k+l)/<br />for k=l,<br />for 1 &lt;k&lt;j,<br />for k=j,<br />(k+l)fl<br />for 1 &lt; k-&lt;_j,<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 23</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />567<br />and therefore<br />as c(x)= x-<br />x/1 + fi/o<br />3/3<br />for k<br />for 1 &lt;k&lt;j,<br />for k =j<br />1,<br />(6.13)<br />X(k)-&quot;x/1--(1--a)txa(k+l)<br />x/1<br />/xa(+)<br />1. We now show that such values of x()and h()pass the acceptance<br />tests (3.1) and (3.2). The constraint test (3.2) is satisfied for all 1 N k Nj because of<br />(6.7), (6.9), (6.12) and because<br />Noand a &lt; 1. Fuhermore, it follows from (6.4)<br />and (6.10) that the gradient of the augmented Lagrangian function at (x(), h()) is<br />--x(k))+2x(k)a(k+2)<br />(1 -x(k))<br />for lNk&lt;j,<br />for k=j.<br />(6.14)<br />V(x(), h (k), I, )=<br />It remains to show that this gradient is acceptably small. First,<br />0&lt;(2-<br />(6.17)(2+a3)a2 N (2+ (o) N3(N6NWo.<br />Thus (6.6), (6.16), and (6.17) imply (3.1). Next, consider any k for which 1 &lt; k &lt;j. Then<br />0 1- (1 a)a(k+’)x(k)1<br />3fl<br />(6.15)<br />x(1)<br />1+5a<br />by dint of (6.13). Thus, from (6.8), (6.14), and (6.15),<br />)3<br />(.6.16)<br />3fl<br />Vx(X(,),<br />(1)<br />I, )&lt;(2+3)<br />But as<br />Noand<br />&lt; 1, definition (6.9) gives<br />(6.18)<br />because of (6.9) and (6.13). Hence, from (6.14) and (6.18)<br />0NV(x()h()<br />(6.19)<br />I, )&lt; (2a +ff(1--ff))ff(k+l)<br />Once again, as<br />Noand a &lt; 1, (6.8) and (6.9) give<br />(2a + g(1 a))a N (2+o) N6NWo.<br />Equations (6.6), (6.19), and (6.20) then imply (3.1). Finally, (6.13) gives<br />(6.20)<br />(6.21)<br />0N<br />ff(j+l)<br />x(j)<br />1.<br />Hence, from (6.14) and (6.21)<br />(6.22)<br />OVx(X(j), h(j)<br />1,)&lt;a<br />(j+l)<br />Then (3.1) follows from (6.6), (6.8), (6.9), and (6.22). Moreover, it follows from (6.15),<br />(6.18), and (6.21) thatxis the only possible limit point of the first j iterates of the<br />cycle. Thus we have shown that the first j iterates in our cycle have the required<br />properties.<br />We now consider case (ii). We have to show that it is possible to have x(j+)&lt;x<br />with ]]c(x(J+))[[ &gt; (J+). Equivalently, we show that inequality (3.1) (but not (3.2)) of<br />step 1 is satisfied for some x(j+)of the form<br />(6.23)<br />x(J+)&lt;-1.<br />We note that (6.5) and (6.7) imply that<br />(j+)<br />(6.24)<br />&quot;<br />&lt; -.<br />2<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 24</p> <p>568<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />Recalling that A(j+l)<br />A*=-o., we thus require that the inequalities (6.23),<br />(6.25)<br />IO(x)<br />2--x(x2-1)-o.x+o<br />(.O(j+l)<br />and<br />(6.26)<br />Ix2-11&gt;r/<br />are satisfied at x<br />(6.26). At the endpoints of the interval, we have that<br />x(j+l).Now observe that any x<br />(-v/1 +o./x,-v/1+ r/(+l) satisfies<br />(6.27)<br />--+o.+<br />&gt;0<br />and<br />(6.28)<br />qt(-v/1 + o./x)<br />o’v/1 +o’/x<br />-1 +4’1 +<br />The continuity of the function q, along with (6.27) and (6.28) implies the existence of<br />a root inside the interval. Any x sufficiently close to this root will therefore satisfy the<br />required inequalities (6.23), (6.25), and (6.26) and we select such a point to define<br />x+l). Because of (6.26), step 3 is executed and A(+2)remains equal to A2*.<br />Furthermore, since the interval (-V’l +o/x,-/1 + r/(+1) of case (ii) shrinks to<br />the single pointx*=-1 as/z tends to zero, this point is the only possible limit point<br />of the sequence of iterates besides x2*.<br />This completes our example for Algorithm 1.<br />We now show that a slightly modified form of this example applies to Algorithm<br />2. Given/Zo, pick o. sufficiently small such that<br />o.&lt;min(i<br />Note that such a o&quot; satisfies (6.8). We construct an infinite sequence of iteration<br />cycles, each of length j+ 2 with j defined as before to be the smallest integer such that<br />inequality (6.5) is satisfied. The first j iterations are identical to those already described<br />in case (i) above. Iteration j+ 1 is identical to case (ii) above, except that the Lagrange<br />multiplier estimate is set toA*<br />x(+2close tox2*is selected and the Lagrange multiplier estimate is reset to A2*<br />Note that each Lagrange multiplier estimate, (6.10) or +0-/2, satisfies<br />I.(’)1--&lt;_u/z<br />because c &lt; 1, sc-&lt;_o., and by choice of o&quot; in (6.29). Moreover, (5.33) is satisfied because<br />the errors in the multiplier estimates (6.10) are bounded by w(kfrom (6.7) and (6.9).<br />It remains to show that we can construct a suitable iterate x+2)at the last iteration<br />of each cycle. We thus require that<br />(6.29)<br />)<br />2zo<br />o’/2 in step 3. For the remaining iteration, a point<br />-o’/2.<br />(6.30)<br />-’/<br />(6.31)<br />2<br />(j+2)2<br />x(+2)(x<br />1)+ o’x(+2)+ o&quot;<br />This may be achieved by choosing x(+2)as the zero<br />(.D(j+2).<br />(6.32)<br />+q’l<br />2o./x<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 25</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />569<br />of that function. The Lagrange multiplier estimate is then reset to A 2&quot;, which is allowed<br />because of (6.30) and the fact that the root (6.32) converges tox*<br />zero. Whether or not (3.6) holds is mainly irrelevant, since its failure only causes a<br />further reduction of the penalty parameter, which suits our purpose. However, if (3.6)<br />is violated, we note that (6.6) and (6.7) must be replaced on the next cycle by<br />1 as /z tends to<br />(6.33)<br />tO(k)<br />0)0aa’+k/3w&gt;O)0a(k+l)/3<br />and<br />+k/<br />(6.34)<br />7<br />(k)<br />7oa 7oa<br />where a<br />in the tests (3.5) and (3.6) on the first j iterations of the next cycle because of the<br />presence of the extra a term in equations (6.12) and (6.14).<br />min (/z, yl)&lt; 1. These slightly more stringent tolerances are still acceptable<br />7. Second-order conditions. It is useful to know how our algorithms behave if we<br />impose further conditions on the iterates generated by the inner iteration. In particular,<br />suppose that x(k)satisfies the following second-order sufficiency condition&quot;<br />(AS7)<br />Suppose that X(k)satisfies (3.1)/(3.5), converges to x* for k<br />J1 andJ2 are as defined by (5.1). Then we assume that V,,a’(k)...<br />uniformly positive definite (that is, its smallest eigenvalue is uniformly<br />bounded away from zero) for all k K sufficiently large.<br />K, and that<br />J,uJ2.J,uJ2is<br />With such a condition we have the following result.<br />THEOREM 7.1. Under (AS1)-(AS3) and (AS7), the iterates x(k), k<br />by either Algorithm 1 or 2 converge to an isolated local solutionof(1.5)-(1.7).<br />K, generated<br />Proof By definition of,<br />(7.1)<br />Letsjbe any nonzero vector satisfying<br />(7.2)<br />A(k)<br />-x[j S[j<br />--0,<br />where J is any set made up from the union ofJ1 and any subset ofJ2. Then for any<br />such vector,<br />(7.3)<br />[j,j]S[j]<br />for some e &gt;0, under (AS7) as J is a subset ofJUJz. It follows from (7.1)-(7.3) that<br />stjH(x(k),<br />By continuity ofHLas x(k)and (k)approach their limits, this gives that<br />THL(X, h*<br />(7.5)<br />stj<br />(7.4)<br />k))j,jstj&gt;- e.<br />&gt;<br />)tj,jstj<br />e<br />for all nonzerosj satisfying (7.2), which implies that x* is an isolated local solution<br />to (1.5)-(1.7) (see, for example, Avriel, (1976, Thm. 3.11)).<br />The importance of (AS7) is that the inner iteration termination test (step 1 of<br />either algorithm) might be tightened so that<br />positive definite, for all floating variablesj(kand all k sufficiently large, in addition<br />to (3.1)/(3.5). If the strict complementary slackness condition (AS6) holds at x*,<br />Theorem 5.4 ensures that the set J is empty andJ identical to the set of floating<br />variables after a finite number of iterations and thus, under this tighter termination<br />test, (AS7) and Theorem 7.1 hold.<br />(k)<br />is required to be uniformly<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 26</p> <p>570<br />A. R. CONN, N.I. M. GOULD, AND P. L. TOINT<br />There is a weaker version of this result, proved in the same way, that if the<br />assumption of uniform positive definiteness in (AS7) is replaced by an assumption of<br />positive semidefiniteness, the limit point then satisfies second-order necessary condi-<br />tions (Avriel (1976, Thm. 3.10)) for a minimizer. This weaker version of (AS7) is easier<br />to ensure in practice as certain methods for solving the inner iteration subproblem,<br />for instance, that of Conn, Gould, and Toint (1988a), guarantee that the second<br />derivative matrix at the limit point of a sequence of generated inner iterates will be<br />positive semidefinite.<br />8. Further comments. We now briefly turn to the more general problem (1.1)-(1.3).<br />As we indicated in our Introduction, the presence of the more general constraints (1.3)<br />does not significantly alter the conclusions that we have drawn so far. If we define the<br />appropriate generalization of the projection (2.1) by<br />li<br />(8.1)<br />(P[x])i<br />if X<br />if X<br />otherwise,<br />li,<br />/’/i<br />xi<br />and let B={xll&lt;=x&lt;u}, we may then use the algorithms of<br />significant modification. Our concept of floating and dominated variables stays essen-<br />tially the same; for any iterate x(k)in B we have three mutually exclusive possibilities<br />for each component xlk), namely,<br />(i)<br />0<br />xl<br />(8.2)<br />(Vx()(k))i<br />xlk)-Ui&lt; (Vx((k))i &lt;Xlk)-li.<br />In case (i) we then have<br />(P(x(), Vx()(k)))i<br />3 without further<br />k)<br />(Vx(I.)(k))i<br />Xl<br />(ii)<br />k)<br />U<br />O,<br />(iii)<br />(8.3)<br />Xlk)-li,<br />whereas in case (ii) we have<br />(8.4)<br />(P(x(), Vxo(k)))<br />Xl<br />k)<br />lXi,<br />and in case (iii)<br />(8.5)<br />Thexl)which satisfy (i) or (ii) are now the dominated variables (the ones satisfying<br />(i) are said to be dominated above and those satisfying (ii) dominated below); those<br />which satisfy (iii) are the floating variables. As a consequence, the sets corresponding<br />to those given in (2.12) are straightforward to define.Inow contains variables that<br />float for all k e K sufficiently large and converge to the interior of B.I2is now the<br />union of the two sets I2, made up of variables that are dominated above for all k e K<br />sufficiently large, and I2,, made up of variables that are dominated below for all k e K<br />sufficiently large. Likewise, I3is the union of the two sets I3, made up of variables<br />that are floating for all sufficiently large k e K but converge to their lower bounds,<br />and I3, made up of variables that are floating for all sufficiently large ke K but<br />converge to their upper bounds. With such definitions, we may reprove all ofthe results<br />of<br />3-7, assumptions (AS5) and (AS6) being extended in the obvious way and<br />Theorem 5.4 being strengthened to say that, for all k e K sufficiently large, I2andI2<br />are precisely the variables that lie at their lower and upper bounds (respectively) at x*.<br />We have not made any statement here about how the scaling matrices S(k)should<br />be constructed, merely that they may be used. We consider that constraint scaling is<br />(P(x(), Vx()(k)))<br />(Vx((k))i.<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 27</p> <p>A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD<br />571<br />essential for any realistic algorithm and believe that it is important that the scaling<br />can be changed (albeit not too drastically) as the computation proceeds. We defer a<br />discussion of the issues of how to choose such scalings until we have performed<br />significant numerical testing of our algorithms. We also note that the results given here<br />are unaltered if the convergence tolerance (3.1)/(3.5) is replaced by<br />(8.6)<br />for any sequence ofpositive diagonal matrices {D )} with uniformly bounded condition<br />number. This is important as the method of Corm, Gould, and Toint (1988a), which<br />we would consider using to solve the inner iteration problem, allows for different<br />scalings for the components of the gradients to cope with variables of differing<br />magnitudes.<br />Finally, although the rules for how the convergence tolerances r/(k and to(k are<br />updated have been made rather rigid in this paper and although the results contained<br />here may be proved under more general updating rules, we have refrained from doing<br />so here as the resulting conditions on the updates seemed rather complicated and are<br />unlikely to provide more practical updates.<br />IID&lt;)P(x&lt;,V&lt;)l}_-&lt;o&lt;<br />Acknowledgments. The authors thank Annick Sartenaer for her careful reading of<br />this paper. We are also grateful to two anonymous referees whose comments have<br />helped us improve the original draft.<br />REFERENCES<br />M. AVRIEL, Nonlinear Programming: Analysis and Methods, Prentice-Hall, Englewood Cliffs, NJ, 1976.<br />M. C. BARTHOLOMEw-BIGGS, Recursive quadraticprogramming methods based on the augmented Lagrangian<br />function, Math. Programming Stud., 31 (1987), pp. 21-42.<br />D. P. BERTSEKAS, Augmented Lagrangian and exact penalty methods, in Nonlinear Optimization 1981, M.<br />J. D. Powell, ed., Academic Press, London, New York, 1982a, pp. 223-234.<br />Constrained Optimization and Lagrange Multiplier Methods, Academic Press, London, New York,<br />1982b.<br />A. R. CONN, N. I. M. GOULD, M. LESCRENIER, AND PH. L. TOINT, Performance ofa multifrontalscheme<br />forpartially separable optimization, Report CSS 218, AERE, Harewell Laboratory, Harwell, U.K., 1987.<br />A. R. CONN, N. I. M. GOULD, AND PH. L. TOINT, Global convergenceofa classoftrust region algorithms<br />for optimization with simple bounds, SIAM J. Numer. Anal., 25 (1988a), pp. 433-460, see also SIAM J.<br />Numer. Anal., 26 (1989), pp. 764-767.<br />Testing a classofmethodsforsolving minimization problems with simple bounds on the variables, Math.<br />Comp., 50 (1988b), 399-430.<br />R. S. DEMBO, S. C. EISENSTAT, AND T. STEIHAUG, Inexact Newton methods, SIAM J. Numer. Anal., 19<br />(1982), pp. 400-408.<br />R. S. DEMBO AND U. TULOWITZKI, Local convergence analysisforsuccessive inexact quadraticprogramming<br />methods, School of Organization and Management Working paper series B no. 78, Yale University,<br />New Haven, CT, 1984.<br />L. C. W. DIXON, P. DOLAN, AND R. PRICE, Finite element optimization: the useofstructured automatic<br />differentiation, in Stimulation and Optimization of Large Systems, A. Osiadacz, ed., Oxford University<br />Press, Oxford, 1988, pp. 117-141.<br />A. DRUD, CONOPT: a GRG codeforlarge sparse dynamic nonlinear optimization problems, Math. Program-<br />ming, 31 (1985), pp. 153-191.<br />R. FLETCHER, Practical MethodsofOptimization, Vol. 2, John Wiley, London, New York, 1981.<br />P. E. GILL, W. MURRAY, M. A. SAUNDERS, AND M. H. WRIGHT, Some theoretical properties ofan<br />augmented Lagrangian meritfunction, Report SOL 86-6, Department of Operations Research, Stanford<br />University, Stanford, CA, 1986.<br />N. I. M. GOULD, On practical conditionsforthe existence and uniquenessofsolutions to the general equality<br />quadratic programming problem, Math. Programming, 32 (1985), pp. 90-99.<br />, On the convergence ofa sequential penaltyfunction methodforconstrained minimization, SIAM J.<br />Numer. Anal., 26 (1989), pp. 107-128.<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <p>Page 28</p> <p>572<br />A. R. CONN, N.<br />I. M. GOULD, AND P. L. TOINT<br />A. GRIEWANK AND PH. L. TOINT, Partitioned variable metric updatesfor large structured optimization<br />problems, Numer. Math., 39 (1982), pp. 119-137.<br />W. A. GRUVER AND E. SACHS, Algorithmic Methods in Optimal Control, Research Notes in Math., 47,<br />Pitman, Boston, 1980.<br />W. W. HAGER, Dual techniquesforconstrained optimization, J. Optim. Theory Appl., 55 (1987), pp. 37-71.<br />M. R. HESTENES, Multiplier and gradient methods, J. Optim. Theory Appl., 4 (1969), pp. 303-320.<br />L. S. LASDON, Reduced gradient methods, in Nonlinear Optimization 1981, M. J. D. Powell, ed., Academic<br />Press, London, New York, 1982, pp. 235-242.<br />D. G. LUENBERGER, Introduction to Linear and Nonlinear Programming, Addison-Wesley, London, 1973.<br />J. J. MOR, Recent developments in algorithms andsoftware for trust region methods, in Mathematical<br />Programming: The State of the Art, A. Bachem, M. GriStschel, and B. Korte, eds., Springer-Verlag,<br />Berlin, 1983, pp. 258-287.<br />W. MURRAY, Analytical expressionsforeigenvalues and eigenvectorsofthe Hessian matricesofbarrier and<br />penalty functions, J. Optim. Theory Appl., 7 (1971), pp. 189-196.<br />B. A. MURTAGH AND M. A. SAUNDERS,MINOSuser’s manual, Report SOL 80-14, Department<br />of Operations Research, Stanford University, Stanford, CA, 1980.<br />J. M. ORTEGA AND W. C. RH El NBOLT, lterative solutionofnonlinear equations in several variables, Academic<br />Press, London, New York, 1970.<br />E. POLAK AND A. L. TITS, A globally convergent, implementable multiplier method with automatic penalty<br />limitation, Appl. Math. Optim., 6 (1980), pp. 335-360.<br />M. J. D. POWELL, A methodfornonlinear constraints in minimization problems in Optimization, R. Fletcher,<br />ed., Academic Press, London, New York, 1969.<br />R. T. ROCKAFELLAR, Augmented Lagrangians and applications ofthe proximal point algorithm in convex<br />programming, Math. Oper. Res.,<br />(1976), pp. 97-116.<br />K. SCHITTKOWSKI, The nonlinear programming methodof Wilson, Han and Powell with an augmented<br />Lagrangian type line search function, Numer. Math., 38 (1981), pp. 83-114.<br />T. STEIHAUG, The conjugate gradient method and trust regions in large scale optimization, SIAM J. Numer.<br />Anal., 20 (1983), pp. 626-637.<br />R. A. TAPlA, Diagonalized multiplier methods and quasi-Newton methodsforconstrained optimization, J.<br />Optim. Theory Appl., 22 (1977), pp. 135-194.<br />PH. L. TOINT, Towards anefficientsparsity exploiting Newton methodforminimization, in Sparse Matrices<br />and Their Uses, I. S. Duff, ed., Academic Press, London, New York, 1981.<br />, Nonlinear optimization in a large numberofvariables, in Simulation and Optimization of Large<br />Systems, A. Osiadacz, ed., Oxford University Press, Oxford, 1988.<br />H. YAMASHITA, A globally convergent constrained quasi-Newton method with an augmented Lagrangian type<br />penalty function, Math. Programming, 23 (1982), pp. 75-86.<br />Downloaded 10/30/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php</p>  <a href="https://www.researchgate.net/profile/Andrew_Conn/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds/links/5452316c0cf2bf864cbb3140.pdf">Download full-text</a> </div> <div id="rgw21_56aba1bfdff61" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56aba1bfdff61">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba1bfdff61"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Andrew_Conn/publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds/links/5452316c0cf2bf864cbb3140.pdf" class="publication-viewer" title="cgt_28_2.pdf">cgt_28_2.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Andrew_Conn">Andrew R. Conn</a> &middot; Oct 30, 2014 </span>   </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56aba1bfdff61" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56aba1bfdff61">  </ul> </div> </div>   <div id="rgw17_56aba1bfdff61" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56aba1bfdff61"> <div> <h5> <a href="publication/2630101_A_Globally_Convergent_Augmented_Lagrangian_Pattern_Search_Algorithm_For_Optimization_With_General_Constraints_And_Simple_Bounds" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Globally Convergent Augmented Lagrangian Pattern Search Algorithm For Optimization With General Constraints And Simple Bounds</span></a>  </h5>  <div class="authors"> <a href="researcher/7302308_Robert_Michael" class="authors ga-similar-publication-author">Robert Michael</a>, <a href="researcher/7302309_Lewis_Virginia_Torczon" class="authors ga-similar-publication-author">Lewis Virginia Torczon</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba1bfdff61"> <div> <h5> <a href="publication/225869214_A_globally_convergent_trust_region_algorithm_for_optimization_with_general_constraints_and_simple_bounds" class="color-inherit ga-similar-publication-title"><span class="publication-title">A globally convergent trust region algorithm for optimization with general constraints and simple bounds</span></a>  </h5>  <div class="authors"> <a href="researcher/2065124739_Chen_Zhongwen" class="authors ga-similar-publication-author">Chen Zhongwen</a>, <a href="researcher/78190203_Han_Jiye" class="authors ga-similar-publication-author">Han Jiye</a>, <a href="researcher/77928804_Han_Qiaoming" class="authors ga-similar-publication-author">Han Qiaoming</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56aba1bfdff61"> <div> <h5> <a href="publication/220577374_A_Globally_Convergent_Lagrangian_Barrier_Algorithm_for_Optimization_with_General_Inequality_Constraints_and_Simple_Bounds" class="color-inherit ga-similar-publication-title"><span class="publication-title">A Globally Convergent Lagrangian Barrier Algorithm for Optimization with General Inequality Constraints and Simple Bounds</span></a>  </h5>  <div class="authors"> <a href="researcher/3222866_Andrew_R_Conn" class="authors ga-similar-publication-author">Andrew R. Conn</a>, <a href="researcher/3213163_Nicholas_I_M_Gould" class="authors ga-similar-publication-author">Nicholas I. M. Gould</a>, <a href="researcher/69746694_Philippe_L_Toint" class="authors ga-similar-publication-author">Philippe L. Toint</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw39_56aba1bfdff61" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw40_56aba1bfdff61">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw41_56aba1bfdff61" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=NTljHROkClw2R6HRGpG_DclKfErTYY7hDVH4gYicO2xuOAoDE0PaONLvfQk7310x" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="AUdZbLrxeokOH9FZQ9DJFyW9E7ZuwrpkLBARDI93s5SPvayt8wZiJg8A5mpDVZ+BCs81nP0+UDhUdOayeaN99R4xRU+nRxfCgmU+Xkd8f2+ocnYnUZEUE9D2Li4MQ3Q5w4BYRr0CBN4aNlshxyhbbCgfTqjKlhS0EJM/rztneUQ5jrkfmHFJtN6VlLlfm7el4rXLdQWIzZAgO817TMuDygyYKBN8m15edvrHmAho2d7KCB9Exh4EL3C7CqXl3QtcwXudSxDef2f17nwkWOGIXODzzy7zrjkKz5VPGErBiYk="/> <input type="hidden" name="urlAfterLogin" value="publication/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjY1NTQzMjYyX0FfR2xvYmFsbHlfQ29udmVyZ2VudF9BdWdtZW50ZWRfTGFncmFuZ2lhbl9BbGdvcml0aG1fZm9yX09wdGltaXphdGlvbl93aXRoX0dlbmVyYWxfQ29uc3RyYWludHNfYW5kX1NpbXBsZV9Cb3VuZHM%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjY1NTQzMjYyX0FfR2xvYmFsbHlfQ29udmVyZ2VudF9BdWdtZW50ZWRfTGFncmFuZ2lhbl9BbGdvcml0aG1fZm9yX09wdGltaXphdGlvbl93aXRoX0dlbmVyYWxfQ29uc3RyYWludHNfYW5kX1NpbXBsZV9Cb3VuZHM%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjY1NTQzMjYyX0FfR2xvYmFsbHlfQ29udmVyZ2VudF9BdWdtZW50ZWRfTGFncmFuZ2lhbl9BbGdvcml0aG1fZm9yX09wdGltaXphdGlvbl93aXRoX0dlbmVyYWxfQ29uc3RyYWludHNfYW5kX1NpbXBsZV9Cb3VuZHM%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw42_56aba1bfdff61"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 607;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Andrew R. Conn","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272408045158413%401441958547518_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Andrew_Conn","institution":"IBM","institutionUrl":false,"widgetId":"rgw4_56aba1bfdff61"},"id":"rgw4_56aba1bfdff61","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=1911634","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba1bfdff61"},"id":"rgw3_56aba1bfdff61","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=265543262","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":265543262,"title":"A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds","journalTitle":"SIAM Journal on Numerical Analysis","journalDetailsTooltip":{"data":{"journalTitle":"SIAM Journal on Numerical Analysis","journalAbbrev":"SIAM J NUMER ANAL","publisher":"Society for Industrial and Applied Mathematics, Society for Industrial and Applied Mathematics","issn":"0036-1429","impactFactor":"1.79","fiveYearImpactFactor":"2.36","citedHalfLife":">10.0","immediacyIndex":"0.26","eigenFactor":"0.02","articleInfluence":"1.80","widgetId":"rgw6_56aba1bfdff61"},"id":"rgw6_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/JournalInfo.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.JournalInfo.html?issn=0036-1429","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"affiliation":false,"type":"Article","details":{"doi":"10.1137\/0728030","journalInfos":{"journal":"","publicationDate":"04\/1991;","publicationDateRobot":"1991-04","article":"28(2).","journalTitle":"SIAM Journal on Numerical Analysis","journalUrl":"journal\/0036-1429_SIAM_Journal_on_Numerical_Analysis","impactFactor":1.79}},"source":false,"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1137\/0728030"},{"key":"rft.atitle","value":"A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds"},{"key":"rft.title","value":"SIAM Journal on Numerical Analysis"},{"key":"rft.jtitle","value":"SIAM Journal on Numerical Analysis"},{"key":"rft.volume","value":"28"},{"key":"rft.issue","value":"2"},{"key":"rft.date","value":"1991"},{"key":"rft.issn","value":"0036-1429"},{"key":"rft.au","value":"Andrew R. Conn,Nicholas I.M. Gould,Philippe L. Toint"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw7_56aba1bfdff61"},"id":"rgw7_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=265543262","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":265543262,"peopleItems":[{"data":{"authorNameOnPublication":"Andrew R. Conn","accountUrl":"profile\/Andrew_Conn","accountKey":"Andrew_Conn","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272408045158413%401441958547518_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Andrew R. Conn","profile":{"professionalInstitution":{"professionalInstitutionName":"IBM","professionalInstitutionUrl":"institution\/IBM2"}},"professionalInstitutionName":"IBM","professionalInstitutionUrl":"institution\/IBM2","url":"profile\/Andrew_Conn","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A272408045158413%401441958547518_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Andrew_Conn","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw10_56aba1bfdff61"},"id":"rgw10_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=1911634&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"IBM","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":265543262,"widgetId":"rgw9_56aba1bfdff61"},"id":"rgw9_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=1911634&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=265543262","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/3213163_Nicholas_IM_Gould","authorNameOnPublication":"Nicholas I.M. Gould","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Nicholas I.M. Gould","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/3213163_Nicholas_IM_Gould","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw12_56aba1bfdff61"},"id":"rgw12_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=3213163&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw11_56aba1bfdff61"},"id":"rgw11_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=3213163&authorNameOnPublication=Nicholas%20I.M.%20Gould","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/69746694_Philippe_L_Toint","authorNameOnPublication":"Philippe L. Toint","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Philippe L. Toint","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/69746694_Philippe_L_Toint","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw14_56aba1bfdff61"},"id":"rgw14_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=69746694&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw13_56aba1bfdff61"},"id":"rgw13_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=69746694&authorNameOnPublication=Philippe%20L.%20Toint","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw8_56aba1bfdff61"},"id":"rgw8_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=265543262&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":265543262,"abstract":"<noscript><\/noscript><div>The global and local convergence properties of a class of augmented Lagrangian methods for solving nonlinear programming problems are considered. In such methods, simple bound constraints are treated separately from more general constraints and the stopping rules for the inner minimization algorithm have this in mind. Global convergence is proved, and it is established that a potentially troublesome penalty parameter is bounded away from zero.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw15_56aba1bfdff61"},"id":"rgw15_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=265543262","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw16_56aba1bfdff61"},"id":"rgw16_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba1bfdff61"},"id":"rgw5_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=265543262&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":7302308,"url":"researcher\/7302308_Robert_Michael","fullname":"Robert Michael","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7302309,"url":"researcher\/7302309_Lewis_Virginia_Torczon","fullname":"Lewis Virginia Torczon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jun 2000","journal":"SIAM Journal on Optimization","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/2630101_A_Globally_Convergent_Augmented_Lagrangian_Pattern_Search_Algorithm_For_Optimization_With_General_Constraints_And_Simple_Bounds","usePlainButton":true,"publicationUid":2630101,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.83","url":"publication\/2630101_A_Globally_Convergent_Augmented_Lagrangian_Pattern_Search_Algorithm_For_Optimization_With_General_Constraints_And_Simple_Bounds","title":"A Globally Convergent Augmented Lagrangian Pattern Search Algorithm For Optimization With General Constraints And Simple Bounds","displayTitleAsLink":true,"authors":[{"id":7302308,"url":"researcher\/7302308_Robert_Michael","fullname":"Robert Michael","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7302309,"url":"researcher\/7302309_Lewis_Virginia_Torczon","fullname":"Lewis Virginia Torczon","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["SIAM Journal on Optimization 06\/2000; 12(4). DOI:10.1137\/S1052623498339727"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/2630101_A_Globally_Convergent_Augmented_Lagrangian_Pattern_Search_Algorithm_For_Optimization_With_General_Constraints_And_Simple_Bounds","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/2630101_A_Globally_Convergent_Augmented_Lagrangian_Pattern_Search_Algorithm_For_Optimization_With_General_Constraints_And_Simple_Bounds\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba1bfdff61"},"id":"rgw18_56aba1bfdff61","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=2630101","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2065124739,"url":"researcher\/2065124739_Chen_Zhongwen","fullname":"Chen Zhongwen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78190203,"url":"researcher\/78190203_Han_Jiye","fullname":"Han Jiye","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":77928804,"url":"researcher\/77928804_Han_Qiaoming","fullname":"Han Qiaoming","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Sep 1999","journal":"Acta Mathematicae Applicatae Sinica","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/225869214_A_globally_convergent_trust_region_algorithm_for_optimization_with_general_constraints_and_simple_bounds","usePlainButton":true,"publicationUid":225869214,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.38","url":"publication\/225869214_A_globally_convergent_trust_region_algorithm_for_optimization_with_general_constraints_and_simple_bounds","title":"A globally convergent trust region algorithm for optimization with general constraints and simple bounds","displayTitleAsLink":true,"authors":[{"id":2065124739,"url":"researcher\/2065124739_Chen_Zhongwen","fullname":"Chen Zhongwen","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":78190203,"url":"researcher\/78190203_Han_Jiye","fullname":"Han Jiye","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":77928804,"url":"researcher\/77928804_Han_Qiaoming","fullname":"Han Qiaoming","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Acta Mathematicae Applicatae Sinica 09\/1999; 15(4):425-432. DOI:10.1007\/BF02684044"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/225869214_A_globally_convergent_trust_region_algorithm_for_optimization_with_general_constraints_and_simple_bounds","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/225869214_A_globally_convergent_trust_region_algorithm_for_optimization_with_general_constraints_and_simple_bounds\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba1bfdff61"},"id":"rgw19_56aba1bfdff61","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=225869214","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":3222866,"url":"researcher\/3222866_Andrew_R_Conn","fullname":"Andrew R. Conn","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3213163,"url":"researcher\/3213163_Nicholas_I_M_Gould","fullname":"Nicholas I. M. Gould","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69746694,"url":"researcher\/69746694_Philippe_L_Toint","fullname":"Philippe L. Toint","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 1997","journal":"Mathematics of Computation","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/220577374_A_Globally_Convergent_Lagrangian_Barrier_Algorithm_for_Optimization_with_General_Inequality_Constraints_and_Simple_Bounds","usePlainButton":true,"publicationUid":220577374,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.49","url":"publication\/220577374_A_Globally_Convergent_Lagrangian_Barrier_Algorithm_for_Optimization_with_General_Inequality_Constraints_and_Simple_Bounds","title":"A Globally Convergent Lagrangian Barrier Algorithm for Optimization with General Inequality Constraints and Simple Bounds","displayTitleAsLink":true,"authors":[{"id":3222866,"url":"researcher\/3222866_Andrew_R_Conn","fullname":"Andrew R. Conn","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":3213163,"url":"researcher\/3213163_Nicholas_I_M_Gould","fullname":"Nicholas I. M. Gould","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":69746694,"url":"researcher\/69746694_Philippe_L_Toint","fullname":"Philippe L. Toint","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Mathematics of Computation 01\/1997; 66(217):261-288. DOI:10.1090\/S0025-5718-97-00777-1"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/220577374_A_Globally_Convergent_Lagrangian_Barrier_Algorithm_for_Optimization_with_General_Inequality_Constraints_and_Simple_Bounds","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/220577374_A_Globally_Convergent_Lagrangian_Barrier_Algorithm_for_Optimization_with_General_Inequality_Constraints_and_Simple_Bounds\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56aba1bfdff61"},"id":"rgw20_56aba1bfdff61","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=220577374","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56aba1bfdff61"},"id":"rgw17_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=265543262&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":265543262,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":265543262,"publicationType":"article","linkId":"5452316c0cf2bf864cbb3140","fileName":"cgt_28_2.pdf","fileUrl":"profile\/Andrew_Conn\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140.pdf","name":"Andrew R. Conn","nameUrl":"profile\/Andrew_Conn","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":true,"uploadDate":"Oct 30, 2014","fileSize":"2.82 MB","widgetId":"rgw23_56aba1bfdff61"},"id":"rgw23_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=265543262&linkId=5452316c0cf2bf864cbb3140&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56aba1bfdff61"},"id":"rgw22_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=265543262&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":404,"valueFormatted":"404","widgetId":"rgw24_56aba1bfdff61"},"id":"rgw24_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=265543262","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56aba1bfdff61"},"id":"rgw21_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=265543262&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":265543262,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56aba1bfdff61"},"id":"rgw26_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=265543262&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":404,"valueFormatted":"404","widgetId":"rgw27_56aba1bfdff61"},"id":"rgw27_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=265543262","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56aba1bfdff61"},"id":"rgw25_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=265543262&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"SIAM J. NUMER. ANAL.\nVol. 28, No. 2, pp. 545-572, April 1991\n(C) 1991 Society for Industrial and Applied Mathematics\n015\nA GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN ALGORITHM\nFOR OPTIMIZATION WITH GENERAL CONSTRAINTS AND\nSIMPLE BOUNDS*\nANDREW R. CONN?, NICHOLAS\nI. M. GOULD$, AND PHILIPPE L. TOINT\nAbstract. The global and local convergence properties of a class of augmented Lagrangian methods\nfor solving nonlinear programming problems are considered. In such methods, simple bound constraints\nare treated separately from more general constraints and the stopping rules for the inner minimization\nalgorithm have this in mind. Global convergence is proved, and it is established that a potentially troublesome\npenalty parameter is bounded away from zero.\nKey words, constrained optimization, augmented Lagrangian, simple bounds, general constraints\nAMS(MOS) subject classifications. 65K05, 90C30\n1. Introduction. In this paper, we consider the problem offinding a local minimizer\nof the function\n(1.1)\nf(x),\nwhere x is required to satisfy the constraints\n(1.2)\nand the simple bounds\nCi(X\nO,\n<--\n<--m,\n(1.3)\nHerefand ei map R\nwe shall assume that the region B\nfurther assume that\n(AS1)\nThe functions f(x) and ci(x) are twice continuously ditterentiable for all\nxB.\nWe assume that any general inequality constraints ci(x)>-0 have already been converted\ninto equations by the introduction of slack variables (see, e.g., Fletcher (1981, p. 8));\nwe wish the combinatorial side of the minimization problem to be represented purely\nin terms of simple bound constraints. We shall attempt to solve our problem by means\nof a sequential minimization of the augmented Lagrangianfunction\nl<=x<=u.\ninto R and inequalities (1.3) are considered componentwise;\n{xll<-x<-_u} is nonempty and may be infinite. We\n1\n(1.4)\n(I)(x, A, S,\/x)-f(x)\/2 l\\ici(x)\/\nSiiCi(X)\n2\ni=1\ni=1\nwhere the components Aiof the vector A are known as Lagrange multiplier estimates,\nwhere the entries sii of the diagonal matrix S are positive scaling factors, and where\n\/z is known as the penalty parameter. Note that we do not include the simple bounds\n(1.3) in the augmented Lagrangian function; rather the intention is that the sequential\nminimization will automatically ensure that these constraints are always satisfied.\nReceived by the editors October 31, 1988; accepted for publication (in revised form) April 4, 1990.\n? Department of Combinatorics and Optimization, University of Waterloo, Ontario, Canada. The\nresearch of this author was supported in part by the Natural Sciences and Engineering Research Council\nof Canada and by the Information Technology Research Centre, which is funded by the Province of Ontario.\n$ Computer Science and Systems Division, United Kingdom Atomic Energy Authority, Harwell, Oxford,\nUnited Kingdom.\nDepartment of Mathematics, Facult6s Universitaires Notre Dame de la Paix, B-5000, Namur, Belgium.\n545\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":2,"text":"546\nA. R. CONN, N.\nI. M. GOULD, AND P. L. TOINT\nOur principal interest is in solving large-scale problems. With a few notable\nexceptions (see, for example, Murtagh and Saunders (1980), Lasdon (1982), Drud\n(1985)), there has been little progress in constructing algorithms for such problems;\nthis is somewhat understandable in view of the lack of a consensus as to the \"best\"\nalgorithm for solving small nonlinear programs. Nevertheless, there are many large-\nscale applications awaiting a suitable algorithm.\nA similar situation existed for unconstrained optimization in the early 1970s.\nHowever, during the past ten years, this deficiency has been redressed primarily through\nthe development ofthree important ideas. The first is the recognition that large problems\nnormally have considerable structure and that such structure usually manifests itself\nas sparsity or low rank of the relevant matrices. This has lead to suitable ways of\nstoring and approximating problem data (function, gradient, and Hessian approxima-\ntions), see, for example, Griewank and Toint (1982). The second development is the\nrealization that, although Newton\u2019s method (or a good approximation to it) is necessary\nfor rapid asymptotic convergence of an algorithm, in early iterations only very crude\napproximations to the solution ofthe Newton equations are needed to guarantee global\nconvergence. In particular, the steepest descent method often makes very good initial\nprogress towards a minimizer. This has led to a study of realistic conditions that suffice\nto guarantee global convergence of an algorithm and also of methods which satisfy\nsuch conditions, the truncated conjugate gradient method being a particularly successful\nexample. This work is described, for example, by Toint (1981), Dembo, Eisenstat, and\nSteihaug (1982), and Steihaug (1983). Third, the development of trust-region methods\n(see, e.g., Mor6 (1983)) has allowed a sensible handling of negative curvature in the\nobjective function; for large-scale problems whose second derivatives are available\n(contrary to popular belief, an extremely common circumstance in many problem\nareas), this enables meaningful steps towards the solution to be made when the Hessian\nmatrix is indefinite. Significantly, these ideas have had an important impact on the\ndesign of algorithms not only for large problems but also for small ones (see, Toint\n(1988), and Dixon, Dolan, and Price (1988)).\nOne issue that is not present in unconstrained minimization, but is in evidence\nhere, is the combinatorial problem of finding which of the variables lie at a bound at\nthe solution (such bound constraints are said to be active). In active-set algorithms,\nthe intention is to predict these variables and to minimize the function with respect\nto the remaining variables. Obviously, an incorrect prediction is undesirable, and it is\nthen useful (indeed essential for large problems) to be able to make rapid changes in\nthe active set to correct for wrong initial choices. Unfortunately, many existing\nalgorithms for constrained optimization only allow very small changes in the active\nset at each iteration, and consequently, for large problems, there is the possibility of\nrequiring a large number of iterations to find the solution. Fortunately, for simple\nbound constraints, it is easy to allow for rapid changes in the active set in the design\nof algorithms (see, e.g., Berksekas (1982b, pp. 76-92), and Conn, Gould, and Toint,\n(1988a)).\nOur intention here is to develop a fairly general algorithm which may benefit from\nthe above-mentioned advances. We have recently developed and tested (Conn et al.\n(1987), Conn, Gould, and Toint (1988a), (1988b)) an algorithm for solving bound\nconstrained minimization problems (problems of the form minimize (1.1) subject to\n(1.3)) which is appropriate in the large-scale case. Our basic idea is now to use this\nalgorithm within an augmented Lagrangian framework, that is to use the algorithm to\nfind an approximation to a minimizer of the augmented Lagrangian function (1.4)\nsubject to the bounds (1.3) for a sequence of different values of S, A, and\/x.The novelty\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":3,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n547\ncomes from being able to solve the augmented Lagrangian problems approximately\nand on being able to deal with the bounds in an efficient manner.\nThe augmented Lagrangian method was proposed independently by Hestenes\n(1969) and Powell (1969), partly as a reaction to the unfortunate side-effects associated\nwith ill-conditioning ofthe simpler ditterentiable penalty and barrier functions (Murray\n(1971)). Indeed, Powell showed, using a very simple device, how to ensure that the\npenalty parameter does not converge to zero and hence that the resulting ill-conditioning\ndoes not occur. A similar device is employed in the algorithms with which we are\nconcerned in this paper with the same consequence. A concise statement of the.salient\nfeatures of augmented Lagrangian methods, or multiplier methods as they are some-\ntimes known, is given, for example, by Fletcher (1981). The most comprehensive\nreferences on augmented Lagrangians are the paper by Tapia (1977) and the book by\nBertsekas (1982b). Globally convergent methods have been given by Powell (1969),\nRockafellar (1976), Bertsekas (1982b), Polak and Tits (1980), Yamashita (1982),\nBartholomew-Biggs (1987), and Hager (1987). Powell\u2019s method requires that the\naugmented Lagrangian be minimized exactly for fixed values of the multipliers and\nparameters. The multiplier estimates are guaranteed to be bounded, but convergence\nis only established in the case where the underlying nonlinear program has a unique\nsolution. Rockafellar, Bertsekas, and Polak and Tits allow inexact minimization of the\naugmented Lagrangian function, but they require that the Lagrange multiplier estimates\nremain bounded--the methods differ in the stopping criteria used. Hager is slightly\nmore restrictive in that he considers a particular multiplier update and specifies the\nmethod used for approximately minimizing the augmented Lagrangian function. His\nanalysis also assumes that a subsequence of the Lagrange multiplier estimates con-\nverges. Yamashita and Bartholomew-Biggs are more specific in the method used for\nthe inner minimization calculation--an appropriate quadratic program is solved--but\ntheir methods allow for more frequent updating ofthe penalty parameter and multiplier\nestimates. Yamashita establishes convergence under the assumption that the Lagrange\nmultipliers for the quadratic programming problem stay bounded; the possibility of\nproving convergence for Bartholomew-Bigg\u2019s method under similar circumstances is\nonly hinted at although encouraging numerical results are presented.\nInterest in augmented Lagrangians declined with the introduction of successive\nquadratic programming (SQP) techniques but recently has gained in popularity. (See\nfor example the papers of Schittkowski (1981) and Gill et al. (1986) which combine\nSQP with an augmented Lagrangian merit function. Both these methods are not pure\naugmented Lagrangian techniques since they perform a line search on the augmented\nLagrangian as a function of both the position x and the multipliers I in contrast to\nthe method described in this paper.)\nOne strong disadvantage ofSQP methods for large-scale problems is that, although\nthere is a theory of how to truncate the solution process in the early iterations (see,\nDembo and Tulowitzki (1984))mas is used so successfully in the unconstrained casemit\nis not clear to us how to construct an efficient algorithm that conforms to this theory.\nWe feel that solving a quadratic programming to completion at each iteration is probably\ntoo expensive a calculation for large-scale problems in the same way that solving the\nNewton equations exactly is considered too expensive in large-scale unconstrained\nminimization. We thus feel there are compelling reasons for trying to use an alternative\nto the SQP approach.\nBertsekas\n(1982a)\nand\nothers, however,\nLagrangians are particularly attractive for large problems, where active set strategies\nare inappropriate, and we tend to agree with this sentiment. In particular, simple\nhave\nremarked that\naugmented\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":4,"text":"548\nA. R. CONN, N.\nI. M. GOULD, AND P. L. TOINT\nmultiplier estimates may be used. In this paper we explore some of the issues involved\nin using an augmented Lagrangian approach for large-scale problems. We have deliber-\nately not included the results of numerical testing as, in our view, the construction of\nappropriate software is by no means trivial and we wish to make a thorough job of\nit. We will report on our numerical experience in due course. We should comment,\nnonetheless, that at the heart of any method for solving nonlinear programming\nproblems, there is a need to find an approximate solution to a system oflinear equations.\nLinear equation solvers may be broadly categorised as either direct or iterative methods.\nIn the former, a factorization of the relevant matrix is used, while in the latter,\nmatrix-vector products involving the relevant matrix are required. Here the coefficient\nmatrix for such systems will typically be symmetric submatrices of the Hessian of the\naugmented Lagrangian function (1.4). For iterative methods, sparsity in the derivatives\nofthe objective function and constraints may be exploited in the matrix-vector products.\nFor direct methods, there is often a concern that the Hessian of an augmented\nLagrangian function may be less sparse than for the Lagrangian function because of\nthe last term in (1.4). While this is certainly true, it is worth noting that variables that\nappear nonlinearly in the constraint functions give rise to nonzeros in the same positions\nin the Hessians of both the Lagrangian and augmented Lagrangian function. It may\ntherefore be worth treating linear constraints in a different way from nonlinear ones;\nwe are currently pursuing this line of research.\nOur exposition will be considerably simplified if we consider the special case\nwhere li=0 and ui=oo for all 1-<i- <n\nmodification required to handle more general constraints will be indicated at the end\nof the paper. Thus we consider the problem:\n(1.5)\nminimizef(x),\nin (1.3). Although straightforward, the\nsubject to the constraints\n(1.6)\nCi(X\nO,\n<= <= m,\nand the nonnegativity restrictions\n(1.7)\nx B ={x R\"[x>=O}.\nThe paper is organised as follows. In\nand then state a pair of related algorithms for solving (1.5)-(1.7) in\nconvergence is established in\n4, while issues of asymptotic convergence follow in\nAn example showing the importance of a certain assumption in\nwhile in\n7 the consequences of satisfying second-order conditions are given. We\nconclude in\n8 by indicating how this theory applies to the original problem (1.1)-(1.3).\n2 we introduce concepts and definitions\n3. Global\n5.\n6,\n5 is given in\n2. Notation. In this section we introduce the notation to be used throughout the\npaper. We will use the projection operator defined componentwise by\n0\nif xg=<0,\notherwise.\n(2.1)\n(P[x]),\nxi\nThis operator projects the point x onto the region B. Furthermore, we will make use\nof the \"projection\"\nx-\/\u2019Ix- v].\nLet g(x) denote the gradient Vxf(X) of f(x) and let H(x) denote its Hessian matrix\nVxxf(x). Let A(x) denote the m-by-n Jacobian of e(x), where\n(2.3)\nc(x)\n[c,(x),\n(2.2)\n\/\u2019(x, v)\n\", c,,(x)]r,\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":5,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n549\nand let Hi(x) denote the Hessian matrix VxxCi(X\nHL(x,A) denote the gradient and Hessian matrix (taken with respect to its first\nargument) of the Lagrangian function\nof Ci(X). Finally, let gL(x, A) and\n(2.4)\nL(x, A) =f(x)+2 ici(x)\ni=1\nWe note that L(x, A) is the Lagrangian function with respect to the C constraints only.\nIf we definefirst-orderLagrange multiplier estimates\n(2.5)\ng(x, h(x,,S, I)).\nNow suppose that {x(k=>0} and {A(k)} are infinite sequences of n-vectors and\nm-vectors, respectively, that {S(k)} is an infinite sequence of positive-definite diagonal\nmatrices, and that {\/(k)} is an infinite sequence of positive scalars. For any function\nF, we shall use the notation that F(k)denotes F evaluated with arguments x(kA(k\nS(k), or\/(k) as appropriate. So, for instance, using the identity (2.6), we have\n(2.7)\nV,(k\nVx(x(k, k, Sk),\/k))\nwhere we have written\nX(x,,S,\/)= h + Sc(x)\/I,\nwe shall make much use of the identity\n(2.6)\nVx(X, A, S, )\ngL(xk, (k)),\n(2.8)\n(2.9)\nFor any x(k, we have two possibilities for each component\n(i)\nOxlk)(VxO(k))i\n(Vx(I)(k))i <Xl\nor\n(ii)\nk)\nnamely,\nIn case (i) we then have\n(2.10)\n(P(x(k), Vxlff(k)))\nXl\nk)\nwhereas in case (ii) we have\n(2.11)\nWe shall refer to anxl\nsatisfies (ii) is known as a floating variable. The algorithms we are about to develop\nconstruct iterates Which force P(x(k),VxtI)(k)) to zero as k increases. Thedomi0ated\nvariables are thus pushed to zero, while the floating variables are allowed to find their\nown level.\nIf, in addition, there is a convergent subsequence {xk)}, k\nx*, we wish to partition the set N\n{1, 2,\nare related to the two possibilities (i) and (ii) above and to the corresponding x*:\nI={i[xlk)are floating for all k K sufficiently large and x\/*>0},\nI2={i[r<k)\n--i\n(2.12)\nI3\n14= N\\(I, U12[,..JI).\nFrom time to time we will slightly abuse notation by saying that a variablexi belongs\nto (for instance) I, when strictly we should say that the index of the variable belongs\nto I. We will also mention the components of a (given) vector in the set I when\nstrictly we mean the components of the vector whose indices lie in I.\n(P(xk), V k))),\n(Vxk))i\"\nk)which satisfies (i) as a dominated variable; a variable which\nK, with limit point\n, n} into the following four subsets which\nare dominated for all k K sufficiently large},\n{ilxl\nk)are floating for all k K sufficiently large but x\/* =0}, and\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":6,"text":"550\nA. R. CONN, N.\nI. M. GOULD, AND P. L. TOINT\nIf the iterates are chosen so that P(x(k), Vx(I.)(k)) approaches zero as k increases,\nwe have the following result.\nLEMMA 2.1. Suppose that {x(k)}, k\nK, is a convergent subsequence with limitpoint\nx*, that h (k), s(k),i(k)I1, 12, I3, andI4are as above, and that P(x(k,Vx(k)) approaches\nzero as k K increases. Then\n(i) The variables in sets I2, 13, and14all converge to their bounds;\n(ii) The components of(V,k))i in the setsIand13 converge to zero\u2019, and\n(iii) Ifa component of(V@k)i in the set14 converges to afinite limit, then the\nlimit is zero.\nProof (i) The result is true for variables in 12 from (2.10), for those in 13 by\ndefinition and for those in 14 as, again from (2.10), there must be a subsequence of\nthe k K for whichXl\n(ii) The result follows for iinIand13from (2.11).\n(iii) This is true for\nfrom (2.11), (Tx((k))i converges to zero.\nIt will sometimes be convenient to group the variables in sets I3 and\/4 together\nand call the resulting set\nk)converges to zero.\nin14as there must be a subsequence of the k K for which,\n(2.13)\nAs we see from Lemma 2.1, I5 gives variables which are zero at the solution and which\nmay correspond to zero components of the gradient of the augmented Lagrangian\nfunction. These variables are potentially (dual) degenerate at the solution of the\nnonlinear programming problem.\nWe will let (x) denote the components of g(x) indexed by I. Similarily,(x)\ndenotes the corresponding columns ofthe Jacobian matrix; indeed any matrix M refers\nto the columns of the generic matrix M indexed by I.In addition, we will define the\nleast-squares Lagrange multiplier estimates (corresponding to the set 11)\n-(A(x)+)(x)\n15 13 U\/4.\n(2.14)\nA (x)\nat all points where the right generalized inverse\n(2.15)\nof \/i(x) is well defined. We note that ,(x) is dilterentiable; for completeness the\nderivative is given in the following lemma.\nLEMMA 2.2. Suppose that (AS1) holds. Ifi(x)(x)\ndifferentiableand its derivative is given by\n(2.16)\nVxA(x)\nwhere the ith rowofR(x) is\nProof The result follows by observing that (2.14) may be rewritten as\n(2.17)\nA(x)+=,(x)(A(x)ft(x))-\u2019\nis nonsingular, A(x)\nis\n(A(x)+)TIIL(x, t (X)) --(A(X)A(X)T)-IR(X),\nand A(x)r(x)=O\nfor some vector r(x). Differentiating (2.17) and eliminating the derivative of r(x) from\nthe resulting equations gives the required result.\nWe stress that, as stated, the Lagrange multiplier estimate (2.14) is not a directly\ncalculable quantity as it requires an a priori knowledge of x*. It is merely introduced\nas an analytical device but we shall show in due course that a variant of this estimate\nmay be calculated and used.\nWe are now in a position to describe more precisely the. algorithms we propose\nto use.\n[3\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":7,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n551\n3. Statement of the algorithms. In order to solve problem (1.5)-(1.7), we consider\nthe following algorithmic models. Here ]t\"\nmatrix norm).\ndenotes any vector norm (or its subordinate\nALGORITHM 1.\nStep 0 [Initialization]. An initial vector of Lagrange multiplier estimates A() is\ngiven. The positive constants r\/o,\/o, too, z < 1, \/1(1, to,<< 1, r\/,<< 1, a,\/3, c%,and\ntoo(a()).,,r\/()= r\/o(a())%, and k=0.\nfinare specified. The diagonal matrices S1 and S=, for which 0<S_-<S=<, are\ngiven (the inequalities are to be understood elementwise for the diagonal elements).\nSet\/x() =\/Xo,a()=min (\/x(), y), to()\nStep 1 [Inner iteration]. Define a diagonal scaling matrix S()for whichS-\nIlP(x\n-<_S()-<_\n$2. Find x()B such that\n(3.1)\nIf\nexecute step 2. Otherwise, execute step 3.\n(3.3)\nStep 2\n[Test for convergence and update Lagrange multiplier\nIlP(x), x7())ll :<o, and IIc(x(\")ll_-<n,, stop. Otherwise, set\nestimates]. If\n\/(k+l)\n(x(k),,(k), S(k), [.1,(k)),\n]\u2019(k+l)\n]\u2019(k),\na(k\/)\nmin (\/(k\/) Yl)\ntok(ag\/)o\nr\/(k)(t(k+l))\/3.,\nto(k+\nr\/(k+l)\nincrement k by one and go to step 1.\n(3.4)\nStep 3 [Reduce the penalty parameter]. Set\nh(k+)\nh(k)\n(k+l)\n(k)\n\/x\n\u2019\/x\n1), \u2019)\/1),\na(k+)\nmin (\/x\nto0(a(k+1)) a,,.,,\nr\/o(a(k+))%,\nto(k+l)\nr\/(k+)\nincrement k by one and go to step 1.\nALGORITHM 2.\nStep 0 [Initialization]. An initial vector of Lagrange multiplier estimates, h(, is\ngiven. The nonnegative constantanand the positive constants r\/o,\/o, z < 1, too, Y< 1,\n7, < 1,to.<< 1, r\/.<< 1, u, ao, flo, andfinare specified. The diagonal matrices Sand\n$2, for which O<S-(<-S<, are given. Set \/z(=\/o, a()=min(\/(), 7), to(o)=\ntoo(a(o))%, r\/(o)\nr\/o(a(o))%, and k\n0.\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":8,"text":"552\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nStep 1 [Inner iteration]. Define a diagonal scaling matrix S(k)for whichS-\n$2. Find x<k)B such that\nIlP(x(\"), X7x(\")II-<_ o\".\nCompute a new vector of Lagrange multiplier estimates <k+l). If\nIIc(x)ll_<-\nII(x,Vx))ll_-<,o, and IIc(x))ll-<_ n,, stop. Otherwise, set\n(k+l)\n(k)\n=\/\n(3.5)\n(3.6)\nexecute step 2. Otherwise, execute step 3.\nStep 2\n[Test for convergence and update Lagrange multiplier\nestimates]. If\nA(k+)\n[A\nk)\notherwise,\n(3.7)\nk+l)\nk+)\n(g)(a(k+)),\nk)(ak+))&\nincrement k by one and go to step 1.\nStep 3 [Refluce the penalty parameter and up,ate Lagrange multiplier estimates], Set\n(k+)\n(k)\nA(+)\n[A)\nmin (+\u2019 y)\nWo(+))\"o,\no(a+)-,,\notherwise,\n(3.8)\n+)\nw+)\n+\nincrement k by one and go to step 1.\nThe motivation for both algorithms is quite straightforward. Traditional augmented\nLagrangian methods are known to be locally convergent if the penalty parameter is\nsufficiently small and if the augmented Lagrangian is approximately minimized at each\nstage (see, for instance, Rockafellar (1976), Bertsekas (1982b,\nthat the method is globally convergent, as a last resort we must drive the penalty\nparameter to zero and ensure that the Lagrange multiplier estimates do not behave\ntoo badly. The convergence of such a scheme is guaranteed, since in this case, the\niteration is essentially that used in the quadratic penalty function method (see, for\nexample, Gould (1989)). We consider this further in\ntraditional multiplier iteration to take over, the test on the size of the constraints\n(3.2)\/(3.6) is based upon the size that might be expected if the multiplier iteration is\nconverging. This aspect is considered in\nThe algorithms differ in their use of multiplier updates. Algorithm 1 is designed\nspecifically for the first-order estimate (2.5); the multiplier estimates are encouraged\nto behave well as a consequence of the test (3.2). For large-scale computations, it is\nlikely that first-order estimates will be used and thus Algorithm 1 is directly applicable.\nAlgorithm 2 allows any multiplier estimate to be used. This extra freedom means that\n2.5)). In order to ensure\n4. In order to try to allow the\n5.\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":9,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n553\ntighter control must be maintained on the acceptance of the estimates to make sure\nthat they do not grow unacceptably fast. In this algorithm, we have in mind using any\nofthe well-known Lagrange multiplier update formulae, including the first-order update\n(2.5) (used in Algorithm 1), the least-squares update (2.14), and other updates summar-\nized, for instance, by Tapia (1977). We note, however, that some of these updates may\nrequire a significant amount ofcomputation and this may prove prohibitively expensive\nfor large-scale problems. Algorithm 2 is identical to Algorithm\nLagrange multiplier updates, the fact that these updates may also occur in step 3 and\nthe presence of the scalars,and y.\nBoth algorithms use a number of free parameters. To give the reader some feel\nfor what might be typical values, we suggest that for well-scaled problems ao =\/o\n\u2019\n4. Gh)balconvergence analysis. In this section we shall make use of the following\nassumptions\"\nThe iterates {x(k)} considered lie within a closed, bounded domain\n(AS3)\nx*, of the sequences {x(k)} considered in this paper.\nNote that (AS3) excludes the possibility that11is empty unless there are no general\nconstraints. In view of Lemma 2.1, this seems reasonable as otherwise we are allowing\nthe possibility that all the constraints and bounds are satisfied as equations at x*. We\nalso observe that (AS3) is equivalent to assuming that the gradients of the general\nconstraints and active bounds at any limit point are linearly independent, an assumption\nthat is commonly made in the analysis of other methods (see Bertsekas (1982b), and\nFletcher (1981)).\nWe shall analyse the convergence of the algorithms of\nconvergence tolerancesto.and 7, are both zero. We require the following pair of\nlemmas in the proof of global convergence of our algorithms. Essentially, the results\nshow that the Lagrange multiplier estimates generated by either algorithm cannot\nbehave too badly.\nLEMMA 4.1. Suppose thatt.t\nis executed. Then the producttz\nh\nconverges to zero.\n(k)\nProof If\nconverges to zero, step 3 ofthe algorithm must be executed infinitely\noften. Let K\n{ko, kl, km,\"\n3 of the algorithm is executed and for which\n\/z(k)__< min (()1\/,\nWe consider how the Lagrange multiplier estimates change between two successive\niterations indexed in the set K. At iteration ki+j, fork<k+j <-ki+l, we have\nexcept for the allowed\ny\n70\ntoo\n1, a, o\nyl\n0.1,\/3,\n0.9, and z\n0.01 are appropriate.\n(ASm)\nThe matrix \/](x*) has column rank no smaller than rn at any limit point,\n3 in the case where the\n(k)converges to zero as k increases when Algorithm 1\nk\nk\n\"} be the set of the indices of the iterations in which step\n(4.1)\nj--1\n(4.2)\nl(ki+J)\nl(ki)-[\ns(ki+l)c(x(ki+l))\/[J,(ki+l)\n\/=1\nand\n(4.3)\ntz(ki+\u2019)\n[J,(ki+J)\n[.lb(ki+l)\n\"\/\u2019\/(ki)\nwhere the summation in (4.2) is null ifj\nof iterationski+ 1,\nfrom (3.2), (4.3), and the recursive definition of7(g), we must also have\n(4.4)\n1. Now suppose that j> 1. Then for the set\n<j, step 2 of the algorithm must have been executed and hence,\n<-\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":10,"text":"554\nA. R. CONN, N.\nI. M. GOULD, AND P. L. TOINT\nCombining equations (4.1) to (4.4) and using the imposed upper bound on S(k)\nobtain the bound\nj-1\nIIh( \u2019+ >ll\nII ( \u2019>ll +Z Ils\n!=1\n(k+l)\nj-1\n(4.5)\nl=l\nwheres2is the norm of $2. Thus we obtain that\nEquation\n(.+)\n(4.6)\n(k+)\nis\nT(.)\nalso\nsatisfied when j=l\n()\nas equations\n(3.4) and\n(4.3)\ngive\nHence from (4.6),\n(4.7)\nEquation (4.7) then gives that <k\u2019)llA<k\u2019)]] converges to zero as k increases. For, if we\ndefine\n(4.8)\nand\nequations (4.3), (4.7), and (4.8)ivethat\n(4.9)\na+_-<\u2019c+\u2019%fl\nand\nand hence that\ni-1\n(4.10)\n0--<_a,-<r\u2019Ceo+(r%)\u2019 E\n\/=0\nIfre,< 1, the sum in (4.10) can be bounded to give\n(4.11\n0\nol\n\u2019l\u2019it)go -J1-\n7\"\nijo\/ 1\nwhereas ifc,> 1, we obtain the alternative\n(4.12)\n0_-<c_-<\"(Co+ \u2019%-\/3o\/(1\n\u2019%-1)),\nand ifa,\n1,\n(4.13)\n0<-_ai<--\u2019iao+fl\u2019io.\nBut, both aoand \/30 are finite. Thus, as\n(4.9) implies thatfl converges to zero. Therefore, as the right-hand side of (4.6)\nconverges to zero, the truth of the lemma is established.\nWe note that Lemmas 4.1 may be proved under much weaker conditions on the\nsequence {r\/(k)} than those imposed in Algorithm 1. All that is needed is that, in the\nproof just given,Zj-ll=l\npositive power of\/x(k\u2019+).\nTurning to Algorithm 2, we have the following easier-to-establish result.\nLEMMA 4.2. Suppose thattx\nis executed. Then the productI\nl\nconverges to zero.\nincreases, c converges to zero; equation\nc(x(k\u2018+\/))]] in (4.5) should be bounded by some multiple of a\n(k)converges to zero as k increases when Algorithm 2\nk\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":11,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n555\nProof Let K\n(4.14)\nand consequently on which A(+\n(4.15)\nIf K is finite, A(k)will be fixed for all k sufficiently large and the result is immediate.\nIfK is infinite, for anyk<k<-ki+l,A(k)=A(ki+l), and\/b\/(k)\/(k,+l). Hence, from (4.15)\n(4.16)\n\/xll\nBy hypothesis, the right-hand side of (4.16) can be made arbitrarily small by choosing\nki large enough, and so \/x(k)[[A(k)[[ converges to zero.\nAs a precursor to our main result, we have the following general convergence\nlemma. The lemma and resulting theorem are in the spirit of Proposition 2.3 of Bertsekas\n(1982b) but do not require that the Lagrange multiplier estimates stay bounded and\nallow for our handling of simple bound constraints.\nLEMMA 4.3. Suppose that (AS1) holds. Let {xk)}\n(AS2) which converges to the point x*forwhich (AS3) holds and let A*\nAsatisfies (2.14). Assume that {A k)}, k\nis any sequenceofdiagonal matrices satisfying 0 < $1\nk\nK,forma nonincreasing sequenceofpositive scalars. Supposefurtherthat\n(4.17)\nIIP(x{,v)ll =< o\nwhere the to(k) are positive scalar parameters which converge to zero as k K increases.\nThen\n(i) There are positive constants a, a, s and an integerkosuch that\n(4.18)\nIIX(x<),\n(4.19)\nand\n{ko, kl, k2,\" \"} be the iterations on which\niiX+lll\n(+l.Then, from (4.14),\n#,+,lla(,+,ll__<(#,+,)l-,.\np(j(.$(k+l))-T\nB, k\nK, be a sequence satisfying\nA (x*), where\nK, is any sequenceofvectors, that {Sk)}, k\n-1<S(k<S2< e, and that {tx(k)},\nK,\n)s\nA (x()\n**11--< a=llx\"-x*l,\n(4.20)\nforall k>-ko,\nSuppose, in addition, that c(x*)=0. Then\n(ii) x* is a Kuhn- Tuckerpoint rst-orderstationary point)forthe problem (1.5)-\n(1.7), A*\n{,(x(k) \/(k)s(k)\n(iii) The gradients Vcb(k) converge to gL(X*, A*)fork\nProof As a consequence of (AS1)-(AS3), we have that for k K sufficiently large,\n,(x(k))+exists, is bounded, and converges to ft.(x*)+. Thus we may write\n(4.21)\n(A(x()+)TII\nfor some constant a > 0. As the variables in the set11are floating, equations (2.7),\n(2.8), (2.11), and the inner iteration termination criterion (4.17) give that\n(4.22)\n?(x) +A(x)X(\nBy assumption, A(x) is bounded for all x in a neighbourhood of x*. Thus we may\ndeduce from (2.14), (4.21), and (4.22) that\nIIX(-a(x()II\n(4.23)\n_<((x()+) II,o__< al,o(>.\nk K ).\nis the corresponding vector of Lagrange multipliers, and the sequences\n(k))} and {A (x(k))} converge to A*fork\nK\"\nK.\n_--<a\n_-< ,o(.\n(A(x()+)?(x()+X(\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":12,"text":"556\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nMoreover, from the integral mean value theorem and Lemma 2.2 we have that\n(4.24)\n1(x()-1(x*)\nV1(x(s)) ds\" (x(-x*),\nwhereVI(x) is given by equation (2.16) and where x(s)=x(+s(x*-x(). Now the\nterms within the integral sign are bounded for all x sufficiently close to x* and hence\n(4.24) gives\nI1(x())\nfor some constanta2> 0, which is just the inequality (4.19). We then have that A (x(k)\nconverges to A*. Combining (4.23) and (4.25) we obtain\nIlX)-*ll<-IlX)-(x))ll\/ll(x))-*ll<-ao\/a211x)-x*ll,\nthe required inequality (4.18). Then, since by construction w(ktends to zero as k\nincreases, (4.18) implies that )converges to A* and from (4.22) we have that\nL(x*, A *)\nMoreover, from the identity (2.6), X7x) converges to gL(x*, A*). Furthermore, multi-\nplying (2.5) byx), we obtain\n(4.28)\n(X(k))\n(4.25)\n*11-<a211x()-x*ll\n(4.26)\n(4.27)\nif(x*) +\/](x*)rA*\n0.\nidb(k)s(k)-l(((k)\n1 :#)+ (I:\nI (k))).\nTaking norms of (4.28) and using (4.26) we derive (4.20), where S\nNow suppose that\nis the norm of $1.\n(4.29)\n(x*) =0\nand consider the status of the variables in the sets 11, 12, and15.Lemma 2.1 and the\nconvergence ofVx)to &(x*, A*) show that the complementary slackness condition\n(4.30)\ngL(x*, A *)rx*\nO\nis satisfied. The variables in the set11 are, by definition, positive at x*. The components\nof g(x*, A*) indexed by 12 are all nonnegative from (2.9) as their corresponding\nvariables are dominated. This then gives the conditions\nx*>0\nand\n(gc(x*,A*))i=O\n(4.31)\nx*=O\nand\n(g(x*,a*))_->O\nx*=O\nand\n(gc(x*,a*)),=O\nEquations (4.29) and (4.31) thus show that x* is a Kuhn-Tucker point and A* are the\ncorresponding set of Lagrange multipliers. Moreover, (4.18) and (4.19) ensure the\nfor\/e\/l,\nfor ieI2,\nand\nfor i\/.\nconvergence of the sequences {(x(k), A(), S,x)} and {A(x)} to A* for k\nHence the lemma is proved.\nWe now show that both Algorithms 1 and 2 possess a powerful global convergence\nproperty under relatively weak conditions.\nTHEOREM 4.4. Assume that (AS1) holds. Let x* be any limit pointofthe sequence\n{X(k)} generated by Algorithm 1 or by Algorithm 2of\nhold and let K be the setofindicesofaninfinitesubsequenceofthe x(whose limit is\nx*. Then conclusions (i), (ii), and (iii) ofLemma 4.3 hold.\nProof The assumptions given are sufficient to reach the conclusions of part (i)\nof Lemma 4.3. We now show that (4.29) holds for Algorithms 1 and 2. To see this, we\nconsider two separate cases:\nK.\n[3\n3forwhich (AS2) and (AS3)\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":13,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n557\n(i) If\/x\n(k)is bounded away from zero, step 2 must be executed every iteration\nfor k sufficiently large. But this implies that (3.2) is always satisfied (k large enough)\nand r\/(k converges to zero.\nHence c(x(k) converges to zero.\n(ii) If\/x(kconverges to zero, Lemma 4.1 for Algorithm 1 and Lemma 4.2 for\nAlgorithm 2 show that \/x(\"ll\n *11converges to zero. But then, inequality (4.20)\ngives the required result.\nHence (4.29)\nholds.\nNote that Theorem 4.4 would remain true regardless of the actual choices of\nand {r\/(k)} provided that both sequences converge to zero.\nis satisfied and thus conclusions\n(ii) and\n(iii) of Lemma 4.3\n5. Asymptotic convergence analysis. We now give our first rate-of-convergence\nresult. It is inconvenient that the estimates (4.18)-(4.20) depend uponIIx<  -x*ll.The\nnext lemma removes this dependence and gives a result similar to the classical theory\nin which the errors in x are bounded by the errors in the multiplier estimates\n(see Bertsekas (1982b, Prop. 2.4)); however, as an inexact minimization of the aug-\nmented Lagrangian function is made, in the spirit of Bertsekas (1982b, Prop. 2.14)),\na term reflecting this is also present in the bound. Once again, the result allows for\nour handling of simple bound constraints. Before giving our result, we need to make\ntwo additional assumptions.\nWe use the notation that, ifJ1andJ2are any subsets of N, Hl(X*, h)[ji,J2] is the\nmatrix formed by taking the rows and columns of Hl(X*, h*) indexed by J1 and J2,\nrespectively, andA(x*)[j,]is the matrix formed by taking the columns ofA(x*) indexed\nby J. We use the following assumptions:\n(AS4)\nThe second derivatives of the functions f(x) and the Ci(X\n(AS5)\n(5.1)\nare Lipschitz\ncontinuous at all points within.\nSuppose that (x*, A*) is a Kuhn-Tucker point for problem (1.5)-(1.7) and\nthat\nJ-{il(gL(X*,h*))i-O\nJ2={i[(gL(x*,A*))i=O\nThen we assume that the matrix\nand\nx\/*>0},\nx*=0}.\nand\nA*)tj,jl\n(A(x*)tjl)\nA(x*)tj]\n0\nis nonsingular for all sets J, where J is any set made up from the union of\nJ and any subset ofJ.\nWe note that assumption (AS5) implies (AS3). Furthermore, ifJ2is empty, any point\nsatisfying the well-known second-order sufficiency condition for a minimizer of (1.5)-\n(1.7) (see, e.g., Fletcher (1981, Thm. 9.3.2)) automatically satisfies (AS5) (see, e.g.,\nGould (1985)). When J2 is nonempty, the connection between (AS5) and Fletcher\u2019s\ncondition is less clear, although (AS5) is certainly implied by the stronger second-order\nsufficiency condition given by Luenberger (1973, pp. 234-235). We believe, however,\nour assumption is reasonable in that small perturbations to the problem can cause\nsome elements ofJe to defect to J while others may drop entirely from J as their\ngradient components become positive. As we do not know which might defect under\nsuch perturbations, (AS5) is a form of \"insurance\" against all possible eventualities.\nLEMMA 5.1. Suppose that (AS1) holds. Let {xk)}\nconverges to the Kuhn-Tucker point x*forwhich (AS2), (AS4), and (AS5) hold, and\nB, k\nK, be a subsequence which\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":14,"text":"558\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nlet A* be the corresponding vectorofLagrange multipliers. Assume that {A (k)}, k\nany sequenceofvectors, that {S(k}, k\nK, is any sequenceofdiagonal matrices satisfying\n0<S-\nscalars, so that theproduct 11A)-A*\nfurtherthat\nIIP(x(, vx<))ll <,\nen there are positive constants,a3, a4, as, a6, ands and an integer valuekoso\nthatif\nK, is\n<-_S(k<_$2 < o, and that {txk}, k\nK,forma nonincreasing sequenceofpositive\nconverges to zero as k increases. Now, suppose\n(5.2)\nwhere the w(are positive scalar parameters whichconveneto zero as k K increases.\n(oN g then\nand\nIIc(x()ll s,(asoo((+((+ a6(\/z(k))2) I(-- *11)\nforall k>-ko,\nProof. We will denote the gradient and Hessian of the Lagrangian function, taken\nwith respect to x, at the limit point (x*, A*) by g* and H*, respectively.\nWe first need to make some observations concerning the status of the variables\nas the limit point is approached. We pick k sufficiently large that the setsI1and 12,\ndefined in (2.12), have been determined. Then, for k\nfloat (variables in I3) or oscillate between floating and being dominated (variables in\nI4). Now recall the definition (2.13) of15and pick an infinite subsequence, K of K\nsuch that:\n(i) 15\n(ii) Variables in16are floating for all k\n(iii) Variables inI7are dominated for all k\nNote that the set13of (2.12) is contained within 16. Note, also, that there are only a\nfinite number (-<2151) of such subsequences \/ and that for k sufficiently large, each\nk K is in one such subsequence. It is thus sufficient to prove the lemma for k\nNow, for k\nK, define\nk K ).\nK, the remaining variables either\n16[_J17with16[\"]17 ;\nK; and\nK.\nK.\n(5.6)\nIv=I,U16\nand\nI=12UI7.\nSo, the variables in I: are floating while those in Io are dominated. We may now\ninvoke Lemma 4.3(i) to obtain inequalities (4.18) and (4.20) for some A*. Furthermore,\nusing (4.20) and the current assumption that txk)llAk)--A*ll converges to zero as k\nincreases, we have that c(x*)= 0. Thus Lemma 4.3(ii) implies that A* is the vector of\nLagrange multipliers corresponding to x*. We thus have\n(5.7)\nand\nfor all sufficiently large k\/ from inequalities (4.18) and (4.20). Moreover,\nconverges to A* and henceVx@k)converges to g*. Therefore, from Lemma 2.1,\n(5.9)\nx\/*\n0\nfor all\nIc,\nand\n(g*L)i\n0\nfor all\nI:.\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":15,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n559\nUsing Taylor\u2019s theorem,\nVxf(k)\ng(k)+\ng(x*) + H(x*)(x(k)-x*)+ A(x*)\n(5.10)\nwhere\n(5.11)\nand\n(5.12)\nr2(x(k), x*, X(k), A*) ,(X}k-Af)t-I(x*)(x(g)-x*).\nj=l\nThe boundedness and Lipschitz continuity of the Hessian matrices offand Cs in a\nneighbourhood of x* along with the convergence of(k) to A* then give that\nr,(x(), x*, X())II-<avlx()-x*ll2,\nrz(x(\"), x*, X(\"), A*)ll <--a8llx(\"-x*ll Ilx(\"-A*11\nfor some positive constants a7andas.In addition, again using Taylor\u2019s theorem and\nthat c(x*)\nO,\nc(x(k))\nA(x*)(x(k)-x*) + r3(x(k), x*),\n(5.13)\n(5.14)\nwhere\n(5.15)\n(r3(x(k),x*))i\nS\n(x(k--X*)rHi(x*+ tS(x(k--X*))(x(k--X*) dtds\no\n(see Gruver and Sachs (1980, p. 11)). The boundedness of the Hessian matrices of the\nc in a neighbourhood of x* then gives that\n(5.16)\nr(x(), x*)ll-<a9llx(k)-x*ll\nfor some constant ag> 0. Combining (5.10) and (5.14), we obtain\n(517)(Ht(x*,A*) Ar(x*))(x(k)--x*) (Vx*(k)--gL(X*,A*)) (r,+r2)\nA(x*)\nwhere we have suppressed the arguments of rl, r2 and r3 for brevity. To proceed\nfurther, we introduce the notation thatyjis the vectorformed by taking the components\nof the vector y indexed by the set J. We may then rewrite (5.17) as\nHc(x*,\n2\n0\nX(k)-A*\nc(x(k))\nr\nA*)[IF,IF]\nHc(x*,,\nH(x* A*)i,i Ar(x*)II\nA(x*)tol\nh*)[lv,l,)]\nAT(x*)[lv]fl(x(k)--X*)[l.][\n(k)(X(k))[ID]h,\nH(x*,h*)i,,,,.\nA(x*)t.j\n0\n(Vx()-g(x*,*)),\nC(X(k))\n(rl+r)o\nr\n]\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":16,"text":"560\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nusing (5.9). Then, rearranging (5.18) and removing the middle horizontal block we\nobtain\n(HL(X*\nA(x*),,:\n(5.19)\nA*)[i_,,.]\nAT(x*)[,:])((x(k)--X*)[,.])\n0\nC(X(k))\nA(x*)[l](x(k))[l\nr3\nRoughly, the rest of the proof proceeds by showing that thet the right-hand side of\n(5.19) is O(w(k))+O((k)llA(k)--A*]). This will then ensure that the vector on the\n\u2019left-hand side is of the same size, which is the result we require. First, observe that\n(5.20\nfrom (2.10) and (5.2) and\n(5.21)\nfrom (2.11). Consequently, again using (5.9),\n(5.22)\nLet x()=II(x(-x*)c,.ll. Combining (5.7) and (5.22), we obtain\n(5.23)\na + a2. Furthermore, from (5.13), (5.16), (5.22), and (5.23),\n((r,+\nwhere a\na7+ a9+aaaz,\na2\n2(a7+ a9)+ as(ao+ a2),\nMoreover, from (5.8), (5.20), (5.21), and (5.22),\nIIx<-x*ll\nIIX(-a*11\nalo(+a2x,\nwhere ao\nall(X(k))2+al2X(k)w(k)+al3(W(k))2\nand\na3\na7+ a9+ aaao.\n(5.25)\nk\nc(x()-A(x*),,)(x(),\n<a4()+s(()[[A()-A*[[ +ao\n()+a2()x(),\nwhere\n(5.26)\na,4\n+\nA(x*),,]\nBy assumption (AS5), the coefficient matrix on the left-hand side of (5.19) is non-\nsingular. Let its inverse have norm M. Multiplying both sides of the equation by this\ninverse and taking norms, we obtain\nx(k)--X*)[I]\n<= M[a 40)(k)\n(k)\n(k)\n(k\n(k)\n(5.27)\n+aow\n+ a,,(ax()+aax((+ a(w()].\n+a2)x\nNow, suppose that k is sufficiently large that\n(5.28)\nw(Nmin (1, 1\/(4Maz)).\nFuhermore, let\n(5.29)\ng =min (1, 1\/(4Mazs,)).\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":17,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n561\nThen, ifIx\n(5.30)\nwhere a-sao+ a3+ a4.As Ax()converges to zero, we have that\n(5.34)\nmx()ll-< 1\/(4Ma11)\nfor all k sufficiently large. Hence inequalities (5.30) and (5.31) give that\n(5.32)\nWriting a3=4Ma15+1 and a4=4Msl, we obtain the desired inequality (5.3) from\n(5.22) and (5.32). Now, using (5.3) and (5.7), we obtain (5.4), wherea5\na6\nmultiplying the inequality byIX(k).\nWe can obtain the following simple corollary.\nCOROLLARY 5.2. Suppose that the conditionsofLemma 5.1 hold and that (k+) is\nany Lagrange multiplier estimateforwhich\n(5.33)\n<-a,6[[x(-x*ll\/a7o\nforsome positive constantsa16anda17and all k K sufficiently large. Then there are\npositive constants12, a3, a4, a5, a6, s1and an integervaluekoso thatifix(k)<--_12then (5.3),\n(5.34)\nand (5.5) holdforall k>-ko, (k\nK).\nProof Inequality (5.34) follows immediately from (5.33) and (5.3).\nWe now show that the penalty parameter will normally be bounded away from\nzero in both Algorithms 1 and 2. This is important as many methods for solving the\ninner iteration subproblem will encounter difficulties if the parameter converges to\nzero since this causes the Hessian of the augmented Lagrangian to become increasingly\nill-conditioned.\nTHEOREM 5.3. Suppose that the iterates {x(k)}ofAlgorithm\nto the single limit point x*, that (AS1), (AS2), (AS4), and (AS5) hold, thataand\nsatisfy\n(5.35)\na,< a-= min (1, a,o),\n(5.36)\nand that (5.33) holdsforall k sufficiently large when Algorithm 2 is used. Then there is\n(k)>\nforall k.\n--ix\nProof Suppose, otherwise, that Ix(k) tends to zero. Then, step 3 of the algorithm\nmust be executed infinitely often. We aim to obtain a contradiction to this statement\nby showing that step 2 is always executed for k sufficiently large. We note that our\nassumptions are sufficient for the conclusions of Theorem 4.4 to hold.\nFirst, we show that the sequence of Lagrange multipliers {h(k)} converges to h*.\nConsider Algorithm 1. The result is clear if step 2 is executed infinitely often as\neach time the step is executed, A(k+)=(k and the inequality (4.18) guarantees that\n(k)converges to A *. Suppose that step 2 is not executed infinitely often. Then ]IA(k)_A\"11\nwill remain fixed for all k>-kfor some k, as step 3 is executed for each remaining\niteration. But then (4.20) implies that IIc(x   )ll_-<a,7 \nk>k2>k As Ix(k) converges to zero as k increases, al7ix(k)<7\/\u2019lO(ix(k))an\nk sufficiently large. But then inequality (3.2) must be satisfied for some k_->kl, which\nis impossible, as this would imply that step 2 is again executed. Hence, step 2 must\nbe executed infinitely often.\n(k)__<\/2, (5.27)-(5.29) give\nAX(k)<=4m(alw(k)+slix(k)l]A(k)--h*[[).\nal+a2a3and\na2a4. Finally, (5.5) follows from (5.4) by substituting for (k), using (2.5), and\n[3\nor 2of\n3 converges\n\/3,< min\n1,\/3,0 ),\na constanttx>Osuch thatIx\nfor some constanta17for all\n(k) for all\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":18,"text":"562\nA. R. CONN, N.\nI. M. GOULD, AND P. L. TOINT\nNow consider Algorithm 2. The result is clear ifthe multiplier updates are accepted\ninfinitely often, as each time the update is performed Ak+l= (k+l and assumption\n(5.33) guarantees that(+1)converges to A*. Suppose that the update is not accepted\ninfinitely often. Then for all k sufficiently large, I](k+l)ll > P((k+l))-y which implies\nthat (k+)]] diverges. But this contradicts assumption (5.33) and henceA(kconverges\nto A*.\nThereforekAk\nLetkbe the smallest integer for which\n(5.37)\nfor all kk. Now let wk)be as generated by either algorithm. Note that, by\nconstruction and inequality (5.37),\n(5.38)\no()\nfor all kk. (This follows by definition if step 2 of either algorithm occurs and\nbecause the penalty parameter is unchanged whileWk)is reduced when step 3 occurs.)\nWe shall apply Lemma 5.1 or Corollary 5.2 to the iterates generated by the algorithm;\nwe identify the set K with the complete set of integers larger thankand the scalars\n(k)with the set of penalty parameters computed in steps 2 and 3 of either algorithm.\nTherefore we can ensure that ) is sufficiently small so that Lemma 5.1 applies to\nstep 1 of Algorithm 1 (or Corollary 5.2 to step 1 of Algorithm 2) and thus that there\nis an integer k2and constants as, a6, andsso that (5.4)\/(5.34) and (5.5) hold for all\nk\nkz. Letk3be the smallest integer such that\n((k))-%\n--WoS(a+2)\u2019\nmin(\na8\u2019 Wos(a+ 2a8)\nand, if Algorithm 2 is used,\nA*\ntends to zero as k increases for both algorithms.\n(k)\nYl<\n(5.39)\no\n(5.40)\n()\u2019-,\no\n)\n(5.41)\n())r\nwherea8= a+a6.Note that (5.37) and (5.40) imply that\n1\n1\n(5.42)\nk())-,\n18\na6\nfor all k\nk3. Fuhermore, letk4be such that\nfor all k\nat iteration k-1 and k\nF has an infinite number of elements.\nIf Algorithm 2 is used, inequality (5.34) gives that\nk4.Now definek\nmax (k, k, k3, k4), let F be the set {kl Step 3 is executed\nk} and let kobe the smallest element of F. By assumption,\nI1 *11+\n+\n *11\n(from\n( rom\n(5.44)\nIA*I+woa,8())\n+\n(+)-\n(from (5.35))\n from\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":19,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n563\nfor all k>ks, the last inequality following from (5.41) and because \/al,(k+l)\/.(k).\nHence, the multiplier update A(k+l)\n(k+l) in Algorithm 2 will always take place when\nk>=ko.\nFor iteration ko, wk=Wo(tzk),and k=o(k))%. Then (5.5) gives\nc(x.o))i\n((.o+a6())=)IIAo)_A*ll+ao)\n(2ollo-*ll+aoo)\n(5.45)\nS(2Wok+aWo(k))l+)\nWoS(a+2)k)\nV0(k)%\nThus, from (5.45), Step 2 of Algorithm 1 or the same step of Algorithm 2 will be\nexecuted\nwith\nA(k+l)\n(X(k), A(k), S(k),\nInequality (5.4)\/(5.34) in conjunction with (5.35), (5.38), and (5.43) guarantee that\n(5.46)\nIlA(o+-A*lla5(+a6(llA(-A*lloa,8(()\nWe shall now suppose that step 2 is executed for iterations ko+ i, (0N Nj), and\nthat\nI1(o+,+)\nInequalities (5.45) and (5.46) show that this is true for j =0. We aim to show that the\nsame is true for i=j+ 1. Under our supposition, we have, for iteration ko+j+ 1, that\n(o++\n(o),w(o++)\nWo((o))(+)+-, and\n(5.5) gives\n(rom (,42))\n(from (5.43))\n(from (5.37))\n(from (5.39)).\nk)\n(k))\nor\nA(k+l)\n(k+l)\nrespectively.\n(5.47)\n,11\noal8\n(o++l\no((o)),(+)+%. Then\nS,(2(+J+l)ll(++l--*[l+a(++l(++l)\nN s(2woa8()(())+e#+ asWo(()\"+(++)\ns(2woa((\n(from (5.42))\n(from 5.47))\n(5.48)\n(from (5.35)-(5.37))\n(from (5.37))\n(from (5.40)).\nos(a+2a8)((o)-,((o),(++%\no((o),(++%\nThus, from (5.48), step 2 of Algorithm 1 or the same step of Algorithm 2 will be\nexecuted with\nrespectively. Inequality (5.4)\/(5.34) then guarantees that\nIla(++=)\n*ll\nwoas(#))\"+j+)+woa6a8#)())\"+\nwoas(#))+.i+)+woa6a8#)(#))\"+\n(5.49)\nwo(a5+ a6a8(#))\nwo(as+a6)(#o))\"+o,j+)\nwoa8(())\"+,(j+),\nwhich establishes (5.47) for i=j+ 1. Hence, step 2 of the appropriate algorithm is\nexecuted for all iterations k\nko. But this implies that F is finite, which contradicts\nthe assumption that step 3\nproved.\nn(o++l\na5w(+i+)+ 06(++\u2019) x<++\u2019)\na*ll\n(from (5.47))\n(from (5.35)-(5.37))\n(from (5.40))\nis executed infinitely often. Hence the theorem\nis\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":20,"text":"564\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nNote, in particular, that if Algorithm 2 is used with t(k+l)chosen as either the\nfirst-order or least-squares multiplier estimates, the penalty parameter \/k) will stay\nbounded away from zero. This follows directly from Theorem 5.3 because of the\ninequalities (4.18) and (4.20).\nOur definition of floating variables has a further desirable consequence ifwe make\nthe following additional assumption.\n(AS6)\n(Strict complementary slackness condition) If the iterates xk), k\nverge to the limit point x* with corresponding Lagrange multipliers A*, we\nassume that the set\nK, con-\n(5.50)\nJz--{il(gL(x*,A*))i=O\nand\nx*=0}\nis empty.\nNote that if inequality constraints ei(x)>-0 have been converted to equations by\nthe subtraction of slack variables (i.e., rewritten as ei(x)-xn+i=O, xn+i>-_O), this\nstatement of strict complementary slackness is equivalent to the more usual one which\nsays that no inequality constraint shall be both active (the constraint function vanishing)\nand have a corresponding zero Lagrange parameter (see, e.g., Fletcher (1981, p. 51)).\nFor it is easy to show that the Lagrange parameter for such a constraint is precisely\nthe corresponding component of the gradient of the Lagrangian function. A constraint\nbeing active and having a corresponding zero Lagrange parameter is thus the same as\nthe slack variable having the value zero, and its corresponding element in the gradient\nof the Lagrangian function vanishing so the latter is excluded under (AS6).\nTHEOREM 5.4. Suppose that the iterates X(k), k\nwith corresponding Lagrange multipliers A*, and that (AS1)-(AS3) and (AS6) hold.\nThenfork sufficiently large, the setoffloating variables are precisely those which lie away\nfromtheir bounds at x*.\nProof From Theorem 4.4,Vk)converges to gl(X*, A*) and from Lemma 2.1,\nthe variables in the set I5then converge to zero and the corresponding components\nofg(x*, A *) are zero. Hence, under (AS6),I5is null. Therefore, each variable ultimately\nremains tied to one of the setsIorIfor all k sufficiently large; a variable inI is,\nby definition, floating and converges to a value away from its bound. Conversely, a\nvariable in12is dominated and converges to its bound.\nAs a consequence of Theorem 5.4, the least-squares multiplier estimates (2.14)\nare implementable. By this we mean that if\/k) and k) are the columns of A(x)\nK, converge to the limit point x*\nand components of g(x) corresponding to the floating variables at x,respectively,\nthe estimates\n(=-((k)+)()\n(5.51)\nare identical to those given by (2.14) for all k sufficiently large. The estimates (5.51),\nunlike (2.14), are well defined when x* is unknown.\nWe conclude the section by giving a rate-of-convergence result for our algorithms.\nFor a comprehensive discussion of convergence, the reader is referred to Ortega and\nRheinboldt (1970).\nTHEOREM 5.5. Under the assumptionsofTheorem 5.3, the iteratesxk, theLagrange\nmultiplier estimates k)ofAlgorithm\nare at least R-linearly convergent withR-factorat most16min(fl\u2019fln), where\/\nand wheretzis the smallest valueofthe penalty parameter generated by the algorithm in\nquestion.\nand anyk)satisfying (5.33)forAlgorithm 2\nmin y\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":21,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n565\nProof The proofparallels that ofLemma 5.1. First, for k sufficiently large, Theorem\n5.3 shows that the penalty parameter x(k) remains fixed at some value z, say, and, for\nall subsequent iterations, inequalities (3.2)\/(3.6) and\n(5.52)\n(5.25) may be replaced by a4w(k+rl,and consequently,\n(5.53)\nAxk)M(aawk)+k)+a(Axk)+aAxkw)+a3(wk))2).\nHence, if k is sufficiently large that\nw(k)Nmin (1, 1\/(2Maz))\n(.O(k+ )-\".)\/3O)(k)\nand\nT\n(k+ 1)\n([)\/3n\"i\n(k)\nhold. Then, from (3.2)\/(3.6), (5.20), and (5.21), the bound on the right-hand side of\n(5.54)\nand\n(5.55)\nAx(k)\n1\/(4Ma,),\ninequalities (5.53)-(5.55) can be rearranged to give\n(5.56)\nx(k)\n4M(a9w(k)+ (k))\nwhereal9\na3+ a4.But then (5.22) and (5.56) give\n[Ix()-x\u2019l[N azow()+ae\n1 +4Ma9andaz\nthat x()converges at least R-linearly, with R-factor m(.,), to X*. That the same\nis true for ) and ) follows directly from (5.7)\/(5.33) and (5.57).\n(5.57)\nwhereao\n4M. As, by assumption,,< 1, (5.52) and (5.57) show\n6. An example. In Theorem 5.3, we showed that, if there is a unique limit point\nfor the iterates generated by the algorithms, the penalty parameter\nbounded away from zero. We now show that, if there is more than a single limit point,\nbut all the other assumptions of Theorem 5.3 are satisfied, it is indeed possible for the\npenalty parameter to become arbitrarily close to zero.\nWe consider the problem\n()\nis necessarily\n(6.1)\nminimize x\nsubject to the single constraint\n(6.2)\nx2-1\n0,\nfor some r> 0. This problem has two stationary points, namely,\n(x*,*)=(-1,)\nNo bounds appear in the problem, and hence P(x(), V,,()) V()for all k. (Of\ncourse, strictly we have not yet defined our algorithms for such a casethis case is\ncovered in\n8; however, we might think of (6.1)-(6.2) as resulting from a transformation\nof variables where the nonnegativity constraint has been shifted so as to play no role\nhere.) For simplicity, we choose S(k=I for all k, and it can be verified that\nVx(x,,,I,\/x)\n(6.3)\nand(x*,*)=(1,-).\n(6.4)\n2x(x2-1) + 2xA + r.\nWe wish to show that Algorithm 1 can generate a sequence of points that oscillate\nbetween neighbourhoods ofx*and x*, and such that the penalty parameter\/x(tends\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":22,"text":"566\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nto zero. The idea is to consider an infinite sequence of iteration cycles, each of length\nj+ 1, where j is the smallest integer such that\n(6.5)\nr\/o(min [\/Zo, y])%+J, <- min [\/Zo, y].\n2\nFor the first j iterations of each cycle, x)lies in a neighbourhood ofx2*and step 2\nis executed; for the iteration that remains, x)has a value less thanx*and the penalty\nparameter is reduced as step 3 is executed. The process is started with A)=A2*.\nIt remains to show that such a sequence can be constructed. For simplicity, we\nshall consider a single cycle. We will denote the sequence of generated iterates and\ncorresponding Lagrange multiplier estimates by {x(k)} and {A (k)}, 1-<k <-j+ 1, respec-\n(k)\ntively, and will let\ndenote the constant penalty parameter throughout the cycle.\nNow definefl=max(1, ao, flo). Note that, under conditions (5.35) and (5.36),\nc% and\/3, are smaller than\/3. Therefore, because our cycle consists ofj iterations in\nwhich step 2 is executed followed by a single iteration with step 3, the convergence\ntolerances satisfy\nm(k)\nm0C%,+(k-1)>_mO0\n(6.6)\nand\nT\/(k\n+(k-1)fl,\nkfl\n(6.7)\noa =oa\nwhere a\nmin (z, Yl)< 1. Furthermore, pick\n(6.8)\nr_-<4\/o\nand define\n(6.9)\n: min(\nmin (1\u2019 7)\n-)\n,o-, r\/o\nThe cycle involves two types of iterate\"\n(i) For the first j iterations,\n(6.10)\nfor k=l,\nx(k)lies in a neighbourhood ofx*and step 2 is executed. (Strictly, for this demonstra-\ntion, the power of a in (6.10) need only be kfl; however, the extra power of\/3 will\nbe important when we discuss Algorithm 2.)\n(ii) For the last iteration, h(+=-r\/2, x(+l)<x*and step 3 is executed.\nTurning to details, consider case (i). We first show that equation (6.10) determines\nx(k). For (6.10) gives that\nt\n(6.11)\nh(k+l=h(k+{--(1--at)a\n{--a(+)t\nBut then equations (2.5), (3.3), and (6.11) imply that\n3\/3\nfor k\nfor 1\nfor k=j.\n1,\n(k+l\n(6.12)\n(X(k)) -:(1 a\/)t(k+l)\/\nfor k=l,\nfor 1 <k<j,\nfor k=j,\n(k+l)fl\nfor 1 < k-<_j,\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":23,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n567\nand therefore\nas c(x)= x-\nx\/1 + fi\/o\n3\/3\nfor k\nfor 1 <k<j,\nfor k =j\n1,\n(6.13)\nX(k)-\"x\/1--(1--a)txa(k+l)\nx\/1\n\/xa(+)\n1. We now show that such values of x()and h()pass the acceptance\ntests (3.1) and (3.2). The constraint test (3.2) is satisfied for all 1 N k Nj because of\n(6.7), (6.9), (6.12) and because\nNoand a < 1. Fuhermore, it follows from (6.4)\nand (6.10) that the gradient of the augmented Lagrangian function at (x(), h()) is\n--x(k))+2x(k)a(k+2)\n(1 -x(k))\nfor lNk<j,\nfor k=j.\n(6.14)\nV(x(), h (k), I, )=\nIt remains to show that this gradient is acceptably small. First,\n0<(2-\n(6.17)(2+a3)a2 N (2+ (o) N3(N6NWo.\nThus (6.6), (6.16), and (6.17) imply (3.1). Next, consider any k for which 1 < k <j. Then\n0 1- (1 a)a(k+\u2019)x(k)1\n3fl\n(6.15)\nx(1)\n1+5a\nby dint of (6.13). Thus, from (6.8), (6.14), and (6.15),\n)3\n(.6.16)\n3fl\nVx(X(,),\n(1)\nI, )<(2+3)\nBut as\nNoand\n< 1, definition (6.9) gives\n(6.18)\nbecause of (6.9) and (6.13). Hence, from (6.14) and (6.18)\n0NV(x()h()\n(6.19)\nI, )< (2a +ff(1--ff))ff(k+l)\nOnce again, as\nNoand a < 1, (6.8) and (6.9) give\n(2a + g(1 a))a N (2+o) N6NWo.\nEquations (6.6), (6.19), and (6.20) then imply (3.1). Finally, (6.13) gives\n(6.20)\n(6.21)\n0N\nff(j+l)\nx(j)\n1.\nHence, from (6.14) and (6.21)\n(6.22)\nOVx(X(j), h(j)\n1,)<a\n(j+l)\nThen (3.1) follows from (6.6), (6.8), (6.9), and (6.22). Moreover, it follows from (6.15),\n(6.18), and (6.21) thatxis the only possible limit point of the first j iterates of the\ncycle. Thus we have shown that the first j iterates in our cycle have the required\nproperties.\nWe now consider case (ii). We have to show that it is possible to have x(j+)<x\nwith ]]c(x(J+))[[ > (J+). Equivalently, we show that inequality (3.1) (but not (3.2)) of\nstep 1 is satisfied for some x(j+)of the form\n(6.23)\nx(J+)<-1.\nWe note that (6.5) and (6.7) imply that\n(j+)\n(6.24)\n\"\n< -.\n2\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":24,"text":"568\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nRecalling that A(j+l)\nA*=-o., we thus require that the inequalities (6.23),\n(6.25)\nIO(x)\n2--x(x2-1)-o.x+o\n(.O(j+l)\nand\n(6.26)\nIx2-11>r\/\nare satisfied at x\n(6.26). At the endpoints of the interval, we have that\nx(j+l).Now observe that any x\n(-v\/1 +o.\/x,-v\/1+ r\/(+l) satisfies\n(6.27)\n--+o.+\n>0\nand\n(6.28)\nqt(-v\/1 + o.\/x)\no\u2019v\/1 +o\u2019\/x\n-1 +4\u20191 +\nThe continuity of the function q, along with (6.27) and (6.28) implies the existence of\na root inside the interval. Any x sufficiently close to this root will therefore satisfy the\nrequired inequalities (6.23), (6.25), and (6.26) and we select such a point to define\nx+l). Because of (6.26), step 3 is executed and A(+2)remains equal to A2*.\nFurthermore, since the interval (-V\u2019l +o\/x,-\/1 + r\/(+1) of case (ii) shrinks to\nthe single pointx*=-1 as\/z tends to zero, this point is the only possible limit point\nof the sequence of iterates besides x2*.\nThis completes our example for Algorithm 1.\nWe now show that a slightly modified form of this example applies to Algorithm\n2. Given\/Zo, pick o. sufficiently small such that\no.<min(i\nNote that such a o\" satisfies (6.8). We construct an infinite sequence of iteration\ncycles, each of length j+ 2 with j defined as before to be the smallest integer such that\ninequality (6.5) is satisfied. The first j iterations are identical to those already described\nin case (i) above. Iteration j+ 1 is identical to case (ii) above, except that the Lagrange\nmultiplier estimate is set toA*\nx(+2close tox2*is selected and the Lagrange multiplier estimate is reset to A2*\nNote that each Lagrange multiplier estimate, (6.10) or +0-\/2, satisfies\nI.(\u2019)1--<_u\/z\nbecause c < 1, sc-<_o., and by choice of o\" in (6.29). Moreover, (5.33) is satisfied because\nthe errors in the multiplier estimates (6.10) are bounded by w(kfrom (6.7) and (6.9).\nIt remains to show that we can construct a suitable iterate x+2)at the last iteration\nof each cycle. We thus require that\n(6.29)\n)\n2zo\no\u2019\/2 in step 3. For the remaining iteration, a point\n-o\u2019\/2.\n(6.30)\n-\u2019\/\n(6.31)\n2\n(j+2)2\nx(+2)(x\n1)+ o\u2019x(+2)+ o\"\nThis may be achieved by choosing x(+2)as the zero\n(.D(j+2).\n(6.32)\n+q\u2019l\n2o.\/x\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":25,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n569\nof that function. The Lagrange multiplier estimate is then reset to A 2\", which is allowed\nbecause of (6.30) and the fact that the root (6.32) converges tox*\nzero. Whether or not (3.6) holds is mainly irrelevant, since its failure only causes a\nfurther reduction of the penalty parameter, which suits our purpose. However, if (3.6)\nis violated, we note that (6.6) and (6.7) must be replaced on the next cycle by\n1 as \/z tends to\n(6.33)\ntO(k)\n0)0aa\u2019+k\/3w>O)0a(k+l)\/3\nand\n+k\/\n(6.34)\n7\n(k)\n7oa 7oa\nwhere a\nin the tests (3.5) and (3.6) on the first j iterations of the next cycle because of the\npresence of the extra a term in equations (6.12) and (6.14).\nmin (\/z, yl)< 1. These slightly more stringent tolerances are still acceptable\n7. Second-order conditions. It is useful to know how our algorithms behave if we\nimpose further conditions on the iterates generated by the inner iteration. In particular,\nsuppose that x(k)satisfies the following second-order sufficiency condition\"\n(AS7)\nSuppose that X(k)satisfies (3.1)\/(3.5), converges to x* for k\nJ1 andJ2 are as defined by (5.1). Then we assume that V,,a\u2019(k)...\nuniformly positive definite (that is, its smallest eigenvalue is uniformly\nbounded away from zero) for all k K sufficiently large.\nK, and that\nJ,uJ2.J,uJ2is\nWith such a condition we have the following result.\nTHEOREM 7.1. Under (AS1)-(AS3) and (AS7), the iterates x(k), k\nby either Algorithm 1 or 2 converge to an isolated local solutionof(1.5)-(1.7).\nK, generated\nProof By definition of,\n(7.1)\nLetsjbe any nonzero vector satisfying\n(7.2)\nA(k)\n-x[j S[j\n--0,\nwhere J is any set made up from the union ofJ1 and any subset ofJ2. Then for any\nsuch vector,\n(7.3)\n[j,j]S[j]\nfor some e >0, under (AS7) as J is a subset ofJUJz. It follows from (7.1)-(7.3) that\nstjH(x(k),\nBy continuity ofHLas x(k)and (k)approach their limits, this gives that\nTHL(X, h*\n(7.5)\nstj\n(7.4)\nk))j,jstj>- e.\n>\n)tj,jstj\ne\nfor all nonzerosj satisfying (7.2), which implies that x* is an isolated local solution\nto (1.5)-(1.7) (see, for example, Avriel, (1976, Thm. 3.11)).\nThe importance of (AS7) is that the inner iteration termination test (step 1 of\neither algorithm) might be tightened so that\npositive definite, for all floating variablesj(kand all k sufficiently large, in addition\nto (3.1)\/(3.5). If the strict complementary slackness condition (AS6) holds at x*,\nTheorem 5.4 ensures that the set J is empty andJ identical to the set of floating\nvariables after a finite number of iterations and thus, under this tighter termination\ntest, (AS7) and Theorem 7.1 hold.\n(k)\nis required to be uniformly\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":26,"text":"570\nA. R. CONN, N.I. M. GOULD, AND P. L. TOINT\nThere is a weaker version of this result, proved in the same way, that if the\nassumption of uniform positive definiteness in (AS7) is replaced by an assumption of\npositive semidefiniteness, the limit point then satisfies second-order necessary condi-\ntions (Avriel (1976, Thm. 3.10)) for a minimizer. This weaker version of (AS7) is easier\nto ensure in practice as certain methods for solving the inner iteration subproblem,\nfor instance, that of Conn, Gould, and Toint (1988a), guarantee that the second\nderivative matrix at the limit point of a sequence of generated inner iterates will be\npositive semidefinite.\n8. Further comments. We now briefly turn to the more general problem (1.1)-(1.3).\nAs we indicated in our Introduction, the presence of the more general constraints (1.3)\ndoes not significantly alter the conclusions that we have drawn so far. If we define the\nappropriate generalization of the projection (2.1) by\nli\n(8.1)\n(P[x])i\nif X\nif X\notherwise,\nli,\n\/\u2019\/i\nxi\nand let B={xll<=x<u}, we may then use the algorithms of\nsignificant modification. Our concept of floating and dominated variables stays essen-\ntially the same; for any iterate x(k)in B we have three mutually exclusive possibilities\nfor each component xlk), namely,\n(i)\n0\nxl\n(8.2)\n(Vx()(k))i\nxlk)-Ui< (Vx((k))i <Xlk)-li.\nIn case (i) we then have\n(P(x(), Vx()(k)))i\n3 without further\nk)\n(Vx(I.)(k))i\nXl\n(ii)\nk)\nU\nO,\n(iii)\n(8.3)\nXlk)-li,\nwhereas in case (ii) we have\n(8.4)\n(P(x(), Vxo(k)))\nXl\nk)\nlXi,\nand in case (iii)\n(8.5)\nThexl)which satisfy (i) or (ii) are now the dominated variables (the ones satisfying\n(i) are said to be dominated above and those satisfying (ii) dominated below); those\nwhich satisfy (iii) are the floating variables. As a consequence, the sets corresponding\nto those given in (2.12) are straightforward to define.Inow contains variables that\nfloat for all k e K sufficiently large and converge to the interior of B.I2is now the\nunion of the two sets I2, made up of variables that are dominated above for all k e K\nsufficiently large, and I2,, made up of variables that are dominated below for all k e K\nsufficiently large. Likewise, I3is the union of the two sets I3, made up of variables\nthat are floating for all sufficiently large k e K but converge to their lower bounds,\nand I3, made up of variables that are floating for all sufficiently large ke K but\nconverge to their upper bounds. With such definitions, we may reprove all ofthe results\nof\n3-7, assumptions (AS5) and (AS6) being extended in the obvious way and\nTheorem 5.4 being strengthened to say that, for all k e K sufficiently large, I2andI2\nare precisely the variables that lie at their lower and upper bounds (respectively) at x*.\nWe have not made any statement here about how the scaling matrices S(k)should\nbe constructed, merely that they may be used. We consider that constraint scaling is\n(P(x(), Vx()(k)))\n(Vx((k))i.\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":27,"text":"A GLOBALLY CONVERGENT AUGMENTED LAGRANGIAN METHOD\n571\nessential for any realistic algorithm and believe that it is important that the scaling\ncan be changed (albeit not too drastically) as the computation proceeds. We defer a\ndiscussion of the issues of how to choose such scalings until we have performed\nsignificant numerical testing of our algorithms. We also note that the results given here\nare unaltered if the convergence tolerance (3.1)\/(3.5) is replaced by\n(8.6)\nfor any sequence ofpositive diagonal matrices {D )} with uniformly bounded condition\nnumber. This is important as the method of Corm, Gould, and Toint (1988a), which\nwe would consider using to solve the inner iteration problem, allows for different\nscalings for the components of the gradients to cope with variables of differing\nmagnitudes.\nFinally, although the rules for how the convergence tolerances r\/(k and to(k are\nupdated have been made rather rigid in this paper and although the results contained\nhere may be proved under more general updating rules, we have refrained from doing\nso here as the resulting conditions on the updates seemed rather complicated and are\nunlikely to provide more practical updates.\nIID<)P(x<,V<)l}_-<o<\nAcknowledgments. The authors thank Annick Sartenaer for her careful reading of\nthis paper. We are also grateful to two anonymous referees whose comments have\nhelped us improve the original draft.\nREFERENCES\nM. AVRIEL, Nonlinear Programming: Analysis and Methods, Prentice-Hall, Englewood Cliffs, NJ, 1976.\nM. C. BARTHOLOMEw-BIGGS, Recursive quadraticprogramming methods based on the augmented Lagrangian\nfunction, Math. Programming Stud., 31 (1987), pp. 21-42.\nD. P. BERTSEKAS, Augmented Lagrangian and exact penalty methods, in Nonlinear Optimization 1981, M.\nJ. D. Powell, ed., Academic Press, London, New York, 1982a, pp. 223-234.\nConstrained Optimization and Lagrange Multiplier Methods, Academic Press, London, New York,\n1982b.\nA. R. CONN, N. I. M. GOULD, M. LESCRENIER, AND PH. L. TOINT, Performance ofa multifrontalscheme\nforpartially separable optimization, Report CSS 218, AERE, Harewell Laboratory, Harwell, U.K., 1987.\nA. R. CONN, N. I. M. GOULD, AND PH. L. TOINT, Global convergenceofa classoftrust region algorithms\nfor optimization with simple bounds, SIAM J. Numer. Anal., 25 (1988a), pp. 433-460, see also SIAM J.\nNumer. Anal., 26 (1989), pp. 764-767.\nTesting a classofmethodsforsolving minimization problems with simple bounds on the variables, Math.\nComp., 50 (1988b), 399-430.\nR. S. DEMBO, S. C. EISENSTAT, AND T. STEIHAUG, Inexact Newton methods, SIAM J. Numer. Anal., 19\n(1982), pp. 400-408.\nR. S. DEMBO AND U. TULOWITZKI, Local convergence analysisforsuccessive inexact quadraticprogramming\nmethods, School of Organization and Management Working paper series B no. 78, Yale University,\nNew Haven, CT, 1984.\nL. C. W. DIXON, P. DOLAN, AND R. PRICE, Finite element optimization: the useofstructured automatic\ndifferentiation, in Stimulation and Optimization of Large Systems, A. Osiadacz, ed., Oxford University\nPress, Oxford, 1988, pp. 117-141.\nA. DRUD, CONOPT: a GRG codeforlarge sparse dynamic nonlinear optimization problems, Math. Program-\nming, 31 (1985), pp. 153-191.\nR. FLETCHER, Practical MethodsofOptimization, Vol. 2, John Wiley, London, New York, 1981.\nP. E. GILL, W. MURRAY, M. A. SAUNDERS, AND M. H. WRIGHT, Some theoretical properties ofan\naugmented Lagrangian meritfunction, Report SOL 86-6, Department of Operations Research, Stanford\nUniversity, Stanford, CA, 1986.\nN. I. M. GOULD, On practical conditionsforthe existence and uniquenessofsolutions to the general equality\nquadratic programming problem, Math. Programming, 32 (1985), pp. 90-99.\n, On the convergence ofa sequential penaltyfunction methodforconstrained minimization, SIAM J.\nNumer. Anal., 26 (1989), pp. 107-128.\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"},{"page":28,"text":"572\nA. R. CONN, N.\nI. M. GOULD, AND P. L. TOINT\nA. GRIEWANK AND PH. L. TOINT, Partitioned variable metric updatesfor large structured optimization\nproblems, Numer. Math., 39 (1982), pp. 119-137.\nW. A. GRUVER AND E. SACHS, Algorithmic Methods in Optimal Control, Research Notes in Math., 47,\nPitman, Boston, 1980.\nW. W. HAGER, Dual techniquesforconstrained optimization, J. Optim. Theory Appl., 55 (1987), pp. 37-71.\nM. R. HESTENES, Multiplier and gradient methods, J. Optim. Theory Appl., 4 (1969), pp. 303-320.\nL. S. LASDON, Reduced gradient methods, in Nonlinear Optimization 1981, M. J. D. Powell, ed., Academic\nPress, London, New York, 1982, pp. 235-242.\nD. G. LUENBERGER, Introduction to Linear and Nonlinear Programming, Addison-Wesley, London, 1973.\nJ. J. MOR, Recent developments in algorithms andsoftware for trust region methods, in Mathematical\nProgramming: The State of the Art, A. Bachem, M. GriStschel, and B. Korte, eds., Springer-Verlag,\nBerlin, 1983, pp. 258-287.\nW. MURRAY, Analytical expressionsforeigenvalues and eigenvectorsofthe Hessian matricesofbarrier and\npenalty functions, J. Optim. Theory Appl., 7 (1971), pp. 189-196.\nB. A. MURTAGH AND M. A. SAUNDERS,MINOSuser\u2019s manual, Report SOL 80-14, Department\nof Operations Research, Stanford University, Stanford, CA, 1980.\nJ. M. ORTEGA AND W. C. RH El NBOLT, lterative solutionofnonlinear equations in several variables, Academic\nPress, London, New York, 1970.\nE. POLAK AND A. L. TITS, A globally convergent, implementable multiplier method with automatic penalty\nlimitation, Appl. Math. Optim., 6 (1980), pp. 335-360.\nM. J. D. POWELL, A methodfornonlinear constraints in minimization problems in Optimization, R. Fletcher,\ned., Academic Press, London, New York, 1969.\nR. T. ROCKAFELLAR, Augmented Lagrangians and applications ofthe proximal point algorithm in convex\nprogramming, Math. Oper. Res.,\n(1976), pp. 97-116.\nK. SCHITTKOWSKI, The nonlinear programming methodof Wilson, Han and Powell with an augmented\nLagrangian type line search function, Numer. Math., 38 (1981), pp. 83-114.\nT. STEIHAUG, The conjugate gradient method and trust regions in large scale optimization, SIAM J. Numer.\nAnal., 20 (1983), pp. 626-637.\nR. A. TAPlA, Diagonalized multiplier methods and quasi-Newton methodsforconstrained optimization, J.\nOptim. Theory Appl., 22 (1977), pp. 135-194.\nPH. L. TOINT, Towards anefficientsparsity exploiting Newton methodforminimization, in Sparse Matrices\nand Their Uses, I. S. Duff, ed., Academic Press, London, New York, 1981.\n, Nonlinear optimization in a large numberofvariables, in Simulation and Optimization of Large\nSystems, A. Osiadacz, ed., Oxford University Press, Oxford, 1988.\nH. YAMASHITA, A globally convergent constrained quasi-Newton method with an augmented Lagrangian type\npenalty function, Math. Programming, 23 (1982), pp. 75-86.\nDownloaded 10\/30\/14 to 18.7.29.240. Redistribution subject to SIAM license or copyright; see http:\/\/www.siam.org\/journals\/ojsa.php"}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Andrew_Conn\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140.pdf","widgetId":"rgw28_56aba1bfdff61"},"id":"rgw28_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=265543262&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56aba1bfdff61"},"id":"rgw29_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=265543262&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":265543262,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"5452316c0cf2bf864cbb3140","name":"Andrew R. Conn","date":"Oct 30, 2014 ","nameLink":"profile\/Andrew_Conn","filename":"cgt_28_2.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Andrew_Conn\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Andrew_Conn\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"7e2e424380eb284f0225f6fff1594a94","showFileSizeNote":false,"fileSize":"2.82 MB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"5452316c0cf2bf864cbb3140","name":"Andrew R. Conn","date":"Oct 30, 2014 ","nameLink":"profile\/Andrew_Conn","filename":"cgt_28_2.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Andrew_Conn\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Andrew_Conn\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"7e2e424380eb284f0225f6fff1594a94","showFileSizeNote":false,"fileSize":"2.82 MB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=8nfpduNv0FKPJK685wIZhwoymuhot8u-yLw-4QxcSUqSjqxJ-siaYJtQjDRtxOQeFB_tp8YHLymIV_RXfFNxvw.ZEVH2xVOTnio6bT6jZaA9X-7qmIqGH7GQc-SscAFLPQJrhotfZB2UY108zcOuEHufhUdswHb4_qy_QAQLTRdxA","clickOnPill":"publication.PublicationFigures.html?_sg=k3qkkN6sotI7rKJf0SD5ku12fiTaJ59-WyWTIyWFZAQron4aMDeXzgKEWmfAPcpkNRtUMOL45RdVQ4r94zmQ4A.L_iNnKVkw6xbHXqXowgLfFPt6ETmX13pDDqI-zVILXM2h5rV6wYT6_AJ26NIxRaqTwMDmGWlszEGSZ-vX2PPbg"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndrew_Conn%2Fpublication%2F265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds%2Flinks%2F5452316c0cf2bf864cbb3140.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=_q2kCbotHkRJrnL1Hk1HYTrUPtxUaZvKe_iuz-LWt-tk32eIVGwLK8KcS2o9VC-LAS7HpraOMO0NeiWom6z9xg","urlHash":"efa1c000e9c6dd8c11ae08e38a24da2d","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=wQfOYKKIGcVyyXf4IYxEJkaGXdbYTRTcPhRIhYoxiQE4DMCb7i-ZC5konv5hNhU7cvRWeuzvVdLc1D2cHTP1JTQpjXi8jWoi1oa6NgAtjCU.ULZ1KrfGhK4MVo_HDeBVFhNVO9-Rh8n14TNVDU14DCtOb5ppKfOKRkeNwXC1u3mok_3RCgfS4Xj9o5UAZqfj5A.lXktfrYlr1WCJzy7s3JEuRVeymbC41LLbKHNXoCAAIe2jBOJkWUjuMmBPSZHFDE7RvYzNY5Vm1Botd0JeZRXZQ","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"5452316c0cf2bf864cbb3140","trackedDownloads":{"5452316c0cf2bf864cbb3140":{"v":false,"d":false}},"assetId":"AS:157963109019648@1414672748657","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":265543262,"commentCursorPromo":null,"widgetId":"rgw31_56aba1bfdff61"},"id":"rgw31_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FAndrew_Conn%2Fpublication%2F265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds%2Flinks%2F5452316c0cf2bf864cbb3140.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A157963109019648%401414672748657&publicationUid=265543262&linkId=5452316c0cf2bf864cbb3140&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=ITLEQyvAN1L7-09y9N8Ujr-wU_t8fmrN_krTLeeNmSk0aauq4CqWvnM67kyr-SwRkV5p7qOtfbS8h_usLeWTaxi_U8vMSgPnSJi9JKRa4Vs.mvbsDHD1L9rZgmU_zNC5WoB0eq6q5dDgJByMGSGT7hNNKUBRMMIPG2B8iECvUwvtSenP83aEVxK6EQXGC4qWyw.UinX9wilMsN5cYwndI_WAritA_PWlQYTbWqoVbi2KuJ9cLbXmQLsdc5s-q9DgH9bDiRmnMqCfJj6XzBycehgvA","publicationUid":265543262,"trackedDownloads":{"5452316c0cf2bf864cbb3140":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw33_56aba1bfdff61"},"id":"rgw33_56aba1bfdff61","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba1bfdff61"},"id":"rgw34_56aba1bfdff61","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw35_56aba1bfdff61"},"id":"rgw35_56aba1bfdff61","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw36_56aba1bfdff61"},"id":"rgw36_56aba1bfdff61","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw37_56aba1bfdff61"},"id":"rgw37_56aba1bfdff61","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw32_56aba1bfdff61"},"id":"rgw32_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw30_56aba1bfdff61"},"id":"rgw30_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba1bfdff61"},"id":"rgw2_56aba1bfdff61","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":265543262},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=265543262&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba1bfdff61"},"id":"rgw1_56aba1bfdff61","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"Z\/Ax4lu3tNsBjiuqDARuT0\/6CpWqUiB\/mVj56kujWoNboDdhl36Vwu8vfgK5KoxNR8Cs807HEM9VX+RpdqMPSaKT4egkSjNNYprQSybudzEfk456GD4iYwbYQJ8aab+8ckzfxFcgBNHg6FL\/xn9CGtA2G3U0C5WH3\/GosoI0\/EKb+1PTyPDLSbEBy6izlSx2icB9A\/Skg4EaazVH2+5vVpZvpn4mw28lEzktyalqvgFNu8+Zro96CGyBL\/UfIWCbYcXHI+5Yd6YGdNWlRI+x1kku9hW6ITCUdivXC3yvKqQ=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds\" \/>\n<meta property=\"og:description\" content=\"The global and local convergence properties of a class of augmented Lagrangian methods for solving nonlinear programming problems are considered. In such methods, simple bound constraints are...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\" \/>\n<meta property=\"rg:id\" content=\"PB:265543262\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1137\/0728030\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"A Globally Convergent Augmented Lagrangian Algorithm for Optimization with General Constraints and Simple Bounds\" \/>\n<meta name=\"citation_author\" content=\"Andrew R. Conn\" \/>\n<meta name=\"citation_author\" content=\"Nicholas I.M. Gould\" \/>\n<meta name=\"citation_author\" content=\"Philippe L. Toint\" \/>\n<meta name=\"citation_publication_date\" content=\"1991\/04\/01\" \/>\n<meta name=\"citation_journal_title\" content=\"SIAM Journal on Numerical Analysis\" \/>\n<meta name=\"citation_issn\" content=\"0036-1429\" \/>\n<meta name=\"citation_volume\" content=\"28\" \/>\n<meta name=\"citation_issue\" content=\"2\" \/>\n<meta name=\"citation_doi\" content=\"10.1137\/0728030\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Andrew_Conn\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\/links\/5452316c0cf2bf864cbb3140.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-acf1f9bb-b938-4a3b-b1f2-c45a65a03891","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":587,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw38_56aba1bfdff61"},"id":"rgw38_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-acf1f9bb-b938-4a3b-b1f2-c45a65a03891", "8c4f3432b397cd84d4ac06c85e3e6abb53292c4b");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-acf1f9bb-b938-4a3b-b1f2-c45a65a03891", "8c4f3432b397cd84d4ac06c85e3e6abb53292c4b");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw39_56aba1bfdff61"},"id":"rgw39_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/265543262_A_Globally_Convergent_Augmented_Lagrangian_Algorithm_for_Optimization_with_General_Constraints_and_Simple_Bounds","requestToken":"AUdZbLrxeokOH9FZQ9DJFyW9E7ZuwrpkLBARDI93s5SPvayt8wZiJg8A5mpDVZ+BCs81nP0+UDhUdOayeaN99R4xRU+nRxfCgmU+Xkd8f2+ocnYnUZEUE9D2Li4MQ3Q5w4BYRr0CBN4aNlshxyhbbCgfTqjKlhS0EJM\/rztneUQ5jrkfmHFJtN6VlLlfm7el4rXLdQWIzZAgO817TMuDygyYKBN8m15edvrHmAho2d7KCB9Exh4EL3C7CqXl3QtcwXudSxDef2f17nwkWOGIXODzzy7zrjkKz5VPGErBiYk=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=NTljHROkClw2R6HRGpG_DclKfErTYY7hDVH4gYicO2xuOAoDE0PaONLvfQk7310x","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjY1NTQzMjYyX0FfR2xvYmFsbHlfQ29udmVyZ2VudF9BdWdtZW50ZWRfTGFncmFuZ2lhbl9BbGdvcml0aG1fZm9yX09wdGltaXphdGlvbl93aXRoX0dlbmVyYWxfQ29uc3RyYWludHNfYW5kX1NpbXBsZV9Cb3VuZHM%3D","signupCallToAction":"Join for free","widgetId":"rgw41_56aba1bfdff61"},"id":"rgw41_56aba1bfdff61","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw40_56aba1bfdff61"},"id":"rgw40_56aba1bfdff61","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw42_56aba1bfdff61"},"id":"rgw42_56aba1bfdff61","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
