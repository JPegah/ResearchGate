<!DOCTYPE html> <html lang="en" class="" id="rgw39_56ab19d0d61f7"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="5iGOgPoCeREjoYcSNU80zsyAVW4TBuMmCMzmhvySKmXlrMhYlBbc+T6dvhpqM5xkmNUz6rr3t4lxvEuRsZEhQ/+XkpBJ3lHVJVg8B8k97ocj6lu9PIvhYRGNT7b9u/pavpPUtNtPc5QZlEKlXsYrSq22NtOx2RTwVYHn+SIEY1GDTcYHE7OK6nyRrw63oQayy13WlosBlnXZtAY2WW6Ini2oKcnCmiS7DDMyBZDwTuG46Mp1lrVC23j2XytqIpt7CSDkYTj15C92j1mQb62GWtGGn1v05xMn/NaRLjClaLo="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-516e4712-fbe1-47b6-9e58-b3b893ee484d",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition" />
<meta property="og:description" content="This paper attempts to recognize spontaneous agreement and disagreement based only on nonverbal multi-modal cues. Related work has mainly used verbal and prosodic cues. We demonstrate that it is..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition/links/0fa8de920cf2bd28793e4b2c/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition" />
<meta property="rg:id" content="PB:224238096" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/10.1109/FG.2011.5771341" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition" />
<meta name="citation_author" content="Konstantinos Bousmalis" />
<meta name="citation_author" content="Louis-Philippe Morency" />
<meta name="citation_author" content="Maja Pantic" />
<meta name="citation_conference_title" content="Automatic Face &amp; Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on" />
<meta name="citation_publication_date" content="2011/04/25" />
<meta name="citation_firstpage" content="746" />
<meta name="citation_lastpage" content="752" />
<meta name="citation_doi" content="10.1109/FG.2011.5771341" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition</title>
<meta name="description" content="Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56ab19d0d61f7" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56ab19d0d61f7" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56ab19d0d61f7">  <div class="type-label"> Conference Paper   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft_id=info%3Adoi%2F10.1109%2FFG.2011.5771341&rft.atitle=Modeling%20hidden%20dynamics%20of%20multimodal%20cues%20for%20spontaneous%20agreement%20and%20disagreement%20recognition&rft.title=2011%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20and%20Workshops%2C%20FG%202011&rft.jtitle=2011%20IEEE%20International%20Conference%20on%20Automatic%20Face%20and%20Gesture%20Recognition%20and%20Workshops%2C%20FG%202011&rft.date=2011&rft.pages=746%20-%20752&rft.au=Konstantinos%20Bousmalis%2CLouis-Philippe%20Morency%2CMaja%20Pantic&rft.genre=inProceedings"></span> <h1 class="pub-title" itemprop="name">Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition</h1> <meta itemprop="headline" content="Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition/links/0fa8de920cf2bd28793e4b2c/smallpreview.png">  <div id="rgw7_56ab19d0d61f7" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56ab19d0d61f7" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Konstantinos_Bousmalis" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://c5.rgstatic.net/m/2671872220764/images/template/default/profile/profile_default_m.jpg" title="Konstantinos Bousmalis" alt="Konstantinos Bousmalis" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Konstantinos Bousmalis</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56ab19d0d61f7" data-account-key="Konstantinos_Bousmalis">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Konstantinos_Bousmalis"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Konstantinos Bousmalis" alt="Konstantinos Bousmalis" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Konstantinos_Bousmalis" class="display-name">Konstantinos Bousmalis</a>    </h5> <div class="truncate-single-line meta">   <a class="meta ga-top-coauthor-institution" href="institution/Imperial_College_London" title="Imperial College London">Imperial College London</a>     </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56ab19d0d61f7"> <a href="researcher/13852141_Louis-Philippe_Morency" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Louis-Philippe Morency" alt="Louis-Philippe Morency" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Louis-Philippe Morency</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56ab19d0d61f7">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/13852141_Louis-Philippe_Morency"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Louis-Philippe Morency" alt="Louis-Philippe Morency" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/13852141_Louis-Philippe_Morency" class="display-name">Louis-Philippe Morency</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56ab19d0d61f7"> <a href="researcher/47824715_Maja_Pantic" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Maja Pantic" alt="Maja Pantic" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Maja Pantic</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56ab19d0d61f7">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/47824715_Maja_Pantic"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Maja Pantic" alt="Maja Pantic" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/47824715_Maja_Pantic" class="display-name">Maja Pantic</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">  <div> Dept. of Comput., Imperial Coll. London, London, UK </div>      DOI:&nbsp;10.1109/FG.2011.5771341     Conference: Automatic Face &amp; Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on      <div class="pub-source"> Source: <a href="http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5771341" rel="nofollow">IEEE Xplore</a> </div>  </div> <div id="rgw14_56ab19d0d61f7" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>This paper attempts to recognize spontaneous agreement and disagreement based only on nonverbal multi-modal cues. Related work has mainly used verbal and prosodic cues. We demonstrate that it is possible to correctly recognize agreement and disagreement without the use of verbal context (i.e. words, syntax). We propose to explicitly model the complex hidden dynamics of the multimodal cues using a sequential discriminative model, the Hidden Conditional Random Field (HCRF). In this paper, we show that the HCRF model is able to capture what makes each of these social attitudes unique. We present an efficient technique to analyze the concepts learned by the HCRF model and show that these coincide with the findings from social psychology regarding which cues are most prevalent in agreement and disagreement. Our experiments are performed on a spontaneous dataset of real televised debates. The HCRF model outperforms conventional approaches such as Hidden Markov Models and Support Vector Machines.</div> </p>  </div>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw27_56ab19d0d61f7">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw26_56ab19d0d61f7"  itemprop="articleBody">  <p>Page 1</p> <p>Modeling Hidden Dynamics of Multimodal Cues for Spontaneous<br />Agreement and Disagreement Recognition<br />Konstantinos BousmalisLouis–Philippe Morency Maja Pantic<br />Abstract—This paper attempts to recognize spontaneous<br />agreement and disagreement based only on nonverbal multi-<br />modal cues. Related work has mainly used verbal and prosodic<br />cues. We demonstrate that it is possible to correctly recognize<br />agreement and disagreement without the use of verbal context<br />(i.e. words, syntax). We propose to explicitly model the complex<br />hidden dynamics of the multimodal cues using a sequential<br />discriminative model, the Hidden Conditional Random Field<br />(HCRF). In this paper, we show that the HCRF model is able<br />to capture what makes each of these social attitudes unique. We<br />present an efficient technique to analyze the concepts learned<br />by the HCRF model and show that these coincide with the<br />findings from social psychology regarding which cues are most<br />prevalent in agreement and disagreement. Our experiments are<br />performed on a spontaneous dataset of real televised debates.<br />The HCRF model outperforms conventional approaches such<br />as Hidden Markov Models and Support Vector Machines.<br />I. INTRODUCTION<br />We have recently witnessed significant advances not only<br />in the machine analysis of nonverbal cues, such as head<br />and hand gestures, facial expressions, auditory cues, but also<br />in the field of affect recognition [1]. However, only few<br />works have so far attempted to recognize social attitudes like<br />interest, politeness and flirting [2]. This is partly so because<br />relevant research in social psychology, which would help<br />identify discriminative combinations of multimodal cues, is<br />at best scarce, and because of the fact that there is a gap of<br />relevant annotated data that can be used for such analyses.<br />Despite these difficulties, achieving such a goal is very im-<br />portant if we are to move towards a more naturalistic human–<br />computer–interaction; machines who are able to detect social<br />attitudes and react according to the needs of their user will be<br />more efficient and welcomed for the rather more naturalistic<br />experience they are bound to offer.<br />Such social attitudes are those of agreement and disagree-<br />ment, which are inevitable in daily human–human interac-<br />tions, from finding a location to dine, to discussions on no-<br />toriously controversial topics like politics. Existing work on<br />the automatic recognition of agreement and/or disagreement<br />(see Table I) has mainly used verbal and prosodic cues, e.g.<br />pitch and energy. To the best of our knowledge, no work has<br />managed to successfully recognize spontaneous agreement<br />and disagreement based solely on nonverbal multimodal<br />K. Bousmalis is with the Department of Computing, Imperial College<br />London, SW7 2AZ, London, UK k.bousmalis@imperial.ac.uk<br />L.–P. Morency is with the Institute for Creative Technologies,<br />University of Southern California, Los Angeles, CA 90094, USA<br />morency@ict.usc.edu<br />M. Pantic is with the Department of Computing, Imperial College<br />London, SW7 2AZ, London, UK, and EEMCS, University of Twente,<br />7522 NB Enschede, The Netherlands m.pantic@imperial.ac.uk<br />P(y=DISAGREEMENT|x)=0.8<br />X2<br />h1<br />h2<br />h3<br />y<br />Head Nod<br />Head Shake<br />Hand Wag<br />Hands Scissor<br />Shoulder Shrug<br />F0<br />X3<br />X1<br />X4<br />X5<br />P(y=AGREEMENT|x)=0.2<br />h4<br />h5<br />Fig. 1: HCRF for spontaneous agreement/disagreement<br />recognition. hirepresents the hidden state that captures the<br />underlying dynamics between features and labels at a given<br />timestamp i. The HCRF model is able to capture fine–grain<br />hidden multimodal dynamics better than other models by<br />learning these hidden states and their relation to each class.<br />Consequently, the HCRF model is able to learn a more<br />suitable mapping between the observations x and each class<br />label y ∈ Y .<br />cues. However, although agreements and disagreements are<br />frequently expressed verbally, the nonverbal behavioral cues<br />that occur during their manifestation play a crucial role in<br />their interpretation. Bousmalis et al. [3] have surveyed and<br />identified such cues that seem to be relevant, as those are<br />evident in social psychology literature (see Tables II and III<br />for a summary). According to this survey, it is the temporal<br />underlying dynamics of multimodal cues that will allow us<br />to recognize agreement and disagreement.<br />This calls for a model capable of capturing these complex<br />dynamics and, based on them, distinguishing these social<br />attitudes from each other. A Hidden Conditional Random<br />Field (HCRF) [4], originally proposed for object recognition,<br />is a model capable of not only capturing the underlying<br />structure of events, but also of learning the combinations<br />of features that are shared by each class and the ones that<br />make each of them unique. Hence, HCRFs could be a good<br />candidate for modeling agreement and disagreement.<br />This paper will show that (i) it is possible to recognize<br />spontaneous agreement and disagreement without the use</p>  <p>Page 2</p> <p>Method<br />Hillard et al. [5] (2003)<br />FeaturesClassifier<br />Decision Tree<br />Data<br />ICSI [6]<br />Spontaneous<br />√<br />Verbal, pause, fundamental frequency(F0),<br />duration<br />Verbal<br />head nod, head shake, head turn, head tilt,<br />AU1, AU2, AU12, AU16, AU19, AU20,<br />AU25, AU26, AU27<br />Verbal<br />head yaw, head pitch, head roll, AU1,<br />AU2, AU12, AU18, AU20, AU25, Gaze,<br />head pose<br />Verbal, pitch, energy, duration, pauses,<br />speech rate<br />Galley et al. [7] (2004)<br />el Kaliouby et al. [8] (2004)<br />Bayesian Network<br />HMM, DBN<br />ICSI [6]<br />√<br />—Mind Reading DVD [9]<br />Hahn et al. [10] (2006)<br />Sheerman–Chase et al. [11] (2009)<br />Contrast Classifier, SVM<br />AdaBoost<br />ICSI [6]<br />own<br />√<br />√<br />Germesin and Wilson [12] (2009) Decision Tree, CRFAMI [13]<br />√<br />TABLE I: Summary of the existing systems that have attempted agreement/disagreement classification.<br />of verbal cues (e.g. spoken words); (ii) HCRFs are indeed<br />able to capture the underlying dynamics of multimodal cues<br />and perform better than conventional models in this task<br />(figure 1); and (iii) HCRFs are able to automatically identify<br />groups of features specific to each attitude in a way that<br />confirms the findings in social psychology literature regard-<br />ing which cues are most prevalent during the expression of<br />agreement and disagreement.<br />In the following section, we discuss agreement, disagree-<br />ment and related work on their automatic recognition. In<br />Section III we present Hidden Conditional Random Fields<br />(HCRFs) and our technique to analyze the concepts learned<br />by the HCRF. In Section IV we explain how our data was<br />collected and what experiments we have conducted. Finally,<br />in Section V, we present and discuss our results.<br />II. AGREEMENT AND DISAGREEMENT<br />A. Definitions and Associated Cues<br />Distinguishing between different kinds of agreement and<br />disagreement is difficult, mainly because of the lack of<br />widely accepted definitions of agreement and disagreement<br />[3]. We can distinguish among at least three ways one could<br />express agreement and disagreement with:<br />• Direct Speaker’s Agreement and Disagreement: A<br />speaker directly expresses his/her agreement or dis-<br />agreement, e.g. “I (dis)agree with you”.<br />• Indirect Speaker’s Agreement and Disagreement:<br />A speaker does not explicitly state her agreement and<br />disagreement, but expresses an opinion that is congru-<br />ent (agreement) or contradictory (disagreement) to an<br />opinion that was expressed earlier in the conversation.<br />• Nonverbal Listener’s Agreement and Disagreement:<br />A listener nonverbally expresses her agreement or dis-<br />agreement to an opinion that is currently or was just<br />expressed. This could be via auditory cues like “mm<br />hmm” or visual cues like a head nod or a smile.<br />It is important to mention at this point that in sponta-<br />neous direct and indirect speaker’s agreement/disagreement,<br />the speaker also exhibits nonverbal behavior which could<br />perhaps be different than the one exhibited during nonverbal<br />listener’s agreement/disagreement.<br />Tables II and III present a full list of the nonverbal<br />cues that can be displayed during agreement and disagree-<br />CUE<br />Head Nod<br />Listener Smile (AU12, AU13)<br />Eyebrow Raise (AU1+AU2)+Head Nod<br />AU1 + AU2 + Smile (AU12, AU13)<br />Sideways Leaning<br />Laughter<br />Mimicry<br />KIND<br />Head Gesture<br />Facial Action<br />Facial Action, Head Gesture<br />Facial Action<br />Body Posture<br />Audiovisual Cue<br />Second–order Cue<br />TABLE II: Cues of Agreement. For relevant descriptions of<br />AUs, see FACS [16].<br />ment [3]1. The most prevalent and straightforward cues seem<br />to be the Head Nod and the Head Shake for agreement<br />and disagreement respectively, with nods intuitively convey-<br />ing affirmation and shakes negation. However, simply the<br />presence of these or any of the other cues alone cannot be<br />discriminative enough, since they could have many other in-<br />terpretations, as studied by Poggi et al. [14] and Kendon [15].<br />B. Related Work on Automatic Recognition<br />There is no work, to the best of our knowledge, that has<br />attempted agreement/disagreement classification on audiovi-<br />sual spontaneous data. Table I summarizes the existing sys-<br />tems that have attempted classification of agreement and/or<br />disagreement in one way or another. However, none of these<br />systems is directly comparable with ours.<br />Hillard et al. [5] attempted speaker agreement and dis-<br />agreement classification on pre–segmented ‘spurts’, speech<br />segments by one speaker with pauses not greater than<br />500ms. The authors used a combination of word–based and<br />prosodic cues to classify each spurt as ‘positive–agreement’,<br />‘negative–disagreement’, ‘backchannel’, or ‘other’. Most of<br />the results reported included word–based cues, however an<br />overall classification accuracy of 62% was reported for a<br />17% confusion rate between the agreement and disagreement<br />classes. Similar works by Galley et al. [7] and Hahn et<br />al. [10] also deal with classifying spurts as disagreement and<br />agreement, with [7] also dealing with finding the addressee<br />of the action. Germesin and Wilson [12] also deal with these<br />issues. However, the features used by these works included<br />1Our discussion of cues for agreement and disagreement is mostly<br />relevant for cultures in Western Europe and North America. Further work<br />might be needed to develop a similar system that targets other cultures.</p>  <p>Page 3</p> <p>CUE<br />Head Shake<br />Head Roll<br />Cut Off<br />Clenched Fist<br />Forefinger Raise<br />Forefinger Wag<br />Hand Chop<br />Hand Cross<br />Hand Wag<br />Hands Scissor<br />Ironic Smile/Smirking [AU12 L/R(+AU14)]<br />Barely noticeable lip–clenching (AU23, AU24)<br />Cheek Crease (AU14)<br />Lowered Eyebrow/Frowning (AU4)<br />Lip Pucker (AU18)<br />Slightly Parted Lips (AU25)<br />Mouth Movement (AU25/AU26)<br />Nose Flare (AU38)<br />Nose Twist (AU9 L/R, AU10 L/R, AU11 L/R)<br />Tongue Show (AU19)<br />Suddenly Narrowed/Slitted Eyes (fast AU7)<br />Eye Roll<br />Gaze Aversion<br />Arm Folding<br />Large Body Shift<br />Leg Clamp<br />Head/Chin Support on Hand<br />Neck Clamp<br />Head Scratch<br />Self–manipulation<br />Feet Pointing Away<br />Sighing<br />Throat Clearing<br />Delays<br />Utterance Length<br />Interruption<br />KIND<br />Head Gesture<br />Head Gesture<br />Head Gesture<br />Hand Action<br />Hand Action<br />Hand Action<br />Hand Action<br />Hand Action<br />Hand Action<br />Hand Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action<br />Facial Action/Gaze<br />Gaze<br />Body Posture<br />Body Action<br />Body Posture<br />Body/Head Posture<br />Hand/Head Action<br />Head/Hand Action<br />Hand/Facial Action<br />Feet Posture<br />Auditory Cue<br />Auditory Cue<br />Auditory Cue<br />Auditory Cue<br />Auditory Cue<br />TABLE III: Cues for Disagreement. For relevant descriptions<br />of AUs, see FACS [16]<br />lexical, structural and durational cues and are not comparable<br />with other systems based on nonverbal cues.<br />The first such system is that by el Kaliouby and Robin-<br />son [8], which attempted agreement/disagreement classifi-<br />cation of acted behavioural displays based on head and<br />facial movements. They used 6 classes: ‘agreeing’, ‘disagree-<br />ing’, ‘concentrating’, ‘interested’, ‘thinking’, and ‘unsure’.<br />They tracked 25 fiducial facial points, out of which they<br />extrapolated rigid head motion (yaw, pitch, and roll), and<br />facial action units (eyebrow raise, lip pull, lip pucker), but<br />also utilized appearance–based features to summarise mouth<br />actions (mouth stretch, jaw drop, and lips parting). They used<br />Hidden Markov Models (HMMs) to detect each head and<br />facial action, and a Dynamic Bayesian Network (DBN) per<br />class was trained to perform the higher–level inference of<br />each of the ‘mental states’ mentioned above, allowing for<br />the co–occurrence of states.<br />Sheerman–Chase et al. [11] are, to our knowledge, the only<br />research group who have attempted recognition of agreement<br />based on non–verbal cues in spontaneous data. However, they<br />did not include disagreement as a class, because of the lack of<br />data. They instead distinguished between ‘thinking’, ‘under-<br />standing’, ‘agreeing’ and ‘questioning’. Their spontaneous<br />data was obtained by capturing the four 12–minute dyadic<br />conversations of 6 males and 2 females. 21 annotators rated<br />the clips with each clip getting on average around 4 ratings<br />that were combined to obtain the ground truth label. For the<br />automatic recognition, they used no auditory features and the<br />tracking of 46 fiducial facial points was used. The output of<br />the tracker was then processed to obtain a number of static<br />and dynamic features to be used for classification. Principal<br />Component Analysis (PCA) was performed on the tracked<br />points in each video frame, and the PCA eigenvalues were<br />used as features. Similarly to el Kaliouby and Robinson [8],<br />the head yaw, pitch and roll, the eyebrow raise, lip pucker<br />and lip parting were calculated as functions of these tracked<br />facial points. Gaze was also estimated in a similar fashion<br />—the eye pupils were among the points tracked.<br />III. HCRFS FOR MULTIMODAL GESTURE RECOGNITION<br />Hidden Conditional Random Fields —discriminative mod-<br />els that contain hidden states— are well–suited to the prob-<br />lem of multimodal cue modeling for agreement/disagreement<br />recognition. Quattoni et al. [4] presented and used HCRFs to<br />capture the spatial dependencies between hidden object parts.<br />Wang et al. [17] used them to capture temporal dependencies<br />across frames and recognize different gesture classes. They<br />did so successfully by learning a state distribution among<br />the different gesture classes in a discriminative manner,<br />allowing them to not only uncover the distinctive config-<br />urations that uniquely identifies each class, but also to learn<br />a shared common structure among the classes. Moreover, as<br />a discriminative model, HCRFs require a fewer number of<br />observations than a generative model like a Hidden–Markov<br />Model (HMM). These were all qualities that prompted us to<br />select HCRFs, as a model to experiment with, in our attempt<br />to recognize agreement and disagreement.<br />A. Model<br />Following the notation of Quattoni et al. [4], [17],<br />we represent m local observations by a vector x<br />{x1,x2,...,xm}. Each local observation xj is represented<br />by a feature vector φ(xj) ∈ ?dwhich includes all input<br />features (e.g., the presence of a head nod or the value<br />of F0-pitch). We wish to learn a mapping between the<br />observations x and the class label y ∈ Y . The class label<br />can be ‘agreement’ or ‘disagreement’. An HCRF models the<br />conditional probability of a class label given an observation<br />sequence by:<br />=<br />P(y | x,θ) =<br />?<br />h<br />P(y,h | x,θ) =<br />?<br />heΨ(y,h,x;θ)<br />y?∈Y,heΨ(y?,h,x;θ).<br />?<br />(1)<br />where h = {h1,h2,...,hm}, each hi∈ H captures certain<br />underlying structure of each class and H is the set of hidden<br />states in the model. The potential function Ψ(y,h,x;θ) ∈ ?<br />is an energy function, parameterized by θ, which measures<br />the compatibility between a label, a sequence of observations<br />and a configuration of the hidden states.</p>  <p>Page 4</p> <p>Ψ(y,h,x;θ)=<br />?<br />+<br />j<br />φ(x,j) · θh(hj) +<br />?<br />?<br />j<br />θy(y,hj)<br />(j,k)∈E<br />θe(y,hj,hk)<br />(2)<br />The graph E is a chain where each node corresponds to<br />a hidden state variable at time t. The paremeter vector θ is<br />made up of three components: θ = [θeθyθh]. We use the<br />notation θh[hj] to refer to the parameters θhthat correspond<br />to state hj ∈ H. Similarly, θy[y,hj] stands for parameters<br />that correspond to class y and state hj and θe[y,hj,hk]<br />measures the compatibility between pairs of consecutive<br />states j and k and the gesture y.<br />B. Training<br />Given a new test sequence x, and parameter values θ∗<br />induced from training examples, we will take the label for<br />the sequence to be:<br />argmax<br />y∈YP(y | x,θ∗).<br />(3)<br />The following objective function is used in training the<br />parameters:<br />L(θ) =<br />?<br />i<br />logP(yi| xi,θ) −<br />1<br />2σ2||θ||2<br />(4)<br />The first term in Eq. 4 is the log-likelihood of the<br />data. The second term is the log of a Gaussian prior with<br />variance σ2, i.e., P(θ) ∼ exp?<br />argmaxθL(θ), under this criterion. For our experiments we<br />used a Quasi-Newton optimization technique to minimize the<br />negative log–likelihood of the data.<br />1<br />2σ2||θ||2?. We use gradient<br />ascent to search for the optimal parameter values, θ∗=<br />C. Analysis<br />The HCRF model is a powerful sequential discriminative<br />model. It can learn the hidden dynamic of a signal using<br />the latent variable hj. For multimodal gesture recognition,<br />this hidden dynamic is usually related to the synchrony and<br />asynchrony between speech and gestures. While previous<br />work has shown the efficiency of HCRF for learning visual<br />gestures [4], [17], none of them described or analysed what<br />the HCRF model learned. In this paper we are presenting an<br />efficient approach to analyze the concepts learned by the<br />HCRF model. This analysis tool enables a new direction<br />of research where machine learning is not simply used as<br />a black box but instead is there to help understand human<br />interactions.<br />To analyze the HCRF model, one has to understand the<br />optimized parameters [θhθeθy]:<br />• θhmodels the relationship between observations xjand<br />hidden states hj. If the HCRF model has 10 input<br />features and 3 hidden states, then the θhparameter will<br />be of length 30 (10x3). By analysing the amplitude of<br />each weights in θh, it is possible to learn the relative<br />importance of each input feature for each hidden state.<br />(a) Forefinger Raise(b) Forefinger Wag<br />(c) Hand Wag(d) Hands Scissors<br />Fig. 2: Some of the gestures used as cues for the experiments.<br />• The parameter θymodels the relationship of the hidden<br />states hjand the label y. If the model contains 3 hidden<br />states and 2 labels, then the θywill be of length 6 (3x2).<br />By analyzing the weights of θy, it is possible to see<br />which hidden states are shared and which ones are not.<br />• The parameter θerepresents the links between hidden<br />states. It is similar to the transition matrix in a Hidden<br />Markov Model. An important difference is that the<br />HCRF model keeps a transition matrix for each label. If<br />the HCRF model contains 3 hidden states and 2 labels,<br />then the θeparameter will be of length 18 (3x3x2).<br />The procedure for analyzing the HCRF model contains<br />three steps: (1) identify the relevant features for each hidden<br />state using θh, (2) determine which hidden states are shared<br />and which ones are not using θy, and (3) analyze the<br />possible transitions between hidden states using θe. In our<br />experiments (see Figure 5), we apply this procedure to<br />identify the relevant concepts learned by the HCRF model<br />to recognize agreement and disgreement behaviors.<br />IV. EXPERIMENTS<br />A. Dataset and Cues<br />Our dataset originated from the Canal 9 Database of<br />Political Debates [18], one that comprises of 43 hours and<br />10 minutes of 72 real televised debates on Canal 9, a local<br />Swiss television station. The debates are moderated by a<br />presenter, and there are two sides that argue around a central<br />issue, with one or more participants on each side. Hence, the<br />database is rather rich in episodes of spontaneous agreement<br />and disagreement.<br />The dataset we used comprises of 53 episodes of agree-<br />ment and 94 episodes of disagreement, which occur over<br />a total of 11 debates. These episodes were selected on the</p>  <p>Page 5</p> <p>basis of verbal content, and thus, only episodes of direct and<br />indirect agreement/disagreement were included (see Section<br />II-A). As the debates were filmed with multiple cameras, and<br />edited live to one feed, the episodes selected for the dataset<br />were only the ones that were contained within one personal,<br />close–up shot of the speaker.<br />We automatically extracted nonverbal auditory features<br />used in related work, specifically the fundamental frequency<br />(F0) and energy, by using a freely–available tool, Open-<br />Ear[19]. Since our main goal is to analyze dynamics of<br />nonverbal cues during agreement/disagreement recognition,<br />our dataset was manuallly annotated to gather as accurate<br />temporal information about the gestures as possible. Based<br />on the results presented in this paper, our future work will<br />evaluate the recognition performance using our automatic<br />nonverbal gesture annotation [20], [21]. The hand and head<br />gestures we included were based off the relevant list of<br />cues from the Social Psychology literature (see Section II-<br />A), with the exception of a number of head and hand<br />gestures that never appeared in the dataset, and the addition<br />of the ’Shoulder Shrug’ and the ‘Forefinger Raise-Like’<br />gestures. The latter is a ‘Forefinger Raise’ without an erect<br />index finger. The cues we finally extracted and used in our<br />experiments are listed in Table IV; the visual cues that may<br />not be self–explanatory from their title are depicted in figure<br />2.<br />CUE<br />Head Nod<br />Head Shake<br />Forefinger Raise<br />‘Forefinger Raise’–Like<br />Forefinger Wag<br />Hand Wag<br />Hands Scissor<br />Shoulder Shrug<br />Fundamental Frequency (F0)<br />Energy<br />KIND<br />Head Gesture<br />Head Gesture<br />Hand Action<br />Hand Action<br />Hand Action<br />Hand Action<br />Hand Action<br />Body Gesture<br />Auditory Cue<br />Auditory Cue<br />TABLE IV: The list of features we used in our experiments.<br />B. Methodology<br />We conducted experiments with Support Vector Ma-<br />chines (SVMs), as our baseline static classifiers, Hidden–<br />Markov Models (HMMs), the most–commonly used dynamic<br />generative model, and Hidden Conditional Random Fields<br />(HCRFs), the dynamic discriminative model we believe is<br />most appropriate for such a task. We conducted different<br />experiments for three groups of cues: only auditory, only<br />visual, and both auditory and visual ones.<br />Our cues were encoded differently for our static and<br />dynamic classifiers, but the same information was available<br />to all classifiers. For SVMs, the features of each gesture were<br />the start frame and the duration (total number of frames) of<br />the gesture within the segment of interest. For the auditory<br />features we used the mean, standard deviation, and the first,<br />second(median), and third quartiles of each. The later values<br />did not take into account the undefined areas of F0, and<br />Fig. 3: Comparisons of recognition performance (total accu-<br />racy) by the classification methods we explored on the three<br />different groups of features used.<br />all values were scaled from -1 to 1. For the experiments<br />with HMMs and HCRFs, we encoded each gesture in a<br />binary manner (1 if the gesture is activated in a certain<br />frame, 0 otherwise), and used the raw values of our auditory<br />features, normalized per subject. Figure 1 allows the reader<br />to visualize the process of reaching a classification decision<br />from our data by using an HCRF.<br />All our experiments were run in a leave–one–debate–out<br />fashion, i.e. the testing set always comprised of examples<br />from the one debate which was not included in the training<br />and validation sets. The optimal model parameters for each<br />test set were chosen by a three–fold validation on the<br />remaining debates. Those were cost and gamma for SVMs,<br />number of mixtures of Gaussians for HMMs, regularization<br />factor for HCRFs and number of hidden states for both<br />HMMs and HCRFs. The HMM and HCRF experiments were<br />run with 10 different random initializations, the best of which<br />was chosen each time during the validation phase (i.e., based<br />on performance on the validation sets). The evaluation metric<br />that we used for all the experiments was the total accuracy<br />in a balanced dataset, i.e. percentage of sequences for which<br />the correct label was predicted in a test set that contains an<br />equal number of agreement and disagreement examples.<br />V. RESULTS AND DISCUSSION<br />Figure 3 summarizes the results of the experiments on<br />spontaneous agreement and disagreement classification using<br />auditory, gestural and both auditory and gestural features. It<br />is clear that:<br />(a) It is possible to perform the task of spontaneous agree-<br />ment and disagreement classification without the use of<br />any verbal features.<br />(b) The temporal dynamics of the cues are vital to the task,<br />as it is evident that SVMs are not able to perform well<br />by using static information alone.<br />(c) HCRFs outperform SVMs and HMMs, especially when<br />the cues used are multimodal and the underlying dy-<br />namics of the different modalities need to be learned.</p>  <p>Page 6</p> <p>Fig. 4: The performance (total accuracy) of HCRFs increases<br />proportionally to the sampling rate of the multimodal data.<br />Figure 4 demonstrates the complexity of the task at hand,<br />and the importance of fine–grain multimodal dynamics to<br />its solution, by summarizing the accuracies achieved with<br />HCRF models when sampling our data at different rates. The<br />fact that the higher the sampling rate, the higher the accuracy<br />achieved by the HCRF models, also demonstrates the ability<br />of the HCRFs to cope with such fine–grain dynamics.<br />We applied our model analysis technique described in<br />Section III-C to the optimal HCRF selected during our<br />experiments. By examination of the weights learned by<br />the HCRF for each of its cues θh, hidden states θy, and<br />transitions θe, we were able to rank, according to importance,<br />the information that the model used. Figure 5 shows the<br />automatically learned concepts of the optimal HCRF model<br />In Figure 5, each hidden state (represented by the white<br />circles) is linked to its highest ranked observed features in a<br />descending order of importance. The relationships between<br />these observed features and the hidden states were identified<br />using the parameter θh. The highest ranked features in these<br />hidden states show that the Head Nod and the Head Shake,<br />which are considered, by social psychologists, the most<br />prevalent cues in agreement and disagreement respectively<br />(see Section II-A), are also the most discriminative cues here.<br />It could be the case that ‘Forefinger Raise-Like’ gestures<br />might in fact play no role in discriminating between the two<br />attitudes.<br />By analyzing the parameter θy, we can see that the HCRF<br />model assigned one state as prevalent for each of the two<br />class labels, and one state as shared between them. The<br />analysis of the transition parameter θeshows that different<br />transitions are learned for each class label. The Figure 5<br />marked the most likely transitions associated to each attitude<br />(class label): green for agreement and red for disagreement.<br />Disagreement will usually end with hidden state h = 2<br />(middle circle) while the agreement can transition directly<br />to a head shake (hidden state h = 1 depicted on the left).<br />VI. CONCLUSION AND FUTURE WORK<br />Related work on spontaneous agreement/disagreement<br />classification has used verbal (e.g. spoken words) and<br />prosodic features. We have shown, in this work, that the task<br />h = 1<br />h = 3<br />h = 2<br />y = DISAGREEMENT<br />Head Shake<br />Forefinger Raise-Like<br />Hand Wag<br />Head Nod<br />F0<br />Forefinger Raise<br />Head Nod<br />Forefinger Raise-Like<br />y = AGREEMENT<br />Disagreement Transitions Agreement Transitions<br />Fig. 5: The features learned for each state by a three-state<br />HCRF model. The green and red connections correspond to<br />the highest–ranked transition from each state in the cases of<br />agreement and disagreement respectively. The middle state<br />is shared among the two classes.<br />is possible without the use of verbal features. Furthermore,<br />we have shown that HCRFs are a good choice for this task,<br />as they outperform SVMs and HMMs, demonstrating the<br />advantages of joint discriminative learning and their ability<br />to model the hidden fine–grain dynamics of the multimodal<br />cues related to agreement and disagreement. Finally, we have<br />shown that HCRFs can be automatically analysed to identify<br />what groups of features are the most discriminative in each<br />class.<br />The next step is to evaluate our recognition algorithm<br />using automatically annotated head and hand gestures[20],<br />[21]. Furthermore, a rating study, which is already under-<br />way, will exhibit how human raters perform at classifying<br />these clips. Finally, another possible research avenue is the<br />inclusion of other groups of cues associated with agree-<br />ment/disagreement (see Tables II and III), especially facial<br />actions.<br />ACKNOWLEDGMENTS<br />This material is based upon work supported by the Na-<br />tional Science Foundation under Grant No. 0917321 and<br />the U.S. Army Research, Development, and Engineering<br />Command (RDECOM). The content does not necessarily<br />reflect the position or the policy of the Government, and<br />no official endorsement should be inferred. This work has<br />been funded in part by the European Community’s 7th<br />Framework Programme [FP7/2007-2013] under the grant<br />agreement no 231287 (SSPNet). The work of Maja Pantic<br />is further funded in part by the European Research Council<br />under the ERC Starting Grant agreement no. ERC-2007-StG-<br />203143 (MAHNOB). Finally, the authors would like to thank<br />Dr. Marc Mehu for his help in creating the dataset used in<br />this work.<br />REFERENCES<br />[1] M. Pantic, A. Nijholt, A. Pentland, and T. S. Huang, “Human–Centred<br />Intelligent Human–Computer Interaction (HCI2): How far are we from</p>  <p>Page 7</p> <p>attaining it?” Journal of Autonomous and Adaptive Communications<br />Systems, vol. 1, no. 2, pp. 168–187, 2008.<br />[2] A. Vinciarelli, M. Pantic, and H. Bourlard, “Social signal processing:<br />Survey of an emerging domain,” Image and Vision Computing, vol. 27,<br />no. 12, pp. 1743–1759, 2009.<br />[3] K. Bousmalis, M. Mehu, and M. Pantic, “Spotting agreement and<br />disagreement: A survey of nonverbal audiovisual cues and tools,” in<br />Proc. IEEE Int’l Conf. Affective Computing and Intelligent Interaction,<br />2009, pp. 1–9.<br />[4] A. Quattoni, M. Collins, and T. Darrell, “Conditional random fields<br />for object recognition,” in Proc. Conf. Neural Information Processing<br />Systems, 2004, pp. 1097–1104.<br />[5] D. Hillard, M. Ostendorf, and E. Shriberg, “Detection of agreement<br />vs. disagreement in meetings: training with unlabeled data,” in Proc.<br />Conf. North American Chapter of the Association for Computational<br />Linguistics on Human Language Technology, 2003, pp. 34–36.<br />[6] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan,<br />B. Peskin, T. Pfau, and E. Shriberg, “ICSI meeting corpus,” in Proc.<br />IEEE Int’l Conf. on Acoustics, Speech, and Signal Processing, 2003.<br />[7] M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg, “Identifying<br />agreement and disagreement in conversational speech: use of bayesian<br />networks to model pragmatic dependencies,” in Proc. Meeting Asso-<br />ciation for Computational Linguistics, 2004, pp. 669–676.<br />[8] R. el Kaliouby and P. Robinson, “Real–time inference of complex<br />mental states from facial expressions and head gestures,” in Proc. IEEE<br />Conf. Computer Vision and Pattern Recognition, vol. 3, 2004, pp. 154–<br />154.<br />[9] S. Baron-Cohen, O. Golan, S. Wheelwright, and J. J. Hill, Mind Read-<br />ing: The Interactive Guide to Emotions.<br />2004.<br />[10] S. Hahn, R. Ladner, and M. Ostendorf, “Agreement/disagreement<br />classification: Exploiting unlabeled data using contrast classifiers,” in<br />Proc. Human Language Technology Conf. of the NAACL, 2006, pp.<br />53–56.<br />[11] T. Sheerman-Chase, E.-J. Ong, and R. Bowden, “Feature selection of<br />facial displays for detection of non verbal communication in natural<br />conversation,” in Proc. IEEE Int’l Workshop on Human–Computer<br />Interaction, 2009.<br />London:Jessica Kingsley,<br />[12] S. Germesin and T. Wilson, “Agreement detection in multiparty<br />conversation,” in Proc. Int’l Conf. on Multimodal Interfaces, 2009,<br />pp. 7–14.<br />[13] J. Carletta, “Unleashing the killer corpus: experiences in creating the<br />multi–everything AMI Meeting Corpus,” Language Resources and<br />Evaluation Journal, vol. 41, no. 2, pp. 181–190, 2007.<br />[14] I. Poggi, F. D’Errico, and L. Vincze, “Types of nods. the polysemy<br />of a social signal,” in Proc. Int’l Conf. Language Resources and<br />Evaluation, 2010.<br />[15] A. Kendon, “Some uses of the head shake,” Gesture, vol. 2, no. 2, pp.<br />147–182, 2002.<br />[16] P. Ekman, W. V. Friesen, and J. C. Hager, “Facial action coding<br />system,” Salt Lake City: Research Nexus, 2002.<br />[17] S. Wang, A. Quattoni, L.-P. Morency, D. Demirdjian, and T. Darrell,<br />“Hidden conditional random fields for gesture recognition,” in Proc.<br />IEEE Conf. Computer Vision and Pattern Recognition, vol. 2, 2006,<br />pp. 1521–1527.<br />[18] A. Vinciarelli, A. Dielmann, S. Favre, and H. Salamin, “Canal9: A<br />database of political debates for analysis of social interactions,” in<br />Proc. IEEE Int’l Conf. Affective Computing and Intelligent Interfaces,<br />vol. 2, 2009, pp. 96–99.<br />[19] F. Eyben, M. W¨ ollmer, and B. Schuller, “openEAR — Introducing the<br />Munich open-source emotion and affect recognition toolkit,” in Proc.<br />IEEE Int’l Conf. Affective Computing and Intelligent Interaction, 2009,<br />pp. 1–6.<br />[20] L.-P. Morency, J. Whitehill, and J. Movellan, “Generalized adaptive<br />view–based appearance model: Integrated framework for monocular<br />head pose estimation,” in Proc. Int’l Conf. Automatic Face and Gesture<br />Recognition, 2008.<br />[21] A. Oikonomopoulos, I. Patras, and M. Pantic, “An implicit spatiotem-<br />poral shape model for human activity localisation and recognition,”<br />in Proc. IEEE Int’l Conf. Computer Vision and Pattern Recognition,<br />vol. 3, 2009, pp. 27–33.</p>   </div> <div id="rgw19_56ab19d0d61f7" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw20_56ab19d0d61f7">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw21_56ab19d0d61f7"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://fundunia.wapka.mobi/0/http://ibug.doc.ic.ac.uk/media/uploads/documents/fg11submission174-final.pdf" target="_blank" rel="nofollow" class="publication-viewer" title="Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition">Modeling hidden dynamics of multimodal cues for sp...</a> </div>  <div class="details">   Available from <a href="http://fundunia.wapka.mobi/0/http://ibug.doc.ic.ac.uk/media/uploads/documents/fg11submission174-final.pdf" target="_blank" rel="nofollow">wapka.mobi</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>  <div id="rgw28_56ab19d0d61f7" class="citations-container"> <div class="tab-container"> <ul class="tab-list"> <li class="lf tab-item  js-citations"> <a href="javascript:void(0);" class="tab-link"> References  </small> </a> </li>  <li class="lf tab-item tab-item-active js-cited-in js-cited-in-tooltip"> <a href="javascript:void(0);" class="tab-link"> Cited In <small> (21) </small> </a> </li>    <li class="rf"> <div class="dropdown js-citations-sorter dropdown-right-align" style="position: relative; bottom: -1px;display:none;"> <a href="javascript:void(0);" class="dropdown-toggle"> Sorted by: <strong class="js-current-sorting"> Order of availability  </strong> <span class="caret"></span> </a> <ul class="dropdown-menu"> <li><a href="javascript:void(0);" data-sort="normal">Order of availability</a></li> <li><a href="javascript:void(0);" data-sort="original">Appearance in publication</a></li> </ul> </div> </li>  </ul> <div class="tab-section tab-section-active js-citations-list-container"> <div id="rgw29_56ab19d0d61f7" class="pub-citations-list">  <ul class="c-list">  <li class="c-list-item li-publication   includes-citation-list"  id="rgw30_56ab19d0d61f7" >  <div class="indent-left">  <div id="rgw31_56ab19d0d61f7" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Petros_Maragos" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Petros Maragos </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw32_56ab19d0d61f7">  <li class="citation-context-item"> "Since earlier pioneering works (Bolt, 1980; Poddar et al., 1998) there has been an explosion of works in the area; this is also due to the introduction of everyday usage depth sensors (e.g., Ren et al., 2011). Such works span a variety of applications such as the recent case of gestures and accompanying speech integration for a problem in geometry (Miki et al., 2014), the integration of nonverbal auditory features with gestures for agreement recognition (Bousmalis et al., 2011), or within the aspect of social signal analysis (Ponce-López et al., 2013); Song et al. (2013) propose a probabilistic extension of first-order logic, integrating multimodal speech/visual data for recognizing complex events such as everyday kitchen activities. The ChaLearn task is an indicative case of the effort recently placed in the field: Published approaches ranked in the first places of this gesture challenge, employ multimodal signals including audio, color, depth and skeletal information; for learning and recognition one finds approaches ranging from hidden Markov models (HMMs)/Gaussian mixture models (GMMs) to boosting, random forests, neural networks and support vector machines among others. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring"> <span class="publication-title js-publication-title">Multimodal gesture recognition via multiple hypotheses rescoring</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/35598148_V_Pitsikalis" class="authors js-author-name ga-publications-authors">V. Pitsikalis</a> &middot;     <a href="researcher/70857240_A_Katsamanis" class="authors js-author-name ga-publications-authors">A. Katsamanis</a> &middot;     <a href="researcher/74960662_S_Theodorakis" class="authors js-author-name ga-publications-authors">S. Theodorakis</a> &middot;     <a href="researcher/35130208_P_Maragos" class="authors js-author-name ga-publications-authors">P. Maragos</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> We present a new framework for multimodal gesture recognition that is based on a multiple hypotheses rescoring fusion scheme. We specifically deal with a demanding Kinect-based multimodal data set, introduced in a recent gesture recognition challenge (ChaLearn 2013), where multiple subjects freely perform multimodal gestures. We employ multiple modalities, that is, visual cues, such as skeleton data, color and depth images, as well as audio, and we extract feature of the hands&#39; movement, handshape, and audio spectral properties. Using a common hidden Markov model framework we build single-stream gesture models based on which we can generate multiple single stream-based hypotheses for an unknown gesture sequence. By multimodally rescoring these hypotheses via constrained decoding and a weighted combination scheme, we end up with a multimodally-selected best hypothesis. This is further refined by means of parallel fusion of the monomodal gesture models applied at a segmental level. In this setup, accurate gesture modeling is proven to be critical and is facilitated by an activity detection system that is also presented. The overall approach achieves 93.3% gesture recognition accuracy in the ChaLearn Kinect-based multimodal data set, significantly outperforming all recently published approaches on the same challenging multimodal gesture recognition task, providing a relative error rate reduction of at least 47.6%. ©2015 Vassilis Pitsikalis, Athanasios Katsamanis, Stavros Theodorakis and Petros Maragos. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Feb 2015  &middot; Journal of Machine Learning Research  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Petros_Maragos/publication/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring/links/564d0ea508aeafc2aaafb7fc.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw33_56ab19d0d61f7" >  <div class="indent-left">  <div id="rgw34_56ab19d0d61f7" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview ga-publication-viewer js-publication-item-fulltext-content" href="publication/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes">       </a>   <div class="preview-source-info"> <a class="details js-show-source ga-source-url" href="profile/Maurizio_Filippone" >Source</a>  <div class="tooltip-content" style="display: none"> Available from: Maurizio Filippone </div> </div>   </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw35_56ab19d0d61f7">  <li class="citation-context-item"> "40-50 Duration, Lexical (dis)agreement categorical 9854 spurts 84% accuracy Speaker Adjacency ICSI Meetings [33] 16 Prosody, Lexical (dis)agreement categorical 20 AMI Meetings F 1 ∼ 45% Dialogue Acts [34] 44 Prosody (dis)agreement categorical 147 Debate clips 64.2% accuracy Gestures from Canal9 [36] 26 Turn Organization conflict categorical 13 Debates 80.0% turn Steady Conversational from Canal9 classification accuracy Periods [37] 138 Overlapping Speech conflict categorical SSPNet Conflict U AR = 83.1% clip to Non-Overlapping Corpus accuracy (2 classes) Speech Ratio [38] (1) 138 Feature Selection conflict categorical SSPNet Conflict U AR = 83.9% clip Over OpenSmile Corpus accuracy (2 classes) Acoustic Features [38] (2) 138 Feature Selection conflict dimensional SSPNet Conflict correlation 0.82 Over OpenSmile Corpus predicted / real Acoustic Features conflict level [39] 26 Lexical blaming categorical 130 Couple &gt; 70.0% acceptance Therapy Sessions classification accuracy " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Article:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes"> <span class="publication-title js-publication-title">Predicting Continuous Conflict Perception with Bayesian Gaussian Processes</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/70610444_Samuel_Kim" class="authors js-author-name ga-publications-authors">Samuel Kim</a> &middot;     <a href="researcher/18329367_Fabio_Valente" class="authors js-author-name ga-publications-authors">Fabio Valente</a> &middot;     <a href="researcher/70871340_Maurizio_Filippone" class="authors js-author-name ga-publications-authors">Maurizio Filippone</a> &middot;     <a href="researcher/8774744_Alessandro_Vinciarelli" class="authors js-author-name ga-publications-authors">Alessandro Vinciarelli</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Conflict is one of the most important phenomena of social life, but it is still largely neglected by the computing community. This work proposes an approach that detects common conversational social signals (loudness, overlapping speech, etc.) and predicts the conflict level perceived by human observers in continuous, non-categorical terms. The proposed regression approach is fully Bayesian and it adopts automatic relevance determination to identify the social signals that influence most the outcome of the prediction. The experiments are performed over the SSPNet Conflict Corpus, a publicly available collection of 1,430 clips extracted from televised political debates (roughly 12 hours of material for 138 subjects in total). The results show that it is possible to achieve a correlation close to 0.8 between actual and predicted conflict perception. </span> </div>    <div class="publication-meta publication-meta">   <span class="ico-publication-fulltext reset-background"></span> Full-text   &middot; Article &middot; Apr 2014  &middot; IEEE Transactions on Affective Computing  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-download primary  open-viewer" href="profile/Maurizio_Filippone/publication/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes/links/0a85e5339402edefb9000000.pdf?origin=publication_list">  <span class="js-btn-label">Download</span> </a>    </div> </div>      </li>  <li class="c-list-item li-publication   includes-citation-list"  id="rgw36_56ab19d0d61f7" >  <div class="indent-left">  <div id="rgw37_56ab19d0d61f7" class="js-publication-item-fulltext fulltext-thumb">    <a class="publication-preview preview-not-available ga-publication-viewer-not-available js-publication-item-fulltext-content" href="publication/261450646_Action_Unit_Models_of_Facial_Expression_of_Emotion_in_the_Presence_of_Speech">       </a>    </div>  </div>  <div class="indent-right">      </div>  <ul class="citation-contexts" id="rgw38_56ab19d0d61f7">  <li class="citation-context-item"> "People convey their emotional state both in their facial expression and their voice, normally in a mix of talking and silence occurring intermittently. A rich tradition of research in multimodal emotion recognition has thoroughly documented the benefit of using simultaneously visual and audio modalities [1] [2] [3] [4] [5] [13]. However there is little research in determining the exact effect that talking has on the recognition of facial expressions and subsequent analysis of emotion. " </li>  </ul>   <div  style="margin-top: -2px">  <h5 class="pub-type-and-title">  <span class="publication-type">Conference Paper:</span>    <a class="js-publication-title-link js-go-to-publication ga-publication-item" href="publication/261450646_Action_Unit_Models_of_Facial_Expression_of_Emotion_in_the_Presence_of_Speech"> <span class="publication-title js-publication-title">Action Unit Models of Facial Expression of Emotion in the Presence of Speech</span> </a>     </h5>  </div>    <div class="authors">     <a href="researcher/2048306749_Miraj_Shah" class="authors js-author-name ga-publications-authors">Miraj Shah</a> &middot;     <a href="researcher/54238848_David_G_Cooper" class="authors js-author-name ga-publications-authors">David G. Cooper</a> &middot;     <a href="researcher/2043366316_Houwei_Cao" class="authors js-author-name ga-publications-authors">Houwei Cao</a> &middot;     <a href="researcher/14930428_Ruben_C_Gur" class="authors js-author-name ga-publications-authors">Ruben C. Gur</a> &middot;     <a href="researcher/10450787_Ani_Nenkova" class="authors js-author-name ga-publications-authors">Ani Nenkova</a> &middot;     <a href="researcher/38908141_Ragini_Verma" class="authors js-author-name ga-publications-authors">Ragini Verma</a>      </div>        <div class="abstract"> <span class="shorten"> <a href="javascript:" class="js-toggle-abstract">[Show abstract]</a> </span> <span class="full"> <a href="javascript:" class="js-toggle-abstract">[Hide abstract]</a><br/>  <strong>ABSTRACT:</strong> Automatic recognition of emotion using facial expressions in the presence of speech poses a unique challenge because talking reveals clues for the affective state of the speaker but distorts the canonical expression of emotion on the face. We introduce a corpus of acted emotion expression where speech is either present (talking) or absent (silent). The corpus is uniquely suited for analysis of the interplay between the two conditions. We use a multimodal decision level fusion classifier to combine models of emotion from talking and silent faces as well as from audio to recognize five basic emotions: anger, disgust, fear, happy and sad. Our results strongly indicate that emotion prediction in the presence of speech from action unit facial features is less accurate when the person is talking. Modeling talking and silent expressions separately and fusing the two models greatly improves accuracy of prediction in the talking setting. The advantages are most pronounced when silent and talking face models are fused with predictions from audio features. In this multi-modal prediction both the combination of modalities and the separate models of talking and silent facial expression of emotion contribute to the improvement. </span> </div>    <div class="publication-meta publication-meta">    No preview  &middot; Conference Paper &middot; Sep 2013  </div>        <div class="publication-actions"> <div class="btn-group">  <a class="btn btn-plain action-request-external  " href="javascript:;" data-context="pubCit">  <span class="js-btn-label">Request full-text</span> </a>    </div> </div>      </li>  </ul>    <a class="show-more-rebranded js-show-more rf text-gray-lighter">Show more</a> <span class="ajax-loading-small list-loading" style="display: none"></span>  <div class="clearfix"></div>  <div class="publication-detail-sidebar-legal">Note: This list is based on the publications in our database and might not be exhaustive.</div> <div class="clearfix"></div>  </div> </div> </div> </div> </div> </div> <div class="clearfix">     <div id="rgw23_56ab19d0d61f7" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw24_56ab19d0d61f7">  </ul> </div> </div>   <div id="rgw15_56ab19d0d61f7" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw16_56ab19d0d61f7"> <div> <h5> <a href="publication/290654827_Recognition_Algorithm_of_Acoustic_Emission_Signals_Based_on_Conditional_Random_Field_Model_in_Storage_Tank_Floor_Inspection_Using_Inner_Detector" class="color-inherit ga-similar-publication-title"><span class="publication-title">Recognition Algorithm of Acoustic Emission Signals Based on Conditional Random Field Model in Storage Tank Floor Inspection Using Inner Detector</span></a>  </h5>  <div class="authors"> <a href="researcher/2094303723_Yibo_Li" class="authors ga-similar-publication-author">Yibo Li</a>, <a href="researcher/2094227426_Yuxiang_Zhang" class="authors ga-similar-publication-author">Yuxiang Zhang</a>, <a href="researcher/2094338341_Huiyu_Zhu" class="authors ga-similar-publication-author">Huiyu Zhu</a>, <a href="researcher/2094268802_Rongxin_Yan" class="authors ga-similar-publication-author">Rongxin Yan</a>, <a href="researcher/2094323055_Yuanyuan_Liu" class="authors ga-similar-publication-author">Yuanyuan Liu</a>, <a href="researcher/2094375818_Liying_Sun" class="authors ga-similar-publication-author">Liying Sun</a>, <a href="researcher/2094218491_Zhoumo_Zeng" class="authors ga-similar-publication-author">Zhoumo Zeng</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56ab19d0d61f7"> <div> <h5> <a href="publication/282270927_Supervised_Single-Microphone_Multi-Talker_Speech_Separation_with_Conditional_Random_Fields" class="color-inherit ga-similar-publication-title"><span class="publication-title">Supervised Single-Microphone Multi-Talker Speech Separation with Conditional Random Fields</span></a>  </h5>  <div class="authors"> <a href="researcher/69795161_Yu_Ting_Yeung" class="authors ga-similar-publication-author">Yu Ting Yeung</a>, <a href="researcher/39859378_Tan_Lee" class="authors ga-similar-publication-author">Tan Lee</a>, <a href="researcher/70757001_Cheung-Chi_Leung" class="authors ga-similar-publication-author">Cheung-Chi Leung</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw18_56ab19d0d61f7"> <div> <h5> <a href="publication/281897391_Human_gesture_recognition_using_a_simplified_dynamic_Bayesian_network" class="color-inherit ga-similar-publication-title"><span class="publication-title">Human gesture recognition using a simplified dynamic Bayesian network</span></a>  </h5>  <div class="authors"> <a href="researcher/11357665_Myung-Cheol_Roh" class="authors ga-similar-publication-author">Myung-Cheol Roh</a>, <a href="researcher/8002799_Seong-Whan_Lee" class="authors ga-similar-publication-author">Seong-Whan Lee</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw40_56ab19d0d61f7" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw41_56ab19d0d61f7">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw42_56ab19d0d61f7" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=VaAAMxaKYL_lW-mWxShNFJLIS0UkEjRekbXb2XnB0V5JIf_IDb-VFK7-j-yn-Opo" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="jA0v11JHtsPFusBMqZyqIop00VWkBNRPlcFIsZtA47OyJSDxIJwBtGHBaOz/7u4xSbwHSKvJDkenleFD8ml5HCoECczUpWmZz5eG3k3fkhCBtkMDWE9Nnabvdi4RM1Slz/SHGs/hfkHdPBC2HBfAfM3Rdj9v5BA8hPf7WDgcnl9eAYN9qMQbCiY6obc6bjaBpJNTT1cWXYGcvbboVXI5kzEJ8j/jRAKhQuYlMPX06S/qd13z9v9b3QmJqurA+1hjMalTELfxs/SrSnzjAAa7pw/NkAcRXIZCWgmqpLbFlPM="/> <input type="hidden" name="urlAfterLogin" value="publication/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MjM4MDk2X01vZGVsaW5nX2hpZGRlbl9keW5hbWljc19vZl9tdWx0aW1vZGFsX2N1ZXNfZm9yX3Nwb250YW5lb3VzX2FncmVlbWVudF9hbmRfZGlzYWdyZWVtZW50X3JlY29nbml0aW9u"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MjM4MDk2X01vZGVsaW5nX2hpZGRlbl9keW5hbWljc19vZl9tdWx0aW1vZGFsX2N1ZXNfZm9yX3Nwb250YW5lb3VzX2FncmVlbWVudF9hbmRfZGlzYWdyZWVtZW50X3JlY29nbml0aW9u"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjI0MjM4MDk2X01vZGVsaW5nX2hpZGRlbl9keW5hbWljc19vZl9tdWx0aW1vZGFsX2N1ZXNfZm9yX3Nwb250YW5lb3VzX2FncmVlbWVudF9hbmRfZGlzYWdyZWVtZW50X3JlY29nbml0aW9u"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw43_56ab19d0d61f7"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 572;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/2448732603281275/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Konstantinos Bousmalis","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Konstantinos_Bousmalis","institution":"Imperial College London","institutionUrl":false,"widgetId":"rgw4_56ab19d0d61f7"},"id":"rgw4_56ab19d0d61f7","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4106096","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56ab19d0d61f7"},"id":"rgw3_56ab19d0d61f7","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=224238096","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":224238096,"title":"Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition","journalTitle":false,"journalDetailsTooltip":false,"affiliation":"Dept. of Comput., Imperial Coll. London, London, UK","type":"Conference Paper","details":{"doi":"10.1109\/FG.2011.5771341","conferenceInfos":"Conference: Automatic Face & Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on"},"source":{"sourceUrl":"http:\/\/ieeexplore.ieee.org\/xpl\/freeabs_all.jsp?arnumber=5771341","sourceName":"IEEE Xplore"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft_id","value":"info:doi\/10.1109\/FG.2011.5771341"},{"key":"rft.atitle","value":"Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition"},{"key":"rft.title","value":"2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011"},{"key":"rft.jtitle","value":"2011 IEEE International Conference on Automatic Face and Gesture Recognition and Workshops, FG 2011"},{"key":"rft.date","value":"2011"},{"key":"rft.pages","value":"746 - 752"},{"key":"rft.au","value":"Konstantinos Bousmalis,Louis-Philippe Morency,Maja Pantic"},{"key":"rft.genre","value":"inProceedings"}],"widgetId":"rgw6_56ab19d0d61f7"},"id":"rgw6_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=224238096","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":224238096,"peopleItems":[{"data":{"authorNameOnPublication":"Konstantinos Bousmalis","accountUrl":"profile\/Konstantinos_Bousmalis","accountKey":"Konstantinos_Bousmalis","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2671872220764\/images\/template\/default\/profile\/profile_default_m.jpg","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Konstantinos Bousmalis","profile":{"professionalInstitution":{"professionalInstitutionName":"Imperial College London","professionalInstitutionUrl":"institution\/Imperial_College_London"}},"professionalInstitutionName":"Imperial College London","professionalInstitutionUrl":"institution\/Imperial_College_London","url":"profile\/Konstantinos_Bousmalis","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Konstantinos_Bousmalis","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56ab19d0d61f7"},"id":"rgw9_56ab19d0d61f7","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4106096&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":"Imperial College London","score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":3,"accountCount":1,"publicationUid":224238096,"widgetId":"rgw8_56ab19d0d61f7"},"id":"rgw8_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4106096&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=3&accountCount=1&publicationUid=224238096","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/13852141_Louis-Philippe_Morency","authorNameOnPublication":"Louis-Philippe Morency","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Louis-Philippe Morency","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/13852141_Louis-Philippe_Morency","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56ab19d0d61f7"},"id":"rgw11_56ab19d0d61f7","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=13852141&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56ab19d0d61f7"},"id":"rgw10_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=13852141&authorNameOnPublication=Louis-Philippe%20Morency","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/47824715_Maja_Pantic","authorNameOnPublication":"Maja Pantic","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Maja Pantic","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/47824715_Maja_Pantic","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56ab19d0d61f7"},"id":"rgw13_56ab19d0d61f7","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=47824715&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56ab19d0d61f7"},"id":"rgw12_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=47824715&authorNameOnPublication=Maja%20Pantic","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56ab19d0d61f7"},"id":"rgw7_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=224238096&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":224238096,"abstract":"<noscript><\/noscript><div>This paper attempts to recognize spontaneous agreement and disagreement based only on nonverbal multi-modal cues. Related work has mainly used verbal and prosodic cues. We demonstrate that it is possible to correctly recognize agreement and disagreement without the use of verbal context (i.e. words, syntax). We propose to explicitly model the complex hidden dynamics of the multimodal cues using a sequential discriminative model, the Hidden Conditional Random Field (HCRF). In this paper, we show that the HCRF model is able to capture what makes each of these social attitudes unique. We present an efficient technique to analyze the concepts learned by the HCRF model and show that these coincide with the findings from social psychology regarding which cues are most prevalent in agreement and disagreement. Our experiments are performed on a spontaneous dataset of real televised debates. The HCRF model outperforms conventional approaches such as Hidden Markov Models and Support Vector Machines.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw14_56ab19d0d61f7"},"id":"rgw14_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=224238096","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition\/links\/0fa8de920cf2bd28793e4b2c\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw5_56ab19d0d61f7"},"id":"rgw5_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=224238096&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2094303723,"url":"researcher\/2094303723_Yibo_Li","fullname":"Yibo Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094227426,"url":"researcher\/2094227426_Yuxiang_Zhang","fullname":"Yuxiang Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094338341,"url":"researcher\/2094338341_Huiyu_Zhu","fullname":"Huiyu Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2094268802,"url":"researcher\/2094268802_Rongxin_Yan","fullname":"Rongxin Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":3,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":"Shock and Vibration","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290654827_Recognition_Algorithm_of_Acoustic_Emission_Signals_Based_on_Conditional_Random_Field_Model_in_Storage_Tank_Floor_Inspection_Using_Inner_Detector","usePlainButton":true,"publicationUid":290654827,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.72","url":"publication\/290654827_Recognition_Algorithm_of_Acoustic_Emission_Signals_Based_on_Conditional_Random_Field_Model_in_Storage_Tank_Floor_Inspection_Using_Inner_Detector","title":"Recognition Algorithm of Acoustic Emission Signals Based on Conditional Random Field Model in Storage Tank Floor Inspection Using Inner Detector","displayTitleAsLink":true,"authors":[{"id":2094303723,"url":"researcher\/2094303723_Yibo_Li","fullname":"Yibo Li","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094227426,"url":"researcher\/2094227426_Yuxiang_Zhang","fullname":"Yuxiang Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094338341,"url":"researcher\/2094338341_Huiyu_Zhu","fullname":"Huiyu Zhu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094268802,"url":"researcher\/2094268802_Rongxin_Yan","fullname":"Rongxin Yan","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094323055,"url":"researcher\/2094323055_Yuanyuan_Liu","fullname":"Yuanyuan Liu","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094375818,"url":"researcher\/2094375818_Liying_Sun","fullname":"Liying Sun","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2094218491,"url":"researcher\/2094218491_Zhoumo_Zeng","fullname":"Zhoumo Zeng","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Shock and Vibration 12\/2015; 2015(1). DOI:10.1155\/2015\/173470"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290654827_Recognition_Algorithm_of_Acoustic_Emission_Signals_Based_on_Conditional_Random_Field_Model_in_Storage_Tank_Floor_Inspection_Using_Inner_Detector","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290654827_Recognition_Algorithm_of_Acoustic_Emission_Signals_Based_on_Conditional_Random_Field_Model_in_Storage_Tank_Floor_Inspection_Using_Inner_Detector\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56ab19d0d61f7"},"id":"rgw16_56ab19d0d61f7","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290654827","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":69795161,"url":"researcher\/69795161_Yu_Ting_Yeung","fullname":"Yu Ting Yeung","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39859378,"url":"researcher\/39859378_Tan_Lee","fullname":"Tan Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70757001,"url":"researcher\/70757001_Cheung-Chi_Leung","fullname":"Cheung-Chi Leung","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Dec 2015","journal":"IEEE\/ACM Transactions on Audio, Speech, and Language Processing ","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/282270927_Supervised_Single-Microphone_Multi-Talker_Speech_Separation_with_Conditional_Random_Fields","usePlainButton":true,"publicationUid":282270927,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/282270927_Supervised_Single-Microphone_Multi-Talker_Speech_Separation_with_Conditional_Random_Fields","title":"Supervised Single-Microphone Multi-Talker Speech Separation with Conditional Random Fields","displayTitleAsLink":true,"authors":[{"id":69795161,"url":"researcher\/69795161_Yu_Ting_Yeung","fullname":"Yu Ting Yeung","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39859378,"url":"researcher\/39859378_Tan_Lee","fullname":"Tan Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70757001,"url":"researcher\/70757001_Cheung-Chi_Leung","fullname":"Cheung-Chi Leung","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE\/ACM Transactions on Audio, Speech, and Language Processing  12\/2015; 23(12):2334-2342. DOI:10.1109\/TASLP.2015.2479039"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/282270927_Supervised_Single-Microphone_Multi-Talker_Speech_Separation_with_Conditional_Random_Fields","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/282270927_Supervised_Single-Microphone_Multi-Talker_Speech_Separation_with_Conditional_Random_Fields\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56ab19d0d61f7"},"id":"rgw17_56ab19d0d61f7","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=282270927","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromReferenceWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":11357665,"url":"researcher\/11357665_Myung-Cheol_Roh","fullname":"Myung-Cheol Roh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8002799,"url":"researcher\/8002799_Seong-Whan_Lee","fullname":"Seong-Whan Lee","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Nov 2015","journal":"Multimedia Systems","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281897391_Human_gesture_recognition_using_a_simplified_dynamic_Bayesian_network","usePlainButton":true,"publicationUid":281897391,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.62","url":"publication\/281897391_Human_gesture_recognition_using_a_simplified_dynamic_Bayesian_network","title":"Human gesture recognition using a simplified dynamic Bayesian network","displayTitleAsLink":true,"authors":[{"id":11357665,"url":"researcher\/11357665_Myung-Cheol_Roh","fullname":"Myung-Cheol Roh","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8002799,"url":"researcher\/8002799_Seong-Whan_Lee","fullname":"Seong-Whan Lee","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Multimedia Systems 11\/2015; 21(6). DOI:10.1007\/s00530-014-0414-9"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281897391_Human_gesture_recognition_using_a_simplified_dynamic_Bayesian_network","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281897391_Human_gesture_recognition_using_a_simplified_dynamic_Bayesian_network\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56ab19d0d61f7"},"id":"rgw18_56ab19d0d61f7","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=281897391","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw15_56ab19d0d61f7"},"id":"rgw15_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=224238096&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":224238096,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":224238096,"publicationType":"inProceedings","linkId":"0fa8de920cf2bd28793e4b2c","fileName":"Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition","fileUrl":"http:\/\/fundunia.wapka.mobi\/0\/http:\/\/ibug.doc.ic.ac.uk\/media\/uploads\/documents\/fg11submission174-final.pdf","name":"wapka.mobi","nameUrl":"http:\/\/fundunia.wapka.mobi\/0\/http:\/\/ibug.doc.ic.ac.uk\/media\/uploads\/documents\/fg11submission174-final.pdf","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":false,"isUserLink":false,"widgetId":"rgw21_56ab19d0d61f7"},"id":"rgw21_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=224238096&linkId=0fa8de920cf2bd28793e4b2c&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw20_56ab19d0d61f7"},"id":"rgw20_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=224238096&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw22_56ab19d0d61f7"},"id":"rgw22_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=224238096","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw19_56ab19d0d61f7"},"id":"rgw19_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=224238096&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":224238096,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw24_56ab19d0d61f7"},"id":"rgw24_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=224238096&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":4,"valueFormatted":"4","widgetId":"rgw25_56ab19d0d61f7"},"id":"rgw25_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=224238096","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw23_56ab19d0d61f7"},"id":"rgw23_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=224238096&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Modeling Hidden Dynamics of Multimodal Cues for Spontaneous\nAgreement and Disagreement Recognition\nKonstantinos BousmalisLouis\u2013Philippe Morency Maja Pantic\nAbstract\u2014This paper attempts to recognize spontaneous\nagreement and disagreement based only on nonverbal multi-\nmodal cues. Related work has mainly used verbal and prosodic\ncues. We demonstrate that it is possible to correctly recognize\nagreement and disagreement without the use of verbal context\n(i.e. words, syntax). We propose to explicitly model the complex\nhidden dynamics of the multimodal cues using a sequential\ndiscriminative model, the Hidden Conditional Random Field\n(HCRF). In this paper, we show that the HCRF model is able\nto capture what makes each of these social attitudes unique. We\npresent an efficient technique to analyze the concepts learned\nby the HCRF model and show that these coincide with the\nfindings from social psychology regarding which cues are most\nprevalent in agreement and disagreement. Our experiments are\nperformed on a spontaneous dataset of real televised debates.\nThe HCRF model outperforms conventional approaches such\nas Hidden Markov Models and Support Vector Machines.\nI. INTRODUCTION\nWe have recently witnessed significant advances not only\nin the machine analysis of nonverbal cues, such as head\nand hand gestures, facial expressions, auditory cues, but also\nin the field of affect recognition [1]. However, only few\nworks have so far attempted to recognize social attitudes like\ninterest, politeness and flirting [2]. This is partly so because\nrelevant research in social psychology, which would help\nidentify discriminative combinations of multimodal cues, is\nat best scarce, and because of the fact that there is a gap of\nrelevant annotated data that can be used for such analyses.\nDespite these difficulties, achieving such a goal is very im-\nportant if we are to move towards a more naturalistic human\u2013\ncomputer\u2013interaction; machines who are able to detect social\nattitudes and react according to the needs of their user will be\nmore efficient and welcomed for the rather more naturalistic\nexperience they are bound to offer.\nSuch social attitudes are those of agreement and disagree-\nment, which are inevitable in daily human\u2013human interac-\ntions, from finding a location to dine, to discussions on no-\ntoriously controversial topics like politics. Existing work on\nthe automatic recognition of agreement and\/or disagreement\n(see Table I) has mainly used verbal and prosodic cues, e.g.\npitch and energy. To the best of our knowledge, no work has\nmanaged to successfully recognize spontaneous agreement\nand disagreement based solely on nonverbal multimodal\nK. Bousmalis is with the Department of Computing, Imperial College\nLondon, SW7 2AZ, London, UK k.bousmalis@imperial.ac.uk\nL.\u2013P. Morency is with the Institute for Creative Technologies,\nUniversity of Southern California, Los Angeles, CA 90094, USA\nmorency@ict.usc.edu\nM. Pantic is with the Department of Computing, Imperial College\nLondon, SW7 2AZ, London, UK, and EEMCS, University of Twente,\n7522 NB Enschede, The Netherlands m.pantic@imperial.ac.uk\nP(y=DISAGREEMENT|x)=0.8\nX2\nh1\nh2\nh3\ny\nHead Nod\nHead Shake\nHand Wag\nHands Scissor\nShoulder Shrug\nF0\nX3\nX1\nX4\nX5\nP(y=AGREEMENT|x)=0.2\nh4\nh5\nFig. 1: HCRF for spontaneous agreement\/disagreement\nrecognition. hirepresents the hidden state that captures the\nunderlying dynamics between features and labels at a given\ntimestamp i. The HCRF model is able to capture fine\u2013grain\nhidden multimodal dynamics better than other models by\nlearning these hidden states and their relation to each class.\nConsequently, the HCRF model is able to learn a more\nsuitable mapping between the observations x and each class\nlabel y \u2208 Y .\ncues. However, although agreements and disagreements are\nfrequently expressed verbally, the nonverbal behavioral cues\nthat occur during their manifestation play a crucial role in\ntheir interpretation. Bousmalis et al. [3] have surveyed and\nidentified such cues that seem to be relevant, as those are\nevident in social psychology literature (see Tables II and III\nfor a summary). According to this survey, it is the temporal\nunderlying dynamics of multimodal cues that will allow us\nto recognize agreement and disagreement.\nThis calls for a model capable of capturing these complex\ndynamics and, based on them, distinguishing these social\nattitudes from each other. A Hidden Conditional Random\nField (HCRF) [4], originally proposed for object recognition,\nis a model capable of not only capturing the underlying\nstructure of events, but also of learning the combinations\nof features that are shared by each class and the ones that\nmake each of them unique. Hence, HCRFs could be a good\ncandidate for modeling agreement and disagreement.\nThis paper will show that (i) it is possible to recognize\nspontaneous agreement and disagreement without the use"},{"page":2,"text":"Method\nHillard et al. [5] (2003)\nFeaturesClassifier\nDecision Tree\nData\nICSI [6]\nSpontaneous\n\u221a\nVerbal, pause, fundamental frequency(F0),\nduration\nVerbal\nhead nod, head shake, head turn, head tilt,\nAU1, AU2, AU12, AU16, AU19, AU20,\nAU25, AU26, AU27\nVerbal\nhead yaw, head pitch, head roll, AU1,\nAU2, AU12, AU18, AU20, AU25, Gaze,\nhead pose\nVerbal, pitch, energy, duration, pauses,\nspeech rate\nGalley et al. [7] (2004)\nel Kaliouby et al. [8] (2004)\nBayesian Network\nHMM, DBN\nICSI [6]\n\u221a\n\u2014Mind Reading DVD [9]\nHahn et al. [10] (2006)\nSheerman\u2013Chase et al. [11] (2009)\nContrast Classifier, SVM\nAdaBoost\nICSI [6]\nown\n\u221a\n\u221a\nGermesin and Wilson [12] (2009) Decision Tree, CRFAMI [13]\n\u221a\nTABLE I: Summary of the existing systems that have attempted agreement\/disagreement classification.\nof verbal cues (e.g. spoken words); (ii) HCRFs are indeed\nable to capture the underlying dynamics of multimodal cues\nand perform better than conventional models in this task\n(figure 1); and (iii) HCRFs are able to automatically identify\ngroups of features specific to each attitude in a way that\nconfirms the findings in social psychology literature regard-\ning which cues are most prevalent during the expression of\nagreement and disagreement.\nIn the following section, we discuss agreement, disagree-\nment and related work on their automatic recognition. In\nSection III we present Hidden Conditional Random Fields\n(HCRFs) and our technique to analyze the concepts learned\nby the HCRF. In Section IV we explain how our data was\ncollected and what experiments we have conducted. Finally,\nin Section V, we present and discuss our results.\nII. AGREEMENT AND DISAGREEMENT\nA. Definitions and Associated Cues\nDistinguishing between different kinds of agreement and\ndisagreement is difficult, mainly because of the lack of\nwidely accepted definitions of agreement and disagreement\n[3]. We can distinguish among at least three ways one could\nexpress agreement and disagreement with:\n\u2022 Direct Speaker\u2019s Agreement and Disagreement: A\nspeaker directly expresses his\/her agreement or dis-\nagreement, e.g. \u201cI (dis)agree with you\u201d.\n\u2022 Indirect Speaker\u2019s Agreement and Disagreement:\nA speaker does not explicitly state her agreement and\ndisagreement, but expresses an opinion that is congru-\nent (agreement) or contradictory (disagreement) to an\nopinion that was expressed earlier in the conversation.\n\u2022 Nonverbal Listener\u2019s Agreement and Disagreement:\nA listener nonverbally expresses her agreement or dis-\nagreement to an opinion that is currently or was just\nexpressed. This could be via auditory cues like \u201cmm\nhmm\u201d or visual cues like a head nod or a smile.\nIt is important to mention at this point that in sponta-\nneous direct and indirect speaker\u2019s agreement\/disagreement,\nthe speaker also exhibits nonverbal behavior which could\nperhaps be different than the one exhibited during nonverbal\nlistener\u2019s agreement\/disagreement.\nTables II and III present a full list of the nonverbal\ncues that can be displayed during agreement and disagree-\nCUE\nHead Nod\nListener Smile (AU12, AU13)\nEyebrow Raise (AU1+AU2)+Head Nod\nAU1 + AU2 + Smile (AU12, AU13)\nSideways Leaning\nLaughter\nMimicry\nKIND\nHead Gesture\nFacial Action\nFacial Action, Head Gesture\nFacial Action\nBody Posture\nAudiovisual Cue\nSecond\u2013order Cue\nTABLE II: Cues of Agreement. For relevant descriptions of\nAUs, see FACS [16].\nment [3]1. The most prevalent and straightforward cues seem\nto be the Head Nod and the Head Shake for agreement\nand disagreement respectively, with nods intuitively convey-\ning affirmation and shakes negation. However, simply the\npresence of these or any of the other cues alone cannot be\ndiscriminative enough, since they could have many other in-\nterpretations, as studied by Poggi et al. [14] and Kendon [15].\nB. Related Work on Automatic Recognition\nThere is no work, to the best of our knowledge, that has\nattempted agreement\/disagreement classification on audiovi-\nsual spontaneous data. Table I summarizes the existing sys-\ntems that have attempted classification of agreement and\/or\ndisagreement in one way or another. However, none of these\nsystems is directly comparable with ours.\nHillard et al. [5] attempted speaker agreement and dis-\nagreement classification on pre\u2013segmented \u2018spurts\u2019, speech\nsegments by one speaker with pauses not greater than\n500ms. The authors used a combination of word\u2013based and\nprosodic cues to classify each spurt as \u2018positive\u2013agreement\u2019,\n\u2018negative\u2013disagreement\u2019, \u2018backchannel\u2019, or \u2018other\u2019. Most of\nthe results reported included word\u2013based cues, however an\noverall classification accuracy of 62% was reported for a\n17% confusion rate between the agreement and disagreement\nclasses. Similar works by Galley et al. [7] and Hahn et\nal. [10] also deal with classifying spurts as disagreement and\nagreement, with [7] also dealing with finding the addressee\nof the action. Germesin and Wilson [12] also deal with these\nissues. However, the features used by these works included\n1Our discussion of cues for agreement and disagreement is mostly\nrelevant for cultures in Western Europe and North America. Further work\nmight be needed to develop a similar system that targets other cultures."},{"page":3,"text":"CUE\nHead Shake\nHead Roll\nCut Off\nClenched Fist\nForefinger Raise\nForefinger Wag\nHand Chop\nHand Cross\nHand Wag\nHands Scissor\nIronic Smile\/Smirking [AU12 L\/R(+AU14)]\nBarely noticeable lip\u2013clenching (AU23, AU24)\nCheek Crease (AU14)\nLowered Eyebrow\/Frowning (AU4)\nLip Pucker (AU18)\nSlightly Parted Lips (AU25)\nMouth Movement (AU25\/AU26)\nNose Flare (AU38)\nNose Twist (AU9 L\/R, AU10 L\/R, AU11 L\/R)\nTongue Show (AU19)\nSuddenly Narrowed\/Slitted Eyes (fast AU7)\nEye Roll\nGaze Aversion\nArm Folding\nLarge Body Shift\nLeg Clamp\nHead\/Chin Support on Hand\nNeck Clamp\nHead Scratch\nSelf\u2013manipulation\nFeet Pointing Away\nSighing\nThroat Clearing\nDelays\nUtterance Length\nInterruption\nKIND\nHead Gesture\nHead Gesture\nHead Gesture\nHand Action\nHand Action\nHand Action\nHand Action\nHand Action\nHand Action\nHand Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\nFacial Action\/Gaze\nGaze\nBody Posture\nBody Action\nBody Posture\nBody\/Head Posture\nHand\/Head Action\nHead\/Hand Action\nHand\/Facial Action\nFeet Posture\nAuditory Cue\nAuditory Cue\nAuditory Cue\nAuditory Cue\nAuditory Cue\nTABLE III: Cues for Disagreement. For relevant descriptions\nof AUs, see FACS [16]\nlexical, structural and durational cues and are not comparable\nwith other systems based on nonverbal cues.\nThe first such system is that by el Kaliouby and Robin-\nson [8], which attempted agreement\/disagreement classifi-\ncation of acted behavioural displays based on head and\nfacial movements. They used 6 classes: \u2018agreeing\u2019, \u2018disagree-\ning\u2019, \u2018concentrating\u2019, \u2018interested\u2019, \u2018thinking\u2019, and \u2018unsure\u2019.\nThey tracked 25 fiducial facial points, out of which they\nextrapolated rigid head motion (yaw, pitch, and roll), and\nfacial action units (eyebrow raise, lip pull, lip pucker), but\nalso utilized appearance\u2013based features to summarise mouth\nactions (mouth stretch, jaw drop, and lips parting). They used\nHidden Markov Models (HMMs) to detect each head and\nfacial action, and a Dynamic Bayesian Network (DBN) per\nclass was trained to perform the higher\u2013level inference of\neach of the \u2018mental states\u2019 mentioned above, allowing for\nthe co\u2013occurrence of states.\nSheerman\u2013Chase et al. [11] are, to our knowledge, the only\nresearch group who have attempted recognition of agreement\nbased on non\u2013verbal cues in spontaneous data. However, they\ndid not include disagreement as a class, because of the lack of\ndata. They instead distinguished between \u2018thinking\u2019, \u2018under-\nstanding\u2019, \u2018agreeing\u2019 and \u2018questioning\u2019. Their spontaneous\ndata was obtained by capturing the four 12\u2013minute dyadic\nconversations of 6 males and 2 females. 21 annotators rated\nthe clips with each clip getting on average around 4 ratings\nthat were combined to obtain the ground truth label. For the\nautomatic recognition, they used no auditory features and the\ntracking of 46 fiducial facial points was used. The output of\nthe tracker was then processed to obtain a number of static\nand dynamic features to be used for classification. Principal\nComponent Analysis (PCA) was performed on the tracked\npoints in each video frame, and the PCA eigenvalues were\nused as features. Similarly to el Kaliouby and Robinson [8],\nthe head yaw, pitch and roll, the eyebrow raise, lip pucker\nand lip parting were calculated as functions of these tracked\nfacial points. Gaze was also estimated in a similar fashion\n\u2014the eye pupils were among the points tracked.\nIII. HCRFS FOR MULTIMODAL GESTURE RECOGNITION\nHidden Conditional Random Fields \u2014discriminative mod-\nels that contain hidden states\u2014 are well\u2013suited to the prob-\nlem of multimodal cue modeling for agreement\/disagreement\nrecognition. Quattoni et al. [4] presented and used HCRFs to\ncapture the spatial dependencies between hidden object parts.\nWang et al. [17] used them to capture temporal dependencies\nacross frames and recognize different gesture classes. They\ndid so successfully by learning a state distribution among\nthe different gesture classes in a discriminative manner,\nallowing them to not only uncover the distinctive config-\nurations that uniquely identifies each class, but also to learn\na shared common structure among the classes. Moreover, as\na discriminative model, HCRFs require a fewer number of\nobservations than a generative model like a Hidden\u2013Markov\nModel (HMM). These were all qualities that prompted us to\nselect HCRFs, as a model to experiment with, in our attempt\nto recognize agreement and disagreement.\nA. Model\nFollowing the notation of Quattoni et al. [4], [17],\nwe represent m local observations by a vector x\n{x1,x2,...,xm}. Each local observation xj is represented\nby a feature vector \u03c6(xj) \u2208 ?dwhich includes all input\nfeatures (e.g., the presence of a head nod or the value\nof F0-pitch). We wish to learn a mapping between the\nobservations x and the class label y \u2208 Y . The class label\ncan be \u2018agreement\u2019 or \u2018disagreement\u2019. An HCRF models the\nconditional probability of a class label given an observation\nsequence by:\n=\nP(y | x,\u03b8) =\n?\nh\nP(y,h | x,\u03b8) =\n?\nhe\u03a8(y,h,x;\u03b8)\ny?\u2208Y,he\u03a8(y?,h,x;\u03b8).\n?\n(1)\nwhere h = {h1,h2,...,hm}, each hi\u2208 H captures certain\nunderlying structure of each class and H is the set of hidden\nstates in the model. The potential function \u03a8(y,h,x;\u03b8) \u2208 ?\nis an energy function, parameterized by \u03b8, which measures\nthe compatibility between a label, a sequence of observations\nand a configuration of the hidden states."},{"page":4,"text":"\u03a8(y,h,x;\u03b8)=\n?\n+\nj\n\u03c6(x,j) \u00b7 \u03b8h(hj) +\n?\n?\nj\n\u03b8y(y,hj)\n(j,k)\u2208E\n\u03b8e(y,hj,hk)\n(2)\nThe graph E is a chain where each node corresponds to\na hidden state variable at time t. The paremeter vector \u03b8 is\nmade up of three components: \u03b8 = [\u03b8e\u03b8y\u03b8h]. We use the\nnotation \u03b8h[hj] to refer to the parameters \u03b8hthat correspond\nto state hj \u2208 H. Similarly, \u03b8y[y,hj] stands for parameters\nthat correspond to class y and state hj and \u03b8e[y,hj,hk]\nmeasures the compatibility between pairs of consecutive\nstates j and k and the gesture y.\nB. Training\nGiven a new test sequence x, and parameter values \u03b8\u2217\ninduced from training examples, we will take the label for\nthe sequence to be:\nargmax\ny\u2208YP(y | x,\u03b8\u2217).\n(3)\nThe following objective function is used in training the\nparameters:\nL(\u03b8) =\n?\ni\nlogP(yi| xi,\u03b8) \u2212\n1\n2\u03c32||\u03b8||2\n(4)\nThe first term in Eq. 4 is the log-likelihood of the\ndata. The second term is the log of a Gaussian prior with\nvariance \u03c32, i.e., P(\u03b8) \u223c exp?\nargmax\u03b8L(\u03b8), under this criterion. For our experiments we\nused a Quasi-Newton optimization technique to minimize the\nnegative log\u2013likelihood of the data.\n1\n2\u03c32||\u03b8||2?. We use gradient\nascent to search for the optimal parameter values, \u03b8\u2217=\nC. Analysis\nThe HCRF model is a powerful sequential discriminative\nmodel. It can learn the hidden dynamic of a signal using\nthe latent variable hj. For multimodal gesture recognition,\nthis hidden dynamic is usually related to the synchrony and\nasynchrony between speech and gestures. While previous\nwork has shown the efficiency of HCRF for learning visual\ngestures [4], [17], none of them described or analysed what\nthe HCRF model learned. In this paper we are presenting an\nefficient approach to analyze the concepts learned by the\nHCRF model. This analysis tool enables a new direction\nof research where machine learning is not simply used as\na black box but instead is there to help understand human\ninteractions.\nTo analyze the HCRF model, one has to understand the\noptimized parameters [\u03b8h\u03b8e\u03b8y]:\n\u2022 \u03b8hmodels the relationship between observations xjand\nhidden states hj. If the HCRF model has 10 input\nfeatures and 3 hidden states, then the \u03b8hparameter will\nbe of length 30 (10x3). By analysing the amplitude of\neach weights in \u03b8h, it is possible to learn the relative\nimportance of each input feature for each hidden state.\n(a) Forefinger Raise(b) Forefinger Wag\n(c) Hand Wag(d) Hands Scissors\nFig. 2: Some of the gestures used as cues for the experiments.\n\u2022 The parameter \u03b8ymodels the relationship of the hidden\nstates hjand the label y. If the model contains 3 hidden\nstates and 2 labels, then the \u03b8ywill be of length 6 (3x2).\nBy analyzing the weights of \u03b8y, it is possible to see\nwhich hidden states are shared and which ones are not.\n\u2022 The parameter \u03b8erepresents the links between hidden\nstates. It is similar to the transition matrix in a Hidden\nMarkov Model. An important difference is that the\nHCRF model keeps a transition matrix for each label. If\nthe HCRF model contains 3 hidden states and 2 labels,\nthen the \u03b8eparameter will be of length 18 (3x3x2).\nThe procedure for analyzing the HCRF model contains\nthree steps: (1) identify the relevant features for each hidden\nstate using \u03b8h, (2) determine which hidden states are shared\nand which ones are not using \u03b8y, and (3) analyze the\npossible transitions between hidden states using \u03b8e. In our\nexperiments (see Figure 5), we apply this procedure to\nidentify the relevant concepts learned by the HCRF model\nto recognize agreement and disgreement behaviors.\nIV. EXPERIMENTS\nA. Dataset and Cues\nOur dataset originated from the Canal 9 Database of\nPolitical Debates [18], one that comprises of 43 hours and\n10 minutes of 72 real televised debates on Canal 9, a local\nSwiss television station. The debates are moderated by a\npresenter, and there are two sides that argue around a central\nissue, with one or more participants on each side. Hence, the\ndatabase is rather rich in episodes of spontaneous agreement\nand disagreement.\nThe dataset we used comprises of 53 episodes of agree-\nment and 94 episodes of disagreement, which occur over\na total of 11 debates. These episodes were selected on the"},{"page":5,"text":"basis of verbal content, and thus, only episodes of direct and\nindirect agreement\/disagreement were included (see Section\nII-A). As the debates were filmed with multiple cameras, and\nedited live to one feed, the episodes selected for the dataset\nwere only the ones that were contained within one personal,\nclose\u2013up shot of the speaker.\nWe automatically extracted nonverbal auditory features\nused in related work, specifically the fundamental frequency\n(F0) and energy, by using a freely\u2013available tool, Open-\nEar[19]. Since our main goal is to analyze dynamics of\nnonverbal cues during agreement\/disagreement recognition,\nour dataset was manuallly annotated to gather as accurate\ntemporal information about the gestures as possible. Based\non the results presented in this paper, our future work will\nevaluate the recognition performance using our automatic\nnonverbal gesture annotation [20], [21]. The hand and head\ngestures we included were based off the relevant list of\ncues from the Social Psychology literature (see Section II-\nA), with the exception of a number of head and hand\ngestures that never appeared in the dataset, and the addition\nof the \u2019Shoulder Shrug\u2019 and the \u2018Forefinger Raise-Like\u2019\ngestures. The latter is a \u2018Forefinger Raise\u2019 without an erect\nindex finger. The cues we finally extracted and used in our\nexperiments are listed in Table IV; the visual cues that may\nnot be self\u2013explanatory from their title are depicted in figure\n2.\nCUE\nHead Nod\nHead Shake\nForefinger Raise\n\u2018Forefinger Raise\u2019\u2013Like\nForefinger Wag\nHand Wag\nHands Scissor\nShoulder Shrug\nFundamental Frequency (F0)\nEnergy\nKIND\nHead Gesture\nHead Gesture\nHand Action\nHand Action\nHand Action\nHand Action\nHand Action\nBody Gesture\nAuditory Cue\nAuditory Cue\nTABLE IV: The list of features we used in our experiments.\nB. Methodology\nWe conducted experiments with Support Vector Ma-\nchines (SVMs), as our baseline static classifiers, Hidden\u2013\nMarkov Models (HMMs), the most\u2013commonly used dynamic\ngenerative model, and Hidden Conditional Random Fields\n(HCRFs), the dynamic discriminative model we believe is\nmost appropriate for such a task. We conducted different\nexperiments for three groups of cues: only auditory, only\nvisual, and both auditory and visual ones.\nOur cues were encoded differently for our static and\ndynamic classifiers, but the same information was available\nto all classifiers. For SVMs, the features of each gesture were\nthe start frame and the duration (total number of frames) of\nthe gesture within the segment of interest. For the auditory\nfeatures we used the mean, standard deviation, and the first,\nsecond(median), and third quartiles of each. The later values\ndid not take into account the undefined areas of F0, and\nFig. 3: Comparisons of recognition performance (total accu-\nracy) by the classification methods we explored on the three\ndifferent groups of features used.\nall values were scaled from -1 to 1. For the experiments\nwith HMMs and HCRFs, we encoded each gesture in a\nbinary manner (1 if the gesture is activated in a certain\nframe, 0 otherwise), and used the raw values of our auditory\nfeatures, normalized per subject. Figure 1 allows the reader\nto visualize the process of reaching a classification decision\nfrom our data by using an HCRF.\nAll our experiments were run in a leave\u2013one\u2013debate\u2013out\nfashion, i.e. the testing set always comprised of examples\nfrom the one debate which was not included in the training\nand validation sets. The optimal model parameters for each\ntest set were chosen by a three\u2013fold validation on the\nremaining debates. Those were cost and gamma for SVMs,\nnumber of mixtures of Gaussians for HMMs, regularization\nfactor for HCRFs and number of hidden states for both\nHMMs and HCRFs. The HMM and HCRF experiments were\nrun with 10 different random initializations, the best of which\nwas chosen each time during the validation phase (i.e., based\non performance on the validation sets). The evaluation metric\nthat we used for all the experiments was the total accuracy\nin a balanced dataset, i.e. percentage of sequences for which\nthe correct label was predicted in a test set that contains an\nequal number of agreement and disagreement examples.\nV. RESULTS AND DISCUSSION\nFigure 3 summarizes the results of the experiments on\nspontaneous agreement and disagreement classification using\nauditory, gestural and both auditory and gestural features. It\nis clear that:\n(a) It is possible to perform the task of spontaneous agree-\nment and disagreement classification without the use of\nany verbal features.\n(b) The temporal dynamics of the cues are vital to the task,\nas it is evident that SVMs are not able to perform well\nby using static information alone.\n(c) HCRFs outperform SVMs and HMMs, especially when\nthe cues used are multimodal and the underlying dy-\nnamics of the different modalities need to be learned."},{"page":6,"text":"Fig. 4: The performance (total accuracy) of HCRFs increases\nproportionally to the sampling rate of the multimodal data.\nFigure 4 demonstrates the complexity of the task at hand,\nand the importance of fine\u2013grain multimodal dynamics to\nits solution, by summarizing the accuracies achieved with\nHCRF models when sampling our data at different rates. The\nfact that the higher the sampling rate, the higher the accuracy\nachieved by the HCRF models, also demonstrates the ability\nof the HCRFs to cope with such fine\u2013grain dynamics.\nWe applied our model analysis technique described in\nSection III-C to the optimal HCRF selected during our\nexperiments. By examination of the weights learned by\nthe HCRF for each of its cues \u03b8h, hidden states \u03b8y, and\ntransitions \u03b8e, we were able to rank, according to importance,\nthe information that the model used. Figure 5 shows the\nautomatically learned concepts of the optimal HCRF model\nIn Figure 5, each hidden state (represented by the white\ncircles) is linked to its highest ranked observed features in a\ndescending order of importance. The relationships between\nthese observed features and the hidden states were identified\nusing the parameter \u03b8h. The highest ranked features in these\nhidden states show that the Head Nod and the Head Shake,\nwhich are considered, by social psychologists, the most\nprevalent cues in agreement and disagreement respectively\n(see Section II-A), are also the most discriminative cues here.\nIt could be the case that \u2018Forefinger Raise-Like\u2019 gestures\nmight in fact play no role in discriminating between the two\nattitudes.\nBy analyzing the parameter \u03b8y, we can see that the HCRF\nmodel assigned one state as prevalent for each of the two\nclass labels, and one state as shared between them. The\nanalysis of the transition parameter \u03b8eshows that different\ntransitions are learned for each class label. The Figure 5\nmarked the most likely transitions associated to each attitude\n(class label): green for agreement and red for disagreement.\nDisagreement will usually end with hidden state h = 2\n(middle circle) while the agreement can transition directly\nto a head shake (hidden state h = 1 depicted on the left).\nVI. CONCLUSION AND FUTURE WORK\nRelated work on spontaneous agreement\/disagreement\nclassification has used verbal (e.g. spoken words) and\nprosodic features. We have shown, in this work, that the task\nh = 1\nh = 3\nh = 2\ny = DISAGREEMENT\nHead Shake\nForefinger Raise-Like\nHand Wag\nHead Nod\nF0\nForefinger Raise\nHead Nod\nForefinger Raise-Like\ny = AGREEMENT\nDisagreement Transitions Agreement Transitions\nFig. 5: The features learned for each state by a three-state\nHCRF model. The green and red connections correspond to\nthe highest\u2013ranked transition from each state in the cases of\nagreement and disagreement respectively. The middle state\nis shared among the two classes.\nis possible without the use of verbal features. Furthermore,\nwe have shown that HCRFs are a good choice for this task,\nas they outperform SVMs and HMMs, demonstrating the\nadvantages of joint discriminative learning and their ability\nto model the hidden fine\u2013grain dynamics of the multimodal\ncues related to agreement and disagreement. Finally, we have\nshown that HCRFs can be automatically analysed to identify\nwhat groups of features are the most discriminative in each\nclass.\nThe next step is to evaluate our recognition algorithm\nusing automatically annotated head and hand gestures[20],\n[21]. Furthermore, a rating study, which is already under-\nway, will exhibit how human raters perform at classifying\nthese clips. Finally, another possible research avenue is the\ninclusion of other groups of cues associated with agree-\nment\/disagreement (see Tables II and III), especially facial\nactions.\nACKNOWLEDGMENTS\nThis material is based upon work supported by the Na-\ntional Science Foundation under Grant No. 0917321 and\nthe U.S. Army Research, Development, and Engineering\nCommand (RDECOM). The content does not necessarily\nreflect the position or the policy of the Government, and\nno official endorsement should be inferred. This work has\nbeen funded in part by the European Community\u2019s 7th\nFramework Programme [FP7\/2007-2013] under the grant\nagreement no 231287 (SSPNet). The work of Maja Pantic\nis further funded in part by the European Research Council\nunder the ERC Starting Grant agreement no. ERC-2007-StG-\n203143 (MAHNOB). Finally, the authors would like to thank\nDr. Marc Mehu for his help in creating the dataset used in\nthis work.\nREFERENCES\n[1] M. Pantic, A. Nijholt, A. Pentland, and T. S. Huang, \u201cHuman\u2013Centred\nIntelligent Human\u2013Computer Interaction (HCI2): How far are we from"},{"page":7,"text":"attaining it?\u201d Journal of Autonomous and Adaptive Communications\nSystems, vol. 1, no. 2, pp. 168\u2013187, 2008.\n[2] A. Vinciarelli, M. Pantic, and H. Bourlard, \u201cSocial signal processing:\nSurvey of an emerging domain,\u201d Image and Vision Computing, vol. 27,\nno. 12, pp. 1743\u20131759, 2009.\n[3] K. Bousmalis, M. Mehu, and M. Pantic, \u201cSpotting agreement and\ndisagreement: A survey of nonverbal audiovisual cues and tools,\u201d in\nProc. IEEE Int\u2019l Conf. Affective Computing and Intelligent Interaction,\n2009, pp. 1\u20139.\n[4] A. Quattoni, M. Collins, and T. Darrell, \u201cConditional random fields\nfor object recognition,\u201d in Proc. Conf. Neural Information Processing\nSystems, 2004, pp. 1097\u20131104.\n[5] D. Hillard, M. Ostendorf, and E. Shriberg, \u201cDetection of agreement\nvs. disagreement in meetings: training with unlabeled data,\u201d in Proc.\nConf. North American Chapter of the Association for Computational\nLinguistics on Human Language Technology, 2003, pp. 34\u201336.\n[6] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan,\nB. Peskin, T. Pfau, and E. Shriberg, \u201cICSI meeting corpus,\u201d in Proc.\nIEEE Int\u2019l Conf. on Acoustics, Speech, and Signal Processing, 2003.\n[7] M. Galley, K. McKeown, J. Hirschberg, and E. Shriberg, \u201cIdentifying\nagreement and disagreement in conversational speech: use of bayesian\nnetworks to model pragmatic dependencies,\u201d in Proc. Meeting Asso-\nciation for Computational Linguistics, 2004, pp. 669\u2013676.\n[8] R. el Kaliouby and P. Robinson, \u201cReal\u2013time inference of complex\nmental states from facial expressions and head gestures,\u201d in Proc. IEEE\nConf. Computer Vision and Pattern Recognition, vol. 3, 2004, pp. 154\u2013\n154.\n[9] S. Baron-Cohen, O. Golan, S. Wheelwright, and J. J. Hill, Mind Read-\ning: The Interactive Guide to Emotions.\n2004.\n[10] S. Hahn, R. Ladner, and M. Ostendorf, \u201cAgreement\/disagreement\nclassification: Exploiting unlabeled data using contrast classifiers,\u201d in\nProc. Human Language Technology Conf. of the NAACL, 2006, pp.\n53\u201356.\n[11] T. Sheerman-Chase, E.-J. Ong, and R. Bowden, \u201cFeature selection of\nfacial displays for detection of non verbal communication in natural\nconversation,\u201d in Proc. IEEE Int\u2019l Workshop on Human\u2013Computer\nInteraction, 2009.\nLondon:Jessica Kingsley,\n[12] S. Germesin and T. Wilson, \u201cAgreement detection in multiparty\nconversation,\u201d in Proc. Int\u2019l Conf. on Multimodal Interfaces, 2009,\npp. 7\u201314.\n[13] J. Carletta, \u201cUnleashing the killer corpus: experiences in creating the\nmulti\u2013everything AMI Meeting Corpus,\u201d Language Resources and\nEvaluation Journal, vol. 41, no. 2, pp. 181\u2013190, 2007.\n[14] I. Poggi, F. D\u2019Errico, and L. Vincze, \u201cTypes of nods. the polysemy\nof a social signal,\u201d in Proc. Int\u2019l Conf. Language Resources and\nEvaluation, 2010.\n[15] A. Kendon, \u201cSome uses of the head shake,\u201d Gesture, vol. 2, no. 2, pp.\n147\u2013182, 2002.\n[16] P. Ekman, W. V. Friesen, and J. C. Hager, \u201cFacial action coding\nsystem,\u201d Salt Lake City: Research Nexus, 2002.\n[17] S. Wang, A. Quattoni, L.-P. Morency, D. Demirdjian, and T. Darrell,\n\u201cHidden conditional random fields for gesture recognition,\u201d in Proc.\nIEEE Conf. Computer Vision and Pattern Recognition, vol. 2, 2006,\npp. 1521\u20131527.\n[18] A. Vinciarelli, A. Dielmann, S. Favre, and H. Salamin, \u201cCanal9: A\ndatabase of political debates for analysis of social interactions,\u201d in\nProc. IEEE Int\u2019l Conf. Affective Computing and Intelligent Interfaces,\nvol. 2, 2009, pp. 96\u201399.\n[19] F. Eyben, M. W\u00a8 ollmer, and B. Schuller, \u201copenEAR \u2014 Introducing the\nMunich open-source emotion and affect recognition toolkit,\u201d in Proc.\nIEEE Int\u2019l Conf. Affective Computing and Intelligent Interaction, 2009,\npp. 1\u20136.\n[20] L.-P. Morency, J. Whitehill, and J. Movellan, \u201cGeneralized adaptive\nview\u2013based appearance model: Integrated framework for monocular\nhead pose estimation,\u201d in Proc. Int\u2019l Conf. Automatic Face and Gesture\nRecognition, 2008.\n[21] A. Oikonomopoulos, I. Patras, and M. Pantic, \u201cAn implicit spatiotem-\nporal shape model for human activity localisation and recognition,\u201d\nin Proc. IEEE Int\u2019l Conf. Computer Vision and Pattern Recognition,\nvol. 3, 2009, pp. 27\u201333."}],"widgetId":"rgw26_56ab19d0d61f7"},"id":"rgw26_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=224238096&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw27_56ab19d0d61f7"},"id":"rgw27_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=224238096&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":224238096,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":{"data":{"publicationUid":224238096,"publicationCitationsList":{"data":{"citationItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":35598148,"url":"researcher\/35598148_V_Pitsikalis","fullname":"V. Pitsikalis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70857240,"url":"researcher\/70857240_A_Katsamanis","fullname":"A. Katsamanis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74960662,"url":"researcher\/74960662_S_Theodorakis","fullname":"S. Theodorakis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":35130208,"url":"researcher\/35130208_P_Maragos","fullname":"P. Maragos","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Feb 2015","journal":"Journal of Machine Learning Research","showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring","usePlainButton":true,"publicationUid":281893586,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.47","url":"publication\/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring","title":"Multimodal gesture recognition via multiple hypotheses rescoring","displayTitleAsLink":true,"authors":[{"id":35598148,"url":"researcher\/35598148_V_Pitsikalis","fullname":"V. Pitsikalis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70857240,"url":"researcher\/70857240_A_Katsamanis","fullname":"A. Katsamanis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":74960662,"url":"researcher\/74960662_S_Theodorakis","fullname":"S. Theodorakis","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":35130208,"url":"researcher\/35130208_P_Maragos","fullname":"P. Maragos","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Machine Learning Research 02\/2015; 16:255-284."],"abstract":"We present a new framework for multimodal gesture recognition that is based on a multiple hypotheses rescoring fusion scheme. We specifically deal with a demanding Kinect-based multimodal data set, introduced in a recent gesture recognition challenge (ChaLearn 2013), where multiple subjects freely perform multimodal gestures. We employ multiple modalities, that is, visual cues, such as skeleton data, color and depth images, as well as audio, and we extract feature of the hands' movement, handshape, and audio spectral properties. Using a common hidden Markov model framework we build single-stream gesture models based on which we can generate multiple single stream-based hypotheses for an unknown gesture sequence. By multimodally rescoring these hypotheses via constrained decoding and a weighted combination scheme, we end up with a multimodally-selected best hypothesis. This is further refined by means of parallel fusion of the monomodal gesture models applied at a segmental level. In this setup, accurate gesture modeling is proven to be critical and is facilitated by an activity detection system that is also presented. The overall approach achieves 93.3% gesture recognition accuracy in the ChaLearn Kinect-based multimodal data set, significantly outperforming all recently published approaches on the same challenging multimodal gesture recognition task, providing a relative error rate reduction of at least 47.6%. \u00a92015 Vassilis Pitsikalis, Athanasios Katsamanis, Stavros Theodorakis and Petros Maragos.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Petros_Maragos\/publication\/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring\/links\/564d0ea508aeafc2aaafb7fc.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Petros_Maragos","sourceName":"Petros Maragos","hasSourceUrl":true},"publicationUid":281893586,"publicationUrl":"publication\/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring\/links\/564d0ea508aeafc2aaafb7fc\/smallpreview.png","linkId":"564d0ea508aeafc2aaafb7fc","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=281893586&reference=564d0ea508aeafc2aaafb7fc&eventCode=&origin=publication_list","widgetId":"rgw31_56ab19d0d61f7"},"id":"rgw31_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=281893586&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"564d0ea508aeafc2aaafb7fc","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224238096,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/281893586_Multimodal_gesture_recognition_via_multiple_hypotheses_rescoring\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["Since earlier pioneering works (Bolt, 1980; Poddar et al., 1998) there has been an explosion of works in the area; this is also due to the introduction of everyday usage depth sensors (e.g., Ren et al., 2011). Such works span a variety of applications such as the recent case of gestures and accompanying speech integration for a problem in geometry (Miki et al., 2014), the integration of nonverbal auditory features with gestures for agreement recognition (Bousmalis et al., 2011), or within the aspect of social signal analysis (Ponce-L\u00f3pez et al., 2013); Song et al. (2013) propose a probabilistic extension of first-order logic, integrating multimodal speech\/visual data for recognizing complex events such as everyday kitchen activities. The ChaLearn task is an indicative case of the effort recently placed in the field: Published approaches ranked in the first places of this gesture challenge, employ multimodal signals including audio, color, depth and skeletal information; for learning and recognition one finds approaches ranging from hidden Markov models (HMMs)\/Gaussian mixture models (GMMs) to boosting, random forests, neural networks and support vector machines among others. "],"widgetId":"rgw32_56ab19d0d61f7"},"id":"rgw32_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw30_56ab19d0d61f7"},"id":"rgw30_56ab19d0d61f7","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=281893586&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70610444,"url":"researcher\/70610444_Samuel_Kim","fullname":"Samuel Kim","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":18329367,"url":"researcher\/18329367_Fabio_Valente","fullname":"Fabio Valente","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8774744,"url":"researcher\/8774744_Alessandro_Vinciarelli","fullname":"Alessandro Vinciarelli","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":true,"isSlurp":false,"isNoText":false,"publicationType":"Article","publicationDate":"Apr 2014","journal":"IEEE Transactions on Affective Computing","showEnrichedPublicationItem":false,"citationCount":7,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes","usePlainButton":true,"publicationUid":261198282,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"2.68","url":"publication\/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes","title":"Predicting Continuous Conflict Perception with Bayesian Gaussian Processes","displayTitleAsLink":true,"authors":[{"id":70610444,"url":"researcher\/70610444_Samuel_Kim","fullname":"Samuel Kim","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":18329367,"url":"researcher\/18329367_Fabio_Valente","fullname":"Fabio Valente","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":70871340,"url":"researcher\/70871340_Maurizio_Filippone","fullname":"Maurizio Filippone","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8774744,"url":"researcher\/8774744_Alessandro_Vinciarelli","fullname":"Alessandro Vinciarelli","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["IEEE Transactions on Affective Computing 04\/2014; to appear(2). DOI:10.1109\/TAFFC.2014.2324564"],"abstract":"Conflict is one of the most important phenomena of social life, but it is still largely neglected by the computing community. This work proposes an approach that detects common conversational social signals (loudness, overlapping speech, etc.) and predicts the conflict level perceived by human observers in continuous, non-categorical terms. The proposed regression approach is fully Bayesian and it adopts automatic relevance determination to identify the social signals that influence most the outcome of the prediction. The experiments are performed over the SSPNet Conflict Corpus, a publicly available collection of 1,430 clips extracted from televised political debates (roughly 12 hours of material for 138 subjects in total). The results show that it is possible to achieve a correlation close to 0.8 between actual and predicted conflict perception.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":true,"actions":[{"type":"download","text":"Download","url":"profile\/Maurizio_Filippone\/publication\/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes\/links\/0a85e5339402edefb9000000.pdf?origin=publication_list","active":false,"primary":true,"extraClass":"open-viewer","icon":null,"data":[]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":{"isPreview":false,"sourceUrl":"profile\/Maurizio_Filippone","sourceName":"Maurizio Filippone","hasSourceUrl":true},"publicationUid":261198282,"publicationUrl":"publication\/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"previewUrl":"https:\/\/i1.rgstatic.net\/publication\/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes\/links\/0a85e5339402edefb9000000\/smallpreview.png","linkId":"0a85e5339402edefb9000000","origin":"publication_list","showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=261198282&reference=0a85e5339402edefb9000000&eventCode=&origin=publication_list","widgetId":"rgw34_56ab19d0d61f7"},"id":"rgw34_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=261198282&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":"0a85e5339402edefb9000000","context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224238096,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/261198282_Predicting_Continuous_Conflict_Perception_with_Bayesian_Gaussian_Processes\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["40-50 Duration, Lexical (dis)agreement categorical 9854 spurts 84% accuracy Speaker Adjacency ICSI Meetings [33] 16 Prosody, Lexical (dis)agreement categorical 20 AMI Meetings F 1 \u223c 45% Dialogue Acts [34] 44 Prosody (dis)agreement categorical 147 Debate clips 64.2% accuracy Gestures from Canal9 [36] 26 Turn Organization conflict categorical 13 Debates 80.0% turn Steady Conversational from Canal9 classification accuracy Periods [37] 138 Overlapping Speech conflict categorical SSPNet Conflict U AR = 83.1% clip to Non-Overlapping Corpus accuracy (2 classes) Speech Ratio [38] (1) 138 Feature Selection conflict categorical SSPNet Conflict U AR = 83.9% clip Over OpenSmile Corpus accuracy (2 classes) Acoustic Features [38] (2) 138 Feature Selection conflict dimensional SSPNet Conflict correlation 0.82 Over OpenSmile Corpus predicted \/ real Acoustic Features conflict level [39] 26 Lexical blaming categorical 130 Couple > 70.0% acceptance Therapy Sessions classification accuracy "],"widgetId":"rgw35_56ab19d0d61f7"},"id":"rgw35_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read full-text","widgetId":"rgw33_56ab19d0d61f7"},"id":"rgw33_56ab19d0d61f7","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":true,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=261198282&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore","clickThrough":"experimentMilestoneClickedToPublicationFromCitationWithNoFulltext","clickThroughWithContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2048306749,"url":"researcher\/2048306749_Miraj_Shah","fullname":"Miraj Shah","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54238848,"url":"researcher\/54238848_David_G_Cooper","fullname":"David G. Cooper","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2043366316,"url":"researcher\/2043366316_Houwei_Cao","fullname":"Houwei Cao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":14930428,"url":"researcher\/14930428_Ruben_C_Gur","fullname":"Ruben C. Gur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":2,"additionalCssClasses":[["includes-citation-list"]],"isFulltext":false,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Sep 2013","journal":null,"showEnrichedPublicationItem":false,"citationCount":1,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/261450646_Action_Unit_Models_of_Facial_Expression_of_Emotion_in_the_Presence_of_Speech","usePlainButton":true,"publicationUid":261450646,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/261450646_Action_Unit_Models_of_Facial_Expression_of_Emotion_in_the_Presence_of_Speech","title":"Action Unit Models of Facial Expression of Emotion in the Presence of Speech","displayTitleAsLink":true,"authors":[{"id":2048306749,"url":"researcher\/2048306749_Miraj_Shah","fullname":"Miraj Shah","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":54238848,"url":"researcher\/54238848_David_G_Cooper","fullname":"David G. Cooper","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2043366316,"url":"researcher\/2043366316_Houwei_Cao","fullname":"Houwei Cao","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":14930428,"url":"researcher\/14930428_Ruben_C_Gur","fullname":"Ruben C. Gur","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":10450787,"url":"researcher\/10450787_Ani_Nenkova","fullname":"Ani Nenkova","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":38908141,"url":"researcher\/38908141_Ragini_Verma","fullname":"Ragini Verma","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Affective Computing and Intelligent Interaction (ACII), 2013 Humaine Association Conference on; 09\/2013"],"abstract":"Automatic recognition of emotion using facial expressions in the presence of speech poses a unique challenge because talking reveals clues for the affective state of the speaker but distorts the canonical expression of emotion on the face. We introduce a corpus of acted emotion expression where speech is either present (talking) or absent (silent). The corpus is uniquely suited for analysis of the interplay between the two conditions. We use a multimodal decision level fusion classifier to combine models of emotion from talking and silent faces as well as from audio to recognize five basic emotions: anger, disgust, fear, happy and sad. Our results strongly indicate that emotion prediction in the presence of speech from action unit facial features is less accurate when the person is talking. Modeling talking and silent expressions separately and fusing the two models greatly improves accuracy of prediction in the talking setting. The advantages are most pronounced when silent and talking face models are fused with predictions from audio features. In this multi-modal prediction both the combination of modalities and the separate models of talking and silent facial expression of emotion contribute to the improvement.","description":false,"swapJournalAndAuthorPositions":false,"showAbstract":true,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/261450646_Action_Unit_Models_of_Facial_Expression_of_Emotion_in_the_Presence_of_Speech","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":false,"actions":[{"type":"request-external","text":"Request full-text","url":"javascript:;","active":false,"primary":false,"extraClass":null,"icon":null,"data":[{"key":"context","value":"pubCit"}]}],"actionWidgets":[],"publicationItemFulltext":{"data":{"isDataset":false,"isResearch":false,"isFulltext":true,"source":null,"publicationUid":261450646,"publicationUrl":"publication\/261450646_Action_Unit_Models_of_Facial_Expression_of_Emotion_in_the_Presence_of_Speech","eventCode":"","citationCount":0,"additionalContentWidgets":[],"disableViewer":true,"showLoggedOutRequestButton":false,"context":null,"showRequestCount":false,"publish":false,"request":false,"showAction":false,"displayAsLink":true,"documentViewerUrl":"publicliterature.PublicLiteratureDocumentViewer.html?publicationId=261450646&eventCode=","widgetId":"rgw37_56ab19d0d61f7"},"id":"rgw37_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemFulltext.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemFulltext.html?publicationUid=261450646&showAction=1&eventCode=&hideSource=0&displayAsLink=1&showPublishAsSelectFile=0","viewClass":"views.publicliterature.PublicationItemFulltextView","yuiModules":["rg.views.publicliterature.PublicationItemFulltextView"],"stylesheets":[],"_isYUI":true},"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":true,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":224238096,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/261450646_Action_Unit_Models_of_Facial_Expression_of_Emotion_in_the_Presence_of_Speech\/review","additionalRightSideTopWidgets":[{"data":{"citationContexts":["People convey their emotional state both in their facial expression and their voice, normally in a mix of talking and silence occurring intermittently. A rich tradition of research in multimodal emotion recognition has thoroughly documented the benefit of using simultaneously visual and audio modalities [1] [2] [3] [4] [5] [13]. However there is little research in determining the exact effect that talking has on the recognition of facial expressions and subsequent analysis of emotion. "],"widgetId":"rgw38_56ab19d0d61f7"},"id":"rgw38_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationItemCitationContexts.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItemCitationContexts.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":true,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw36_56ab19d0d61f7"},"id":"rgw36_56ab19d0d61f7","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationItem.html?showEnrichedPublicationItem=0&showRequestButton=1&publicationUid=261450646&additionalCssClasses%5B0%5D=includes-citation-list&citationContextItemVersion=old&isIncomingCitation=1","viewClass":"views.publicliterature.PublicationItemView","yuiModules":["rg.views.publicliterature.PublicationItemView"],"stylesheets":[],"_isYUI":true}],"hasCitations":true,"isPublicationAuthor":false,"isPublicationVisitor":false,"publicationUid":224238096,"publicationLink":"publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition","hasShowMore":true,"newOffset":3,"pageSize":10,"widgetId":"rgw29_56ab19d0d61f7"},"id":"rgw29_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationIncomingCitationsList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationIncomingCitationsList.html?publicationUid=224238096&citedInPage=1&swapJournalAndAuthorPositions=0&showAbstract=1&showType=1&showPublicationPreview=1&totalCount=21","viewClass":"views.publicliterature.PublicationIncomingCitationsListView","yuiModules":["rg.views.publicliterature.PublicationIncomingCitationsListView"],"stylesheets":[],"_isYUI":true},"hasCitations":false,"citationsCount":0,"hasIncomingCitations":true,"incomingCitationsCount":21,"showCitationsSorter":true,"showAbstract":true,"showType":true,"showPublicationPreview":true,"swapJournalAndAuthorPositions":false,"sort":"","sortOriginal":false,"citationList":"incoming","showsIncoming":true,"showSorting":false,"usePlainButton":null,"useEnrichedContext":null,"widgetId":"rgw28_56ab19d0d61f7"},"id":"rgw28_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicationCitations.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCitations.html?publicationUid=224238096&citationList=&citedInPage=1&sort=","viewClass":"views.publicliterature.PublicationCitationsView","yuiModules":["rg.views.publicliterature.PublicationCitationsView"],"stylesheets":[],"_isYUI":true},"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56ab19d0d61f7"},"id":"rgw2_56ab19d0d61f7","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":224238096},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=224238096&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56ab19d0d61f7"},"id":"rgw1_56ab19d0d61f7","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"5iGOgPoCeREjoYcSNU80zsyAVW4TBuMmCMzmhvySKmXlrMhYlBbc+T6dvhpqM5xkmNUz6rr3t4lxvEuRsZEhQ\/+XkpBJ3lHVJVg8B8k97ocj6lu9PIvhYRGNT7b9u\/pavpPUtNtPc5QZlEKlXsYrSq22NtOx2RTwVYHn+SIEY1GDTcYHE7OK6nyRrw63oQayy13WlosBlnXZtAY2WW6Ini2oKcnCmiS7DDMyBZDwTuG46Mp1lrVC23j2XytqIpt7CSDkYTj15C92j1mQb62GWtGGn1v05xMn\/NaRLjClaLo=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition\" \/>\n<meta property=\"og:description\" content=\"This paper attempts to recognize spontaneous agreement and disagreement based only on nonverbal multi-modal cues. Related work has mainly used verbal and prosodic cues. We demonstrate that it is...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition\/links\/0fa8de920cf2bd28793e4b2c\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition\" \/>\n<meta property=\"rg:id\" content=\"PB:224238096\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/10.1109\/FG.2011.5771341\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Modeling hidden dynamics of multimodal cues for spontaneous agreement and disagreement recognition\" \/>\n<meta name=\"citation_author\" content=\"Konstantinos Bousmalis\" \/>\n<meta name=\"citation_author\" content=\"Louis-Philippe Morency\" \/>\n<meta name=\"citation_author\" content=\"Maja Pantic\" \/>\n<meta name=\"citation_conference_title\" content=\"Automatic Face &amp; Gesture Recognition and Workshops (FG 2011), 2011 IEEE International Conference on\" \/>\n<meta name=\"citation_publication_date\" content=\"2011\/04\/25\" \/>\n<meta name=\"citation_firstpage\" content=\"746\" \/>\n<meta name=\"citation_lastpage\" content=\"752\" \/>\n<meta name=\"citation_doi\" content=\"10.1109\/FG.2011.5771341\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Conference Paper');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-516e4712-fbe1-47b6-9e58-b3b893ee484d","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":550,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw39_56ab19d0d61f7"},"id":"rgw39_56ab19d0d61f7","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-516e4712-fbe1-47b6-9e58-b3b893ee484d", "9ef37c2ba0a90c2e03694ba123e65678b78ac338");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-516e4712-fbe1-47b6-9e58-b3b893ee484d", "9ef37c2ba0a90c2e03694ba123e65678b78ac338");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw40_56ab19d0d61f7"},"id":"rgw40_56ab19d0d61f7","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/224238096_Modeling_hidden_dynamics_of_multimodal_cues_for_spontaneous_agreement_and_disagreement_recognition","requestToken":"jA0v11JHtsPFusBMqZyqIop00VWkBNRPlcFIsZtA47OyJSDxIJwBtGHBaOz\/7u4xSbwHSKvJDkenleFD8ml5HCoECczUpWmZz5eG3k3fkhCBtkMDWE9Nnabvdi4RM1Slz\/SHGs\/hfkHdPBC2HBfAfM3Rdj9v5BA8hPf7WDgcnl9eAYN9qMQbCiY6obc6bjaBpJNTT1cWXYGcvbboVXI5kzEJ8j\/jRAKhQuYlMPX06S\/qd13z9v9b3QmJqurA+1hjMalTELfxs\/SrSnzjAAa7pw\/NkAcRXIZCWgmqpLbFlPM=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=VaAAMxaKYL_lW-mWxShNFJLIS0UkEjRekbXb2XnB0V5JIf_IDb-VFK7-j-yn-Opo","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjI0MjM4MDk2X01vZGVsaW5nX2hpZGRlbl9keW5hbWljc19vZl9tdWx0aW1vZGFsX2N1ZXNfZm9yX3Nwb250YW5lb3VzX2FncmVlbWVudF9hbmRfZGlzYWdyZWVtZW50X3JlY29nbml0aW9u","signupCallToAction":"Join for free","widgetId":"rgw42_56ab19d0d61f7"},"id":"rgw42_56ab19d0d61f7","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw41_56ab19d0d61f7"},"id":"rgw41_56ab19d0d61f7","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw43_56ab19d0d61f7"},"id":"rgw43_56ab19d0d61f7","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Conference Paper","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1o9o3/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
