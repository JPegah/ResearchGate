<!DOCTYPE html> <html lang="en" class="" id="rgw35_56aba0b5cfaf0"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="JAelfKfQw+xVixAL7hoAT3Jx6zuEMoAzFcDYoUlBUKad1/LjHsy0Y57RU0O5wm8vHcALRJFQHclaRaFEmxnG42GuGBBQcbIQGZ/jFb4UtIyPemzQINsVuv3zjKspMkTjStYDUhEANtNZ3P+GZ7DjDucswAx484rwSoLdhAk5iRTfkIfkzodrC4AI+r2HfR9AS+iK/ph49PHj8j2hdy/6WiL6fXz5n3557Dwh66d2BCfhJsQK8U3Udhnm5Ku5JikqsQdQPfnRNegxhUhKiqGfzBCmLpgjr9pvN5OBH8Pir7w="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-375f014e-5319-4b45-8b6d-487cdcd5736a",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)" />
<meta property="og:description" content="In applications of Gaussian processes where quantification of uncertainty is
of primary interest, it is necessary to accurately characterize the posterior
distribution over covariance parameters...." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE/links/54c76f2e0cf289f0cecd23d5/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE" />
<meta property="rg:id" content="PB:271218362" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)" />
<meta name="citation_author" content="Maurizio Filippone" />
<meta name="citation_author" content="Raphael Engler" />
<meta name="citation_publication_date" content="2015/01/22" />
<meta name="citation_pdf_url" content="https://www.researchgate.net/profile/Maurizio_Filippone/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE/links/54c76f2e0cf289f0cecd23d5.pdf" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21993347442549/styles/pow/publicliterature/FollowPublicationPromo.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/282514599719602/styles/pow/application/PdfJsReader.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/23819663151220/styles/pow/publicliterature/PublicationInlineReader.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE) (PDF Download Available)</title>
<meta name="description" content="Official Full-Text Publication: Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE) on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba0b5cfaf0" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba0b5cfaf0" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw5_56aba0b5cfaf0">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Enabling%20scalable%20stochastic%20gradient-based%20inference%20for%20Gaussian%20processes%20by%20employing%20the%20Unbiased%20LInear%20System%20SolvEr%20(ULISSE)&rft.date=2015&rft.au=Maurizio%20Filippone%2CRaphael%20Engler&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)</h1> <meta itemprop="headline" content="Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE/links/54c76f2e0cf289f0cecd23d5/smallpreview.png">  <div id="rgw7_56aba0b5cfaf0" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw8_56aba0b5cfaf0" itemprop="author" itemscope itemtype="http://schema.org/Person"> <a itemprop="sameAs" href="profile/Maurizio_Filippone" class="pub-detail-item account-item"> <div class="indent-left"> <div class="people-img"> <img  src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_m" title="Maurizio Filippone" alt="Maurizio Filippone" height="20px" width="20px" style="height: 20px;"/> </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Maurizio Filippone</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item  remove-action-indent   " id="rgw9_56aba0b5cfaf0" data-account-key="Maurizio_Filippone">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="profile/Maurizio_Filippone"> <img class="lazyload" data-src="https://i1.rgstatic.net/ii/profile.image/AS%3A273666076311560%401442258485613_l" title="Maurizio Filippone" alt="Maurizio Filippone" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">     </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="profile/Maurizio_Filippone" class="display-name">Maurizio Filippone</a>    </h5> <div class="truncate-single-line meta">   </div>  <a href="javascript:" class="btn btn-promote btn-large people-item-contact-author js-contact-author">Message author</a>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw10_56aba0b5cfaf0"> <a href="researcher/2064343467_Raphael_Engler" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Raphael Engler" alt="Raphael Engler" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Raphael Engler</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56aba0b5cfaf0">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2064343467_Raphael_Engler"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Raphael Engler" alt="Raphael Engler" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2064343467_Raphael_Engler" class="display-name">Raphael Engler</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">        <meta itemprop="datePublished" content="2015-01">  01/2015;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1501.05427" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw12_56aba0b5cfaf0" class="pub-abstract">  <div class="clearfix">   <div style="max-height: 54px;" class="js-expander-container js-expander-collapsed">  <p itemprop="description"> <strong>ABSTRACT</strong> <div>In applications of Gaussian processes where quantification of uncertainty is<br />
of primary interest, it is necessary to accurately characterize the posterior<br />
distribution over covariance parameters. This paper proposes stochastic<br />
gradient-based inference to draw samples from the posterior distribution over<br />
covariance parameters with negligible bias and without the need to compute the<br />
marginal likelihood. In Gaussian process regression, this has the enormous<br />
advantage that stochastic gradients can be computed by solving linear systems<br />
only. A novel unbiased linear systems solver based on parallelizable covariance<br />
matrix-vector products is developed to accelerate the unbiased estimation of<br />
gradients. The results demonstrate the possibility to enable scalable and exact<br />
(in a Monte Carlo sense) quantification of uncertainty in Gaussian processes<br />
without imposing any special structures on the covariance or reducing the<br />
number of input vectors.</div> </p>  </div>  </div>   </div>      <div class="action-container"> <div id="rgw13_56aba0b5cfaf0" class="follow-publication-promo"> <table> <tr> <td class="follow-publication-promo-text-cell"> <p>Get notified about updates to this publication</p> <a class="btn btn-large btn-promote js-follow-publication ga-follow-publication-new-promo">Follow publication</a> </td> <td> <div class="follow-publication-publication-image"></div> </td> </tr> </table> </div>  <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw26_56aba0b5cfaf0">  </div> </div>  </div>  <div class="clearfix"> <div class="pdf-js-container clearfix " id="rgw27_56aba0b5cfaf0">  <div class="pdf-js-header js-sticky-header clear">  <a class="blue-link js-download rf btn btn-promote" href="https://www.researchgate.net/profile/Maurizio_Filippone/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE/links/54c76f2e0cf289f0cecd23d5.pdf?inViewer=0&amp;pdfJsDownload=0&amp;origin=publication_detail">Download full-text</a>  <h3>Full-text</h3> <span class="publication-info">  <span class="js-doi-container" style="display: none;"> DOI: <span class="js-doi"></span> &middot; </span> Available from: <a class="js-name" href="profile/Maurizio_Filippone">Maurizio Filippone</a>, <span class="js-publication-date"> Jan 27, 2015 </span>   </span>  </div>  <div class="social-share-container"><div id="rgw29_56aba0b5cfaf0" class="social-share js-social-share"> <div class="social-share-heading">Share</div>  <a id="rgw30_56aba0b5cfaf0" href="javascript:" title="Share on Facebook" class="link-nostyle js-share-item share-icon "  data-url="http://www.facebook.com/share.php?u={{url}}{{#title}}&amp;t={{title}}{{/title}}" data-width="600" data-height="350" data-name="Facebook"> <span class="share-icon ico-share-facebook-round reset-background ga-share-blog-facebook"></span> </a>  <a id="rgw31_56aba0b5cfaf0" href="javascript:" title="Share on Twitter" class="link-nostyle js-share-item share-icon "  data-url="http://twitter.com/intent/tweet?text={{#title}}{{title}}: {{/title}}{{url}}&amp;via=researchgate" data-width="600" data-height="350" data-name="Twitter"> <span class="share-icon ico-share-twitter-round reset-background ga-share-blog-twitter"></span> </a>  <a id="rgw32_56aba0b5cfaf0" href="javascript:" title="Share on Google+" class="link-nostyle js-share-item share-icon "  data-url="https://plus.google.com/share?url={{url}}" data-width="600" data-height="600" data-name="Google+"> <span class="share-icon ico-share-gplus-round reset-background ga-share-blog-gplus"></span> </a>  <a id="rgw33_56aba0b5cfaf0" href="javascript:" title="Share on LinkedIn" class="link-nostyle js-share-item share-icon "  data-url="http://www.linkedin.com/shareArticle?mini=true&amp;url={{url}}{{#title}}&amp;title={{title}}{{/title}}&amp;source=ResearchGate" data-width="520" data-height="570" data-name="LinkedIn"> <span class="share-icon ico-share-linkedin-round reset-background ga-share-blog-linkedin"></span> </a>  <a id="rgw34_56aba0b5cfaf0" href="javascript:" title="Share on Reddit" class="link-nostyle js-share-item share-icon "  data-url="https://www.reddit.com/submit?url={{url}}{{#title}}&amp;title={{title}}{{/title}}" data-width="600" data-height="600" data-name="Reddit"> <span class="share-icon ico-share-reddit reset-background ga-share-blog-reddit"></span> </a>  </div></div>    <iframe id="rgw28_56aba0b5cfaf0" src="https://www.researchgate.net/c/o1q2er/javascript/lib/pdfjs/web/viewer.html?file=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE%2Flinks%2F54c76f2e0cf289f0cecd23d5.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail" allowfullscreen frameborder="0" style="width: 100%; height: 100%; box-sizing: border-box;"></iframe>    </div> <noscript> <div id="rgw25_56aba0b5cfaf0"  itemprop="articleBody">  <p>Page 1</p> <p>arXiv:1501.05427v1  [stat.ME]  22 Jan 2015<br />Enabling scalable stochastic gradient-based inference for Gaussian processes<br />by employing the Unbiased LInear System SolvEr (ULISSE)<br />Maurizio Filippone<br />School of Computing Science, University of Glasgow, UK<br />MAURIZIO.FILIPPONE@GLASGOW.AC.UK<br />Raphael Engler<br />School of Computing Science, University of Glasgow, UK<br />RAPHAEL.ENGLER@WEB.DE<br />Abstract<br />In applications of Gaussian processes where<br />quantification of uncertainty is of primary in-<br />terest, it is necessary to accurately characterize<br />the posterior distribution over covariance param-<br />eters. This paper proposes stochastic gradient-<br />based inference to draw samples from the poste-<br />rior distribution over covariance parameters with<br />negligible bias and without the need to compute<br />the marginal likelihood. In Gaussian process re-<br />gression, this has the enormous advantage that<br />stochastic gradients can be computed by solving<br />linear systems only. A novel unbiased linear sys-<br />tems solver based on parallelizable covariance<br />matrix-vector products is developed to acceler-<br />ate the unbiased estimation of gradients. The re-<br />sults demonstrate the possibility to enable scal-<br />able and exact (in a Monte Carlo sense) quantifi-<br />cation of uncertaintyin Gaussian processes with-<br />outimposinganyspecialstructuresonthecovari-<br />ance or reducing the number of input vectors.<br />1. Introduction<br />Probabilistic kernel machines based on Gaussian Pro-<br />cesses (GPs) (Rasmussen &amp; Williams, 2006) are popular<br />in a number of applied domains as they offer the possi-<br />bility to flexibly model complex data and, depending on<br />the choice of covariance function, to gain some under-<br />standing on the underlying behavior of the system under<br />study. When quantification of uncertainty is of primary<br />interest, it is necessary to accurately characterize the pos-<br />terior distribution over covariance parameters. This has<br />been argued in a number of papers where this is done by<br />means of Markov chain Monte Carlo (MCMC) methods<br />(Williams &amp; Rasmussen, 1995; Williams &amp; Barber, 1998;<br />Neal, 1999; Murray &amp; Adams, 2010; Taylor &amp; Diggle,<br />2012; Filippone et al., 2013; Filippone &amp; Girolami, 2014).<br />Thelimitation ofMCMC approachesto drawsamples from<br />the posteriordistributionovercovarianceparametersis that<br />they need to compute the marginal likelihood at every it-<br />eration. In GP regression, a standard way to compute<br />the marginal likelihood involves storing and factorizing an<br />n×n matrix, leadingto O(n3) time and O(n2) space com-<br />plexities, where n is the size of the data set. For large<br />data sets this quickly becomes unfeasible, so a large num-<br />ber of contributions can be found in the literature on how<br />to make these calculations tractable. For example, when<br />the GP covariance matrix has some particular properties,<br />e.g., it has sparse inverse (Rue et al., 2009; Simpson et al.,<br />2013; Lyne et al., 2014), it is computedon regularlyspaced<br />inputs (Saatc ¸i, 2011), it is computed on univariate in-<br />puts (Gilboa et al., 2013), it is possible to considerably re-<br />duce the complexity in computing the marginal likelihood.<br />When these properties do not hold, which is common in<br />severalMachineLearningapplications, approximationsare<br />usually employed. Some examples involve the use subsets<br />of the data (Candela &amp; Rasmussen, 2005), the determina-<br />tion of a small number of surrogate input vectors that rep-<br />resent the full set of inputs (Titsias, 2009; Hensman et al.,<br />2013), and the application of GPs to subsets of the data<br />obtained by partitioning the input space (Gramacy et al.,<br />2004), to name a few. Unfortunately, it is difficult to as-<br />sess to what extent approximationsaffect the quantification<br />of uncertainty in predictions, although some interesting re-<br />sults in this directionarereportedin(Banerjee et al., 2013).<br />The focus of this paper are applications of GP regression<br />where the structure of the covariance matrix is not neces-<br />sarily special and quantification of uncertainty is of pri-<br />mary interest, so that approximations should be avoided.<br />This paper proposes the use of Stochastic GRadient-based<br />INference (S-GRIN) (Welling &amp; Teh, 2011) to draw sam-<br />ples from the posterior distribution over GP covariance pa-<br />rameters. S-GRIN does not require the computation of<br />the marginal likelihood and yields samples from the pos-<br />terior distribution of interest with negligible bias. This</p>  <p>Page 2</p> <p>has the enormous advantage that stochastic gradients can<br />be computed by solving linear systems only (Gibbs, 1997;<br />Gibbs &amp; MacKay, 1997; Stein et al., 2013). Linear sys-<br />tems can be solved by means of iterative methods, such as<br />the Conjugate Gradient (CG) algorithm, that are based on<br />parallelizable covariance matrix-vector products (Higham,<br />2008; Skilling, 1993; Seeger, 2000). Similar ideas were<br />previously put forward to optimize GP covariance param-<br />eters (Chen et al., 2011; Anitescu et al., 2012; Stein et al.,<br />2013). Despite the O(n2) in time and O(n) in space com-<br />plexities of these methods compare well with the O(n3)<br />in time and O(n2) in space complexities of traditional<br />MCMC-based inference, solving dense linear systems at<br />each iteration makes the whole inference framework too<br />slow to be of practical use. We compare a number of<br />standard ways to speed up the solution of dense linear<br />systems, such as fast covariance matrix-vector products<br />(Gray &amp; Moore, 2000; Moore, 2000) and preconditioning<br />(Srinivasan et al., 2014), and in line with what argued in<br />(Murray, 2009), we observe that they yield little gain in<br />computational speed compared to the standard CG algo-<br />rithm. In order to enable practical inference for GPs ap-<br />plied to large data sets, we therefore developed an Unbi-<br />ased LInear Systems SolvEr (ULISSE) that essentially al-<br />lows the CG algorithm to stop early while retaining unbi-<br />asedness of the solution.<br />We highlight here that (i) in (Welling &amp; Teh, 2011), an<br />unbiased estimate of the gradient is computed by consid-<br />ering small batches of data. Recent alternative contribu-<br />tions on scaling Bayesian inference by analyzing small<br />batches of data can be found in (Banterle et al., 2014;<br />Maclaurin &amp; Adams, 2014). GPs do not lend themselves<br />to this treatment, due to the covariancestructuremaking all<br />data dependenton one another. (ii) ULISSE is complemen-<br />taryto recentapproachesin thearea ofprobabilisticnumer-<br />ics that aim at infering, rather than computing, solutions to<br />linearsystems(Hennig, 2014). (iii) Theproposedinference<br />method is based on “noisy” gradients and is complemen-<br />tary to recent inference approaches based on noisy likeli-<br />hoods (Lyne et al., 2014; Filippone, 2014). In GP regres-<br />sion, iterative methods akin to the CG algorithm (Higham,<br />2008) can be employed to obtain an unbiased estimate of<br />the log-determinant of the covariance matrix, but this re-<br />mains an extremely onerous calculation needed to get an<br />unbiased estimate of the log-marginallikelihood. A further<br />and perhaps more challengingissue is transformingthe un-<br />biased estimate of the log-marginal likelihood in an unbi-<br />ased estimate of the marginal likelihood (Kennedy &amp; Kuti,<br />1985; Liu, 2000; Lyne et al., 2014).<br />This paper demonstrates that employingULISSE within S-<br />GRIN makes it possible to accurately carry out inference<br />of covarianceparameters in GPs and effectively scale these<br />computations to large data sets. We report results on a<br />data set with about 23 thousandinputvectors where we can<br />draw ten thousand samples per day from the posterior dis-<br />tribution over covariance parameters on a desktop machine<br />with standard hardware1. To the best of our knowledge,<br />this paper reports the first real attempt to enable full quan-<br />tification of uncertainty of covariance parameters of GPs<br />without the use of deterministic approximations to reduce<br />the number of input vectors and without imposing sparsity<br />on the GP covariance or its inverse.<br />Thepaperis organizedas follows. Section2 brieflyreviews<br />GPs and motivates the adoption of S-GRIN to infer GP co-<br />variance parameters. Section 3 describes and evaluates the<br />CG algorithm to solve linear systems and some variants<br />based on fast covariancematrix-vectorproductandprecon-<br />ditioning. Section 4 presents ULISSE and its use to obtain<br />an unbiased estimate of the gradient of the log-marginal<br />likelihood in GPs. Section 5 demonstrates the ability of the<br />proposed methodology to accurately infer covariance pa-<br />rameters in GPs and its scalability to a large data set where<br />traditional inference methods cannot be applied. Finally,<br />Section 6 draws the conclusions.<br />2. Inference of covariance parameters in GPs<br />In GP regression, a set of continuous labels y<br />{y1,...,yn} is associated with a set of input vectors X =<br />{x1,...,xn}. Throughout the paper, we will employ zero<br />mean GPs with the following covariance function:<br />=<br />k(xi,xj) = σexp?τ?xi− xj?2?+ λδij<br />with δij = 1 if i = j and zero otherwise. The parameter<br />τ determines the rate of decay of the covariance function,<br />whereas σ represents the marginal variance of each Gaus-<br />sian random variable comprising the GP. The parameter λ<br />is the variance of the (Gaussian) noise in the labels.<br />(1)<br />Let K be the covariance matrix with Kij= k(xi,xj) and<br />denote by θ the vector comprising all parameters of the<br />covariance matrix K, namely θ = (σ,τ,λ).<br />In a Bayesian sense, we would like to carryany uncertainty<br />in parameters estimates forward to predictions over the la-<br />bel y∗for a new input vector x∗. In particular, this would<br />amountin integratingouthyper-parametersfromthe model<br />in order to compute the predictive distribution:<br />p(y∗|y,X,x∗) =<br />?<br />p(y∗|y,θ,X,x∗)p(θ|y,X)dθ. (2)<br />This shows that in order to compute the predictive dis-<br />tribution for new input vectors, we need to characterize<br />the posterior distribution over the covariance parameters<br />1Code to reproduce all the results can be found here:<br />www.dcs.gla.ac.uk/˜maurizio/pages/code.html</p>  <p>Page 3</p> <p>p(θ|y,X). The integral above is analytically intractable,<br />so it is necessary to resort to some approximations.<br />A standard way to tackle this intractability is to draw sam-<br />ples from p(θ|y,X) using MCMC methods, and approxi-<br />mate the integral with the Monte Carlo estimate<br />p(y∗|y,X,x∗) ≃<br />1<br />N<br />N<br />?<br />i=1<br />p(y∗|θ(i),X,x∗),<br />(3)<br />where θ(i)denotes the ith of a set of samples from<br />p(θ|y,X). Drawing samples from the posterior distribu-<br />tion can be done using several MCMC algorithms that es-<br />sentially are based on a proposal mechanism and on an<br />accept/reject step that requires the evaluation of the log-<br />marginal likelihood:<br />log[p(y|θ,X)] = −1<br />2log(|K|) −1<br />2yTK−1y + const.<br />(4)<br />A standard way to proceed, is to factorize the covari-<br />ance matrix K = LLTusing the Cholesky algorithm<br />(Golub &amp; Van Loan, 1996). The factorization costs O(n3)<br />operations and requires the storage of O(n2) entries of<br />the covariance matrix, but after that computing the log-<br />determinant and the inverse of K multiplied by y can be<br />done using O(n2) operations.<br />The computational complexities above pose a constraint in<br />the scalability of GPs to large data sets. Iterative methods<br />based on covariancematrix-vectorproducts(CMVPs) have<br />been proposed to obtain an unbiased estimate of the log-<br />marginal likelihood. Even though these methods scale in<br />O(n2) in time and O(n) in space, they are of little practical<br />use in the task of sampling from p(θ|y,X), as the number<br />of iterations needed to estimate the log-determinant term<br />can be prohibitively large. We now illustrate our proposal<br />to obtain samples fromthe posterior distributionp(θ|y,X)<br />with negligible bias and avoiding having to estimate log-<br />determinants and marginal likelihoods.<br />2.1. Stochastic GRadient-based INference (S-GRIN)<br />We<br />(Welling &amp; Teh, 2011) to obtain samples from p(θ|y,X).<br />The idea behindS-GRIN is to modifythe standard stochas-<br />tic gradient optimization algorithm (Robbins &amp; Monro,<br />1951) by injecting Gaussian noise in a way that ensures<br />transition into a Langevin dynamics phase yielding<br />samples from the posterior distribution of interest.<br />particular, the proposal of a new set of parameters is<br />θt+1= θt+εt<br />2M {˜ gi+ ∇θlog[p(θ)]} + ηt<br />with ηt∼ N(ηt|0,εtM). The term in curly brackets is<br />an unbiased estimate of log[p(θ|y,X)]. We have also in-<br />troduced a preconditioning matrix M that can be chosen<br />briefly describe theS-GRINalgorithmin<br />In<br />(5)<br />to improve convergence of S-GRIN. The update equation,<br />except for ηt, is the standard update used in stochastic gra-<br />dient optimization. The parameters εtare chosen to satisfy<br />the standard conditions<br />∞<br />?<br />t=1<br />εt= ∞<br />and<br />∞<br />?<br />t=1<br />ε2<br />t&lt; ∞<br />(6)<br />as these, along with some other technical assumptions,<br />guarantee convergence to a local maximum. In this work,<br />we define εt = a(b + t)−γwith γ = 1. The injected<br />noise ηtis Gaussian with covariance εtM ensuring that<br />the algorithm transitions into a discretized version of a<br />Langevin dynamics with target distribution given by the<br />posterior over θ. In principle, it would be necessary to ac-<br />ceptorrejecttheproposals,whichwouldrequireevaluating<br />the marginal likelihood. The key result in (Welling &amp; Teh,<br />2011) is that when S-GRIN reaches the Langevin dynam-<br />ics phase, the step-size εtis small enough that the accep-<br />tance rate would be almost one. Therefore, in this phase<br />we can accept all proposals without the need to evaluate<br />the marginal likelihood at the cost of introducinga negligi-<br />ble amount of bias.<br />Following (Welling &amp; Teh, 2011), we can estimate when<br />the algorithm reaches the Langevin dynamics phase by<br />monitoring the ratio between the variance of the stochastic<br />gradients and the variance of the injected noise. Defining<br />V to be the sampling covariance of the stochastic gradi-<br />ents and denoting with λmax(A) the largest eigenvalue of a<br />matrix A, we can write such a ratio as<br />εt<br />4λmax<br />?<br />M<br />1<br />2V M<br />1<br />2<br />?<br />(7)<br />When this ratio is small enough the algorithm is in its<br />Langevin dynamics phase and produces samples from the<br />posterior distribution over θ.<br />The motivation for employing S-GRIN for inference of GP<br />covariance parameters comes from inspecting the gradient<br />of the log-marginal likelihood that has components<br />gi= −1<br />2Tr<br />?<br />K−1∂K<br />∂θi<br />?<br />+ yTK−1∂K<br />∂θiK−1y<br />(8)<br />Computingthe gi’s requiresagainoperationsthathavetime<br />complexity in O(n3) due to the trace term and the linear<br />system K−1y. However, we can introduce Nrvectors r(i)<br />with componentsdrawn from{−1,1}with probability1/2<br />and unbiasedly estimate the trace term obtaining (Gibbs,<br />1997):<br />˜ gi= −<br />1<br />2Nr<br />Nr<br />?<br />i=1<br />r(i)TK−1∂K<br />∂θir(i)+ yTK−1∂K<br />∂θiK−1y<br />(9)</p>  <p>Page 4</p> <p>Algorithm 1 The Conjugate Gradient algorithm<br />Input: data X, vectorb, convergencethreshold ǫ, initial<br />vector s0, maximum number of iterations T<br />e0= b − Ks0;<br />for i = 0 to T do<br />eT<br />iei<br />dT<br />si+1= si+ αidi;<br />ei+1= ei− αiKdi;<br />if ?ei+1? &lt; ǫ then<br />return s = si+1;<br />end if<br />βi=eT<br />eT<br />iei<br />di+1= ei+1+ βidi;<br />end for<br />d0= e0;<br />αi=<br />iKdi;<br />i+1ei+1<br />;<br />We can easily verify that the expectation of each<br />of the terms in the sum is E[r(i)TK−1 ∂K<br />∂θiE[r(i)r(i)T] which is the trace term in eq. 8 given<br />that E[r(i)r(i)T] = I. Hence, in order to compute an unbi-<br />ased version of the gradient of the log-marginal likelihood<br />we need to solve one linear system for y and one for each<br />of the Nrvectors r(i)used to estimate the trace term. This<br />consideration forms the basis of the proposed methodol-<br />ogy. Computing an unbiased version of the gradient in-<br />volves solving linear systems only, which is much easier<br />and cheaper than estimating log-determinants.<br />∂θir(i)]=<br />K−1 ∂K<br />3. Solving Linear Systems<br />We have discussed that S-GRIN to infer covariance pa-<br />rameters in GPs in general requires solving dense linear<br />systems. In this section, we briefly review the Conjugate<br />Gradient (CG) algorithm that is a popular method to it-<br />eratively solve linear systems based on Covariance Ma-<br />trix Vector Product (CMVP) operations (time and space<br />complexities in O(n2) and O(n), respectively). We also<br />discuss and evaluate a few variants to speed up com-<br />putations/convergence, such as preconditioning and fast<br />CMVPs. Throughout this section we will evaluate the ef-<br />fectiveness of these alternatives on a GP regression task<br />applied to the Concrete data set from the UCI repository<br />(Asuncion &amp; Newman, 2007). The Concrete data set con-<br />tainsdataaboutthecompressivestrengthofn = 1030sam-<br />ples of concrete described by d = 8 features.<br />3.1. The Conjugate Gradient (CG) algorithm<br />Given a linear system of the form Ks = b with K and b<br />given, the CG algorithm (Golub &amp; Van Loan, 1996) yields<br />the solution s without having to invert or factorize the ma-<br />trix K. The idea is to calculate the solution s as the mini-<br />02468 1012<br />log10(κ)<br />a = 1, b = 1<br />a = 1, b = 0.1<br />a = 1, b = 0.01<br />a = 0.5, b = 0.01<br />Figure 1. Distribution of the condition number κ of the covari-<br />ance matrix for different choices of shape and rate parameters of<br />a Gamma prior on each covariance parameter θ.<br />mizer of<br />φ(s) =1<br />2sTKs − sTb<br />(10)<br />which can be obtained by employing gradient-based opti-<br />mization. The CG algorithm is initialized from an initial<br />guess s0. After that, the iterations refine the solution s by<br />updates in directions di. The CG algorithm, in comparison<br />with the standard gradient descent, is characterized by the<br />fact that K-orthogonality(or conjugacy with respect to K)<br />of the search directions is imposed, namely dT<br />when i ?= j. This condition yields a sequence of residuals<br />ei = b − Ksithat are mutually orthogonal, and guaran-<br />tees convergence in at most n iterations. Remarkably, the<br />CG algorithm can be implemented in a way that requires a<br />single CMVP (Kdi) at each iteration (see Algorithm 1).<br />iKdj = 0<br />The trade-off between accuracy for speed is governed by<br />the threshold ǫ, which in this paper we set to ǫ = 10−8.<br />Theoretically, the CG algorithm is guaranteed to converge<br />in at most n iterations, but in practice, due to the repre-<br />sentation in finite numerical precision, orthogonalityof the<br />directions can be lost, especially in badly conditioned sys-<br />tems, and the CG algorithmcan take more than n iterations<br />to converge. The condition number of a matrix is defined<br />as the ratio between its largest and smallest eigenvalues:<br />κ =λmax(K)<br />λmin(K)<br />Fig. 1 shows the distribution of the condition numberwhen<br />each covariance parameter θiis sampled form a Gamma<br />distribution with shape and rate parameters a and b. The<br />distributions are reasonably vague and give a rough idea of<br />thetypicalconditionnumbersencounteredduringtheinfer-<br />enceofGP covarianceparametersfortheConcretedataset.<br />We can expect slower convergence speed when the condi-<br />tion number is large due to numerical instabilities; we are<br />interested in quantifying to what extent this applies to GPs<br />and what is the impact of cheap CMVPs and precondition-<br />ing on convergencespeed. In the remainder of this section,<br />we will consider the problem of solving the linear system<br />Ks = y that is needed in the calculation of part of the<br />gradient in eq. 9. The results pertaining to the solution of<br />the linear systems Ks = r(i)are quite similar (results not<br />reported), so for the sake of brevity, we will omit them.</p>  <p>Page 5</p> <p>3.2. Fast CMVPs<br />We consider here the use of two fast CMVPs based on<br />efficient representation of input data that we will call<br />“kdtree” (Gray &amp; Moore, 2000) and “anchors” (Moore,<br />2000)2. These methods yield fast CMVPs at the price of<br />a lower accuracy.<br />In the top row of Fig. 2 we show the number of iterations<br />required by the CG algorithm to reach convergence versus<br />the condition number and the error in the solution versus<br />the condition number. The error is defined as the norm of<br />the difference between the solution obtained by the CG al-<br />gorithm and the one obtained by factorizing K using the<br />Choleskyalgorithmandcarryingoutforwardandbacksub-<br />stitutions with y. We compare a baseline CG algorithm<br />with CMVPs performed in double precision with CG al-<br />gorithms implemented with (i) “kdtree” CMVPs (ii) “an-<br />chors” CMVPs and (iii) single precision CMVPs. The con-<br />vergencethresholdof the CG algorithmwas set to 10−8, so<br />in order to be able to satisfy this criterion when employing<br />“kdtree” and “anchors” CMVPs, we selected the relative<br />and absolute tolerance parameters to be 10−10.<br />The results indicate that double precision calculations lead<br />to the lowest number of iterations compared to the other<br />methods, especially when κ is large. Double precision<br />calculations offer the lowest error. Single precision cal-<br />culation lead to a very poor error compared to the other<br />methods. The CG algorithm with “kdtree” CMVPs seems<br />to take longer to converge than the one with “anchor”<br />CMVPs, but it achieves a lower error.<br />Drawing definitive conclusions on whether fast CMVPs<br />yield any gain in computing time is far from trivial, as this<br />very much depends on implementation details and hard-<br />ware where the code is run. What we can say, however, is<br />that gaining orders of magnitude speed-ups would require<br />reducing the accuracy of fast CMVPs, but this would re-<br />quire loosening up the convergence criterion in order for<br />the CG algorithm to converge. As a result, we would be<br />able to obtain solutions of linear systems faster but at the<br />cost of a reduced accuracy in the solution, which in turn<br />would bias the estimation of gradients.<br />3.3. Preconditioned CG<br />The Preconditioned CG (PCG) is a variant of the CG algo-<br />rithm that aims at mitigating the issues associated with the<br />rate of convergence of the CG algorithm when the condi-<br />tion number κ is large. A (left) preconditioning matrix J<br />operates on the linear system yielding<br />J−1Ks = J−1b<br />2code implementing these methods can be found here:<br />www.cs.ubc.ca/˜awll/nbody_methods.html<br />0123456<br />1<br />2<br />3<br />4<br />5<br />log10(κ)<br />log10(number iterations)<br />double<br />float<br />anchors<br />kdtree<br />0123456<br />−15<br />−10<br />−5<br />0<br />log10(κ)<br />log10(norm residual)<br />0123456<br />1<br />2<br />3<br />4<br />log10(κ)<br />log10(number iterations)<br />CG<br />PCG double<br />PCG float<br />0123456<br />−14<br />−10<br />−6<br />log10(κ)<br />log10(norm residual)<br />Figure 2. Top row: Comparison of fast CMVPson number of iter-<br />ations and error versus condition number. - Bottom row: Compar-<br />ison of the CG algorithm and two PCG algorithms using double<br />and single precision CMVPs to solve inner linear systems.<br />The success of PCG is based on the possibility to construct<br />J so that J−1K is well conditioned. This can be achieved<br />when J−1well approximatesK−1, and a complicationim-<br />mediatelyarisesonhowtodosoforgeneralkernelmatrices<br />without carrying out expensive operations (in O(n3)).<br />In (Srinivasan et al., 2014) it was proposed to define J =<br />K + δI with δ &gt; 0. Compared to the standard CG al-<br />gorithm, the PCG algorithm introduces the solution of a<br />linear system of the form J−1b at each iteration, that can<br />be solved again using the CG algorithm. A large value of<br />δ makes K + δI well conditioned and makes convergence<br />speed of the inner CG algorithm faster whereas it makes<br />J−1and K−1quite different leading to the necessity to<br />run the outer CG algorithm for several iterations. For small<br />values of δ the situation is reversed, so δ needs to be tuned<br />to find an optimal compromise.<br />In the bottom row of Fig. 2, we compare the standard CG<br />algorithmwith two versions of the PCG algorithmon num-<br />ber of iterations and accuracy of the solution. In the first<br />versionofthePCG algorithmwe useddoubleprecisioncal-<br />culations when solvingthe innerlinear systems, whereas in<br />the second version we used single precision calculations.<br />In both versions of the PCG algorithm we set δ to yield the<br />lowest number of iterations in order to show whether it is<br />possible to reduce the number of computations.<br />The results show that the standard CG algorithm takes less<br />iterations to converge than the PCG algorithm (counting</p>  <p>Page 6</p> <p>both inner and outer iterations). Even in the case of sin-<br />gle precision calculations in the inner CG algorithm, we<br />did not experience any gain in computing time due to the<br />increased number of iterations. For other data and in dif-<br />ferent experimental conditions there might be a compu-<br />tational advantage in using a preconditioner, as shown in<br />(Srinivasan et al., 2014), but the gain is generally modest.<br />4. Unbiased LInear System SolvEr (ULISSE)<br />From the analysis in the previous sections it is evident that<br />none of the standard ways to speedupcalculations and con-<br />vergence of the CG algorithm offers substantial gains in<br />computing time. As a result, employing iterative meth-<br />ods as an alternative to traditional factorization techniques<br />seems beyond practicality as pointed out, e.g., in (Murray,<br />2009). One of the novel contributions of this paper is to<br />accelerate the CG algorithm by orders of magnitude at the<br />expenses of obtaining an unbiased estimate of the solution.<br />The idea is to stop the CG algorithm before the conver-<br />gence criterion is satisfied and apply some corrections to<br />ensure unbiasedness of the solution. We note here that our<br />proposal can be applied to any of the variants of the CG<br />algorithm presented earlier and to dense as well as sparse<br />linear systems.<br />We canrewritethefinalsolutionofalinearsystemobtained<br />by the CG algorithm as a sum of incremental updates<br />s = s0+ δ1+ ... + δT<br />(11)<br />assuming that it takes T iterations to satisfy the conver-<br />gence threshold ǫ. We can define an “early stop” thresh-<br />old α &gt; ǫ that will be reached after l &lt; T iterations, and<br />rewrite the final solution by introducing a series of coeffi-<br />cients as follows<br />s<br />=<br />s0+<br />l−1<br />?<br />i=1<br />δi+<br />1<br />w0<br />?<br />w0δl+0+<br />1<br />w1<br />?<br />w0w1δl+1+<br />+1<br />w2(w0w1w2δl+2+ ...)<br />??<br />(12)<br />We will focuson coefficientsdefinedas wr= exp(βr), but<br />this choice is not restrictive. We can now obtain an unbi-<br />ased estimate of the solution of the linear system by setting<br />˜ s = s0+?l−1<br />ing two steps (i) draw uj ∼ U[0,1] (ii) if uj &lt;<br />˜ s = ˜ s +?j<br />is clearly s and the rate of decay β in the expression of wr<br />determines the average number of steps that are carried out<br />after early stopping the CG algorithm. The accumulation<br />of the terms forming ˜ s is carried out during the execution<br />of the standard CG algorithm.<br />For simplicity, we set the early stop threshold to α = q√n<br />as q gives a rough indication of the average error that we<br />i=1δiand iterate for j = 0,1,... the follow-<br />1<br />wjthen<br />r=0wrδl+j, else return ˜ s. The expectation of ˜ s<br />0123456<br />0.5<br />1.5<br />2.5<br />3.5<br />log10(κ)<br />log10(number iterations)<br />CG<br />ULISSE q=0.1<br />ULISSE q=1.0<br />0123456<br />−8<br />−6<br />−4<br />−2<br />0<br />2<br />4<br />log10(κ)<br />log10(avg std dev)<br />0123456<br />0.5<br />1.5<br />2.5<br />3.5<br />log10(κ)<br />log10(number iterations)<br />CG<br />ULISSE q=0.1<br />ULISSE q=1.0<br />0123456<br />−15<br />−10<br />−5<br />0<br />log10(κ)<br />log10(avg std dev)<br />Figure 3. Comparison of the CG algorithm and ULISSE with<br />early stop thresholds α calculated with q = 0.1 and q = 1 on<br />number of iterations and standard deviation of the solution. The<br />top row corresponds to β = 1 in the calculation of the weights<br />wr, whereas the bottom row corresponds to β = 100.<br />are expecting in each element of the solution. In Fig. 3<br />we report number of iterations and average standard devia-<br />tion across the elements of the solution. ULISSE with two<br />different values of β is compared with the baseline CG al-<br />gorithm without early stop q = 0. We stress again that the<br />error is such that the solution is unbiased.<br />4.1. Variance of the gradient<br />We conclude this section by showing the behavior of<br />ULISSE in the calculation of an unbiased version of the<br />gradient in GPs. Applying the proposed unbiased solver to<br />the first term of ˜ giin eq. 9 is straightforwardand it requires<br />solving Nrlinear systems, one for each of the r(i)vectors.<br />For the quadratic term in y, instead, we need to obtain two<br />independent unbiased estimates of K−1y in order for the<br />expectation of the whole term to be unbiased. This can be<br />implemented by running a single instance of the CG algo-<br />rithm and keeping track of two solutions obtained by in-<br />dependent draws of the uniform variables ujused to early<br />stop the CG algorithm. We remark that the unbiased esti-<br />mationofthegradientinvolvesnowtwosourcesofstochas-<br />ticity: onedueto theintroductionofvectorsr(i)toestimate<br />the trace term in eq. 8, and one due to the proposed way to<br />unbiasedly solve linear systems.<br />Fig. 4 reports the average, taken with respect to 100 repeti-</p>  <p>Page 7</p> <p>0123456<br />1.0<br />2.0<br />3.0<br />log10(κ)<br />log10(number iterations)<br />CG<br />ULISSE q=0.1<br />ULISSE q=1.0<br />0123456<br />−8<br />−6<br />−4<br />−2<br />0<br />log10(κ)<br />log10(avg rel square norm)<br />Figure 4. Comparison of the CG algorithm and ULISSE and early<br />stop thresholds computed with q = 0.1 and q = 1 to estimate the<br />gradient of the log-marginal likelihood in eq. 9. In ULISSE, the<br />weights wr are calculated with β = 1.<br />tion of the log10of the relative square norm of the error:<br />?g(θ) − ˜ g(θ)?2<br />?g(θ)?2<br />(13)<br />as a function of the condition number κ. We used one sin-<br />gle vector r(1)to estimate the gradient in eq. 9. The figure<br />shows a number of interesting facts. The estimate in eq. 9<br />(q = 0 in the figure) is quite accurate, as the relative error<br />is small in a wide range of values of κ. At the expenses of<br />a larger variance in the estimate of the gradient that is af-<br />fected by the early stop threshold α, ULISSE yields orders<br />of magnitude improvements in the number of iterations.<br />5. Experimental validation<br />In this section, we infer covariance parameters of GP re-<br />gression models using S-GRIN with ULISSE. We start<br />by considering the concrete data set where it is possible<br />to compare the results of our proposal and the standard<br />Metropolis-Hastings (MH) algorithm.<br />strate the scalability of the proposed methodology by con-<br />sidering a data set comprising n = 22,784 input vectors in<br />d = 8 dimensions.<br />We then demon-<br />5.1. Comparison with MCMC<br />We ran a standard Metropolis-Hastings algorithm for fifty-<br />thousanditerationsto the GP regressionmodelwith covari-<br />ance in eq. 1 applied to the concrete data set. We allowed<br />for an initial adaptive phase to reach an average acceptance<br />rate between 0.2 and 0.4, and we discarded the first ten<br />thousand iterations in order to estimate the mean and stan-<br />dard deviation of the posterior distribution over the three<br />parameters. Fig. 5 shows the running mean and the inter-<br />val correspondingto plus/minustwice the runningstandard<br />deviation for the three parameters (solid red lines).<br />We compare the run from the MH algorithm with S-GRIN,<br />where we made the following design choices. We em-<br />010000<br />Iteration number<br />2000030000<br />1.0<br />2.5<br />4.0<br />log(σ)<br />0e+00 1e+04<br />Iteration number<br />2e+043e+04<br />1.0<br />2.5<br />4.0<br />log(σ)<br />PSRF median<br />PSRF 97.5%<br />0 10000<br />Iteration number<br />2000030000<br />−2.9<br />−2.6<br />log(λ)<br />0e+001e+04<br />Iteration number<br />2e+04 3e+04<br />1.0<br />2.5<br />4.0<br />log(λ)<br />PSRF median<br />PSRF 97.5%<br />010000<br />Iteration number<br />2000030000<br />−3.5<br />−2.5<br />log(τ)<br />0e+001e+04<br />Iteration number<br />2e+043e+04<br />1.0<br />2.5<br />4.0<br />log(τ)<br />PSRF median<br />PSRF 97.5%<br />Figure 5. Concrete data - Left panel: Comparison of MCMC<br />(red) and S-GRIN with ULISSE (black) on running mean and<br />plus/minus two standard deviations. The trace of one chain of<br />S-GRIN is also shown (gray). - Right panel: Convergence anal-<br />ysis of S-GRIN with ULISSE reporting the PSRF computed over<br />ten chains.<br />ployed ULISSE within the CG algorithm with double pre-<br />cision CMVPs. We set the early stop threshold α to√n<br />and the parameter β in the computation of the weights wr<br />to 1. Stochastic gradients were estimated using Nr = 4<br />vectors r(i). We ran S-GRIN for 40 thousanditerations and<br />the εtwere chosen to start from 10−1and reduce to 10−4<br />at the last iteration with γ = 1. During the execution of S-<br />GRIN we monitored the quantityεt<br />discussed in section 4, and we froze the value of εtwhen it<br />was less than 0.002; the covariance of the gradients V was<br />estimated on batches of 100 iterations. Finally, in order to<br />speed up computations, we decided to redraw the vectors<br />r(i)every 20 iterations and to keep them fixed in between.<br />The advantage of this is that the solutions of the linear sys-<br />tems Kr(i)can be used to initialize the same systems when<br />proposing a new θ thus speeding up convergence.<br />4λmax<br />?<br />M<br />1<br />2V M<br />1<br />2<br />?<br />as<br />We set the preconditioning matrix M in S-GRIN as the in-<br />verse of the negative Hessian of the log of the posterior<br />density at its mode computed on a subset of 500 input vec-<br />tors, as this is cheap way to obtain a rough idea of the co-<br />variance structure of the posterior distribution for the full<br />data set. In Fig. 5 we report the running statistics for the<br />three parameters (solid black lines), and the trace-plot of<br />one run of S-GRIN (solid gray lines), where we discarded<br />all iterations prior to the freezing of the step-size εt. The<br />figure shows a striking match between the results obtained<br />by a standardMCMC approachand S-GRIN with ULISSE.<br />This demonstrates that our proposal is a valid alternative to<br />faithfully quantify uncertainty in GPs.<br />In orderto checkconvergencespeedof S-GRIN, we ran ten</p>  <p>Page 8</p> <p>0 5000<br />Iteration number<br />1000015000<br />0.85<br />1.05<br />log(σ)<br />0 5000<br />Iteration number<br />10000 15000<br />1.0<br />2.5<br />4.0<br />log(σ)<br />PSRF median<br />PSRF 97.5%<br />0 5000<br />Iteration number<br />1000015000<br />−1.80<br />−1.70<br />log(λ)<br />0 5000<br />Iteration number<br />10000 15000<br />1.0<br />2.5<br />4.0<br />log(λ)<br />PSRF median<br />PSRF 97.5%<br />0 5000<br />Iteration number<br />10000 15000<br />−0.95<br />−0.75<br />log(τ)<br />0 5000<br />Iteration number<br />1000015000<br />1.0<br />2.5<br />4.0<br />log(τ)<br />PSRF median<br />PSRF 97.5%<br />Figure 6. Census data - Left panel: Running mean and plus/minus<br />two standard deviations (black) and trace of one chain of S-GRIN<br />(gray). - Right panel: Convergence analysis of S-GRIN with<br />ULISSE reporting the PSRF computed over five chains.<br />parallel chainsand computedthe Potential Scale Reduction<br />Factor (PSRF) (Gelman &amp; Rubin, 1992). The chains were<br />initialized by drawing from a Gaussian with mean on the<br />MAP solution over a subset of 500 input vectors and co-<br />variance M. Fig. 5 shows the median and the 97.5th per-<br />centile of the PSRF across the ten chains. The analysis of<br />these plots reveals that S-GRIN achieves convergenceafter<br />few thousands of iterations.<br />5.2. Demonstration on a larger dataset<br />We now present the application of S-GRIN with ULISSE<br />to a data set where it is not possible to run any MCMC<br />algorithm on a conventional desktop machine. This data<br />set contains data collected as part of the 1990 US census.<br />In this study, we used the 8L data set3where the regression<br />task associates the median house price in a given region<br />with demographic composition and housing market. We<br />kept the same experimental conditions as in the case of the<br />concretedata, exceptthat εtwas chosen to go from 5·10−2<br />to 5·10−6to copewiththe largergradientsobtainedforthis<br />data set, and the preconditioner M was estimated based on<br />the MAP on 1000 data points rather than 500. The running<br />statistics for the three parameters are reported in Fig. 6,<br />along with the PSRF computed across five chains. Again,<br />convergencewas reached after few thousands of iterations.<br />S-GRIN with ULISSE was ran on a desktop machine with<br />an eight core (i7-2600 CPU at 3.40GHz) processor, and an<br />NVIDIA GeForce GTX 590 graphics card. The two GPUs<br />in the graphics card are used to carry out CMVPs. With<br />this arrangement, we were able to draw roughly ten thou-<br />3www.cs.toronto.edu/˜delve/data/datasets.html<br />sand samples per day from the posterior distribution over<br />covariance parameters.<br />6. Conclusions<br />This paper presented a novel way to accurately infer co-<br />variance parameters in GPs. The novelty stems from the<br />combination of stochastic gradient-based inference and a<br />fast unbiased solver of dense linear systems. The results<br />demonstrate that it is possible to tackle the inference prob-<br />lemoveradatasetcomprisingabout23thousandinputvec-<br />tors in a day on a desktop machine with standard hardware.<br />The proposed methodology can exploit parallelism when<br />computing covariance matrix-vector products, so there is<br />an opportunityto scale “exact” inference(in a Monte Carlo<br />sense) to even larger data sets. We are not aware of any<br />method that is capable of carrying out full quantification of<br />uncertaintyof GP covarianceparameterson such large data<br />sets without imposing any special structures on the covari-<br />ance or reducing the number of input vectors. These re-<br />sults are important not only in Machine Learning, but also<br />in a number of areas where quantification of uncertainty is<br />of primary interest and GPs are routinely employed, such<br />as calibration of computer codes (Kennedy &amp; O’Hagan,<br />2001) and optimization (Jones et al., 1998).<br />Theresults reportedinthis paper,althoughpromising,indi-<br />cate some directions for improvements. S-GRIN requires<br />the tuning of a preconditioning matrix M. Choosing M<br />to be similar to the covariance of the posterior speeds up<br />convergence of S-GRIN when it reaches the Langevin dy-<br />namics phase. However, M also affect the scaling of the<br />gradient in the proposal. During the first phase of S-GRIN<br />this might not be optimal, and ideally, gradients should be<br />scaled in a way similar to AdaGrad (Duchi et al., 2011). In<br />(Welling &amp; Teh, 2011), it was possible to establish a con-<br />nection between the variance of the gradient, the Fisher In-<br />formation, and M due to the fact that the gradient is eval-<br />uated on a subset of the data. We were unable to do so<br />for GPs due to the different way stochasticity is introduced<br />in the computation of the gradients. Despite this compli-<br />cation, we demonstrated that it is still possible to obtain<br />convergence to the posterior distribution over covariance<br />parameters in a reasonable number of iterations, which is<br />of ultimate importance in any inference task.<br />We are currently investigating the possibility to extend<br />this methodology to scale inference for other GP models,<br />e.g., GPs classification and GPs for spatio-temporal data.<br />Other interesting aspects to explore would be the introduc-<br />tion of mixed precision calculations within the CG algo-<br />rithm to improve convergence and computation speed as<br />presented, e.g., in (Jang et al., 2011; Cevahir et al., 2009;<br />Baboulin et al., 2009).</p>  <p>Page 9</p> <p>Acknowledgments<br />MF gratefully acknowledges support from EPSRC grant<br />EP/L020319/1.<br />References<br />Anitescu, M., Chen, J., and Wang, L.<br />Approach for Solving the Parametric Gaussian Process<br />Maximum LikelihoodProblem. SIAM Journal on Scien-<br />tific Computing, 34(1):A240–A262,2012.<br />A Matrix-free<br />Asuncion, A. and Newman, D. J. UCI machine learning<br />repository, 2007.<br />Baboulin, M., Buttari, A., Dongarra, J., Kurzak, J., Lan-<br />gou, J., Langou, J., Luszczek, P., and Tomov, S. Accel-<br />erating scientific computations with mixed precision al-<br />gorithms. Computer Physics Communications, 180(12):<br />2526–2533,2009.<br />Banerjee, A., Dunson, D. B., and Tokdar, S. T.<br />ficient Gaussian process regression for large datasets.<br />Biometrika, 100(1):75–89,2013.<br />Ef-<br />Banterle, M., Grazian, C., and Robert, C. P.<br />ating Metropolis-Hastings algorithms: Delayed accep-<br />tance with prefetching, June 2014. arXiv:1406.2660.<br />Acceler-<br />Candela, J. Q. and Rasmussen, C. E. A Unifying View of<br />SparseApproximateGaussianProcessRegression. Jour-<br />nal of Machine Learning Research, 6:1939–1959,2005.<br />Cevahir, A., Nukada, A., and Matsuoka, S. Fast Conjugate<br />Gradients with Multiple GPUs. In Allen, G., Nabrzyski,<br />J., Seidel, E., van Albada, G., Dongarra, J., and Sloot,<br />P. (eds.), Computational Science ICCS 2009, volume<br />5544 of Lecture Notes in Computer Science, pp. 893–<br />903. Springer Berlin Heidelberg, 2009.<br />Chen, J., Anitescu, M., and Saad, Y. Computing f(A)b via<br />Least Squares Polynomial Approximations. SIAM Jour-<br />nal on Scientific Computing, 33(1):195–222,2011.<br />Duchi, J., Hazan, E., and Singer, Y. Adaptive Subgradient<br />Methods for Online Learning and Stochastic Optimiza-<br />tion. J. Mach. Learn. Res., 12:2121–2159,July 2011.<br />Filippone, M.<br />classifiers with annealing and pseudo-marginal MCMC.<br />In 22nd International Conference on Pattern Recogni-<br />tion, ICPR 2014, Stockholm, Sweden, August 24-28,<br />2014, pp. 614–619. IEEE, 2014.<br />Bayesian inference for Gaussian process<br />Filippone, M. and Girolami, M. Pseudo-marginalBayesian<br />inference for Gaussian processes. IEEE Transactions<br />on Pattern Analysis and Machine Intelligence, 36(11):<br />2214–2226,2014.<br />Filippone, M., Zhong, M., and Girolami, M. A compara-<br />tive evaluationofstochastic-basedinferencemethodsfor<br />Gaussian process models. Machine Learning, 93(1):93–<br />114, 2013.<br />Gelman, A. and Rubin, D. B. Inference from iterative sim-<br />ulation using multiple sequences. Statistical Science, 7<br />(4):457–472,1992.<br />Gibbs, M. and MacKay, D. J. C. Efficient Implementa-<br />tion of Gaussian Processes. Technical report, Cavendish<br />Laboratory, Cambridge, UK, 1997.<br />Gibbs, M. N. Bayesian Gaussian processes for regression<br />and classification. PhD thesis, University of Cambridge,<br />1997.<br />Gilboa, E., Saatci, Y., andCunningham,J. ScalingMultidi-<br />mensional Inference for Structured Gaussian Processes.<br />IEEE Transactions on Pattern Analysis and Machine In-<br />telligence, to appear, 2013.<br />Golub, G. H. and Van Loan, C. F. Matrix computations.<br />The Johns Hopkins University Press, 3rd edition, Octo-<br />ber 1996.<br />Gramacy, R. B., Lee, H. K. H., and Macready, W. G. Pa-<br />rameter space exploration with Gaussian process trees.<br />In ICML ’04: Proceedings of the twenty-first interna-<br />tional conference on Machine learning, pp. 45, New<br />York, NY, USA, 2004. ACM.<br />Gray, A. G. and Moore, A. W.<br />Statistical Learning. In Leen, T. K., Dietterich, T. G.,<br />and Tresp, V. (eds.), Advances in Neural Information<br />Processing Systems 13, Papers from Neural Information<br />Processing Systems (NIPS) 2000, Denver, CO, USA, pp.<br />521–527. MIT Press, 2000.<br />’N-Body’ Problems in<br />Hennig, P. Probabilistic Interpretation of Linear Solvers,<br />October 2014. arXiv:1402.2058.<br />Hensman, J., Fusi, N., and Lawrence, N. D. Gaussian Pro-<br />cesses for Big Data, September 2013. arXiv:1309.6835.<br />Higham, N. J. Functions of Matrices: Theory and Compu-<br />tation. Society for Industrial and Applied Mathematics,<br />Philadelphia, PA, USA, 2008.<br />Jang, Y.-C., Kim, H.-J., and Lee, W.<br />Performance of Conjugate Gradient Solver with Stag-<br />gered Fermions in Mixed Precision, November 2011.<br />arXiv:1111.0125.<br />Multi GPU<br />Jones, D. R., Schonlau, M., and Welch, W. J. Efficient<br />GlobalOptimizationofExpensiveBlack-BoxFunctions.<br />J. of Global Optimization, 13(4):455–492,1998.</p>  <p>Page 10</p> <p>Kennedy, A. D. and Kuti, J. Noise without Noise: A New<br />MonteCarloMethod. PhysicalReviewLetters, 54:2473–<br />2476, 1985.<br />Kennedy, M. C. and O’Hagan, A. Bayesian calibration of<br />computer models. Journal of the Royal Statistical Soci-<br />ety: Series B (Statistical Methodology), 63(3):425–464,<br />2001.<br />Liu, K.-F. A Noisy Monte Carlo Algorithm with Fermion<br />Determinant. In Frommer, A., Lippert, T., Medeke, B.,<br />and Schilling, K. (eds.), Numerical Challenges in Lat-<br />tice Quantum Chromodynamics, volume 15 of Lecture<br />Notes in Computational Science and Engineering, pp.<br />142–152. Springer Berlin Heidelberg, 2000.<br />Lyne, A.-M., Girolami, M., Atchade, Y., Strathmann, H.,<br />and Simpson, D. Playing Russian Roulette with In-<br />tractable Likelihoods, July 2014. arXiv:1306.4032.<br />Maclaurin, D. and Adams, R. P.<br />Exact MCMC with Subsets of Data, March 2014.<br />arXiv:1403.5693.<br />Firefly Monte Carlo:<br />Moore, A. The Anchors Hierarchy: Using the Triangle In-<br />equality to SurviveHigh-DimensionalData. In Proceed-<br />ings of the Twelfth Conference on Uncertainty in Artifi-<br />cial Intelligence, pp. 397–405. AAAI Press, 2000.<br />Murray, I. Gaussian processes and fast matrix-vector mul-<br />tiplies, 2009. Presented at the Numerical Mathematics<br />in Machine Learning workshop at the 26th International<br />Conference on Machine Learning (ICML 2009), Mon-<br />treal, Canada.<br />Murray, I. and Adams, R. P. Slice sampling covariance<br />hyperparameters of latent Gaussian models.<br />ferty, J. D., Williams, C. K. I., Shawe-Taylor, J., Zemel,<br />R. S., and Culotta, A. (eds.), Advances in Neural Infor-<br />mation Processing Systems 23: 24th Annual Conference<br />on Neural Information Processing Systems 2010. Pro-<br />ceedings of a meeting held 6-9 December 2010, Vancou-<br />ver, British Columbia, Canada., pp. 1732–1740. Curran<br />Associates, Inc., 2010.<br />In Laf-<br />Neal, R. M. Regression and classification using Gaussian<br />process priors (with discussion). Bayesian Statistics, 6:<br />475–501, 1999.<br />Rasmussen, C. E. and Williams, C. Gaussian Processes for<br />Machine Learning. MIT Press, 2006.<br />Robbins, H. and Monro, S. A Stochastic Approximation<br />Method. The Annals of MathematicalStatistics, 22:400–<br />407, 1951.<br />Rue, H., Martino, S., and Chopin, N.<br />Bayesian inference for latent Gaussian models by using<br />Approximate<br />integratednestedLaplaceapproximations.Journalofthe<br />Royal Statistical Society: Series B (Statistical Methodol-<br />ogy), 71(2):319–392,2009.<br />Saatc ¸i, Y. Scalable Inference for Structured Gaussian Pro-<br />cess Models. PhD thesis, University of Cambridge,<br />2011.<br />Seeger, M.<br />Technical report, Institute for ANC, Edinburgh, UK,<br />2000.<br />Skilling techniques for Bayesian analysis.<br />Simpson, D. P., Turner, I. W., Strickland, C. M., and<br />Pettitt, A. N. Scalable iterative methods for sampling<br />frommassiveGaussianrandomvectors,December2013.<br />arXiv:1312.1476.<br />Skilling, J. Bayesian NumericalAnalysis. In Grandy,W. T.<br />and Milonni, P. W. (eds.), Physics and Probability, pp.<br />207–222.CambridgeUniversityPress, 1993. Cambridge<br />Books Online.<br />Srinivasan, B. V., Hu, Q., Gumerov, N. A., Murtugudde,<br />R., and Duraiswami, R. Preconditioned Krylov solvers<br />for kernel regression, August 2014. arXiv:1408.1237.<br />Stein, M. L., Chen, J., and Anitescu, M. Stochastic approx-<br />imation of score functions for Gaussian processes. The<br />Annals of Applied Statistics, 7(2):1162–1191,2013.<br />Taylor, M. B. and Diggle, J. P. INLA or MCMC? A Tu-<br />torial and Comparative Evaluation for Spatial Prediction<br />in log-Gaussian Cox Processes, 2012. arXiv:1202.1738.<br />Titsias, M.K. VariationalLearningofInducingVariablesin<br />Sparse Gaussian Processes. In Dyk, D. A. and Welling,<br />M. (eds.), Proceedings of the Twelfth International Con-<br />ference on Artificial Intelligence and Statistics, AISTATS<br />2009, Clearwater Beach, Florida, USA, April 16-18,<br />2009, volume 5 of JMLR Proceedings, pp. 567–574.<br />JMLR.org, 2009.<br />Welling, M. andTeh, Y. W. BayesianLearningviaStochas-<br />tic Gradient Langevin Dynamics.<br />Scheffer, T. (eds.), Proceedings of the 28th International<br />ConferenceonMachineLearning,ICML 2011,Bellevue,<br />Washington, USA, June 28 - July 2, 2011, pp. 681–688.<br />Omnipress, 2011.<br />In Getoor, L. and<br />Williams, C. K. I. and Barber, D. Bayesian classification<br />with Gaussian processes. IEEE Transactions on Pat-<br />tern Analysis and Machine Intelligence, 20:1342–1351,<br />1998.<br />Williams, C. K. I. and Rasmussen, C. E. Gaussian Pro-<br />cesses for Regression. In Touretzky, D. S., Mozer,<br />M., and Hasselmo, M. E. (eds.), Advances in Neural<br />Information Processing Systems 8, NIPS, Denver, CO,<br />November 27-30, 1995, pp. 514–520. MIT Press, 1995.</p>  <a href="https://www.researchgate.net/profile/Maurizio_Filippone/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE/links/54c76f2e0cf289f0cecd23d5.pdf">Download full-text</a> </div> <div id="rgw18_56aba0b5cfaf0" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw19_56aba0b5cfaf0">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw20_56aba0b5cfaf0"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="profile/Maurizio_Filippone/publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE/links/54c76f2e0cf289f0cecd23d5.pdf" class="publication-viewer" title="ulisse15.pdf">ulisse15.pdf</a> </div>  <div class="details">  <span> Available from <a href="profile/Maurizio_Filippone">Maurizio Filippone</a> &middot; Jan 27, 2015 </span>   </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw22_56aba0b5cfaf0" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw23_56aba0b5cfaf0">  </ul> </div> </div>   <div id="rgw14_56aba0b5cfaf0" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw15_56aba0b5cfaf0"> <div> <h5> <a href="publication/265578442_Multi-class_inference_with_Gaussian_processes" class="color-inherit ga-similar-publication-title"><span class="publication-title">Multi-class inference with Gaussian processes</span></a>  </h5>  <div class="authors"> <a href="researcher/2054028011_Botond_Cseke" class="authors ga-similar-publication-author">Botond Cseke</a>, <a href="researcher/2053996059_Lehel_Csato" class="authors ga-similar-publication-author">Lehel Csató</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw16_56aba0b5cfaf0"> <div> <h5> <a href="publication/220380399_A_New_and_Faster_Gaussian_Elimination_Based_Fault_Tolerant_Systolic_Linear_System_Solver" class="color-inherit ga-similar-publication-title"><span class="publication-title">A New and Faster Gaussian Elimination Based Fault Tolerant Systolic Linear System Solver</span></a>  </h5>  <div class="authors"> <a href="researcher/71125121_K_Bhuvaneswari" class="authors ga-similar-publication-author">K. Bhuvaneswari</a>, <a href="researcher/7163068_K_N_Balasubramanya_Murthy" class="authors ga-similar-publication-author">K. N. Balasubramanya Murthy</a>, <a href="researcher/7579434_C_Siva_Ram_Murthy" class="authors ga-similar-publication-author">C. Siva Ram Murthy</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw17_56aba0b5cfaf0"> <div> <h5> <a href="publication/271418556_A_parallel_sparse_linear_system_solver_for_large-scale_circuit_simulation_based_on_Schur_Complement" class="color-inherit ga-similar-publication-title"><span class="publication-title">A parallel sparse linear system solver for large-scale circuit simulation based on Schur Complement</span></a>  </h5>  <div class="authors"> <a href="researcher/70221778_Liuxi_Qian" class="authors ga-similar-publication-author">Liuxi Qian</a>, <a href="researcher/8430652_Dian_Zhou" class="authors ga-similar-publication-author">Dian Zhou</a>, <a href="researcher/8430654_Xuan_Zeng" class="authors ga-similar-publication-author">Xuan Zeng</a>, <a href="researcher/2035734320_Fan_Yang" class="authors ga-similar-publication-author">Fan Yang</a>, <a href="researcher/2059971487_Shengguo_Wang" class="authors ga-similar-publication-author">Shengguo Wang</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw36_56aba0b5cfaf0" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw37_56aba0b5cfaf0">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw38_56aba0b5cfaf0" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=ElozmA0UOMBGTOZkm5G1bLH5s6yLnJ_aXEJh2Fy2R8n1hhuKcI1c0m5xyq7DSLcq" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="bxz+JiXaH39dd66ZiVgNeUweX0vN69ngr1WuLyGYbUg+SzyT5ZAiYMIYGMEGrmNsUOsNzbeYf+i779L5JmCnqw23UaonymKWJpJyZ1KZVGCcaEkysedYx36GP853CwMH7CLkjEZb8YOg2QxODSMP+cqUYcTrW+FwyFERHw2ECz1tZxQKFy07cMXk31kOkaL0GBJ9H85TwbrSAS9bMfTVmG5f0+jLu1MBk0Fcw1onDEB9kbc7xKC0qdHxfF3NOVU35ND9nVZ0Y/c/tYaN0APPpRrvPQZOjJEk1SWsSD7wuyY="/> <input type="hidden" name="urlAfterLogin" value="publication/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjcxMjE4MzYyX0VuYWJsaW5nX3NjYWxhYmxlX3N0b2NoYXN0aWNfZ3JhZGllbnQtYmFzZWRfaW5mZXJlbmNlX2Zvcl9HYXVzc2lhbl9wcm9jZXNzZXNfYnlfZW1wbG95aW5nX3RoZV9VbmJpYXNlZF9MSW5lYXJfU3lzdGVtX1NvbHZFcl9VTElTU0U%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjcxMjE4MzYyX0VuYWJsaW5nX3NjYWxhYmxlX3N0b2NoYXN0aWNfZ3JhZGllbnQtYmFzZWRfaW5mZXJlbmNlX2Zvcl9HYXVzc2lhbl9wcm9jZXNzZXNfYnlfZW1wbG95aW5nX3RoZV9VbmJpYXNlZF9MSW5lYXJfU3lzdGVtX1NvbHZFcl9VTElTU0U%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjcxMjE4MzYyX0VuYWJsaW5nX3NjYWxhYmxlX3N0b2NoYXN0aWNfZ3JhZGllbnQtYmFzZWRfaW5mZXJlbmNlX2Zvcl9HYXVzc2lhbl9wcm9jZXNzZXNfYnlfZW1wbG95aW5nX3RoZV9VbmJpYXNlZF9MSW5lYXJfU3lzdGVtX1NvbHZFcl9VTElTU0U%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw39_56aba0b5cfaf0"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 518;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile","css-pow-publicliterature-FollowPublicationPromo","css-pow-application-PdfJsReader","css-pow-publicliterature-PublicationInlineReader"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"fullname":"Maurizio Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m","profileStats":[],"profileFollowButton":null,"profileReputationScoreNumber":null,"profileUrl":"profile\/Maurizio_Filippone","institution":null,"institutionUrl":false,"widgetId":"rgw4_56aba0b5cfaf0"},"id":"rgw4_56aba0b5cfaf0","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileBadge.html?accountId=4709876","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"widgetId":"rgw3_56aba0b5cfaf0"},"id":"rgw3_56aba0b5cfaf0","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=271218362","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":271218362,"title":"Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"","publicationDate":"01\/2015;","publicationDateRobot":"2015-01","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1501.05427","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)"},{"key":"rft.date","value":"2015"},{"key":"rft.au","value":"Maurizio Filippone,Raphael Engler"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw6_56aba0b5cfaf0"},"id":"rgw6_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=271218362","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":271218362,"peopleItems":[{"data":{"authorNameOnPublication":"Maurizio Filippone","accountUrl":"profile\/Maurizio_Filippone","accountKey":"Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_m","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Maurizio Filippone","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":false}},"professionalInstitutionName":false,"professionalInstitutionUrl":false,"url":"profile\/Maurizio_Filippone","imageUrl":"https:\/\/i1.rgstatic.net\/ii\/profile.image\/AS%3A273666076311560%401442258485613_l","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":false,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":true,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"accountKey":"Maurizio_Filippone","hasInfoPopup":false,"hasTeaserPopup":true,"showContactAuthorButton":true,"widgetId":"rgw9_56aba0b5cfaf0"},"id":"rgw9_56aba0b5cfaf0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAccountItem.html?entityId=4709876&enableUnfollow=0&imageSize=l&enableFollowButton=0&showContactAuthorButton=1","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"accountInstitution":false,"score":null,"largeTooltip":false,"useRebrandedImageStyle":null,"authorCount":2,"accountCount":1,"publicationUid":271218362,"widgetId":"rgw8_56aba0b5cfaf0"},"id":"rgw8_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAccountItem.html","templateExtensions":[],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAccountItem.html?accountId=4709876&context=pubdetail_authors_xflw&showContactAuthorButton=1&authorCount=2&accountCount=1&publicationUid=271218362","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/2064343467_Raphael_Engler","authorNameOnPublication":"Raphael Engler","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Raphael Engler","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2064343467_Raphael_Engler","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56aba0b5cfaf0"},"id":"rgw11_56aba0b5cfaf0","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2064343467&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56aba0b5cfaf0"},"id":"rgw10_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2064343467&authorNameOnPublication=Raphael%20Engler","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw7_56aba0b5cfaf0"},"id":"rgw7_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=271218362&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":271218362,"abstract":"<noscript><\/noscript><div>In applications of Gaussian processes where quantification of uncertainty is<br \/>\nof primary interest, it is necessary to accurately characterize the posterior<br \/>\ndistribution over covariance parameters. This paper proposes stochastic<br \/>\ngradient-based inference to draw samples from the posterior distribution over<br \/>\ncovariance parameters with negligible bias and without the need to compute the<br \/>\nmarginal likelihood. In Gaussian process regression, this has the enormous<br \/>\nadvantage that stochastic gradients can be computed by solving linear systems<br \/>\nonly. A novel unbiased linear systems solver based on parallelizable covariance<br \/>\nmatrix-vector products is developed to accelerate the unbiased estimation of<br \/>\ngradients. The results demonstrate the possibility to enable scalable and exact<br \/>\n(in a Monte Carlo sense) quantification of uncertainty in Gaussian processes<br \/>\nwithout imposing any special structures on the covariance or reducing the<br \/>\nnumber of input vectors.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":false,"widgetId":"rgw12_56aba0b5cfaf0"},"id":"rgw12_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=271218362","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":{"data":{"widgetId":"rgw13_56aba0b5cfaf0"},"id":"rgw13_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/FollowPublicationPromo.html","templateExtensions":[],"attrs":{"context":null,"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.FollowPublicationPromo.html","viewClass":null,"yuiModules":["css-pow-publicliterature-FollowPublicationPromo"],"stylesheets":["pow\/publicliterature\/FollowPublicationPromo.css"],"_isYUI":true},"widgetId":"rgw5_56aba0b5cfaf0"},"id":"rgw5_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=271218362&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=1","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2054028011,"url":"researcher\/2054028011_Botond_Cseke","fullname":"Botond Cseke","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2053996059,"url":"researcher\/2053996059_Lehel_Csato","fullname":"Lehel Csat\u00f3","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2005","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/265578442_Multi-class_inference_with_Gaussian_processes","usePlainButton":true,"publicationUid":265578442,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/265578442_Multi-class_inference_with_Gaussian_processes","title":"Multi-class inference with Gaussian processes","displayTitleAsLink":true,"authors":[{"id":2054028011,"url":"researcher\/2054028011_Botond_Cseke","fullname":"Botond Cseke","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2053996059,"url":"researcher\/2053996059_Lehel_Csato","fullname":"Lehel Csat\u00f3","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/265578442_Multi-class_inference_with_Gaussian_processes","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/265578442_Multi-class_inference_with_Gaussian_processes\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw15_56aba0b5cfaf0"},"id":"rgw15_56aba0b5cfaf0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=265578442","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":71125121,"url":"researcher\/71125121_K_Bhuvaneswari","fullname":"K. Bhuvaneswari","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7163068,"url":"researcher\/7163068_K_N_Balasubramanya_Murthy","fullname":"K. N. Balasubramanya Murthy","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7579434,"url":"researcher\/7579434_C_Siva_Ram_Murthy","fullname":"C. Siva Ram Murthy","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Aug 1997","journal":"Journal of Parallel and Distributed Computing","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/220380399_A_New_and_Faster_Gaussian_Elimination_Based_Fault_Tolerant_Systolic_Linear_System_Solver","usePlainButton":true,"publicationUid":220380399,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"1.18","url":"publication\/220380399_A_New_and_Faster_Gaussian_Elimination_Based_Fault_Tolerant_Systolic_Linear_System_Solver","title":"A New and Faster Gaussian Elimination Based Fault Tolerant Systolic Linear System Solver","displayTitleAsLink":true,"authors":[{"id":71125121,"url":"researcher\/71125121_K_Bhuvaneswari","fullname":"K. Bhuvaneswari","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7163068,"url":"researcher\/7163068_K_N_Balasubramanya_Murthy","fullname":"K. N. Balasubramanya Murthy","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":7579434,"url":"researcher\/7579434_C_Siva_Ram_Murthy","fullname":"C. Siva Ram Murthy","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Journal of Parallel and Distributed Computing 08\/1997; 44(2):107-122. DOI:10.1006\/jpdc.1997.1352"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/220380399_A_New_and_Faster_Gaussian_Elimination_Based_Fault_Tolerant_Systolic_Linear_System_Solver","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/220380399_A_New_and_Faster_Gaussian_Elimination_Based_Fault_Tolerant_Systolic_Linear_System_Solver\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw16_56aba0b5cfaf0"},"id":"rgw16_56aba0b5cfaf0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=220380399","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":70221778,"url":"researcher\/70221778_Liuxi_Qian","fullname":"Liuxi Qian","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8430652,"url":"researcher\/8430652_Dian_Zhou","fullname":"Dian Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8430654,"url":"researcher\/8430654_Xuan_Zeng","fullname":"Xuan Zeng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":2035734320,"url":"researcher\/2035734320_Fan_Yang","fullname":"Fan Yang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Conference Paper","publicationDate":"Oct 2013","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/271418556_A_parallel_sparse_linear_system_solver_for_large-scale_circuit_simulation_based_on_Schur_Complement","usePlainButton":true,"publicationUid":271418556,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/271418556_A_parallel_sparse_linear_system_solver_for_large-scale_circuit_simulation_based_on_Schur_Complement","title":"A parallel sparse linear system solver for large-scale circuit simulation based on Schur Complement","displayTitleAsLink":true,"authors":[{"id":70221778,"url":"researcher\/70221778_Liuxi_Qian","fullname":"Liuxi Qian","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8430652,"url":"researcher\/8430652_Dian_Zhou","fullname":"Dian Zhou","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":8430654,"url":"researcher\/8430654_Xuan_Zeng","fullname":"Xuan Zeng","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2035734320,"url":"researcher\/2035734320_Fan_Yang","fullname":"Fan Yang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2059971487,"url":"researcher\/2059971487_Shengguo_Wang","fullname":"Shengguo Wang","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["2013 IEEE 10th International Conference on ASIC (ASICON 2013); 10\/2013"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Conference Paper","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/271418556_A_parallel_sparse_linear_system_solver_for_large-scale_circuit_simulation_based_on_Schur_Complement","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/271418556_A_parallel_sparse_linear_system_solver_for_large-scale_circuit_simulation_based_on_Schur_Complement\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw17_56aba0b5cfaf0"},"id":"rgw17_56aba0b5cfaf0","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=271418556","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw14_56aba0b5cfaf0"},"id":"rgw14_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=271218362&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":271218362,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":271218362,"publicationType":"article","linkId":"54c76f2e0cf289f0cecd23d5","fileName":"ulisse15.pdf","fileUrl":"profile\/Maurizio_Filippone\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5.pdf","name":"Maurizio Filippone","nameUrl":"profile\/Maurizio_Filippone","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":true,"uploadDate":"Jan 27, 2015","fileSize":"476.64 KB","widgetId":"rgw20_56aba0b5cfaf0"},"id":"rgw20_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=271218362&linkId=54c76f2e0cf289f0cecd23d5&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw19_56aba0b5cfaf0"},"id":"rgw19_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=271218362&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":31,"valueFormatted":"31","widgetId":"rgw21_56aba0b5cfaf0"},"id":"rgw21_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=271218362","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw18_56aba0b5cfaf0"},"id":"rgw18_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=271218362&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":271218362,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw23_56aba0b5cfaf0"},"id":"rgw23_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=271218362&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":31,"valueFormatted":"31","widgetId":"rgw24_56aba0b5cfaf0"},"id":"rgw24_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=271218362","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw22_56aba0b5cfaf0"},"id":"rgw22_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=271218362&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"arXiv:1501.05427v1  [stat.ME]  22 Jan 2015\nEnabling scalable stochastic gradient-based inference for Gaussian processes\nby employing the Unbiased LInear System SolvEr (ULISSE)\nMaurizio Filippone\nSchool of Computing Science, University of Glasgow, UK\nMAURIZIO.FILIPPONE@GLASGOW.AC.UK\nRaphael Engler\nSchool of Computing Science, University of Glasgow, UK\nRAPHAEL.ENGLER@WEB.DE\nAbstract\nIn applications of Gaussian processes where\nquantification of uncertainty is of primary in-\nterest, it is necessary to accurately characterize\nthe posterior distribution over covariance param-\neters. This paper proposes stochastic gradient-\nbased inference to draw samples from the poste-\nrior distribution over covariance parameters with\nnegligible bias and without the need to compute\nthe marginal likelihood. In Gaussian process re-\ngression, this has the enormous advantage that\nstochastic gradients can be computed by solving\nlinear systems only. A novel unbiased linear sys-\ntems solver based on parallelizable covariance\nmatrix-vector products is developed to acceler-\nate the unbiased estimation of gradients. The re-\nsults demonstrate the possibility to enable scal-\nable and exact (in a Monte Carlo sense) quantifi-\ncation of uncertaintyin Gaussian processes with-\noutimposinganyspecialstructuresonthecovari-\nance or reducing the number of input vectors.\n1. Introduction\nProbabilistic kernel machines based on Gaussian Pro-\ncesses (GPs) (Rasmussen & Williams, 2006) are popular\nin a number of applied domains as they offer the possi-\nbility to flexibly model complex data and, depending on\nthe choice of covariance function, to gain some under-\nstanding on the underlying behavior of the system under\nstudy. When quantification of uncertainty is of primary\ninterest, it is necessary to accurately characterize the pos-\nterior distribution over covariance parameters. This has\nbeen argued in a number of papers where this is done by\nmeans of Markov chain Monte Carlo (MCMC) methods\n(Williams & Rasmussen, 1995; Williams & Barber, 1998;\nNeal, 1999; Murray & Adams, 2010; Taylor & Diggle,\n2012; Filippone et al., 2013; Filippone & Girolami, 2014).\nThelimitation ofMCMC approachesto drawsamples from\nthe posteriordistributionovercovarianceparametersis that\nthey need to compute the marginal likelihood at every it-\neration. In GP regression, a standard way to compute\nthe marginal likelihood involves storing and factorizing an\nn\u00d7n matrix, leadingto O(n3) time and O(n2) space com-\nplexities, where n is the size of the data set. For large\ndata sets this quickly becomes unfeasible, so a large num-\nber of contributions can be found in the literature on how\nto make these calculations tractable. For example, when\nthe GP covariance matrix has some particular properties,\ne.g., it has sparse inverse (Rue et al., 2009; Simpson et al.,\n2013; Lyne et al., 2014), it is computedon regularlyspaced\ninputs (Saatc \u00b8i, 2011), it is computed on univariate in-\nputs (Gilboa et al., 2013), it is possible to considerably re-\nduce the complexity in computing the marginal likelihood.\nWhen these properties do not hold, which is common in\nseveralMachineLearningapplications, approximationsare\nusually employed. Some examples involve the use subsets\nof the data (Candela & Rasmussen, 2005), the determina-\ntion of a small number of surrogate input vectors that rep-\nresent the full set of inputs (Titsias, 2009; Hensman et al.,\n2013), and the application of GPs to subsets of the data\nobtained by partitioning the input space (Gramacy et al.,\n2004), to name a few. Unfortunately, it is difficult to as-\nsess to what extent approximationsaffect the quantification\nof uncertainty in predictions, although some interesting re-\nsults in this directionarereportedin(Banerjee et al., 2013).\nThe focus of this paper are applications of GP regression\nwhere the structure of the covariance matrix is not neces-\nsarily special and quantification of uncertainty is of pri-\nmary interest, so that approximations should be avoided.\nThis paper proposes the use of Stochastic GRadient-based\nINference (S-GRIN) (Welling & Teh, 2011) to draw sam-\nples from the posterior distribution over GP covariance pa-\nrameters. S-GRIN does not require the computation of\nthe marginal likelihood and yields samples from the pos-\nterior distribution of interest with negligible bias. This"},{"page":2,"text":"has the enormous advantage that stochastic gradients can\nbe computed by solving linear systems only (Gibbs, 1997;\nGibbs & MacKay, 1997; Stein et al., 2013). Linear sys-\ntems can be solved by means of iterative methods, such as\nthe Conjugate Gradient (CG) algorithm, that are based on\nparallelizable covariance matrix-vector products (Higham,\n2008; Skilling, 1993; Seeger, 2000). Similar ideas were\npreviously put forward to optimize GP covariance param-\neters (Chen et al., 2011; Anitescu et al., 2012; Stein et al.,\n2013). Despite the O(n2) in time and O(n) in space com-\nplexities of these methods compare well with the O(n3)\nin time and O(n2) in space complexities of traditional\nMCMC-based inference, solving dense linear systems at\neach iteration makes the whole inference framework too\nslow to be of practical use. We compare a number of\nstandard ways to speed up the solution of dense linear\nsystems, such as fast covariance matrix-vector products\n(Gray & Moore, 2000; Moore, 2000) and preconditioning\n(Srinivasan et al., 2014), and in line with what argued in\n(Murray, 2009), we observe that they yield little gain in\ncomputational speed compared to the standard CG algo-\nrithm. In order to enable practical inference for GPs ap-\nplied to large data sets, we therefore developed an Unbi-\nased LInear Systems SolvEr (ULISSE) that essentially al-\nlows the CG algorithm to stop early while retaining unbi-\nasedness of the solution.\nWe highlight here that (i) in (Welling & Teh, 2011), an\nunbiased estimate of the gradient is computed by consid-\nering small batches of data. Recent alternative contribu-\ntions on scaling Bayesian inference by analyzing small\nbatches of data can be found in (Banterle et al., 2014;\nMaclaurin & Adams, 2014). GPs do not lend themselves\nto this treatment, due to the covariancestructuremaking all\ndata dependenton one another. (ii) ULISSE is complemen-\ntaryto recentapproachesin thearea ofprobabilisticnumer-\nics that aim at infering, rather than computing, solutions to\nlinearsystems(Hennig, 2014). (iii) Theproposedinference\nmethod is based on \u201cnoisy\u201d gradients and is complemen-\ntary to recent inference approaches based on noisy likeli-\nhoods (Lyne et al., 2014; Filippone, 2014). In GP regres-\nsion, iterative methods akin to the CG algorithm (Higham,\n2008) can be employed to obtain an unbiased estimate of\nthe log-determinant of the covariance matrix, but this re-\nmains an extremely onerous calculation needed to get an\nunbiased estimate of the log-marginallikelihood. A further\nand perhaps more challengingissue is transformingthe un-\nbiased estimate of the log-marginal likelihood in an unbi-\nased estimate of the marginal likelihood (Kennedy & Kuti,\n1985; Liu, 2000; Lyne et al., 2014).\nThis paper demonstrates that employingULISSE within S-\nGRIN makes it possible to accurately carry out inference\nof covarianceparameters in GPs and effectively scale these\ncomputations to large data sets. We report results on a\ndata set with about 23 thousandinputvectors where we can\ndraw ten thousand samples per day from the posterior dis-\ntribution over covariance parameters on a desktop machine\nwith standard hardware1. To the best of our knowledge,\nthis paper reports the first real attempt to enable full quan-\ntification of uncertainty of covariance parameters of GPs\nwithout the use of deterministic approximations to reduce\nthe number of input vectors and without imposing sparsity\non the GP covariance or its inverse.\nThepaperis organizedas follows. Section2 brieflyreviews\nGPs and motivates the adoption of S-GRIN to infer GP co-\nvariance parameters. Section 3 describes and evaluates the\nCG algorithm to solve linear systems and some variants\nbased on fast covariancematrix-vectorproductandprecon-\nditioning. Section 4 presents ULISSE and its use to obtain\nan unbiased estimate of the gradient of the log-marginal\nlikelihood in GPs. Section 5 demonstrates the ability of the\nproposed methodology to accurately infer covariance pa-\nrameters in GPs and its scalability to a large data set where\ntraditional inference methods cannot be applied. Finally,\nSection 6 draws the conclusions.\n2. Inference of covariance parameters in GPs\nIn GP regression, a set of continuous labels y\n{y1,...,yn} is associated with a set of input vectors X =\n{x1,...,xn}. Throughout the paper, we will employ zero\nmean GPs with the following covariance function:\n=\nk(xi,xj) = \u03c3exp?\u03c4?xi\u2212 xj?2?+ \u03bb\u03b4ij\nwith \u03b4ij = 1 if i = j and zero otherwise. The parameter\n\u03c4 determines the rate of decay of the covariance function,\nwhereas \u03c3 represents the marginal variance of each Gaus-\nsian random variable comprising the GP. The parameter \u03bb\nis the variance of the (Gaussian) noise in the labels.\n(1)\nLet K be the covariance matrix with Kij= k(xi,xj) and\ndenote by \u03b8 the vector comprising all parameters of the\ncovariance matrix K, namely \u03b8 = (\u03c3,\u03c4,\u03bb).\nIn a Bayesian sense, we would like to carryany uncertainty\nin parameters estimates forward to predictions over the la-\nbel y\u2217for a new input vector x\u2217. In particular, this would\namountin integratingouthyper-parametersfromthe model\nin order to compute the predictive distribution:\np(y\u2217|y,X,x\u2217) =\n?\np(y\u2217|y,\u03b8,X,x\u2217)p(\u03b8|y,X)d\u03b8. (2)\nThis shows that in order to compute the predictive dis-\ntribution for new input vectors, we need to characterize\nthe posterior distribution over the covariance parameters\n1Code to reproduce all the results can be found here:\nwww.dcs.gla.ac.uk\/\u02dcmaurizio\/pages\/code.html"},{"page":3,"text":"p(\u03b8|y,X). The integral above is analytically intractable,\nso it is necessary to resort to some approximations.\nA standard way to tackle this intractability is to draw sam-\nples from p(\u03b8|y,X) using MCMC methods, and approxi-\nmate the integral with the Monte Carlo estimate\np(y\u2217|y,X,x\u2217) \u2243\n1\nN\nN\n?\ni=1\np(y\u2217|\u03b8(i),X,x\u2217),\n(3)\nwhere \u03b8(i)denotes the ith of a set of samples from\np(\u03b8|y,X). Drawing samples from the posterior distribu-\ntion can be done using several MCMC algorithms that es-\nsentially are based on a proposal mechanism and on an\naccept\/reject step that requires the evaluation of the log-\nmarginal likelihood:\nlog[p(y|\u03b8,X)] = \u22121\n2log(|K|) \u22121\n2yTK\u22121y + const.\n(4)\nA standard way to proceed, is to factorize the covari-\nance matrix K = LLTusing the Cholesky algorithm\n(Golub & Van Loan, 1996). The factorization costs O(n3)\noperations and requires the storage of O(n2) entries of\nthe covariance matrix, but after that computing the log-\ndeterminant and the inverse of K multiplied by y can be\ndone using O(n2) operations.\nThe computational complexities above pose a constraint in\nthe scalability of GPs to large data sets. Iterative methods\nbased on covariancematrix-vectorproducts(CMVPs) have\nbeen proposed to obtain an unbiased estimate of the log-\nmarginal likelihood. Even though these methods scale in\nO(n2) in time and O(n) in space, they are of little practical\nuse in the task of sampling from p(\u03b8|y,X), as the number\nof iterations needed to estimate the log-determinant term\ncan be prohibitively large. We now illustrate our proposal\nto obtain samples fromthe posterior distributionp(\u03b8|y,X)\nwith negligible bias and avoiding having to estimate log-\ndeterminants and marginal likelihoods.\n2.1. Stochastic GRadient-based INference (S-GRIN)\nWe\n(Welling & Teh, 2011) to obtain samples from p(\u03b8|y,X).\nThe idea behindS-GRIN is to modifythe standard stochas-\ntic gradient optimization algorithm (Robbins & Monro,\n1951) by injecting Gaussian noise in a way that ensures\ntransition into a Langevin dynamics phase yielding\nsamples from the posterior distribution of interest.\nparticular, the proposal of a new set of parameters is\n\u03b8t+1= \u03b8t+\u03b5t\n2M {\u02dc gi+ \u2207\u03b8log[p(\u03b8)]} + \u03b7t\nwith \u03b7t\u223c N(\u03b7t|0,\u03b5tM). The term in curly brackets is\nan unbiased estimate of log[p(\u03b8|y,X)]. We have also in-\ntroduced a preconditioning matrix M that can be chosen\nbriefly describe theS-GRINalgorithmin\nIn\n(5)\nto improve convergence of S-GRIN. The update equation,\nexcept for \u03b7t, is the standard update used in stochastic gra-\ndient optimization. The parameters \u03b5tare chosen to satisfy\nthe standard conditions\n\u221e\n?\nt=1\n\u03b5t= \u221e\nand\n\u221e\n?\nt=1\n\u03b52\nt< \u221e\n(6)\nas these, along with some other technical assumptions,\nguarantee convergence to a local maximum. In this work,\nwe define \u03b5t = a(b + t)\u2212\u03b3with \u03b3 = 1. The injected\nnoise \u03b7tis Gaussian with covariance \u03b5tM ensuring that\nthe algorithm transitions into a discretized version of a\nLangevin dynamics with target distribution given by the\nposterior over \u03b8. In principle, it would be necessary to ac-\nceptorrejecttheproposals,whichwouldrequireevaluating\nthe marginal likelihood. The key result in (Welling & Teh,\n2011) is that when S-GRIN reaches the Langevin dynam-\nics phase, the step-size \u03b5tis small enough that the accep-\ntance rate would be almost one. Therefore, in this phase\nwe can accept all proposals without the need to evaluate\nthe marginal likelihood at the cost of introducinga negligi-\nble amount of bias.\nFollowing (Welling & Teh, 2011), we can estimate when\nthe algorithm reaches the Langevin dynamics phase by\nmonitoring the ratio between the variance of the stochastic\ngradients and the variance of the injected noise. Defining\nV to be the sampling covariance of the stochastic gradi-\nents and denoting with \u03bbmax(A) the largest eigenvalue of a\nmatrix A, we can write such a ratio as\n\u03b5t\n4\u03bbmax\n?\nM\n1\n2V M\n1\n2\n?\n(7)\nWhen this ratio is small enough the algorithm is in its\nLangevin dynamics phase and produces samples from the\nposterior distribution over \u03b8.\nThe motivation for employing S-GRIN for inference of GP\ncovariance parameters comes from inspecting the gradient\nof the log-marginal likelihood that has components\ngi= \u22121\n2Tr\n?\nK\u22121\u2202K\n\u2202\u03b8i\n?\n+ yTK\u22121\u2202K\n\u2202\u03b8iK\u22121y\n(8)\nComputingthe gi\u2019s requiresagainoperationsthathavetime\ncomplexity in O(n3) due to the trace term and the linear\nsystem K\u22121y. However, we can introduce Nrvectors r(i)\nwith componentsdrawn from{\u22121,1}with probability1\/2\nand unbiasedly estimate the trace term obtaining (Gibbs,\n1997):\n\u02dc gi= \u2212\n1\n2Nr\nNr\n?\ni=1\nr(i)TK\u22121\u2202K\n\u2202\u03b8ir(i)+ yTK\u22121\u2202K\n\u2202\u03b8iK\u22121y\n(9)"},{"page":4,"text":"Algorithm 1 The Conjugate Gradient algorithm\nInput: data X, vectorb, convergencethreshold \u01eb, initial\nvector s0, maximum number of iterations T\ne0= b \u2212 Ks0;\nfor i = 0 to T do\neT\niei\ndT\nsi+1= si+ \u03b1idi;\nei+1= ei\u2212 \u03b1iKdi;\nif ?ei+1? < \u01eb then\nreturn s = si+1;\nend if\n\u03b2i=eT\neT\niei\ndi+1= ei+1+ \u03b2idi;\nend for\nd0= e0;\n\u03b1i=\niKdi;\ni+1ei+1\n;\nWe can easily verify that the expectation of each\nof the terms in the sum is E[r(i)TK\u22121 \u2202K\n\u2202\u03b8iE[r(i)r(i)T] which is the trace term in eq. 8 given\nthat E[r(i)r(i)T] = I. Hence, in order to compute an unbi-\nased version of the gradient of the log-marginal likelihood\nwe need to solve one linear system for y and one for each\nof the Nrvectors r(i)used to estimate the trace term. This\nconsideration forms the basis of the proposed methodol-\nogy. Computing an unbiased version of the gradient in-\nvolves solving linear systems only, which is much easier\nand cheaper than estimating log-determinants.\n\u2202\u03b8ir(i)]=\nK\u22121 \u2202K\n3. Solving Linear Systems\nWe have discussed that S-GRIN to infer covariance pa-\nrameters in GPs in general requires solving dense linear\nsystems. In this section, we briefly review the Conjugate\nGradient (CG) algorithm that is a popular method to it-\neratively solve linear systems based on Covariance Ma-\ntrix Vector Product (CMVP) operations (time and space\ncomplexities in O(n2) and O(n), respectively). We also\ndiscuss and evaluate a few variants to speed up com-\nputations\/convergence, such as preconditioning and fast\nCMVPs. Throughout this section we will evaluate the ef-\nfectiveness of these alternatives on a GP regression task\napplied to the Concrete data set from the UCI repository\n(Asuncion & Newman, 2007). The Concrete data set con-\ntainsdataaboutthecompressivestrengthofn = 1030sam-\nples of concrete described by d = 8 features.\n3.1. The Conjugate Gradient (CG) algorithm\nGiven a linear system of the form Ks = b with K and b\ngiven, the CG algorithm (Golub & Van Loan, 1996) yields\nthe solution s without having to invert or factorize the ma-\ntrix K. The idea is to calculate the solution s as the mini-\n02468 1012\nlog10(\u03ba)\na = 1, b = 1\na = 1, b = 0.1\na = 1, b = 0.01\na = 0.5, b = 0.01\nFigure 1. Distribution of the condition number \u03ba of the covari-\nance matrix for different choices of shape and rate parameters of\na Gamma prior on each covariance parameter \u03b8.\nmizer of\n\u03c6(s) =1\n2sTKs \u2212 sTb\n(10)\nwhich can be obtained by employing gradient-based opti-\nmization. The CG algorithm is initialized from an initial\nguess s0. After that, the iterations refine the solution s by\nupdates in directions di. The CG algorithm, in comparison\nwith the standard gradient descent, is characterized by the\nfact that K-orthogonality(or conjugacy with respect to K)\nof the search directions is imposed, namely dT\nwhen i ?= j. This condition yields a sequence of residuals\nei = b \u2212 Ksithat are mutually orthogonal, and guaran-\ntees convergence in at most n iterations. Remarkably, the\nCG algorithm can be implemented in a way that requires a\nsingle CMVP (Kdi) at each iteration (see Algorithm 1).\niKdj = 0\nThe trade-off between accuracy for speed is governed by\nthe threshold \u01eb, which in this paper we set to \u01eb = 10\u22128.\nTheoretically, the CG algorithm is guaranteed to converge\nin at most n iterations, but in practice, due to the repre-\nsentation in finite numerical precision, orthogonalityof the\ndirections can be lost, especially in badly conditioned sys-\ntems, and the CG algorithmcan take more than n iterations\nto converge. The condition number of a matrix is defined\nas the ratio between its largest and smallest eigenvalues:\n\u03ba =\u03bbmax(K)\n\u03bbmin(K)\nFig. 1 shows the distribution of the condition numberwhen\neach covariance parameter \u03b8iis sampled form a Gamma\ndistribution with shape and rate parameters a and b. The\ndistributions are reasonably vague and give a rough idea of\nthetypicalconditionnumbersencounteredduringtheinfer-\nenceofGP covarianceparametersfortheConcretedataset.\nWe can expect slower convergence speed when the condi-\ntion number is large due to numerical instabilities; we are\ninterested in quantifying to what extent this applies to GPs\nand what is the impact of cheap CMVPs and precondition-\ning on convergencespeed. In the remainder of this section,\nwe will consider the problem of solving the linear system\nKs = y that is needed in the calculation of part of the\ngradient in eq. 9. The results pertaining to the solution of\nthe linear systems Ks = r(i)are quite similar (results not\nreported), so for the sake of brevity, we will omit them."},{"page":5,"text":"3.2. Fast CMVPs\nWe consider here the use of two fast CMVPs based on\nefficient representation of input data that we will call\n\u201ckdtree\u201d (Gray & Moore, 2000) and \u201canchors\u201d (Moore,\n2000)2. These methods yield fast CMVPs at the price of\na lower accuracy.\nIn the top row of Fig. 2 we show the number of iterations\nrequired by the CG algorithm to reach convergence versus\nthe condition number and the error in the solution versus\nthe condition number. The error is defined as the norm of\nthe difference between the solution obtained by the CG al-\ngorithm and the one obtained by factorizing K using the\nCholeskyalgorithmandcarryingoutforwardandbacksub-\nstitutions with y. We compare a baseline CG algorithm\nwith CMVPs performed in double precision with CG al-\ngorithms implemented with (i) \u201ckdtree\u201d CMVPs (ii) \u201can-\nchors\u201d CMVPs and (iii) single precision CMVPs. The con-\nvergencethresholdof the CG algorithmwas set to 10\u22128, so\nin order to be able to satisfy this criterion when employing\n\u201ckdtree\u201d and \u201canchors\u201d CMVPs, we selected the relative\nand absolute tolerance parameters to be 10\u221210.\nThe results indicate that double precision calculations lead\nto the lowest number of iterations compared to the other\nmethods, especially when \u03ba is large. Double precision\ncalculations offer the lowest error. Single precision cal-\nculation lead to a very poor error compared to the other\nmethods. The CG algorithm with \u201ckdtree\u201d CMVPs seems\nto take longer to converge than the one with \u201canchor\u201d\nCMVPs, but it achieves a lower error.\nDrawing definitive conclusions on whether fast CMVPs\nyield any gain in computing time is far from trivial, as this\nvery much depends on implementation details and hard-\nware where the code is run. What we can say, however, is\nthat gaining orders of magnitude speed-ups would require\nreducing the accuracy of fast CMVPs, but this would re-\nquire loosening up the convergence criterion in order for\nthe CG algorithm to converge. As a result, we would be\nable to obtain solutions of linear systems faster but at the\ncost of a reduced accuracy in the solution, which in turn\nwould bias the estimation of gradients.\n3.3. Preconditioned CG\nThe Preconditioned CG (PCG) is a variant of the CG algo-\nrithm that aims at mitigating the issues associated with the\nrate of convergence of the CG algorithm when the condi-\ntion number \u03ba is large. A (left) preconditioning matrix J\noperates on the linear system yielding\nJ\u22121Ks = J\u22121b\n2code implementing these methods can be found here:\nwww.cs.ubc.ca\/\u02dcawll\/nbody_methods.html\n0123456\n1\n2\n3\n4\n5\nlog10(\u03ba)\nlog10(number iterations)\ndouble\nfloat\nanchors\nkdtree\n0123456\n\u221215\n\u221210\n\u22125\n0\nlog10(\u03ba)\nlog10(norm residual)\n0123456\n1\n2\n3\n4\nlog10(\u03ba)\nlog10(number iterations)\nCG\nPCG double\nPCG float\n0123456\n\u221214\n\u221210\n\u22126\nlog10(\u03ba)\nlog10(norm residual)\nFigure 2. Top row: Comparison of fast CMVPson number of iter-\nations and error versus condition number. - Bottom row: Compar-\nison of the CG algorithm and two PCG algorithms using double\nand single precision CMVPs to solve inner linear systems.\nThe success of PCG is based on the possibility to construct\nJ so that J\u22121K is well conditioned. This can be achieved\nwhen J\u22121well approximatesK\u22121, and a complicationim-\nmediatelyarisesonhowtodosoforgeneralkernelmatrices\nwithout carrying out expensive operations (in O(n3)).\nIn (Srinivasan et al., 2014) it was proposed to define J =\nK + \u03b4I with \u03b4 > 0. Compared to the standard CG al-\ngorithm, the PCG algorithm introduces the solution of a\nlinear system of the form J\u22121b at each iteration, that can\nbe solved again using the CG algorithm. A large value of\n\u03b4 makes K + \u03b4I well conditioned and makes convergence\nspeed of the inner CG algorithm faster whereas it makes\nJ\u22121and K\u22121quite different leading to the necessity to\nrun the outer CG algorithm for several iterations. For small\nvalues of \u03b4 the situation is reversed, so \u03b4 needs to be tuned\nto find an optimal compromise.\nIn the bottom row of Fig. 2, we compare the standard CG\nalgorithmwith two versions of the PCG algorithmon num-\nber of iterations and accuracy of the solution. In the first\nversionofthePCG algorithmwe useddoubleprecisioncal-\nculations when solvingthe innerlinear systems, whereas in\nthe second version we used single precision calculations.\nIn both versions of the PCG algorithm we set \u03b4 to yield the\nlowest number of iterations in order to show whether it is\npossible to reduce the number of computations.\nThe results show that the standard CG algorithm takes less\niterations to converge than the PCG algorithm (counting"},{"page":6,"text":"both inner and outer iterations). Even in the case of sin-\ngle precision calculations in the inner CG algorithm, we\ndid not experience any gain in computing time due to the\nincreased number of iterations. For other data and in dif-\nferent experimental conditions there might be a compu-\ntational advantage in using a preconditioner, as shown in\n(Srinivasan et al., 2014), but the gain is generally modest.\n4. Unbiased LInear System SolvEr (ULISSE)\nFrom the analysis in the previous sections it is evident that\nnone of the standard ways to speedupcalculations and con-\nvergence of the CG algorithm offers substantial gains in\ncomputing time. As a result, employing iterative meth-\nods as an alternative to traditional factorization techniques\nseems beyond practicality as pointed out, e.g., in (Murray,\n2009). One of the novel contributions of this paper is to\naccelerate the CG algorithm by orders of magnitude at the\nexpenses of obtaining an unbiased estimate of the solution.\nThe idea is to stop the CG algorithm before the conver-\ngence criterion is satisfied and apply some corrections to\nensure unbiasedness of the solution. We note here that our\nproposal can be applied to any of the variants of the CG\nalgorithm presented earlier and to dense as well as sparse\nlinear systems.\nWe canrewritethefinalsolutionofalinearsystemobtained\nby the CG algorithm as a sum of incremental updates\ns = s0+ \u03b41+ ... + \u03b4T\n(11)\nassuming that it takes T iterations to satisfy the conver-\ngence threshold \u01eb. We can define an \u201cearly stop\u201d thresh-\nold \u03b1 > \u01eb that will be reached after l < T iterations, and\nrewrite the final solution by introducing a series of coeffi-\ncients as follows\ns\n=\ns0+\nl\u22121\n?\ni=1\n\u03b4i+\n1\nw0\n?\nw0\u03b4l+0+\n1\nw1\n?\nw0w1\u03b4l+1+\n+1\nw2(w0w1w2\u03b4l+2+ ...)\n??\n(12)\nWe will focuson coefficientsdefinedas wr= exp(\u03b2r), but\nthis choice is not restrictive. We can now obtain an unbi-\nased estimate of the solution of the linear system by setting\n\u02dc s = s0+?l\u22121\ning two steps (i) draw uj \u223c U[0,1] (ii) if uj <\n\u02dc s = \u02dc s +?j\nis clearly s and the rate of decay \u03b2 in the expression of wr\ndetermines the average number of steps that are carried out\nafter early stopping the CG algorithm. The accumulation\nof the terms forming \u02dc s is carried out during the execution\nof the standard CG algorithm.\nFor simplicity, we set the early stop threshold to \u03b1 = q\u221an\nas q gives a rough indication of the average error that we\ni=1\u03b4iand iterate for j = 0,1,... the follow-\n1\nwjthen\nr=0wr\u03b4l+j, else return \u02dc s. The expectation of \u02dc s\n0123456\n0.5\n1.5\n2.5\n3.5\nlog10(\u03ba)\nlog10(number iterations)\nCG\nULISSE q=0.1\nULISSE q=1.0\n0123456\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\nlog10(\u03ba)\nlog10(avg std dev)\n0123456\n0.5\n1.5\n2.5\n3.5\nlog10(\u03ba)\nlog10(number iterations)\nCG\nULISSE q=0.1\nULISSE q=1.0\n0123456\n\u221215\n\u221210\n\u22125\n0\nlog10(\u03ba)\nlog10(avg std dev)\nFigure 3. Comparison of the CG algorithm and ULISSE with\nearly stop thresholds \u03b1 calculated with q = 0.1 and q = 1 on\nnumber of iterations and standard deviation of the solution. The\ntop row corresponds to \u03b2 = 1 in the calculation of the weights\nwr, whereas the bottom row corresponds to \u03b2 = 100.\nare expecting in each element of the solution. In Fig. 3\nwe report number of iterations and average standard devia-\ntion across the elements of the solution. ULISSE with two\ndifferent values of \u03b2 is compared with the baseline CG al-\ngorithm without early stop q = 0. We stress again that the\nerror is such that the solution is unbiased.\n4.1. Variance of the gradient\nWe conclude this section by showing the behavior of\nULISSE in the calculation of an unbiased version of the\ngradient in GPs. Applying the proposed unbiased solver to\nthe first term of \u02dc giin eq. 9 is straightforwardand it requires\nsolving Nrlinear systems, one for each of the r(i)vectors.\nFor the quadratic term in y, instead, we need to obtain two\nindependent unbiased estimates of K\u22121y in order for the\nexpectation of the whole term to be unbiased. This can be\nimplemented by running a single instance of the CG algo-\nrithm and keeping track of two solutions obtained by in-\ndependent draws of the uniform variables ujused to early\nstop the CG algorithm. We remark that the unbiased esti-\nmationofthegradientinvolvesnowtwosourcesofstochas-\nticity: onedueto theintroductionofvectorsr(i)toestimate\nthe trace term in eq. 8, and one due to the proposed way to\nunbiasedly solve linear systems.\nFig. 4 reports the average, taken with respect to 100 repeti-"},{"page":7,"text":"0123456\n1.0\n2.0\n3.0\nlog10(\u03ba)\nlog10(number iterations)\nCG\nULISSE q=0.1\nULISSE q=1.0\n0123456\n\u22128\n\u22126\n\u22124\n\u22122\n0\nlog10(\u03ba)\nlog10(avg rel square norm)\nFigure 4. Comparison of the CG algorithm and ULISSE and early\nstop thresholds computed with q = 0.1 and q = 1 to estimate the\ngradient of the log-marginal likelihood in eq. 9. In ULISSE, the\nweights wr are calculated with \u03b2 = 1.\ntion of the log10of the relative square norm of the error:\n?g(\u03b8) \u2212 \u02dc g(\u03b8)?2\n?g(\u03b8)?2\n(13)\nas a function of the condition number \u03ba. We used one sin-\ngle vector r(1)to estimate the gradient in eq. 9. The figure\nshows a number of interesting facts. The estimate in eq. 9\n(q = 0 in the figure) is quite accurate, as the relative error\nis small in a wide range of values of \u03ba. At the expenses of\na larger variance in the estimate of the gradient that is af-\nfected by the early stop threshold \u03b1, ULISSE yields orders\nof magnitude improvements in the number of iterations.\n5. Experimental validation\nIn this section, we infer covariance parameters of GP re-\ngression models using S-GRIN with ULISSE. We start\nby considering the concrete data set where it is possible\nto compare the results of our proposal and the standard\nMetropolis-Hastings (MH) algorithm.\nstrate the scalability of the proposed methodology by con-\nsidering a data set comprising n = 22,784 input vectors in\nd = 8 dimensions.\nWe then demon-\n5.1. Comparison with MCMC\nWe ran a standard Metropolis-Hastings algorithm for fifty-\nthousanditerationsto the GP regressionmodelwith covari-\nance in eq. 1 applied to the concrete data set. We allowed\nfor an initial adaptive phase to reach an average acceptance\nrate between 0.2 and 0.4, and we discarded the first ten\nthousand iterations in order to estimate the mean and stan-\ndard deviation of the posterior distribution over the three\nparameters. Fig. 5 shows the running mean and the inter-\nval correspondingto plus\/minustwice the runningstandard\ndeviation for the three parameters (solid red lines).\nWe compare the run from the MH algorithm with S-GRIN,\nwhere we made the following design choices. We em-\n010000\nIteration number\n2000030000\n1.0\n2.5\n4.0\nlog(\u03c3)\n0e+00 1e+04\nIteration number\n2e+043e+04\n1.0\n2.5\n4.0\nlog(\u03c3)\nPSRF median\nPSRF 97.5%\n0 10000\nIteration number\n2000030000\n\u22122.9\n\u22122.6\nlog(\u03bb)\n0e+001e+04\nIteration number\n2e+04 3e+04\n1.0\n2.5\n4.0\nlog(\u03bb)\nPSRF median\nPSRF 97.5%\n010000\nIteration number\n2000030000\n\u22123.5\n\u22122.5\nlog(\u03c4)\n0e+001e+04\nIteration number\n2e+043e+04\n1.0\n2.5\n4.0\nlog(\u03c4)\nPSRF median\nPSRF 97.5%\nFigure 5. Concrete data - Left panel: Comparison of MCMC\n(red) and S-GRIN with ULISSE (black) on running mean and\nplus\/minus two standard deviations. The trace of one chain of\nS-GRIN is also shown (gray). - Right panel: Convergence anal-\nysis of S-GRIN with ULISSE reporting the PSRF computed over\nten chains.\nployed ULISSE within the CG algorithm with double pre-\ncision CMVPs. We set the early stop threshold \u03b1 to\u221an\nand the parameter \u03b2 in the computation of the weights wr\nto 1. Stochastic gradients were estimated using Nr = 4\nvectors r(i). We ran S-GRIN for 40 thousanditerations and\nthe \u03b5twere chosen to start from 10\u22121and reduce to 10\u22124\nat the last iteration with \u03b3 = 1. During the execution of S-\nGRIN we monitored the quantity\u03b5t\ndiscussed in section 4, and we froze the value of \u03b5twhen it\nwas less than 0.002; the covariance of the gradients V was\nestimated on batches of 100 iterations. Finally, in order to\nspeed up computations, we decided to redraw the vectors\nr(i)every 20 iterations and to keep them fixed in between.\nThe advantage of this is that the solutions of the linear sys-\ntems Kr(i)can be used to initialize the same systems when\nproposing a new \u03b8 thus speeding up convergence.\n4\u03bbmax\n?\nM\n1\n2V M\n1\n2\n?\nas\nWe set the preconditioning matrix M in S-GRIN as the in-\nverse of the negative Hessian of the log of the posterior\ndensity at its mode computed on a subset of 500 input vec-\ntors, as this is cheap way to obtain a rough idea of the co-\nvariance structure of the posterior distribution for the full\ndata set. In Fig. 5 we report the running statistics for the\nthree parameters (solid black lines), and the trace-plot of\none run of S-GRIN (solid gray lines), where we discarded\nall iterations prior to the freezing of the step-size \u03b5t. The\nfigure shows a striking match between the results obtained\nby a standardMCMC approachand S-GRIN with ULISSE.\nThis demonstrates that our proposal is a valid alternative to\nfaithfully quantify uncertainty in GPs.\nIn orderto checkconvergencespeedof S-GRIN, we ran ten"},{"page":8,"text":"0 5000\nIteration number\n1000015000\n0.85\n1.05\nlog(\u03c3)\n0 5000\nIteration number\n10000 15000\n1.0\n2.5\n4.0\nlog(\u03c3)\nPSRF median\nPSRF 97.5%\n0 5000\nIteration number\n1000015000\n\u22121.80\n\u22121.70\nlog(\u03bb)\n0 5000\nIteration number\n10000 15000\n1.0\n2.5\n4.0\nlog(\u03bb)\nPSRF median\nPSRF 97.5%\n0 5000\nIteration number\n10000 15000\n\u22120.95\n\u22120.75\nlog(\u03c4)\n0 5000\nIteration number\n1000015000\n1.0\n2.5\n4.0\nlog(\u03c4)\nPSRF median\nPSRF 97.5%\nFigure 6. Census data - Left panel: Running mean and plus\/minus\ntwo standard deviations (black) and trace of one chain of S-GRIN\n(gray). - Right panel: Convergence analysis of S-GRIN with\nULISSE reporting the PSRF computed over five chains.\nparallel chainsand computedthe Potential Scale Reduction\nFactor (PSRF) (Gelman & Rubin, 1992). The chains were\ninitialized by drawing from a Gaussian with mean on the\nMAP solution over a subset of 500 input vectors and co-\nvariance M. Fig. 5 shows the median and the 97.5th per-\ncentile of the PSRF across the ten chains. The analysis of\nthese plots reveals that S-GRIN achieves convergenceafter\nfew thousands of iterations.\n5.2. Demonstration on a larger dataset\nWe now present the application of S-GRIN with ULISSE\nto a data set where it is not possible to run any MCMC\nalgorithm on a conventional desktop machine. This data\nset contains data collected as part of the 1990 US census.\nIn this study, we used the 8L data set3where the regression\ntask associates the median house price in a given region\nwith demographic composition and housing market. We\nkept the same experimental conditions as in the case of the\nconcretedata, exceptthat \u03b5twas chosen to go from 5\u00b710\u22122\nto 5\u00b710\u22126to copewiththe largergradientsobtainedforthis\ndata set, and the preconditioner M was estimated based on\nthe MAP on 1000 data points rather than 500. The running\nstatistics for the three parameters are reported in Fig. 6,\nalong with the PSRF computed across five chains. Again,\nconvergencewas reached after few thousands of iterations.\nS-GRIN with ULISSE was ran on a desktop machine with\nan eight core (i7-2600 CPU at 3.40GHz) processor, and an\nNVIDIA GeForce GTX 590 graphics card. The two GPUs\nin the graphics card are used to carry out CMVPs. With\nthis arrangement, we were able to draw roughly ten thou-\n3www.cs.toronto.edu\/\u02dcdelve\/data\/datasets.html\nsand samples per day from the posterior distribution over\ncovariance parameters.\n6. Conclusions\nThis paper presented a novel way to accurately infer co-\nvariance parameters in GPs. The novelty stems from the\ncombination of stochastic gradient-based inference and a\nfast unbiased solver of dense linear systems. The results\ndemonstrate that it is possible to tackle the inference prob-\nlemoveradatasetcomprisingabout23thousandinputvec-\ntors in a day on a desktop machine with standard hardware.\nThe proposed methodology can exploit parallelism when\ncomputing covariance matrix-vector products, so there is\nan opportunityto scale \u201cexact\u201d inference(in a Monte Carlo\nsense) to even larger data sets. We are not aware of any\nmethod that is capable of carrying out full quantification of\nuncertaintyof GP covarianceparameterson such large data\nsets without imposing any special structures on the covari-\nance or reducing the number of input vectors. These re-\nsults are important not only in Machine Learning, but also\nin a number of areas where quantification of uncertainty is\nof primary interest and GPs are routinely employed, such\nas calibration of computer codes (Kennedy & O\u2019Hagan,\n2001) and optimization (Jones et al., 1998).\nTheresults reportedinthis paper,althoughpromising,indi-\ncate some directions for improvements. S-GRIN requires\nthe tuning of a preconditioning matrix M. Choosing M\nto be similar to the covariance of the posterior speeds up\nconvergence of S-GRIN when it reaches the Langevin dy-\nnamics phase. However, M also affect the scaling of the\ngradient in the proposal. During the first phase of S-GRIN\nthis might not be optimal, and ideally, gradients should be\nscaled in a way similar to AdaGrad (Duchi et al., 2011). In\n(Welling & Teh, 2011), it was possible to establish a con-\nnection between the variance of the gradient, the Fisher In-\nformation, and M due to the fact that the gradient is eval-\nuated on a subset of the data. We were unable to do so\nfor GPs due to the different way stochasticity is introduced\nin the computation of the gradients. Despite this compli-\ncation, we demonstrated that it is still possible to obtain\nconvergence to the posterior distribution over covariance\nparameters in a reasonable number of iterations, which is\nof ultimate importance in any inference task.\nWe are currently investigating the possibility to extend\nthis methodology to scale inference for other GP models,\ne.g., GPs classification and GPs for spatio-temporal data.\nOther interesting aspects to explore would be the introduc-\ntion of mixed precision calculations within the CG algo-\nrithm to improve convergence and computation speed as\npresented, e.g., in (Jang et al., 2011; Cevahir et al., 2009;\nBaboulin et al., 2009)."},{"page":9,"text":"Acknowledgments\nMF gratefully acknowledges support from EPSRC grant\nEP\/L020319\/1.\nReferences\nAnitescu, M., Chen, J., and Wang, L.\nApproach for Solving the Parametric Gaussian Process\nMaximum LikelihoodProblem. SIAM Journal on Scien-\ntific Computing, 34(1):A240\u2013A262,2012.\nA Matrix-free\nAsuncion, A. and Newman, D. J. UCI machine learning\nrepository, 2007.\nBaboulin, M., Buttari, A., Dongarra, J., Kurzak, J., Lan-\ngou, J., Langou, J., Luszczek, P., and Tomov, S. Accel-\nerating scientific computations with mixed precision al-\ngorithms. Computer Physics Communications, 180(12):\n2526\u20132533,2009.\nBanerjee, A., Dunson, D. B., and Tokdar, S. T.\nficient Gaussian process regression for large datasets.\nBiometrika, 100(1):75\u201389,2013.\nEf-\nBanterle, M., Grazian, C., and Robert, C. P.\nating Metropolis-Hastings algorithms: Delayed accep-\ntance with prefetching, June 2014. arXiv:1406.2660.\nAcceler-\nCandela, J. Q. and Rasmussen, C. E. A Unifying View of\nSparseApproximateGaussianProcessRegression. Jour-\nnal of Machine Learning Research, 6:1939\u20131959,2005.\nCevahir, A., Nukada, A., and Matsuoka, S. Fast Conjugate\nGradients with Multiple GPUs. In Allen, G., Nabrzyski,\nJ., Seidel, E., van Albada, G., Dongarra, J., and Sloot,\nP. (eds.), Computational Science ICCS 2009, volume\n5544 of Lecture Notes in Computer Science, pp. 893\u2013\n903. Springer Berlin Heidelberg, 2009.\nChen, J., Anitescu, M., and Saad, Y. Computing f(A)b via\nLeast Squares Polynomial Approximations. SIAM Jour-\nnal on Scientific Computing, 33(1):195\u2013222,2011.\nDuchi, J., Hazan, E., and Singer, Y. Adaptive Subgradient\nMethods for Online Learning and Stochastic Optimiza-\ntion. J. Mach. Learn. Res., 12:2121\u20132159,July 2011.\nFilippone, M.\nclassifiers with annealing and pseudo-marginal MCMC.\nIn 22nd International Conference on Pattern Recogni-\ntion, ICPR 2014, Stockholm, Sweden, August 24-28,\n2014, pp. 614\u2013619. IEEE, 2014.\nBayesian inference for Gaussian process\nFilippone, M. and Girolami, M. Pseudo-marginalBayesian\ninference for Gaussian processes. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 36(11):\n2214\u20132226,2014.\nFilippone, M., Zhong, M., and Girolami, M. A compara-\ntive evaluationofstochastic-basedinferencemethodsfor\nGaussian process models. Machine Learning, 93(1):93\u2013\n114, 2013.\nGelman, A. and Rubin, D. B. Inference from iterative sim-\nulation using multiple sequences. Statistical Science, 7\n(4):457\u2013472,1992.\nGibbs, M. and MacKay, D. J. C. Efficient Implementa-\ntion of Gaussian Processes. Technical report, Cavendish\nLaboratory, Cambridge, UK, 1997.\nGibbs, M. N. Bayesian Gaussian processes for regression\nand classification. PhD thesis, University of Cambridge,\n1997.\nGilboa, E., Saatci, Y., andCunningham,J. ScalingMultidi-\nmensional Inference for Structured Gaussian Processes.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence, to appear, 2013.\nGolub, G. H. and Van Loan, C. F. Matrix computations.\nThe Johns Hopkins University Press, 3rd edition, Octo-\nber 1996.\nGramacy, R. B., Lee, H. K. H., and Macready, W. G. Pa-\nrameter space exploration with Gaussian process trees.\nIn ICML \u201904: Proceedings of the twenty-first interna-\ntional conference on Machine learning, pp. 45, New\nYork, NY, USA, 2004. ACM.\nGray, A. G. and Moore, A. W.\nStatistical Learning. In Leen, T. K., Dietterich, T. G.,\nand Tresp, V. (eds.), Advances in Neural Information\nProcessing Systems 13, Papers from Neural Information\nProcessing Systems (NIPS) 2000, Denver, CO, USA, pp.\n521\u2013527. MIT Press, 2000.\n\u2019N-Body\u2019 Problems in\nHennig, P. Probabilistic Interpretation of Linear Solvers,\nOctober 2014. arXiv:1402.2058.\nHensman, J., Fusi, N., and Lawrence, N. D. Gaussian Pro-\ncesses for Big Data, September 2013. arXiv:1309.6835.\nHigham, N. J. Functions of Matrices: Theory and Compu-\ntation. Society for Industrial and Applied Mathematics,\nPhiladelphia, PA, USA, 2008.\nJang, Y.-C., Kim, H.-J., and Lee, W.\nPerformance of Conjugate Gradient Solver with Stag-\ngered Fermions in Mixed Precision, November 2011.\narXiv:1111.0125.\nMulti GPU\nJones, D. R., Schonlau, M., and Welch, W. J. Efficient\nGlobalOptimizationofExpensiveBlack-BoxFunctions.\nJ. of Global Optimization, 13(4):455\u2013492,1998."},{"page":10,"text":"Kennedy, A. D. and Kuti, J. Noise without Noise: A New\nMonteCarloMethod. PhysicalReviewLetters, 54:2473\u2013\n2476, 1985.\nKennedy, M. C. and O\u2019Hagan, A. Bayesian calibration of\ncomputer models. Journal of the Royal Statistical Soci-\nety: Series B (Statistical Methodology), 63(3):425\u2013464,\n2001.\nLiu, K.-F. A Noisy Monte Carlo Algorithm with Fermion\nDeterminant. In Frommer, A., Lippert, T., Medeke, B.,\nand Schilling, K. (eds.), Numerical Challenges in Lat-\ntice Quantum Chromodynamics, volume 15 of Lecture\nNotes in Computational Science and Engineering, pp.\n142\u2013152. Springer Berlin Heidelberg, 2000.\nLyne, A.-M., Girolami, M., Atchade, Y., Strathmann, H.,\nand Simpson, D. Playing Russian Roulette with In-\ntractable Likelihoods, July 2014. arXiv:1306.4032.\nMaclaurin, D. and Adams, R. P.\nExact MCMC with Subsets of Data, March 2014.\narXiv:1403.5693.\nFirefly Monte Carlo:\nMoore, A. The Anchors Hierarchy: Using the Triangle In-\nequality to SurviveHigh-DimensionalData. In Proceed-\nings of the Twelfth Conference on Uncertainty in Artifi-\ncial Intelligence, pp. 397\u2013405. AAAI Press, 2000.\nMurray, I. Gaussian processes and fast matrix-vector mul-\ntiplies, 2009. Presented at the Numerical Mathematics\nin Machine Learning workshop at the 26th International\nConference on Machine Learning (ICML 2009), Mon-\ntreal, Canada.\nMurray, I. and Adams, R. P. Slice sampling covariance\nhyperparameters of latent Gaussian models.\nferty, J. D., Williams, C. K. I., Shawe-Taylor, J., Zemel,\nR. S., and Culotta, A. (eds.), Advances in Neural Infor-\nmation Processing Systems 23: 24th Annual Conference\non Neural Information Processing Systems 2010. Pro-\nceedings of a meeting held 6-9 December 2010, Vancou-\nver, British Columbia, Canada., pp. 1732\u20131740. Curran\nAssociates, Inc., 2010.\nIn Laf-\nNeal, R. M. Regression and classification using Gaussian\nprocess priors (with discussion). Bayesian Statistics, 6:\n475\u2013501, 1999.\nRasmussen, C. E. and Williams, C. Gaussian Processes for\nMachine Learning. MIT Press, 2006.\nRobbins, H. and Monro, S. A Stochastic Approximation\nMethod. The Annals of MathematicalStatistics, 22:400\u2013\n407, 1951.\nRue, H., Martino, S., and Chopin, N.\nBayesian inference for latent Gaussian models by using\nApproximate\nintegratednestedLaplaceapproximations.Journalofthe\nRoyal Statistical Society: Series B (Statistical Methodol-\nogy), 71(2):319\u2013392,2009.\nSaatc \u00b8i, Y. Scalable Inference for Structured Gaussian Pro-\ncess Models. PhD thesis, University of Cambridge,\n2011.\nSeeger, M.\nTechnical report, Institute for ANC, Edinburgh, UK,\n2000.\nSkilling techniques for Bayesian analysis.\nSimpson, D. P., Turner, I. W., Strickland, C. M., and\nPettitt, A. N. Scalable iterative methods for sampling\nfrommassiveGaussianrandomvectors,December2013.\narXiv:1312.1476.\nSkilling, J. Bayesian NumericalAnalysis. In Grandy,W. T.\nand Milonni, P. W. (eds.), Physics and Probability, pp.\n207\u2013222.CambridgeUniversityPress, 1993. Cambridge\nBooks Online.\nSrinivasan, B. V., Hu, Q., Gumerov, N. A., Murtugudde,\nR., and Duraiswami, R. Preconditioned Krylov solvers\nfor kernel regression, August 2014. arXiv:1408.1237.\nStein, M. L., Chen, J., and Anitescu, M. Stochastic approx-\nimation of score functions for Gaussian processes. The\nAnnals of Applied Statistics, 7(2):1162\u20131191,2013.\nTaylor, M. B. and Diggle, J. P. INLA or MCMC? A Tu-\ntorial and Comparative Evaluation for Spatial Prediction\nin log-Gaussian Cox Processes, 2012. arXiv:1202.1738.\nTitsias, M.K. VariationalLearningofInducingVariablesin\nSparse Gaussian Processes. In Dyk, D. A. and Welling,\nM. (eds.), Proceedings of the Twelfth International Con-\nference on Artificial Intelligence and Statistics, AISTATS\n2009, Clearwater Beach, Florida, USA, April 16-18,\n2009, volume 5 of JMLR Proceedings, pp. 567\u2013574.\nJMLR.org, 2009.\nWelling, M. andTeh, Y. W. BayesianLearningviaStochas-\ntic Gradient Langevin Dynamics.\nScheffer, T. (eds.), Proceedings of the 28th International\nConferenceonMachineLearning,ICML 2011,Bellevue,\nWashington, USA, June 28 - July 2, 2011, pp. 681\u2013688.\nOmnipress, 2011.\nIn Getoor, L. and\nWilliams, C. K. I. and Barber, D. Bayesian classification\nwith Gaussian processes. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 20:1342\u20131351,\n1998.\nWilliams, C. K. I. and Rasmussen, C. E. Gaussian Pro-\ncesses for Regression. In Touretzky, D. S., Mozer,\nM., and Hasselmo, M. E. (eds.), Advances in Neural\nInformation Processing Systems 8, NIPS, Denver, CO,\nNovember 27-30, 1995, pp. 514\u2013520. MIT Press, 1995."}],"fullTextUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5.pdf","widgetId":"rgw25_56aba0b5cfaf0"},"id":"rgw25_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=271218362&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw26_56aba0b5cfaf0"},"id":"rgw26_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=271218362&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":271218362,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":{"data":{"isPreview":false,"licenseUrl":null,"licenseInfo":null,"defaultLinkData":{"linkId":"54c76f2e0cf289f0cecd23d5","name":"Maurizio Filippone","date":"Jan 27, 2015 ","nameLink":"profile\/Maurizio_Filippone","filename":"ulisse15.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"9f219048b7715cf153533f6af4aecc72","showFileSizeNote":false,"fileSize":"476.64 KB","noFollow":false,"isDefault":true,"doi":null},"displayableLinks":[{"linkId":"54c76f2e0cf289f0cecd23d5","name":"Maurizio Filippone","date":"Jan 27, 2015 ","nameLink":"profile\/Maurizio_Filippone","filename":"ulisse15.pdf","downloadLink":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5.pdf?inViewer=0&pdfJsDownload=0&origin=publication_detail","viewerUrl":"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5.pdf?inViewer=1&pdfJsDownload=1&origin=publication_detail","downloadHash":"9f219048b7715cf153533f6af4aecc72","showFileSizeNote":false,"fileSize":"476.64 KB","noFollow":false,"isDefault":true,"doi":null}],"hasDisplayableLinks":false,"reader":{"data":{"pdfCommentsActive":false,"publicationType":"Article","onlyRenderFirstPage":false,"readMoreExperimentGoal":"goalPublicationPdfClicksReadMore","readMoreExperimentViewId":null,"comments":null,"figures":[],"figureAssetIds":[],"figureOverlayUrls":{"clickOnOverlay":"publication.PublicationFigures.html?_sg=sjOGcg1LH3e1SXX1hM84SBPpAEZ8UP90ENew0pc5PN2enqHoYbJu9YxOGcH-ArL3cBLX5juYc5EHk8s6t57TjQ.5fXwnZjCwN3akfEEC4Gl8yXsTn4bGG8mQTVjPhAP_aN_ciU67Gik5VN5eiXTrzROhGBV2y6MDsKVMx8w-dkL8w","clickOnPill":"publication.PublicationFigures.html?_sg=JTpSlJXUAb7Epb08cQlro85aMXkY67Ui1b24X1qADqi6tfelem5_XTQFXtUgIfo3IuK4LXATfs4pZqExq4KQrA.XS7bueYu69WGb10XznJtP-f6snHkUuTnLHpRCDdICq2Re4mPpJqgnRe_Y7nsyKVHNcM4-O3tAlK-2Ww-Htw3oQ"},"canSelect":false,"javascriptPath":"https:\/\/www.researchgate.net\/c\/o1q2er\/","downloadUrl":"https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE%2Flinks%2F54c76f2e0cf289f0cecd23d5.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail","viewerUrl":"https:\/\/www.researchgate.net\/c\/o1q2er\/javascript\/lib\/pdfjs\/web\/viewer.html","commentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/comment","experimentEndpoint":"https:\/\/www.researchgate.net\/rgformat\/api\/experiment?key=nPn1qEfCItS01bQ9IGvGKUOGHbCRoEJuREUaiwyI83ohHv6qjyHoG9wTHjvprxtfD8YBazg61uue9mrI6JCtEg","urlHash":"c29a31b16074d1677c7254150ab8ef1b","downloadTrackUrl":"application.PdfJsReader.ajaxTrackDownload.html?msrp=WoHcMJrmPvxOeztvoKh8nFGJKIQvq4_tMWebwxOH7AVsK3mYp5vE_sVcrNFqldei3WlY0ZevmXspjTbC2lWV_fU7y1ADaglva2zhkDxf7SE.7Lm5UBy_WnN3l1Gi4XrspMlvml-FgcpaDJW_ECo_0JVnK0ophbedhfOtqMxMeRcqicG-ru9_H2jag-c34yWKlg.X9S4uXo33Emb9Aqepo5jN5j7cfIraqKgOxyBEFhF2Z58duu9Q2lCgXDsAL3U8vPAunQYK00NIe-V5ip06Gm50Q","viewportMilestoneTrackUrl":"application.PdfJsReader.ajaxTrackViewportMilestone.html","linkId":"54c76f2e0cf289f0cecd23d5","trackedDownloads":{"54c76f2e0cf289f0cecd23d5":{"v":false,"d":false}},"assetId":"AS:190190136078340@1422356270675","readerDocId":null,"assetType":"fulltext","interactionType":{"comment":"comment","highlight":"highlight"},"publicationUid":271218362,"commentCursorPromo":null,"widgetId":"rgw28_56aba0b5cfaf0"},"id":"rgw28_56aba0b5cfaf0","partials":[],"templateName":"application\/stubs\/PdfJsReader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PdfJsReader.html?fileHref=https%3A%2F%2Fwww.researchgate.net%2Fprofile%2FMaurizio_Filippone%2Fpublication%2F271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE%2Flinks%2F54c76f2e0cf289f0cecd23d5.pdf%3FinViewer%3D1%26pdfJsDownload%3D1%26origin%3Dpublication_detail&assetId=AS%3A190190136078340%401422356270675&publicationUid=271218362&linkId=54c76f2e0cf289f0cecd23d5&onlyShowFirstPage=0","viewClass":null,"yuiModules":["css-pow-application-PdfJsReader"],"stylesheets":["pow\/application\/PdfJsReader.css"],"_isYUI":true},"showHeader":true,"title":"Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)","publicationType":"Article","downloadTrackUrl":"publicliterature.PublicationInlineReader.ajaxTrackDownload.html?msrp=BFGXONqMxRGedaG7jXD8zZp0-Jz73KUbv2X2SslceCV_v4QnKEefVD7EK-LOnYqHru6YJ3qEh00QJvq3j_glWBWfdCqCDk1M04E62mEVkWM.pHyS1RBbAYczs8_Nwb7VNScBUCsbA_qmmXQp3-CTy9gBXZOOIaa6A72uzYhCm9TqL4X4_FWJHOCvIfyjJ5xV2g.RqzNEKd7d26g6qiOTDPskG24HZCzTy-20W7n_BJH_6dyE4nTqKoagILGY-etLnX2bqAw0dT7IuiKZ-iBVefYOw","publicationUid":271218362,"trackedDownloads":{"54c76f2e0cf289f0cecd23d5":{"v":false,"d":false}},"inlinePdf":false,"publicationComments":null,"showDownloadButton":true,"socialShare":{"data":{"shareItems":[{"data":{"name":"Facebook","url":"http:\/\/www.facebook.com\/share.php?u={{url}}{{#title}}&t={{title}}{{\/title}}","width":600,"height":350,"useUtmTags":true,"utmSource":"facebook","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.ajaxTrackSocialShare.html","widgetId":"rgw30_56aba0b5cfaf0"},"id":"rgw30_56aba0b5cfaf0","partials":{"shareIcon":"application\/stubs\/partials\/shareFacebookBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareFacebook.html?provider=Facebook&shareIcon=shareIconBlog&utmSource=facebook&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Twitter","url":"http:\/\/twitter.com\/intent\/tweet?text={{#title}}{{title}}: {{\/title}}{{url}}&via=researchgate","width":600,"height":350,"useUtmTags":true,"utmSource":"twitter","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.ajaxTrackSocialShare.html","widgetId":"rgw31_56aba0b5cfaf0"},"id":"rgw31_56aba0b5cfaf0","partials":{"shareIcon":"application\/stubs\/partials\/shareTwitterBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareTwitter.html?provider=Twitter&shareIcon=shareIconBlog&utmSource=twitter&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Google+","url":"https:\/\/plus.google.com\/share?url={{url}}","width":600,"height":600,"useUtmTags":true,"utmSource":"googleplus","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.ajaxTrackSocialShare.html","widgetId":"rgw32_56aba0b5cfaf0"},"id":"rgw32_56aba0b5cfaf0","partials":{"shareIcon":"application\/stubs\/partials\/shareGooglePlusBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareGooglePlus.html?provider=Google%2B&shareIcon=shareIconBlog&utmSource=googleplus&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"LinkedIn","url":"http:\/\/www.linkedin.com\/shareArticle?mini=true&url={{url}}{{#title}}&title={{title}}{{\/title}}&source=ResearchGate","width":520,"height":570,"useUtmTags":true,"utmSource":"linkedin","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.ajaxTrackSocialShare.html","widgetId":"rgw33_56aba0b5cfaf0"},"id":"rgw33_56aba0b5cfaf0","partials":{"shareIcon":"application\/stubs\/partials\/shareLinkedInBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareLinkedIn.html?provider=LinkedIn&shareIcon=shareIconBlog&utmSource=linkedin&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true},{"data":{"name":"Reddit","url":"https:\/\/www.reddit.com\/submit?url={{url}}{{#title}}&title={{title}}{{\/title}}","width":600,"height":600,"useUtmTags":true,"utmSource":"reddit","utmMedium":"rgShare","utmCampaign":"shareFullTextPublication","cssClass":"","trackClick":true,"trackUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.ajaxTrackSocialShare.html","widgetId":"rgw34_56aba0b5cfaf0"},"id":"rgw34_56aba0b5cfaf0","partials":{"shareIcon":"application\/stubs\/partials\/shareRedditBlogIcon.html"},"templateName":"application\/stubs\/BaseShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.ShareReddit.html?provider=Reddit&shareIcon=shareIconBlog&utmSource=reddit&utmMedium=rgShare&utmCampaign=shareFullTextPublication&trackClick=1","viewClass":"views.application.BaseShareView","yuiModules":["rg.views.application.BaseShareView"],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw29_56aba0b5cfaf0"},"id":"rgw29_56aba0b5cfaf0","partials":[],"templateName":"application\/stubs\/SocialShare.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.SocialShare.html?campaign=shareFullTextPublication&trackClick=1&shareIcon=shareIconBlog","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw27_56aba0b5cfaf0"},"id":"rgw27_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicationInlineReader.html","templateExtensions":["generalHelpers"],"attrs":{"showFulltextDownloadedSignupDialog":true,"preSignUpDialogContext":null,"requestFulltext":false},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationInlineReader.html","viewClass":"views.publicliterature.PublicationInlineReaderView","yuiModules":["rg.views.publicliterature.PublicationInlineReaderView","css-pow-publicliterature-PublicationInlineReader"],"stylesheets":["pow\/publicliterature\/PublicationInlineReader.css"],"_isYUI":true},"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba0b5cfaf0"},"id":"rgw2_56aba0b5cfaf0","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":271218362},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=271218362&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba0b5cfaf0"},"id":"rgw1_56aba0b5cfaf0","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"JAelfKfQw+xVixAL7hoAT3Jx6zuEMoAzFcDYoUlBUKad1\/LjHsy0Y57RU0O5wm8vHcALRJFQHclaRaFEmxnG42GuGBBQcbIQGZ\/jFb4UtIyPemzQINsVuv3zjKspMkTjStYDUhEANtNZ3P+GZ7DjDucswAx484rwSoLdhAk5iRTfkIfkzodrC4AI+r2HfR9AS+iK\/ph49PHj8j2hdy\/6WiL6fXz5n3557Dwh66d2BCfhJsQK8U3Udhnm5Ku5JikqsQdQPfnRNegxhUhKiqGfzBCmLpgjr9pvN5OBH8Pir7w=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)\" \/>\n<meta property=\"og:description\" content=\"In applications of Gaussian processes where quantification of uncertainty is\nof primary interest, it is necessary to accurately characterize the posterior\ndistribution over covariance parameters....\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\" \/>\n<meta property=\"rg:id\" content=\"PB:271218362\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Enabling scalable stochastic gradient-based inference for Gaussian processes by employing the Unbiased LInear System SolvEr (ULISSE)\" \/>\n<meta name=\"citation_author\" content=\"Maurizio Filippone\" \/>\n<meta name=\"citation_author\" content=\"Raphael Engler\" \/>\n<meta name=\"citation_publication_date\" content=\"2015\/01\/22\" \/>\n<meta name=\"citation_pdf_url\" content=\"https:\/\/www.researchgate.net\/profile\/Maurizio_Filippone\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\/links\/54c76f2e0cf289f0cecd23d5.pdf\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21993347442549\/styles\/pow\/publicliterature\/FollowPublicationPromo.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/282514599719602\/styles\/pow\/application\/PdfJsReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/23819663151220\/styles\/pow\/publicliterature\/PublicationInlineReader.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication full-text');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-375f014e-5319-4b45-8b6d-487cdcd5736a","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":503,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw35_56aba0b5cfaf0"},"id":"rgw35_56aba0b5cfaf0","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-375f014e-5319-4b45-8b6d-487cdcd5736a", "4568381910b81c92c42d4602f9dca2b7aa0d874b");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-375f014e-5319-4b45-8b6d-487cdcd5736a", "4568381910b81c92c42d4602f9dca2b7aa0d874b");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw36_56aba0b5cfaf0"},"id":"rgw36_56aba0b5cfaf0","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/271218362_Enabling_scalable_stochastic_gradient-based_inference_for_Gaussian_processes_by_employing_the_Unbiased_LInear_System_SolvEr_ULISSE","requestToken":"bxz+JiXaH39dd66ZiVgNeUweX0vN69ngr1WuLyGYbUg+SzyT5ZAiYMIYGMEGrmNsUOsNzbeYf+i779L5JmCnqw23UaonymKWJpJyZ1KZVGCcaEkysedYx36GP853CwMH7CLkjEZb8YOg2QxODSMP+cqUYcTrW+FwyFERHw2ECz1tZxQKFy07cMXk31kOkaL0GBJ9H85TwbrSAS9bMfTVmG5f0+jLu1MBk0Fcw1onDEB9kbc7xKC0qdHxfF3NOVU35ND9nVZ0Y\/c\/tYaN0APPpRrvPQZOjJEk1SWsSD7wuyY=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=ElozmA0UOMBGTOZkm5G1bLH5s6yLnJ_aXEJh2Fy2R8n1hhuKcI1c0m5xyq7DSLcq","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjcxMjE4MzYyX0VuYWJsaW5nX3NjYWxhYmxlX3N0b2NoYXN0aWNfZ3JhZGllbnQtYmFzZWRfaW5mZXJlbmNlX2Zvcl9HYXVzc2lhbl9wcm9jZXNzZXNfYnlfZW1wbG95aW5nX3RoZV9VbmJpYXNlZF9MSW5lYXJfU3lzdGVtX1NvbHZFcl9VTElTU0U%3D","signupCallToAction":"Join for free","widgetId":"rgw38_56aba0b5cfaf0"},"id":"rgw38_56aba0b5cfaf0","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw37_56aba0b5cfaf0"},"id":"rgw37_56aba0b5cfaf0","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw39_56aba0b5cfaf0"},"id":"rgw39_56aba0b5cfaf0","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication full-text","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
