<!DOCTYPE html> <html lang="en" class="" id="rgw30_56aba1f479f68"> <head > <meta charset="utf-8"/> <meta http-equiv="content-type" content="text/html; charset=UTF-8"/> <meta name="Rg-Request-Token" id="Rg-Request-Token" content="4nvqRNb6r5f44lJYzzbRqOi9kCiUq6tYPxKk0wmXzJ56bbDL1lldZN6BnKMdWZrfjucrkHTLDbQahxx7c7kw5aEyZQ06Y49jofnxc6lsu1qZhzNxE5F5qq7ta/5ej3NGWBWFos0UPVlD9XESpE1ZwXaIZiAhyHGS8IV1VSvjgRPXZwl8dSGQMEUSDG7NTpLQINArFyUly7xi0sClVzDa/TYy2Dbr7ttuOKjb/5OA1wiPHJaUbsP/eptgB4ovwec9iD8JurSvume5BuFTgQUn5rv2kzwMk/TQmmBSlQjgBeI="/> <meta http-equiv="expires" content="0"/> <link rel="apple-touch-icon" sizes="57x57" href="https://www.researchgate.net/apple-touch-icon-57x57.png"> <link rel="apple-touch-icon" sizes="60x60" href="https://www.researchgate.net/apple-touch-icon-60x60.png"> <link rel="apple-touch-icon" sizes="72x72" href="https://www.researchgate.net/apple-touch-icon-72x72.png"> <link rel="apple-touch-icon" sizes="76x76" href="https://www.researchgate.net/apple-touch-icon-76x76.png"> <link rel="apple-touch-icon" sizes="114x114" href="https://www.researchgate.net/apple-touch-icon-114x114.png"> <link rel="apple-touch-icon" sizes="120x120" href="https://www.researchgate.net/apple-touch-icon-120x120.png"> <link rel="apple-touch-icon" sizes="144x144" href="https://www.researchgate.net/apple-touch-icon-144x144.png"> <link rel="apple-touch-icon" sizes="152x152" href="https://www.researchgate.net/apple-touch-icon-152x152.png"> <link rel="apple-touch-icon" sizes="180x180" href="https://www.researchgate.net/apple-touch-icon-180x180.png"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-32x32.png" sizes="32x32"> <link rel="icon" type="image/png" href="https://www.researchgate.net/android-chrome-192x192.png" sizes="192x192"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-96x96.png" sizes="96x96"> <link rel="icon" type="image/png" href="https://www.researchgate.net/favicon-16x16.png" sizes="16x16"> <link rel="shortcut icon" type="image/x-icon" href="https://c5.rgstatic.net/m/2390829798215018/images/favicon.ico"/> <link rel="manifest" href="https://www.researchgate.net/manifest.json"> <meta name="msapplication-TileColor" content="#da532c"> <meta name="msapplication-TileImage" content="https://www.researchgate.net/mstile-144x144.png"> <meta name="theme-color" content="#444444"> <link rel="search" type="application/opensearchdescription+xml" title="ResearchGate search" href="https://www.researchgate.net/application.DownloadOpenSearchPlugin.html"/> <link rel="meta" type="application/rdf+xml" title="ICRA labels" href="https://www.researchgate.net/application.DownloadLabels.html"/> <link rel="http://oexchange.org/spec/0.8/rel/related-target" type="application/xrd+xml" href="https://www.researchgate.net/application.DownloadOExchange.html"/> <base href="https://www.researchgate.net/"/> <script>
    var rgConfig = {
        correlationId: "rgreq-237ad02b-41ed-47ad-8792-b39f65bfca40",
        accountId: "",
        module: "publicliterature",
        action: "publicliterature.PublicPublicationDetails",
        page: "publicationDetail",
        product: "publications",
        continent: "Asia",
        stylesHome: "//c5.rgstatic.net/m/",
        staticHost: "c5.rgstatic.net",
        longRunningRequestIdentifier: "LongRunningRequest.publicliterature.PublicPublicationDetails",
        longRunningRequestFp: "e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b"
    };
    window.rootUrl = "https://www.researchgate.net/";
</script> <link rel="canonical" href="https://www.researchgate.net/publication/261065571_Bayesian_Optimization_with_Unknown_Constraints" />
<link rel="dns-prefetch" href="//c5.rgstatic.net" />
<link rel="dns-prefetch" href="//i1.rgstatic.net" />
<meta property="twitter:card" content="summary" />
<meta property="twitter:site" content="@ResearchGate" />
<meta property="og:title" content="Bayesian Optimization with Unknown Constraints" />
<meta property="og:description" content="Recent work on Bayesian optimization has shown its effectiveness in global
optimization of difficult black-box objective functions. Many real-world
optimization problems of interest also have..." />
<meta property="og:site_name" content="ResearchGate" />
<meta property="og:image" content="https://i1.rgstatic.net/publication/261065571_Bayesian_Optimization_with_Unknown_Constraints/links/03353e7e0cf216bc595c8d9c/smallpreview.png" />
<meta property="og:url" content="https://www.researchgate.net/publication/261065571_Bayesian_Optimization_with_Unknown_Constraints" />
<meta property="rg:id" content="PB:261065571" />
<meta name="DC.identifier" scheme="DCTERMS.URI" content="http://dx.doi.org/" />
<meta name="gs_meta_revision" content="1.1" />
<meta name="citation_title" content="Bayesian Optimization with Unknown Constraints" />
<meta name="citation_author" content="Michael A. Gelbart" />
<meta name="citation_author" content="Jasper Snoek" />
<meta name="citation_author" content="Ryan P. Adams" />
<meta name="citation_publication_date" content="2014/03/21" />
<meta name="citation_abstract_html_url" content="https://www.researchgate.net/publication/261065571_Bayesian_Optimization_with_Unknown_Constraints" />
<meta name="citation_fulltext_html_url" content="https://www.researchgate.net/publication/261065571_Bayesian_Optimization_with_Unknown_Constraints" />
<meta name="viewport" content="width=device-width,initial-scale=1" />
<link href="//c5.rgstatic.net/m/22664197317151888/styles/rg.css" type="text/css" rel="stylesheet"/>
<link href="//c5.rgstatic.net/m/21004998181197492/styles/rg2.css" type="text/css" rel="stylesheet"/>
<!--[if lt IE 9]><link href="//c5.rgstatic.net/m/238176252723686/styles/ie.css" type="text/css" rel="stylesheet"/><![endif]-->
<link href="//c5.rgstatic.net/m/217752362214895/styles/modules/publicprofile.css" type="text/css" rel="stylesheet"/>
<script src="//c5.rgstatic.net/m/2321000301012716/javascript/vendor/webfontloader/webfontloader.js" type="text/javascript"></script>
 <script>(function(i,s,o,g,r,a,m){i["GoogleAnalyticsObject"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,"script","//www.google-analytics.com/analytics.js","ga");
 ga("create","UA-58591210-1");ga("set","anonymizeIp",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga("send","pageview");</script>
  <script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['//c5.rgstatic.net/m/231392577336386/styles/fonts.css'] } }; WebFont.load(WebFontConfig); </script><noscript></noscript>

<title>Bayesian Optimization with Unknown Constraints</title>
<meta name="description" content="Bayesian Optimization with Unknown Constraints on ResearchGate, the professional network for scientists."/>
<meta name="keywords" content="scientific network, scientific platform, scientific community, research partner, research collaboration, journal articles, international collaboration, find researcher, lifescience researcher, interdisciplinary research, research collaboration"/>
</head>
<body class="use-svg-icons logged-out">
<div id="page-container">
<script type="text/javascript">var googletag = googletag || {}; googletag.cmd = googletag.cmd || [];
(function() { var gads = document.createElement("script"); gads.async = true; gads.type = "text/javascript"; var useSSL = "https:" == document.location.protocol; gads.src = (useSSL ? "https:" : "http:") + "//www.googletagservices.com/tag/js/gpt.js"; var node =document.getElementsByTagName("script")[0]; node.parentNode.insertBefore(gads, node); })();</script><div id="main" class="logged-out-header-support">
<div id="content" class="">

<noscript>
<div class="c-box-warning full-width-element" style="text-align: center; ">
    <div style="margin: auto; padding:10px;" class="container">
        <b>For full functionality of ResearchGate it is necessary to enable JavaScript.
            Here are the <a href="http://www.enable-javascript.com/" rel="nofollow" target="_blank">
                instructions how to enable JavaScript in your web browser</a>.</b>
    </div>
</div>
</noscript>

<div id="rgw1_56aba1f479f68" itemscope itemtype="http://schema.org/ScholarlyArticle"><div class="publication-wrapper publication-wrapper-onecol" id="rgw2_56aba1f479f68" itemscope itemtype="http://schema.org/ScholarlyArticle"> <div class="c-col-content"> <div class="c-content"> <div class="clearfix">  <div class="publication-header"> <div id="rgw7_56aba1f479f68">  <div class="type-label"> Article   </div> <span class="Z3988" title="ctx_ver=Z39.88-2004&rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&rfr_id=info%3Asid%2Fresearchgate.net%3Aresearchgate&rft.atitle=Bayesian%20Optimization%20with%20Unknown%20Constraints&rft.title=Uncertainty%20in%20Artificial%20Intelligence%20-%20Proceedings%20of%20the%2030th%20Conference%2C%20UAI%202014&rft.jtitle=Uncertainty%20in%20Artificial%20Intelligence%20-%20Proceedings%20of%20the%2030th%20Conference%2C%20UAI%202014&rft.date=2014&rft.au=Michael%20A.%20Gelbart%2CJasper%20Snoek%2CRyan%20P.%20Adams&rft.genre=article"></span> <h1 class="pub-title" itemprop="name">Bayesian Optimization with Unknown Constraints</h1> <meta itemprop="headline" content="Bayesian Optimization with Unknown Constraints">  <meta itemprop="image" content="https://i1.rgstatic.net/publication/261065571_Bayesian_Optimization_with_Unknown_Constraints/links/03353e7e0cf216bc595c8d9c/smallpreview.png">  <div id="rgw9_56aba1f479f68" class="publication-detail-author-list"> <div> <ul class="clearfix js-people-list">  <li id="rgw10_56aba1f479f68"> <a href="researcher/2043445261_Michael_A_Gelbart" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Michael A. Gelbart" alt="Michael A. Gelbart" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Michael A. Gelbart</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw11_56aba1f479f68">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/2043445261_Michael_A_Gelbart"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Michael A. Gelbart" alt="Michael A. Gelbart" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/2043445261_Michael_A_Gelbart" class="display-name">Michael A. Gelbart</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw12_56aba1f479f68"> <a href="researcher/70743985_Jasper_Snoek" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Jasper Snoek" alt="Jasper Snoek" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Jasper Snoek</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw13_56aba1f479f68">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/70743985_Jasper_Snoek"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Jasper Snoek" alt="Jasper Snoek" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/70743985_Jasper_Snoek" class="display-name">Jasper Snoek</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>  <li id="rgw14_56aba1f479f68"> <a href="researcher/71165702_Ryan_P_Adams" class="pub-detail-item author-item" itemprop="author" itemscope itemtype="http://schema.org/Person"> <div class="indent-left"> <div class="people-img">  <img src="https://c5.rgstatic.net/m/2951093203564/images/template/default/profile/profile_default_s.jpg" title="Ryan P. Adams" alt="Ryan P. Adams" height="20px" width="20px" style="height: 20px;"/>  </div> </div> <div class="indent-content"> <div class="item-name"> <span itemprop="name">Ryan P. Adams</span> </div> </div> </a>  <div style="display: none"> <div class="js-tooltip-content"> <ul class="pub-detail-item-tooltip"> <li class="people-item js-list-item   not-claimed " id="rgw15_56aba1f479f68">   <div class="indent-left"> <div class="c-img-l people-img ga-top-coauthor-image">  <a href="researcher/71165702_Ryan_P_Adams"> <img class="lazyload" data-src="https://c5.rgstatic.net/m/237738464651637/images/template/default/profile/profile_default_l.jpg" title="Ryan P. Adams" alt="Ryan P. Adams" height="90px" width="90px"/> </a>   </div> </div> <div class="indent-right">       </div> <div class="indent-content"> <h5 class="ga-top-coauthor-name">  <a href="researcher/71165702_Ryan_P_Adams" class="display-name">Ryan P. Adams</a>    </h5> <div class="truncate-single-line meta">   </div>  <div class="hide-button-container hide-button" style="display: none;"> <a href="javascript:" class="details js-hide action-hide">Remove suggestion</a> </div>  </div>    </li> </ul> </div> </div>  </li>   </ul> <div class="js-loading"></div>  </div> </div> <div class="pub-details js-pub-details">     Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014   <meta itemprop="datePublished" content="2014-03">  03/2014;               <div class="pub-source"> Source: <a href="http://arxiv.org/abs/1403.5607" rel="nofollow">arXiv</a> </div>  </div> <div id="rgw16_56aba1f479f68" class="pub-abstract">  <div class="clearfix">   <p itemprop="description"> <strong>ABSTRACT</strong> <div>Recent work on Bayesian optimization has shown its effectiveness in global<br />
optimization of difficult black-box objective functions. Many real-world<br />
optimization problems of interest also have constraints which are unknown a<br />
priori. In this paper, we study Bayesian optimization for constrained problems<br />
in the general case that noise may be present in the constraint functions, and<br />
the objective and constraints may be evaluated independently. We provide<br />
motivating practical examples, and present a general framework to solve such<br />
problems. We demonstrate the effectiveness of our approach on optimizing the<br />
performance of online latent Dirichlet allocation subject to topic sparsity<br />
constraints, tuning a neural network given test-time memory constraints, and<br />
optimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed<br />
time, subject to passing standard convergence diagnostics.</div> </p>  </div>   </div>      <div class="action-container">   <div class="clear"></div> <div class="share-dialog-container" style="display: none;">  </div> </div> </div> <div style="margin-left: -20px; margin-right: -20px;">  </div> </div>  <div class="publication-detail-dfp-container rf"> <div id="rgw29_56aba1f479f68">  </div> </div>  </div>  <div class="clearfix">  <noscript> <div id="rgw28_56aba1f479f68"  itemprop="articleBody">  <p>Page 1</p> <p>Bayesian Optimization with Unknown Constraints<br />Michael A. Gelbart, Jasper Snoek, and Ryan P. Adams<br />School of Engineering and Applied Sciences, Harvard University<br />{mgelbart, jsnoek, rpa} @ seas.harvard.edu<br />Abstract<br />Recent work on Bayesian optimization has shown its effectiveness in global optimization<br />of difficult black-box objective functions. Many real-world optimization problems of interest<br />also have constraints which are unknown a priori. In this paper, we study Bayesian optimiza-<br />tion for constrained problems in the general case that noise may be present in the constraint<br />functions, and the objective and constraints may be evaluated independently. We provide<br />motivating practical examples, and present a general framework to solve such problems. We<br />demonstrate the effectiveness of our approach on optimizing the performance of online la-<br />tent Dirichlet allocation subject to topic sparsity constraints, tuning a neural network given<br />test-time memory constraints, and optimizing Hamiltonian Monte Carlo to achieve maximal<br />effectiveness in a fixed time, subject to passing standard convergence diagnostics.<br />1 Introduction<br />Bayesian optimization (Mockus et al., 1978) is a method for performing global optimization of<br />unknown “black box” objectives that is particularly appropriate when objective function evalua-<br />tions are expensive (in any sense, such as time or money). For example, consider a food company<br />trying to design a low-calorie variant of a popular cookie. In this case, the design space is the<br />space of possible recipes and might include several key parameters such as quantities of various<br />ingredients and baking times. Each evaluation of a recipe entails computing (or perhaps actually<br />measuring) the number of calories in the proposed cookie. Bayesian optimization can be used to<br />propose new candidate recipes such that good results are found with few evaluations.<br />Now suppose the company also wants to ensure the taste of the cookie is not compromised<br />when calories are reduced. Therefore, for each proposed low-calorie recipe, they perform a taste<br />test with sample customers. Because different people, or the same people at different times, have<br />differing opinions about the taste of cookies, the company decides to require that at least 95% of<br />test subjects must like the new cookie. This is a constrained optimization problem:<br />min<br />x<br />c(x) s.t. ρ(x) ≥ 1 − ? ,<br />where x is a real-valued vector representing a recipe, c(x) is the number of calories in recipe<br />x, ρ(x) is the fraction of test subjects that like recipe x, and 1 − ? is the minimum acceptable<br />fraction, i.e., 95%.<br />This paper presents a general formulation of constrained Bayesian optimization that is suitable<br />for a large class of problems such as this one.<br />recognition performance on a smart phone such that the user’s speech is transcribed within some<br />acceptable time limit, or minimizing the cost of materials for a new bridge, subject to the constraint<br />that all safety margins are met.<br />Another use of constraints arises when the search space is known a priori but occupies a<br />complicated volume that cannot be expressed as simple coordinate-wise bounds on the search<br />variables. For example, in a chemical synthesis experiment, it may be known that certain com-<br />binations of reagents cause an explosion to occur. This constraint is not unknown in the sense<br />Other examples might include tuning speech<br />1<br />arXiv:1403.5607v1  [stat.ML]  22 Mar 2014</p>  <p>Page 2</p> <p>of being a discovered property of the environment as in the examples above—we do not want to<br />discover the constraint boundary by trial and error explosions of our laboratory. Rather, we would<br />like to specify this constraint using a boolean noise-free oracle function that declares input vectors<br />as valid or invalid. Our formulation of constrained Bayesian optimization naturally encapsulates<br />such constraints.<br />1.1 Bayesian Optimization<br />Bayesian optimization proceeds by iteratively developing a global statistical model of the unknown<br />objective function. Starting with a prior over functions and a likelihood, at each iteration a<br />posterior distribution is computed by conditioning on the previous evaluations of the objective<br />function, treating them as observations in a Bayesian nonlinear regression. An acquisition function<br />is used to map beliefs about the objective function to a measure of how promising each location in<br />input space is, if it were to be evaluated next. The goal is then to find the input that maximizes<br />the acquisition function, and submit it for function evaluation.<br />Maximizing the acquisition function is ideally a relatively easy proxy optimization problem:<br />evaluations of the acquisition function are often inexpensive, do not require the objective to be<br />queried, and may have gradient information available. Under the assumption that evaluating the<br />objective function is expensive, the time spent computing the best next evaluation via this inner<br />optimization problem is well spent. Once a new result is obtained, the model is updated, the<br />acquisition function is recomputed, and a new input is chosen for evaluation. This completes one<br />iteration of the Bayesian optimization loop.<br />For an in-depth discussion of Bayesian optimization, see Brochu et al. (2010b) or Lizotte (2008).<br />Recent work has extended Bayesian optimization to multiple tasks and objectives (Krause and<br />Ong, 2011; Swersky et al., 2013; Zuluaga et al., 2013) and high dimensional problems (Wang et al.,<br />2013; Djolonga et al., 2013). Strong theoretical results have also been developed (Srinivas et al.,<br />2010; Bull, 2011; de Freitas et al., 2012). Bayesian optimization has been shown to be a powerful<br />method for the meta-optimization of machine learning algorithms (Snoek et al., 2012; Bergstra<br />et al., 2011) and algorithm configuration (Hutter et al., 2011).<br />1.2 Expected Improvement<br />An acquisition function for Bayesian optimization should address the exploitation vs. exploration<br />tradeoff: the idea that we are interested both in regions where the model believes the objective<br />function is low (“exploitation”) and regions where uncertainty is high (“exploration”). One such<br />choice is the Expected Improvement (EI) criterion (Mockus et al., 1978), an acquisition function<br />shown to have strong theoretical guarantees (Bull, 2011) and empirical effectiveness (e.g., Snoek<br />et al., 2012). The expected improvement, EI(x), is defined as the expected amount of improvement<br />over some target t, if we were to evaluate the objective function at x:<br />?∞<br />where p(y |x) is the predictive marginal density of the objective function at x, and (t − y)+≡ max(0,t − y)<br />is the improvement (in the case of minimization) over the target t. EI encourages both exploitation<br />and exploration because it is large for inputs with a low predictive mean (exploitation) and/or a<br />high predictive variance (exploration). Often, t is set to be the minimum over previous observa-<br />tions (e.g., Snoek et al., 2012), or the minimum of the expected value of the objective (Brochu<br />et al., 2010a). Following our formulation of the problem, we use the minimum expected value of<br />the objective such that the probabilistic constraints are satisfied (see Section 1.5, Eq., 6).<br />When the predictive distribution under the model is Gaussian, EI has a closed-form expression<br />(Jones, 2001):<br />EI(x) = E[(t − y)+] =<br />−∞<br />(t − y)+p(y |x)dy ,(1)<br />EI(x) = σ(x)(z(x)Φ(z(x)) + φ(z(x))) (2)<br />where z(x) ≡t−µ(x)<br />is the standard normal CDF, and φ(·) is the standard normal PDF. This function is differentiable<br />σ(x), µ(x) is the predictive mean at x, σ2(x) is the predictive variance at x, Φ(·)<br />2</p>  <p>Page 3</p> <p>and fast to compute, and can therefore be maximized with a standard gradient-based optimizer.<br />In Section 3 we present an acquisition function for constrained Bayesian optimization based on<br />EI.<br />1.3 Our Contributions<br />The main contribution of this paper is a general formulation for constrained Bayesian optimiza-<br />tion, along with an acquisition function that enables efficient optimization of such problems. Our<br />formulation is suitable for addressing a large class of constrained problems, including those con-<br />sidered in previous work. The specific improvements are enumerated below.<br />First, our formulation allows the user to manage uncertainty when constraint observations are<br />noisy. By reformulating the problem with probabilistic constraints, the user can directly address<br />this uncertainty by specifying the required confidence that constraints are satisfied.<br />Second, we consider the class of problems for which the objective function and constraint<br />function need not be evaluated jointly. In the cookie example, the number of calories might be<br />predicted very cheaply with a simple calculation, while evaluating the taste is a large under-<br />taking requiring human trials. Previous methods, which assume joint evaluations, might query a<br />particular recipe only to discover that the objective (calorie) function for that recipe is highly unfa-<br />vorable. The resources spent simultaneously evaluating the constraint (taste) function would then<br />be very poorly spent. We present an acquisition function for such problems, which incorporates<br />this user-specified cost information.<br />Third, our framework, which supports an arbitrary number of constraints, provides an expres-<br />sive language for specifying arbitrarily complicated restrictions on the parameter search spaces.<br />For example if the total memory usage of a neural network must be within some bound, this<br />restriction could be encoded as a separate, noise-free constraint with very low cost. As described<br />above, evaluating this low-cost constraint would take priority over the more expensive constraints<br />and/or objective function.<br />1.4 Prior Work<br />There has been some previous work on constrained Bayesian optimization. Gramacy and Lee<br />(2010) propose an acquisition function called the integrated expected conditional improvement<br />(IECI), defined as<br />?<br />In the above, EI(x?) is the expected improvement at x?, EI(x?|x) is the expected improvement at<br />x?given that the objective has been observed at x (but without making any assumptions about<br />the observed value), and h(x?) is an arbitrary density over x?. In words, the IECI at x is the<br />expected reduction in EI at x?, under the density h(x?), caused by observing the objective at<br />x. Gramacy and Lee use IECI for constrained Bayesian optimization by setting h(x?) to the<br />probability of satisfying the constraint. This formulation encourages evaluations that inform the<br />model in places that are likely to satisfy the constraint.<br />Zuluaga et al. (2013) propose the Pareto Active Learning (PAL) method for finding Pareto-<br />optimal solutions when multiple objective functions are present and the input space is a discrete<br />set. Their algorithm classifies each design candidate as either Pareto-optimal or not, and proceeds<br />iteratively until all inputs are classified. The user may specify a confidence parameter determining<br />the tradeoff between the number of function evaluations and prediction accuracy. Constrained<br />optimization can be considered a special case of multi-objective optimization in which the user’s<br />utility function for the “constraint objectives” is an infinite step function: constant over the<br />feasible region and negative infinity elsewhere. However, PAL solves different problems than those<br />we intend to solve, because it is limited to discrete sets and aims to classify each point in the set<br />versus finding a single optimal solution.<br />Snoek (2013) discusses constrained Bayesian optimization for cases in which constraint viola-<br />tions arise from a failure mode of the objective function, such as a simulation crashing or failing to<br />terminate. The thesis introduces the weighted expected improvement acquisition function, namely<br />IECI(x) =<br />X<br />[EI(x?) − EI(x?|x)]h(x?)dx?<br />(3)<br />3</p>  <p>Page 4</p> <p>expected improvement weighted by the predictive probability that the constraint is satisfied at<br />that input.<br />1.5 Formalizing the Problem<br />In Bayesian optimization, the objective and constraint functions are in general unknown for two<br />reasons. First, the functions have not been observed everywhere, and therefore we must interpolate<br />or extrapolate their values to new inputs. Second, our observations may be noisy; even after<br />multiple observations at the same input, the true function is not known. Accounting for this<br />uncertainty is the role of the model, see Section 2.<br />However, before solving the problem, we must first define it. Returning to the cookie example,<br />each taste test yields an estimate of ρ(x), the fraction of test subjects that like recipe x. But<br />uncertainty is always present, even after many measurements. Therefore, it is impossible to be<br />certain that the constraint ρ(x) ≥ 1 − ? is satisfied for any x. Likewise, the objective function can<br />only be evaluated point-wise and, if noise is present, it may never be determined with certainty.<br />This is a stochastic programming problem: namely, an optimization problem in which the ob-<br />jective and/or constraints contain uncertain quantities whose probability distributions are known<br />or can be estimated (see e.g., Shapiro et al., 2009). A natural formulation of these problems is to<br />minimize the objective function in expectation, while satisfying the constraints with high probabil-<br />ity. The condition that the constraint be satisfied with high probability is called a probabilistic<br />constraint. This concept is formalized below.<br />Let f(x) represent the objective function. Let C(x) represent the the constraint condition,<br />namely the boolean function indicating whether or not the constraint is satisfied for input x. For<br />example, in the cookie problem, C(x) ⇐⇒ ρ(x) ≥ 1 − ?. Then, our probabilistic constraint is<br />Pr(C(x)) ≥ 1 − δ ,<br />for some user-specified minimum confidence 1 − δ.<br />If K constraints are present, for each constraint k ∈ (1,...,K) we define Ck(x) to be the<br />constraint condition for constraint k. Each constraint may also have its own tolerance δk, so we<br />have K probabilistic constraints of the form<br />(4)<br />Pr(Ck(x)) ≥ 1 − δk. (5)<br />All K probabilistic constraints must ultimately be satisfied at a solution to the optimization<br />problem.1<br />Given these definitions, a general class of constrained Bayesian optimization problems can be<br />formulated as<br />min<br />x<br />E[f(x)] s.t. ∀k Pr(Ck(x)) ≥ 1 − δk. (6)<br />The remainder of this paper proposes methods for solving problems in this class using Bayesian<br />optimization. Two key ingredients are needed: a model of the objective and constraint functions<br />(Section 2), and an acquisition function that determines which input x would be most beneficial<br />to observe next (Section 3).<br />2 Modeling the Constraints<br />2.1 Gaussian Processes<br />We use Gaussian processes (GPs) to model both the objective function f(x) and the constraint<br />functions. A GP is a generalization of the multivariate normal distribution to arbitrary index sets,<br />including infinite length vectors or functions, and is specified by its positive definite covariance<br />kernel function K(x,x?). GPs allow us to condition on observed data and tractably compute the<br />1Note: this formulation is based on individual constraint satisfaction for all constraints. Another reasonable<br />formulation requires the (joint) probability that all constraints are satisfied to be above some single threshold.<br />4</p>  <p>Page 5</p> <p>posterior distribution of the model for any finite number of query points. A consequence of this<br />property is that the marginal distribution at any single point is univariate Gaussian with a known<br />mean and variance. See Rasmussen and Williams (2006) for an in-depth treatment of GPs for<br />machine learning.<br />We assume the objective and all constraints are independent and model them with independent<br />GPs. Note that since the objective and constraints are all modeled independently, they need not<br />all be modeled with GPs or even with the same types of models as each other. Any combination<br />of models suffices, so long as each one represents its uncertainty about the true function values.<br />2.2 The latent constraint function, g(x)<br />In order to model constraint conditions Ck(x), we introduce real-valued latent constraint functions<br />gk(x) such that for each constraint k, the constraint condition Ck(x) is satisfied if and only<br />if gk(x) ≥ 0.2<br />below. By computing the posterior distribution of gk(x) for each constraint, we can then compute<br />Pr(Ck(x)) = Pr(gk(x) ≥ 0) by simply evaluating the Gaussian CDF using the predictive marginal<br />mean and variance of the GP at x.<br />Different constraints require different definitions of the constraint function g(x). When the<br />nature of the problem permits constraint observations to be modeled with a Gaussian likelihood,<br />the posterior distribution of g(x) can be computed in closed form. If not, approximations or<br />sampling methods are needed (see Rasmussen and Williams, 2006, p. 41-75). We discuss two<br />examples below, one of each type, respectively.<br />Different observation models lead to different likelihoods on g(x), as discussed<br />2.3 Example I: bounded running time<br />Consider optimizing some property of a computer program such that its running time τ(x) must<br />not exceed some value τmax. Because τ(x) is a measure of time, it is nonnegative for all x<br />and thus not well-modeled by a GP prior. We therefore choose to model time in logarithmic<br />units. In particular, we define g(x) = logτmax− logτ, so that the condition g(x) ≥ 0 corresponds<br />to our constraint condition τ ≤ τmax, and place a GP prior on g(x). For every problem, this<br />transformation implies a particular prior on the original variables; in this case, the implied prior<br />on τ(x) is the log-normal distribution. In this problem we may also posit a Gaussian likelihood<br />for observations of g(x). This corresponds to the generative model that constraint observations<br />are generated by some true latent function corrupted with i.i.d. Gaussian noise. As with the<br />prior, this choice implies something about the original function τ(x), in this case a log-normal<br />likelihood. The basis for these choices is their computational convenience. Given a Gaussian prior<br />and likelihood, the posterior distribution is also Gaussian and can be computed in closed form<br />using the standard GP predictive equations.<br />2.4Example II: modeling cookie tastiness<br />Recall the cookie optimization, and let us assume that constraint observations arrive as a set of<br />counts indicating the numbers of people who did and did not like the cookies. We call these<br />binomial constraint observations. Because these observations are discrete, they are not modeled<br />well by a GP prior. Instead, we model the (unknown) binomial probability ρ(x) that a test subject<br />likes cookie x, which is linked to the observations through a binomial likelihood.3<br />1.5, we selected the constraint condition ρ(x) ≥ 1 − ?, where 1 − ? is the user-specified threshold<br />representing the minimum allowable probability that a test subject likes the new cookie. Because<br />ρ(x) ∈ (0,1) and g(x) ∈ R, we define g(x) = s−1(ρ(x)), where s(·) is a monotonically increasing<br />sigmoid function mapping R → (0,1) as in logistic or probit regression.4In our implementation,<br />we use s(z) = Φ(z), the Gaussian CDF. The likelihood of g(x) given the binomial observations<br />In Section<br />2Any inequality constraint g(x) ≤ g0or g(x) ≥ g1can be represented this way by transforming to a new variable<br />ˆ g(x) ≡ g0− g(x) ≥ 0 or ˆ g(x) ≡ g(x) − g1≥ 0, respectively, so we set the right-hand side to zero without loss of<br />generality.<br />3We use the notation ρ(x) both for the fraction of test subjects who like recipe x and for its generative inter-<br />pretation as the probability that a subject likes recipe x.<br />4When the number of binomial trials is one, this model is called Gaussian Process Classification.<br />5</p>  <p>Page 6</p> <p>is then the binomial likelihood composed with s−1. Because this likelihood is non-Gaussian, the<br />posterior distribution cannot be computed in closed form, and therefore approximation or sampling<br />methods are needed.<br />2.5 Integrating out the GP hyperparameters<br />Following Snoek et al. (2012), we use the Mat´ ern 5/2 kernel for the Gaussian process prior, which<br />corresponds to the assumption that the function being modeled is twice differentiable. This kernel<br />has D + 1 hyperparameters in D dimensions: one characteristic length scale per dimension, and<br />an overall amplitude. Again following Snoek et al. (2012), we perform a fully-Bayesian treatment<br />by integrating out these kernel hyperparameters with Markov chain Monte Carlo (MCMC) via<br />slice sampling (Neal, 2000).<br />When the posterior distribution cannot be computed in closed form due to a non-Gaussian<br />likelihood, we use elliptical slice sampling (Murray et al., 2010) to sample g(x). We also use the<br />prior whitening procedure described in Murray and Adams (2010) to avoid poor mixing due to<br />the strong coupling of the latent values and the kernel hyperparameters.<br />3 Acquisition Functions<br />3.1 Constraint weighted expected improvement<br />Given the probabilistic constraints and the model for a particular problem, it remains to specify an<br />acquisition function that leads to efficient optimization. Here, we present an acquisition function<br />for constrained Bayesian optimization under the Expected Improvement (EI) criterion (Section<br />1.2). However, the general framework presented here does not depend on this specific choice and<br />can be used in conjunction with any improvement criterion.<br />Because improvement is not possible when the constraint is violated, we can define an ac-<br />quisition function for constrained Bayesian optimization by extending the expectation in Eq. 1<br />to include the additional constraint uncertainty. This results in a constraint-weighted expected<br />improvement criterion, a(x):<br />a(x) = EI(x)Pr(C(x))(7)<br />= EI(x)<br />K<br />?<br />k=1<br />Pr(Ck(x))(8)<br />where the second line follows from the assumed independence of the constraints.<br />Then, the full acquisition function a(x), after integrating out the GP hyperparameters, is given<br />by<br />?<br />where θ is the set of GP hyperparameters for the objective function model, ω is the set of GP<br />hyperparameters for the constraint model(s), D = {xn,yn}N<br />observations, and D?are the constraint function observations.<br />a(x) =EI(x|θ)p(θ|D)p(C(x)|x,D?,ω)p(ω|D?)dθdω,<br />n=1are the previous objective function<br />3.2 Finding the feasible region<br />The acquisition function given above is not defined when at least one probabilistic constraint is<br />violated for all x, because in this case the EI target does not exist and therefore EI cannot be<br />computed. In this case we take the acquisition function to include only the second factor,<br />a(x) =<br />K<br />?<br />k=1<br />Pr(gk(x) ≥ 0) (9)<br />Intuitively, if the probabilistic constraint is violated everywhere, we ignore the objective function<br />and try to satisfy the probabilistic constraint until it is satisfied somewhere. This acquisition<br />6</p>  <p>Page 7</p> <p>-50510<br />0<br />5<br />10<br />15<br />(a) True objective<br />-50510<br />0<br />5<br />10<br />15<br />(b) True constraint<br />-50510<br />0<br />5<br />10<br />15<br />(c) Objective GP mean<br />-50510<br />0<br />5<br />10<br />15<br />(d) Objective GP variance<br />-50510<br />0<br />5<br />10<br />15<br />0.00<br />0.25<br />0.50<br />0.75<br />1.00<br />(e) Pr(g(x) ≥ 0)<br />-5 0510<br />0<br />5<br />10<br />15<br />(f) Pr(g(x) ≥ 0) ≥ 0.99<br />-5 0510<br />0<br />5<br />10<br />15<br />(g) a(x)<br />-50510<br />0<br />5<br />10<br />15<br />(h) pmin(x)<br />Figure 1: Constrained Bayesian optimization on the 2D Branin-Hoo function with a disk con-<br />straint, after 50 iterations (33 objective evaluations and 17 constraint evaluations): (a) Branin-Hoo<br />function, (b) true constraint, (c) mean of objective function GP, (d) variance of objective function<br />GP, (e) probability of constraint satisfaction, (f) probabilistic constraint, Pr(g(x) ≥ 0) ≥ 0.99,<br />(g) acquisition function, a(x), and (h) probability distribution over the location of the mini-<br />mum, pmin(x). Lighter colors indicate lower values. Objective function observations are indicated<br />with black circles in (c) and (d). Constraint observations are indicated with black ×’s (violations)<br />and o’s (satisfactions) in (e). Orange stars: (a) unique true minimum of the constrained problem,<br />(c) best solution found by Bayesian optimization, (g) input chosen for the next evaluation, in this<br />case an objective evaluation because ∆So(x) &gt; ∆Sc(x) at the next observation location x.<br />function may also be used if no objective function exists, i.e., if the problem is just to search for<br />any feasible input. This feasibility search is purely exploitative: it searches where the probability<br />of satisfying the constraints is highest. This is possible because the true probability of constraint<br />satisfaction is either zero or one. Therefore, as the algorithm continues to probe a particular<br />region, it will either discover that the region is feasible, or the probability will drop and it will<br />move on to a more promising region.<br />3.3Acquisition function for decoupled observations<br />In some problems, the objective and constraint functions may be evaluated independently. We<br />call this property the decoupling of the objective and constraint functions. In decoupled problems,<br />we must choose to evaluate either the objective function or one of the constraint functions at each<br />iteration of Bayesian optimization. As discussed in Section 1.3, it is important to identify problems<br />with this decoupled structure, because often some of the functions are much more expensive to<br />evaluate than others. Bayesian optimization with decoupled constraints is a form of multi-task<br />Bayesian optimization (Swersky et al., 2013), in which the different black-boxes or tasks are<br />the objective and decoupled constraint(s), represented by the set {objective,1,2,...,K} for K<br />constraints.<br />3.3.1Chicken and Egg Pathology<br />One possible acquisition function for decoupled constraints is the expected improvement of indi-<br />vidually evaluating each task. However, the myopic nature of the EI criterion causes a pathology<br />in this formulation that prevents exploration of the design space. Consider a situation, with a<br />single constraint, in which some feasible region has been identified and thus the current best input<br />is defined, but a large unexplored region remains. Evaluating only the objective in this region<br />could not cause improvement as our belief about Pr(g(x) ≥ 0) will follow the prior and not ex-<br />ceed the threshold 1 − δ. Likewise, evaluating only the constraint would not cause improvement<br />because our belief about the objective will follow the prior and is unlikely to become the new<br />7</p>  <p>Page 8</p> <p>best. This is a causality dilemma: we must learn that both the objective and the constraint are<br />favorable for improvement to occur, but this is not possible when only a single task is observed.<br />This difficulty suggests a non-myopic aquisition function which assesses the improvement after a<br />sequence of objective and constraint observations. However, such a multi-step acquisition function<br />is intractable in general (Ginsbourger and Riche, 2010).<br />Instead, to address this pathology, we propose to use the coupled acquisition function (Eq. 7)<br />to select an input x for observation, followed by a second step to determine which task will be<br />evaluated at x. Following Swersky et al. (2013), we use the entropy search criterion (Hennig and<br />Schuler, 2012) to select a task. However, our framework does not depend on this choice.<br />3.3.2Entropy Search Criterion<br />Entropy search works by considering pmin(x), the probability distribution over the location of the<br />minimum of the objective function. Here, we extend the definition of pminto be the probability<br />distribution over the location of the solution to the constrained problem. Entropy search seeks<br />the action that, in expectation, most reduces the relative entropy between pmin(x) and an un-<br />informative base distribution such as the uniform distribution. Intuitively speaking, we want to<br />reduce our uncertainty about pminas much as possible at each step, or, in other words, maximize<br />our information gain at each step. Following Hennig and Schuler (2012), we choose b(x) to be<br />the uniform distribution on the input space. Given this choice, the relative entropy of pminand<br />b is the differential entropy of pminup to a constant that does not affect the choice of task. Our<br />decision criterion is then<br />?<br />where T is one of the tasks in {objective,1,2,...,K}, T∗is the selected task, S(·) is the differential<br />entropy functional, and p(yT)<br />minis pmin conditioned on observing the value yT for task T. When<br />integrating out the GP covariance hyperparameters, the full form is<br />?<br />where yT is a possible observed outcome of selecting task T and θ and ω are the objective and<br />constraint GP hyperparameters respectively.5<br />T∗= argmin<br />T<br />Ey<br />S<br />?<br />p(yT)<br />min<br />?<br />− S(pmin)<br />?<br />, (10)<br />T∗= argmin<br />T<br />S<br />?<br />p(yT)<br />min<br />?<br />p(yT|θ,ω)dyTdθdω(11)<br />3.3.3Entropy Search in Practice<br />Solving Eq. 11 poses several practical difficulties, which we address here in turn. First, estimat-<br />ing pmin(x) requires a discretization of the space. In the spirit of Hennig and Schuler (2012),<br />we form a discretization of Nd points by taking the top Nd points according to the weighted<br />expected improvement criterion. Second, pmincannot be computed in closed form and must be<br />either estimated or approximated. Swersky et al. (2013) use Monte Carlo sampling to estimate<br />pminby drawing samples from the GP on the discretization set and finding the minimum. We use<br />the analogous method for constrained optimization: we sample from the objective function GP<br />and all K constraint GPs, and then find the minimum of the objective for which the constraint is<br />satisfied for all K constraint samples.<br />3.3.4 Incorporating cost information<br />Following Swersky et al. (2013), we incorporate information about the relative cost of the tasks<br />by simply scaling the acquisition functions by these costs (provided by the user). In doing so, we<br />pick the task with the most information gain per unit cost. If λAis the cost of observing task A,<br />then Eq. 10 becomes<br />?<br />A∗= argmin<br />A<br />1<br />λAEy<br />S<br />?<br />p(yA)<br />min<br />?<br />− S(pmin)<br />?<br />. (12)<br />5For brevity, we have omitted the base entropy term (which does not affect the decision T∗) and the explicit<br />dependence of pminon θ and ω.<br />8</p>  <p>Page 9</p> <p>1020304050<br />7.5<br />8<br />8.5<br />9<br />9.5<br />Function Evaluations<br />Min Function Value<br /> <br /> <br />Constrained GP EI MCMC<br />GP EI MCMC<br />(a) Online LDA<br />2040 6080100<br />0.1<br />0.2<br />0.3<br />0.4<br />0.5<br />0.6<br />0.7<br />0.8<br />Function Evaluations<br />Min Function Value<br /> <br /> <br />Constrained GP EI MCMC<br />GP EI MCMC<br />(b) Neural Network<br />Figure 2: Empirical performance of constrained Bayesian optimization for (a) Online Latent<br />Dirichlet Allocation and (b) turning a deep neural network.<br />curve: unconstrained Bayesian optimization with constraint violations as large values. Errors<br />bars indicate standard error from 5 independent runs.<br />Blue curve: our method.Red<br />01234<br />log10τ<br />-3<br />-2<br />-1<br />0<br />log10?<br />-5.4<br />-3<br />-4.8<br />-4.2<br />-3.6<br />-2<br />-3.0<br />log10?<br />-2.4<br />-1<br />-1.8<br />-1.2<br />(a) Objective<br />tion<br />Func-<br />01234<br />log10τ<br />0<br />0.00<br />-3<br />0.25<br />0.50<br />-2<br />0.75<br />-1<br />log10?<br />1.00<br />(b) Geweke<br />01234<br />log10τ<br />0<br />0.00<br />-3<br />0.25<br />0.50<br />-2<br />0.75<br />-1<br />log10?<br />1.00<br />(c) Gelman-Rubin<br />01234<br />log10τ<br />0<br />0.00<br />-3<br />0.25<br />0.50<br />-2<br />0.75<br />-1<br />log10?<br />1.00<br />(d) Stability<br />01234<br />log10τ<br />0<br />0.00<br />0.25<br />0.50<br />0.75<br />1.00<br />(e) Overall<br />Figure 3: Tuning Hamiltonian Monte Carlo with constrained Bayesian optimization: (a) objective<br />function model, (b-e) constraint satisfaction probability surfaces for (b) Geweke test, (c) Gelman-<br />Rubin test, (d) stability of the numerical integration, (d) overall, which is the product of the<br />preceding three probability surfaces. In (a), lighter colors correspond to more effective samples,<br />circles indicate function evaluations, and the orange star indicates the best solution. In (b-e),<br />constraint observations are indicated with black ×’s (violations) and o’s (satisfactions). Vertical<br />axis label at left is for all subplots. Probability colormap at right is for (b-d).<br />4 Experiments<br />4.1Branin-Hoo function<br />We first illustrate constrained Bayesian optimization on the Branin-Hoo function, a 2D function<br />with three global minima (Fig. 1(a)). We add a decoupled disk constraint (x1− 2.5)2+ (x2− 7.5)2) ≤ 50,<br />shown in Fig. 1(b). This constraint eliminates the upper-left and lower-right solutions, leaving a<br />unique global minimum at x = (π,2.275), indicated by the orange star in Fig. 1(a). After 33 ob-<br />jective function evaluations and 17 constraint evaluations, the best solution is (3.01,2.36), which<br />satisfies the constraint and has value 0.48 (true best value = 0.40).<br />4.2Online LDA with sparse topics<br />Online Latent Dirichlet Allocation (LDA, Hoffman et al., 2010) is an efficient variational formu-<br />lation of a popular topic model for learning topics and corresponding word distributions given<br />a corpus of documents. In order for topics to have meaningful semantic interpretations, it is<br />desirable for the word distributions to exhibit sparsity. In this experiment we optimize the hy-<br />perparameters of online LDA subject to the constraint that the entropy of the per-topic word<br />distribution averaged over topics is less than log2200 bits, which is achieved, for example by al-<br />locating uniform density over 200 words. We used the online LDA implementation from Agarwal<br />et al. (2011) and optimized five hyperparameters corresponding to the number of topics (from<br />9</p>  <p>Page 10</p> <p>2 to 100), two Dirichlet distribution prior base measures (from 0 to 2), and two learning rate pa-<br />rameters (rate from 0.1 to 1, decay from 10−5to 1). As a baseline, we compare with unconstrained<br />Bayesian optimization in which constraint violations are set to the worst possible value for this<br />LDA problem. Fig. 2(a) shows that constrained Bayesian optimization significantly outperforms<br />the baseline. Intuitively, the baseline is poor because the GP has difficulty modeling the sharp<br />discontinuities caused by the large values.<br />Table 1: Tuning Hamiltonian Monte Carlo.<br />Experimentburn-inτ?mass# samples accepteffective samples<br />Baseline<br />BayesOpt<br />10%<br />3.8%<br />100<br />2<br />0.047<br />0.048<br />1<br />1.55<br />8.3 × 103<br />3.3 × 105<br />85%<br />70%<br />1.1 × 103<br />9.7 × 104<br />4.3Memory-limited neural net<br />In the final experiment, we optimize the hyperparameters of a deep neural network on the MNIST<br />handwritten digit classification task in a memory-constrained scenario.<br />parameters: 1 learning rate, 2 momentum parameters (initial and final), the number of hidden<br />units per layer (2 layers), the maximum norm on model weights (for 3 sets of weights), and<br />the dropout regularization probabilities (for the inputs and 2 hidden layers). We optimize the<br />classification error on a withheld validation set under the constraint that the total number of<br />model parameters (weights in the network) must be less than one million. This constraint is<br />decoupled from the objective and inexpensive to evaluate, because the number of weights can<br />be calculated directly from the parameters, without training the network. We train the neural<br />network using momentum-based stochastic gradient descent which is notoriously difficult to tune<br />as training can diverge under various combinations of the momentum and learning rate. When<br />training diverges, the objective function cannot be measured. Reporting the constraint violation<br />as a large objective value performs poorly because it introduces sharp discontinuities that are<br />hard to model (Fig. 2). This necessitates a second noisy, binary constraint which is violated when<br />training diverges, for example when the both the learning rate and momentum are too large. The<br />network is trained6for 25,000 weight updates and the objective is reported as classification error<br />on the standard validation set. Our Bayesian optimization routine can thus choose between two<br />decoupled tasks, evaluating the memory constraint or the validation error after a full training<br />run.Evaluating the validation error can still cause a constraint violation when the training<br />diverges, which is treated as a binary constraint in our model. Fig. 2(b) shows a comparison of<br />our constrained Bayesian optimization against a baseline standard Bayesian optimization where<br />constraint violations are treated as resulting in a random classifier (90% error). Only the objective<br />evaluations are presented, since constraint evaluations are extremely inexpensive compared to an<br />entire training run. In the event that training diverges on an objective evaluation, we report 90%<br />error. The optimized net has a learning rate of 0.1, dropout probabilities of 0.17 (inputs), 0.30<br />(first layer), and 0 (second layer), initial momentum 0.86, and final momentum 0.81. Interestingly,<br />the optimization chooses a small first layer (size 312) and a large second layer (size 1772).<br />We optimize over 11<br />4.4 Tuning Markov chain Monte Carlo<br />Hamiltonian Monte Carlo (HMC) is a popular MCMC sampling technique that takes advantage<br />of gradient information for rapid mixing. However, HMC contains several parameters that re-<br />quire careful tuning. The two basic parameters are the number of leapfrog steps τ, and the step<br />size ?. HMC may also include a mass matrix which introduces O(D2) additional parameters in<br />D dimensions, although the matrix is often chosen to be diagonal (D parameters) or a multiple<br />of the identity matrix (1 parameter) (Neal, 2011). In this experiment, we optimize the perfor-<br />mance of HMC using Bayesian optimization; see Mahendran et al. (2012) for a similar approach.<br />6We use the Deepnet package: https://github.com/nitishsrivastava/deepnet.<br />10</p>  <p>Page 11</p> <p>We optimize the following parameters: τ, ?, a mass parameter, and the fraction of the allotted<br />computation time spent burning in the chain.<br />Our experiment measures the number of effective samples (ES) in a fixed computation time;<br />this corresponds to finding chains that minimize estimator variance. We impose the constraints<br />that the generated samples must pass the Geweke (Geweke, 1992) and Gelman-Rubin (Gelman<br />and Rubin, 1992) convergence diagnostics. In particular, we require the worst (largest absolute<br />value) Geweke test score across all variables and chains to be at most 2.0, and the worst (largest)<br />Gelman-Rubin score between chains and across all variables to be at most 1.2. We use PyMC<br />(Patil et al., 2010) for the convergence diagnostics and the LaplacesDemon R package to compute<br />effective sample size. The chosen thresholds for the convergence diagnostics are based on the<br />PyMC and LaplacesDemon documentation. The HMC integration may also diverge for large<br />values of ?; we treat this as an additional constraint, and set δ = 0.05 for all constraints. We<br />optimize HMC sampling from the posterior of a logistic regression binary classification problem<br />using the German credit data set from the UCI repository (Frank and Asuncion, 2010). The data<br />set contains 1000 data points, and is normalized to have unit variance. We initialize each chain<br />randomly with D independent draws from a Gaussian distribution with mean zero and standard<br />deviation 10−3. For each set of inputs, we compute two chains, each with 5 minutes of computation<br />time on a single core of a compute node.<br />Fig. 3 shows the constraint surfaces discovered by Bayesian optimization for a simpler ex-<br />periment in which only τ and ? are varied; burn-in is fixed at 10% and the mass is fixed at 1.<br />These diagrams yield interpretations of the feasible region; for example, Fig. 3(d) shows that<br />the numerical integration diverges for values of ? above ≈ 10−1. Table 1 shows the results of<br />our 4-parameter optimization after 50 iterations, compared with a baseline that is reflective of a<br />typical HMC configuration: 10% burn in, 100 leapfrog steps, and the step size chosen to yield an<br />85% proposal accept rate. Each row in the table was produced by averaging 5 independent runs<br />with the given parameters. The optimization chooses to perform very few (τ = 2) leapfrog steps<br />and spend relatively little time (3.8%) burning in the chain, and chooses an acceptance rate of<br />70%. In contrast, the baseline spends much more time generating each proposal (τ = 100), which<br />produces many fewer total samples and, correspondingly, significantly fewer effective samples.<br />5Conclusion<br />In this paper we extended Bayesian optimization to constrained optimization problems. Because<br />constraint observations may be noisy, we formulate the problem using probabilistic constraints,<br />allowing the user to directly express the tradeoff between cost and risk by specifying the confidence<br />parameter δ. We then propose an acquisition function to perform constrained Bayesian optimiza-<br />tion, including the case where the objective and constraint(s) may be observed independently. We<br />demonstrate the effectiveness of our system on the meta-optimization of machine learning algo-<br />rithms and sampling techniques. Constrained optimization is a ubiquitous problem and we believe<br />this work has applications in areas such as product design (e.g. designing a low-calorie cookie),<br />machine learning meta-optimization (as in our experiments), real-time systems (such as a speech<br />recognition system on a mobile device with speed, memory, and/or energy usage constraints),<br />or any optimization problem in which the objective function and/or constraints are expensive to<br />evaluate and possibly noisy.<br />Acknowledgements<br />The authors would like to thank Geoffrey Hinton, George Dahl, and Oren Rippel for helpful<br />discussions, and Robert Nishihara for help with the experiments. This work was partially funded<br />by DARPA Young Faculty Award N66001-12-1-4219. Jasper Snoek is a fellow in the Harvard<br />Center for Research on Computation and Society.<br />11</p>  <p>Page 12</p> <p>References<br />Alekh Agarwal, Olivier Chapelle, Miroslav Dud´ ık, and John Langford. A reliable effective terascale<br />linear learning system, 2011. arXiv: 1110.4198 [cs.LG].<br />James S. Bergstra, R´ emi Bardenet, Yoshua Bengio, and B´ al´ azs K´ egl. Algorithms for hyper-<br />parameter optimization. In NIPS. 2011.<br />Eric Brochu, Tyson Brochu, and Nando de Freitas. A Bayesian interactive optimization approach<br />to procedural animation design. In ACM SIGGRAPH/Eurographics Symposium on Computer<br />Animation, 2010a.<br />Eric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of expen-<br />sive cost functions, 2010b. arXiv:1012.2599 [cs.LG].<br />Adam D. Bull. Convergence rates of efficient global optimization algorithms. JMLR, (3-4):2879–<br />2904, 2011.<br />Nando de Freitas, Alex Smola, and Masrour Zoghi. Exponential regret bounds for Gaussian<br />process bandits with deterministic observations. In ICML, 2012.<br />Josip Djolonga, Andreas Krause, and Volkan Cevher. High dimensional Gaussian Process bandits.<br />In NIPS, 2013.<br />Andrew Frank and Arthur Asuncion. UCI machine learning repository, 2010.<br />Andrew Gelman and Donald R. Rubin. A single series from the Gibbs sampler provides a false<br />sense of security. In Bayesian Statistics, pages 625–32. Oxford University Press, 1992.<br />John Geweke. Evaluating the accuracy of sampling-based approaches to the calculation of posterior<br />moments. In Bayesian Statistics, pages 169–193. University Press, 1992.<br />David Ginsbourger and Rodolphe Riche. Towards Gaussian process-based optimization with finite<br />time horizon. In Advances in Model-Oriented Design and Analysis. Physica-Verlag HD, 2010.<br />Robert B. Gramacy and Herbert K. H. Lee. Optimization under unknown constraints, 2010.<br />arXiv:1004.4027 [stat.ME].<br />Philipp Hennig and Christian J. Schuler. Entropy search for information-efficient global optimiza-<br />tion. JMLR, 13, 2012.<br />Matthew Hoffman, David M. Blei, and Francis Bach. Online learning for latent Dirichlet allocation.<br />In NIPS, 2010.<br />Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization<br />for general algorithm configuration. In LION, 2011.<br />Donald R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal<br />of Global Optimization, 21, 2001.<br />Andreas Krause and Cheng Soon Ong. Contextual Gaussian Process bandit optimization. In<br />NIPS, 2011.<br />Dan Lizotte. Practical Bayesian Optimization. PhD thesis, University of Alberta, Edmonton,<br />Alberta, 2008.<br />Nimalan Mahendran, Ziyu Wang, Firas Hamze, and Nando de Freitas. Adaptive MCMC with<br />Bayesian optimization. In AISTATS, 2012.<br />Jonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for<br />seeking the extremum. Towards Global Optimization, 2, 1978.<br />12</p>  <p>Page 13</p> <p>Iain Murray and Ryan P. Adams. Slice sampling covariance hyperparameters of latent Gaussian<br />models. In NIPS, 2010.<br />Iain Murray, Ryan P. Adams, and David J.C. MacKay. Elliptical slice sampling. JMLR, 9:541–548,<br />2010.<br />Radford Neal. Slice sampling. Annals of Statistics, 31:705–767, 2000.<br />Radford Neal. MCMC using Hamiltonian dynamics. In Handbook of Markov Chain Monte Carlo.<br />Chapman and Hall/CRC, 2011.<br />Anand Patil, David Huard, and Christopher Fonnesbeck. PyMC: Bayesian stochastic modelling<br />in Python. Journal of Statistical Software, 2010.<br />Carl Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning. MIT Press,<br />2006.<br />A. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on stochastic programming: modeling<br />and theory. MPS-SIAM Series on Optimization, Philadelphia, USA, 2009.<br />Jasper Snoek. Bayesian Optimization and Semiparametric Models with Applications to Assistive<br />Technology. PhD thesis, University of Toronto, Toronto, Canada, 2013.<br />Jasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine<br />learning algorithms. In NIPS, 2012.<br />Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process opti-<br />mization in the bandit setting: no regret and experimental design. In ICML, 2010.<br />Kevin Swersky, Jasper Snoek, and Ryan P. Adams. Multi-task Bayesian optimization. In NIPS,<br />2013.<br />Ziyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando de Freitas. Bayesian<br />optimization in high dimensions via random embeddings. In IJCAI, 2013.<br />Marcela Zuluaga, Andreas Krause, Guillaume Sergent, and Markus P¨ uschel. Active learning for<br />multi-objective optimization. In ICML, 2013.<br />13</p>   </div> <div id="rgw21_56aba1f479f68" class="c-box pub-resource-container js-toggle" style=""> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw22_56aba1f479f68">  <li class="c-list-item pub-resource-item"  data-type="fulltext" id="rgw23_56aba1f479f68"> <span class="ico-pub"></span> <div class="file-content"> <div class="download"> <a href="http://de.arxiv.org/pdf/1403.5607" target="_blank" rel="nofollow" class="publication-viewer" title="Bayesian Optimization with Unknown Constraints">Bayesian Optimization with Unknown Constraints</a> </div>  <div class="details">   Available from <a href="http://de.arxiv.org/pdf/1403.5607" target="_blank" rel="nofollow">de.arxiv.org</a>  </div>    </div> </li>  </ul> </div> </div> </noscript> <div class="clearfix"> <div class="action-container">  </div> <div class="pub-legal"> Data provided are for informational purposes only. Although carefully collected, accuracy cannot be guaranteed. The impact factor represents a rough estimation of the journal's impact factor and does not reflect the actual current impact factor. Publisher conditions are provided by RoMEO. Differing provisions from the publisher's actual policy or licence agreement may be applicable. </div>   </div> </div> <div class="clearfix">     <div id="rgw25_56aba1f479f68" class="c-box pub-resource-container js-toggle" style="display: none;"> <h4 class="lf js-expand-list"><small><a href="javascript:" class="lf">View other sources <span class="ico-expand-list"></span></a></small></h4> <h4 class="lf js-collapse-list hidden"><small><a href="javascript:" class="lf">Hide other sources <span class="ico-collapse-list"></span></a></small></h4> <div class="clear"></div> <div class="scroll-wrapper hidden"> <ul class="files-list c-list" id="rgw26_56aba1f479f68">  </ul> </div> </div>   <div id="rgw17_56aba1f479f68" class="similar-publications"> <h2>Similar Publications</h2> <ul class="list-bordered">  <li class="c-list-item li-publication-teaser" id="rgw18_56aba1f479f68"> <div> <h5> <a href="publication/290452676_Technology_Global_Optimization_of_User_Computer_Program_Code" class="color-inherit ga-similar-publication-title"><span class="publication-title">Technology Global Optimization of User Computer Program Code</span></a>  </h5>  <div class="authors"> <a href="researcher/2094120902_Murat_Hasanbekovich_Tomaev" class="authors ga-similar-publication-author">Murat Hasanbekovich Tomaev</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw19_56aba1f479f68"> <div> <h5> <a href="publication/290478537_Preface_to_the_Special_Issue_Recent_developments_in_non-linear_and_global_optimization" class="color-inherit ga-similar-publication-title"><span class="publication-title">Preface to the Special Issue “Recent developments in non-linear and global optimization”</span></a>  </h5>  <div class="authors"> <a href="researcher/2094078770_Yaroslav_D_Sergeyev" class="authors ga-similar-publication-author">Yaroslav D. Sergeyev</a>  </div>  </div> <div class="clear"></div> </li>  <li class="c-list-item li-publication-teaser" id="rgw20_56aba1f479f68"> <div> <h5> <a href="publication/291437208_Link_community_detection_through_global_optimization_and_the_inverse_resolution_limit_of_partition_density" class="color-inherit ga-similar-publication-title"><span class="publication-title">Link community detection through global optimization and the inverse resolution limit of partition density</span></a>  </h5>  <div class="authors"> <a href="researcher/14313139_Juyong_Lee" class="authors ga-similar-publication-author">Juyong Lee</a>, <a href="researcher/79533785_Zhong-Yuan_Zhang" class="authors ga-similar-publication-author">Zhong-Yuan Zhang</a>, <a href="researcher/39648239_Jooyoung_Lee" class="authors ga-similar-publication-author">Jooyoung Lee</a>, <a href="researcher/42954827_Bernard_R_Brooks" class="authors ga-similar-publication-author">Bernard R. Brooks</a>, <a href="researcher/2042554358_Yong-Yeol_Ahn" class="authors ga-similar-publication-author">Yong-Yeol Ahn</a>  </div>  </div> <div class="clear"></div> </li>  </ul> </div> </div> </div> </div> </div></div></div>
<div class="clear"></div><div id="rgw31_56aba1f479f68" class="default-footer"> <div id="footer" class="clearfix"> <span class="footer-left"> &copy; 2008&dash;2016 researchgate.net. All rights reserved. </span> <span class="footer-right"> <a href="https://www.researchgate.net/about">About us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="https://www.researchgate.net/contact">Contact us</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="careers">Careers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="developers">Developers</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="blog" target="_blank">News</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.PrivacyPolicy.html">Privacy</a><span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="application.TermsAndConditions.html">Terms</a> <span class="footer-link-separator" style="padding: 0 5px;">&nbsp;|&nbsp;</span> <a href="advertising?_ref=ft">Advertising</a> <span class="footer-link-separator">&nbsp;&middot;&nbsp;</span> <a href="recruiters?_ref=ft">Recruiting</a> </span> </div>  </div></div>
<div id="rgw32_56aba1f479f68">  <div class="header-wrapper-logged-out"> <div id="header"> <div class="header-content"> <a href="" class="g-l-logo"> <svg width="149" height="19"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="149" height="19"/> </svg> </a> <a href="" class="g-l-logo-ico"> <svg width="127" height="17" viewbox="0 0 127 22"> <image xlink:href="https://c5.rgstatic.net/m/235107188705592/images/template/brand-header-logo.svg" src="https://c5.rgstatic.net/m/238113351022438/images/template/brand-header-logo.png" width="127" height="17"/> </svg> </a> <div id="rgw33_56aba1f479f68" class="header-login-wrapper js-header-login"> <div class="dropdown-right-align"> <div class="login-signup-container lf"> <a href="https://www.researchgate.net/signup.SignUp.html?ev=su_chnl_index&amp;hdrsu=1&amp;_sg=X1C_axkLrbX-2A-ELv22LpvCWNdaTVyvp-Yau5ZPoWkQrZHLKfnAcjWKUOV1F5g7" class="dropdown-toggle lf">Join for free</a> </div> <div class="dropdown lf" style="height: 20px;"> <a href="https://www.researchgate.net/application.Login.html" class="js-login-url dropdown-toggle lf">Log in <span class="caret"></span></a> <div class="dropdown-menu"> <div class="header-login-form-wrapper"> <!--[if IE 6]><p class="box-warning" style="margin-bottom: 0;">Sorry, ResearchGate no longer supports the version of Internet Explorer you are using. <a href="http://whatbrowser.org/" rel="nofollow" target="_blank">Update your web browser</a> and then log in. </p><![endif]--> <form method="post" action="https://www.researchgate.net/application.Login.html" class="form-big header-login-form js-login-form" name="loginForm" id="headerLoginForm"> <input type="hidden" name="request_token" value="RilBMm9dtoMY34doR46E8za0vGML06V5qhHM/reN7BMqn/ZQdEIq3tWEBIZe4BK1yn7M6AVNJkGnLk5eZzNcff0aNWwKHFim9YAjrgnMKTfLezFwwm/Dff+wlPylZEF9bS+4T9dpt23Dm+726y2HMXBgXxhWTJQ3JVYnYiNOyLo0MiebecYdNNNtBBlclYhUYVAHGY0Z1BPpmDu9Ku7M+u10Ib1S1ijalf3hUt9/H8HiWRapRSI7d/O+btjG8TsBsqWaR6XoA8NnycXP04g7+QM+sIJVuKJtuijz5ZuToiU="/> <input type="hidden" name="urlAfterLogin" value="publication/261065571_Bayesian_Optimization_with_Unknown_Constraints"/> <input type="hidden" name="invalidPasswordCount" value="0"/> <input type="hidden" name="headerLogin" value="yes"/> <label for="input-header-login">Email</label> <div class="login-input"> <div class="info-tip-wrapper"> <span class="ico-info js-info"></span> </div> <input type="email" value="" name="login" class="login js-login-input text" id="input-header-login" tabindex="1"/> </div> <div class="clear"></div> <label class="lf" for="input-header-password"> Password </label> <a class="rf forgot-password js-forgot-password" href="application.LostPassword.html">Forgot password?</a> <div class="clear"></div> <input type="password" value="" name="password" class="password js-password-input text" id="input-header-password" tabindex="2"/> <div class="clear"></div> <label class="remember-me" for="headerLoginCookie"> <input type="checkbox" checked="checked" value="yes" name="setLoginCookie" class="lf checkbox" id="headerLoginCookie" tabindex="3"/> Keep me logged in </label> <div class="clear"></div> <input value="Log in" name="loginSubmit" class="btn btn-promote btn-fullwidth btn-large allow-leave js-submit-button" type="submit" tabindex="4"/> </form> <div class="connectors"> <div class="text">or log in with</div> <div class="connector-actions"> <a href="connector/linkedin/" class="li-connect js-li-connect" data-redirect-url="cHVibGljYXRpb24vMjYxMDY1NTcxX0JheWVzaWFuX09wdGltaXphdGlvbl93aXRoX1Vua25vd25fQ29uc3RyYWludHM%3D"> <span class="icon ico-linkedin-round-grey"></span> <span class="icon ico-linkedin-round"></span> </a> <a href="connector/facebook/" class="fb-connect middle js-fb-connect" data-redirect-url="cHVibGljYXRpb24vMjYxMDY1NTcxX0JheWVzaWFuX09wdGltaXphdGlvbl93aXRoX1Vua25vd25fQ29uc3RyYWludHM%3D"> <span class="icon ico-facebook-round-grey"></span> <span class="icon ico-facebook-round"></span> </a> <a href="connector/google/" class="g-connect js-g-connect" data-redirect-url="cHVibGljYXRpb24vMjYxMDY1NTcxX0JheWVzaWFuX09wdGltaXphdGlvbl93aXRoX1Vua25vd25fQ29uc3RyYWludHM%3D"> <span class="icon ico-google-round-grey"></span> <span class="icon ico-google-round"></span> </a> </div> </div> </div> </div> </div> </div> </div> </div> </div> <script type="application/ld+json">
{ "@context" : "http://schema.org",
  "@type" : "Organization",
  "name" : "ResearchGate",
  "url" : "http://www.researchgate.net",
  "logo" : "http://www.researchgate.net/images/template/rg_logo_square_brand.png",
  "sameAs" : [ "https://www.facebook.com/ResearchGate",
    "https://twitter.com/ResearchGate",
    "https://plus.google.com/+researchgate",
    "https://www.linkedin.com/company/researchgate"] 
}
</script> </div> </div><div class="c-signup-bar" id="rgw34_56aba1f479f68"> <div class="banner-contents">   <span class="message">ResearchGate is the professional network for scientists and researchers.</span> <a href="signup.SignUp.html?ev=su_banner" class="btn btn-large btn-promote">Join for free</a>  </div> </div></div>
<script>
rgConfig.backendTime = 403;
</script>
<script src="//c5.rgstatic.net/m/2277196935388619/javascript/lib/yui3/yui/yui-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/21832295316281274/javascript/yuiLoaderConfig-min.js" type="text/javascript"></script>
        <script src="//c5.rgstatic.net/m/23178613132105398/javascript/vendor/babel-core/browser-polyfill.min.js" type="text/javascript"></script>
<script>
(function (){
if (typeof YRG === "undefined") {
var xmlHttpRequest = new XMLHttpRequest();
xmlHttpRequest.open("post", "go.Error.html");
xmlHttpRequest.setRequestHeader("Content-Type", "application/json");
xmlHttpRequest.setRequestHeader("Accept", "application/json"); var loadedScripts = "";
if (window.performance && window.performance.getEntriesByType) {
    var result = [];
    var resources = performance.getEntriesByType("resource");
    for (var i in resources) {
        if (resources.hasOwnProperty(i)) {
            result.push({
                name: resources[i].name,
                duration: resources[i].duration
            });
        }
    }
    loadedScripts += "&loadedScripts=" + encodeURIComponent(JSON.stringify(result));
}
if (typeof YUI === "undefined") {
    loadedScripts += "&yuiLoaded=false";
} else {
    loadedScripts += "&yuiLoaded=true";
}
xmlHttpRequest.send("Type=InformationException&message=" + encodeURIComponent("Error loading YUI") + loadedScripts);
}
})();
</script>
<script>if (typeof YRG !== 'undefined') { YRG.use('rg-base',function(Y){Y.applyConfig({ignore: ["css-rg","css-rg2","css-ie","css-modules-publicprofile"]});Y.use(["rg.core.pagespeed.Monitoring"],function(Y){(function(){Y.rg.createInitialWidget({"data":{"content":{"data":{"profileSmallHeader":{"data":{"profileBadge":{"data":{"profileUrl":"researcher\/2043445261_Michael_A_Gelbart","fullname":"Michael A. Gelbart","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2549355721578\/images\/template\/default\/profile\/profile_default_m.png","profileStats":[{"data":{"impactPoints":"24.13","widgetId":"rgw5_56aba1f479f68"},"id":"rgw5_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorImpactPoints.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorImpactPoints.html?authorUid=2043445261","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"publicationCount":9,"widgetId":"rgw6_56aba1f479f68"},"id":"rgw6_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicLiteratureAuthorPublicationCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorPublicationCount.html?authorUid=2043445261","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},null],"widgetId":"rgw4_56aba1f479f68"},"id":"rgw4_56aba1f479f68","partials":[],"templateName":"publicprofile\/stubs\/ProfileBadge.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicLiteratureAuthorBadge.html?authorUid=2043445261","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw3_56aba1f479f68"},"id":"rgw3_56aba1f479f68","partials":[],"templateName":"publicprofile\/stubs\/ProfileSmallHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicprofile.ProfileSmallHeader.html?publicationUid=261065571","viewClass":null,"yuiModules":["css-modules-publicprofile"],"stylesheets":["modules\/publicprofile.css"],"_isYUI":true},"publication":{"data":{"publicationUid":261065571,"title":"Bayesian Optimization with Unknown Constraints","journalTitle":false,"journalDetailsTooltip":false,"affiliation":false,"type":"Article","details":{"journalInfos":{"journal":"Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014","publicationDate":"03\/2014;","publicationDateRobot":"2014-03","article":""}},"source":{"sourceUrl":"http:\/\/arxiv.org\/abs\/1403.5607","sourceName":"arXiv"},"publicationActions":null,"publicationCoins":{"data":{"tags":[{"key":"ctx_ver","value":"Z39.88-2004"},{"key":"rft_val_fmt","value":"info:ofi\/fmt:kev:mtx:journal"},{"key":"rfr_id","value":"info:sid\/researchgate.net:researchgate"},{"key":"rft.atitle","value":"Bayesian Optimization with Unknown Constraints"},{"key":"rft.title","value":"Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014"},{"key":"rft.jtitle","value":"Uncertainty in Artificial Intelligence - Proceedings of the 30th Conference, UAI 2014"},{"key":"rft.date","value":"2014"},{"key":"rft.au","value":"Michael A. Gelbart,Jasper Snoek,Ryan P. Adams"},{"key":"rft.genre","value":"article"}],"widgetId":"rgw8_56aba1f479f68"},"id":"rgw8_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationCoins.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationCoins.html?publicationUid=261065571","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationAuthors":{"data":{"publicationUid":261065571,"peopleItems":[{"data":{"authorUrl":"researcher\/2043445261_Michael_A_Gelbart","authorNameOnPublication":"Michael A. Gelbart","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Michael A. Gelbart","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/2043445261_Michael_A_Gelbart","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw11_56aba1f479f68"},"id":"rgw11_56aba1f479f68","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=2043445261&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw10_56aba1f479f68"},"id":"rgw10_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=2043445261&authorNameOnPublication=Michael%20A.%20Gelbart","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/70743985_Jasper_Snoek","authorNameOnPublication":"Jasper Snoek","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Jasper Snoek","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/70743985_Jasper_Snoek","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw13_56aba1f479f68"},"id":"rgw13_56aba1f479f68","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=70743985&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw12_56aba1f479f68"},"id":"rgw12_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=70743985&authorNameOnPublication=Jasper%20Snoek","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true},{"data":{"authorUrl":"researcher\/71165702_Ryan_P_Adams","authorNameOnPublication":"Ryan P. Adams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg","imageSize":"s","imageWidth":20,"imageHeight":20,"peopleItem":{"data":{"displayName":"Ryan P. Adams","profile":{"professionalInstitution":{"professionalInstitutionName":false,"professionalInstitutionUrl":null}},"professionalInstitutionName":false,"professionalInstitutionUrl":null,"url":"researcher\/71165702_Ryan_P_Adams","imageUrl":"https:\/\/c5.rgstatic.net\/m\/237738464651637\/images\/template\/default\/profile\/profile_default_l.jpg","imageSize":"l","imageHeight":90,"imageWidth":90,"enableFollowButton":true,"enableHideButton":false,"enableConnectionButton":false,"followButton":null,"isClaimedAuthor":false,"hasExtraContainer":false,"showStatsWidgets":false,"statsWidgets":[],"additionalCssClasses":null,"contentWidgetsRight":[],"contentWidgetsMain":[],"showHideButton":false,"widgetId":"rgw15_56aba1f479f68"},"id":"rgw15_56aba1f479f68","partials":[],"templateName":"application\/stubs\/PeopleItem.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.PeopleAuthorItem.html?entityId=71165702&imageSize=l&enableAuthorInviteButton=0","viewClass":"views.application.PeopleItemView","yuiModules":["rg.views.application.PeopleItemView"],"stylesheets":[],"_isYUI":true},"peopleItemRenderable":true,"largeTooltip":false,"useRebrandedImageStyle":null,"widgetId":"rgw14_56aba1f479f68"},"id":"rgw14_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorItem.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorItem.html?authorUid=71165702&authorNameOnPublication=Ryan%20P.%20Adams","viewClass":"views.publicliterature.PublicationDetailAccountItemView","yuiModules":["rg.views.publicliterature.PublicationDetailAccountItemView"],"stylesheets":[],"_isYUI":true}],"hasMore":false,"nextOffset":6,"useRebrandedImageStyle":null,"widgetId":"rgw9_56aba1f479f68"},"id":"rgw9_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAuthorList.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAuthorList.html?publicationUid=261065571&context=pubdetail_authors_xflw&showContactAuthorButton=1","viewClass":"views.publicliterature.PublicationDetailAuthorListView","yuiModules":["rg.views.publicliterature.PublicationDetailAuthorListView"],"stylesheets":[],"_isYUI":true},"publicationAbstract":{"data":{"publicationUid":261065571,"abstract":"<noscript><\/noscript><div>Recent work on Bayesian optimization has shown its effectiveness in global<br \/>\noptimization of difficult black-box objective functions. Many real-world<br \/>\noptimization problems of interest also have constraints which are unknown a<br \/>\npriori. In this paper, we study Bayesian optimization for constrained problems<br \/>\nin the general case that noise may be present in the constraint functions, and<br \/>\nthe objective and constraints may be evaluated independently. We provide<br \/>\nmotivating practical examples, and present a general framework to solve such<br \/>\nproblems. We demonstrate the effectiveness of our approach on optimizing the<br \/>\nperformance of online latent Dirichlet allocation subject to topic sparsity<br \/>\nconstraints, tuning a neural network given test-time memory constraints, and<br \/>\noptimizing Hamiltonian Monte Carlo to achieve maximal effectiveness in a fixed<br \/>\ntime, subject to passing standard convergence diagnostics.<\/div>","canEdit":false,"isAdmin":false,"isArtifact":false,"showFullAbstract":true,"widgetId":"rgw16_56aba1f479f68"},"id":"rgw16_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationAbstract.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationAbstract.html?publicationUid=261065571","viewClass":"views.publicliterature.PublicationAbstractView","yuiModules":["rg.views.publicliterature.PublicationAbstractView"],"stylesheets":[],"_isYUI":true},"publicationKeywords":null,"publicationState":null,"isGuest":true,"isAdminEditingAllowed":false,"isArtifact":false,"figureListWidget":null,"previewImage":"https:\/\/i1.rgstatic.net\/publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints\/links\/03353e7e0cf216bc595c8d9c\/smallpreview.png","nativeAdDisclosure":null,"showFollowPublicationButton":false,"followPublicationPromo":"","widgetId":"rgw7_56aba1f479f68"},"id":"rgw7_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailItem.html","templateExtensions":["generalHelpers"],"attrs":{"preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailItem.html?publicationUid=261065571&showActionBar=0&showContactAuthorButton=1&showRequestFulltextExperience=0&showNoRgAuthorsRequestFulltextExperience=0&showFollowPublicationPromo=0","viewClass":"views.publicliterature.PublicationDetailItemView","yuiModules":["rg.views.publicliterature.PublicationDetailItemView"],"stylesheets":[],"_isYUI":true},"similarPublications":{"data":{"publicationListItems":[{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2094120902,"url":"researcher\/2094120902_Murat_Hasanbekovich_Tomaev","fullname":"Murat Hasanbekovich Tomaev","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290452676_Technology_Global_Optimization_of_User_Computer_Program_Code","usePlainButton":true,"publicationUid":290452676,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/290452676_Technology_Global_Optimization_of_User_Computer_Program_Code","title":"Technology Global Optimization of User Computer Program Code","displayTitleAsLink":true,"authors":[{"id":2094120902,"url":"researcher\/2094120902_Murat_Hasanbekovich_Tomaev","fullname":"Murat Hasanbekovich Tomaev","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["01\/2016;  DOI:10.12731\/2306-1561-2015-3-2"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290452676_Technology_Global_Optimization_of_User_Computer_Program_Code","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290452676_Technology_Global_Optimization_of_User_Computer_Program_Code\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw18_56aba1f479f68"},"id":"rgw18_56aba1f479f68","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290452676","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":2094078770,"url":"researcher\/2094078770_Yaroslav_D_Sergeyev","fullname":"Yaroslav D. Sergeyev","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":null,"surplusAuthors":null,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":"Optimization Letters","showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/290478537_Preface_to_the_Special_Issue_Recent_developments_in_non-linear_and_global_optimization","usePlainButton":true,"publicationUid":290478537,"feedStubs":null,"hasFeedStubs":false,"impactPoints":"0.93","url":"publication\/290478537_Preface_to_the_Special_Issue_Recent_developments_in_non-linear_and_global_optimization","title":"Preface to the Special Issue \u201cRecent developments in non-linear and global optimization\u201d","displayTitleAsLink":true,"authors":[{"id":2094078770,"url":"researcher\/2094078770_Yaroslav_D_Sergeyev","fullname":"Yaroslav D. Sergeyev","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":["Optimization Letters 01\/2016;  DOI:10.1007\/s11590-015-0995-z"],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/290478537_Preface_to_the_Special_Issue_Recent_developments_in_non-linear_and_global_optimization","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/290478537_Preface_to_the_Special_Issue_Recent_developments_in_non-linear_and_global_optimization\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw19_56aba1f479f68"},"id":"rgw19_56aba1f479f68","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=290478537","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},{"data":{"milestones":{"moreContext":"experimentMilestoneClickedCitationContextReadMore"},"nextPublicationMilestone":"experimentMilestoneRequestFulltextNoPreview","nextPublicationViewId":null,"authorsPartOne":[{"id":14313139,"url":"researcher\/14313139_Juyong_Lee","fullname":"Juyong Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":79533785,"url":"researcher\/79533785_Zhong-Yuan_Zhang","fullname":"Zhong-Yuan Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39648239,"url":"researcher\/39648239_Jooyoung_Lee","fullname":"Jooyoung Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"authorsPartTwo":{"id":42954827,"url":"researcher\/42954827_Bernard_R_Brooks","fullname":"Bernard R. Brooks","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},"surplusAuthors":1,"additionalCssClasses":null,"isFulltext":null,"isSlurp":false,"isNoText":true,"publicationType":"Article","publicationDate":"Jan 2016","journal":null,"showEnrichedPublicationItem":false,"citationCount":0,"commentCount":0,"requestFulltextButton":null,"publicationUrl":"publication\/291437208_Link_community_detection_through_global_optimization_and_the_inverse_resolution_limit_of_partition_density","usePlainButton":true,"publicationUid":291437208,"feedStubs":null,"hasFeedStubs":false,"impactPoints":false,"url":"publication\/291437208_Link_community_detection_through_global_optimization_and_the_inverse_resolution_limit_of_partition_density","title":"Link community detection through global optimization and the inverse resolution limit of partition density","displayTitleAsLink":true,"authors":[{"id":14313139,"url":"researcher\/14313139_Juyong_Lee","fullname":"Juyong Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":79533785,"url":"researcher\/79533785_Zhong-Yuan_Zhang","fullname":"Zhong-Yuan Zhang","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":39648239,"url":"researcher\/39648239_Jooyoung_Lee","fullname":"Jooyoung Lee","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":42954827,"url":"researcher\/42954827_Bernard_R_Brooks","fullname":"Bernard R. Brooks","last":false,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"},{"id":2042554358,"url":"researcher\/2042554358_Yong-Yeol_Ahn","fullname":"Yong-Yeol Ahn","last":true,"imageUrl":"https:\/\/c5.rgstatic.net\/m\/2951093203564\/images\/template\/default\/profile\/profile_default_s.jpg"}],"displayAuthorsAsLinks":true,"displayAuthors":true,"displayJournalMetadata":true,"collapseTitle":false,"collapseAuthors":false,"collapseAbstract":false,"details":[],"abstract":false,"description":false,"swapJournalAndAuthorPositions":false,"showAbstract":false,"type":"Article","showType":true,"showPublicationIcon":false,"showSeeInContextCta":false,"publicationUrlWithContext":"publication\/291437208_Link_community_detection_through_global_optimization_and_the_inverse_resolution_limit_of_partition_density","isReader":null,"isAuthor":null,"isRequester":false,"hasFulltext":null,"actions":[],"actionWidgets":[],"publicationItemFulltext":null,"linkId":null,"context":null,"contextId":null,"eventCode":"","isCitation":false,"isPendingCitationRequest":false,"canRemoveCitation":false,"citationSourcePublicationUid":false,"publicationCitationUid":false,"showPublicationDownloadDataUsagePermissionDialog":false,"origin":"publication_list","keywordList":null,"hasKeywordList":false,"showOpenReviewButton":false,"openReviewUrl":"publication\/291437208_Link_community_detection_through_global_optimization_and_the_inverse_resolution_limit_of_partition_density\/review","additionalRightSideTopWidgets":[],"additionalRightSideBottomWidgets":[],"requestEndpointPrimary":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","requestEndpointImage":"literature.PublicationPromoRequestFulltextAction.html?dbw=true","doi":false,"hasMicrodata":false,"microdataPropertyName":null,"showPublicationPreview":true,"showPublicationPreviewOnRightSide":false,"showActionsOnRightSide":false,"swapJournalAndAbstractPositions":false,"displayUpdatedExpanderPlugin":false,"showActions":false,"showAuthors":true,"isResearch":false,"publicationLinkText":"Read preview","widgetId":"rgw20_56aba1f479f68"},"id":"rgw20_56aba1f479f68","partials":{"publicationItem":"publicliterature\/stubs\/partials\/publicationItem.html"},"templateName":"publicliterature\/stubs\/PublicationTeaserItem.html","templateExtensions":["generalHelpers"],"attrs":{"opensViewer":false,"requestFulltextInvitationDialogUrl":"literature.PublicationRequestFulltextInvitationDialog.html","followContext":"publication_item","publicationBookmarkEndpointUrl":"literature.AjaxFollowPublication.html","preSignUpDialogContext":null},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationTeaserItem.html?options%5BshowImpactPoints%5D=1&options%5BshowActorActions%5D=1&options%5BshowAbstract%5D=0&options%5BeventCode%5D=&publicationUid=291437208","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"viewClass":"sidebar","widgetId":"rgw17_56aba1f479f68"},"id":"rgw17_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/SimilarPublications.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.SimilarPublications.html?referencePublicationId=261065571&view=sidebar","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationFulltextPreview":null,"publicationResourcesFulltext":{"data":{"publicationUid":261065571,"publicationResourceList":{"data":{"publicationResourceItems":[{"data":{"publicationUid":261065571,"publicationType":"article","linkId":"03353e7e0cf216bc595c8d9c","fileName":"Bayesian Optimization with Unknown Constraints","fileUrl":"http:\/\/de.arxiv.org\/pdf\/1403.5607","name":"de.arxiv.org","nameUrl":"http:\/\/de.arxiv.org\/pdf\/1403.5607","canRemove":false,"fileIcon":"ico-pub","canPreview":true,"hasPublisherLink":null,"hasMetaData":true,"hide":false,"fulltext":"ico-pub","origin":"publication_list","isLastLink":true,"isUserLink":false,"widgetId":"rgw23_56aba1f479f68"},"id":"rgw23_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceItem.html","templateExtensions":["generalHelpers"],"attrs":{"fileRequiredPublicationTypes":["code","coverPage","experimentFindings","method","negativeResults","presentation","rawData","researchProposal","workingPaper","artifact","dataset"]},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceItem.html?publicationUid=261065571&linkId=03353e7e0cf216bc595c8d9c&hide=0&disableJavascript=disableJavascript&useAlternativeTemplate=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true}],"widgetId":"rgw22_56aba1f479f68"},"id":"rgw22_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=261065571&limit=3&disableJavascript=disableJavascript&type=fulltextFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":1,"hidden":false,"showMore":false,"fulltext":true,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw24_56aba1f479f68"},"id":"rgw24_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=261065571","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw21_56aba1f479f68"},"id":"rgw21_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=261065571&type=fulltextFile&disableJavascript=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationResourcesAttachments":{"data":{"publicationUid":261065571,"publicationResourceList":{"data":{"publicationResourceItems":[],"widgetId":"rgw26_56aba1f479f68"},"id":"rgw26_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationResourceList.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResourceList.html?publicationUid=261065571&limit=3&disableJavascript=disableJavascript&type=attachmentFile","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"resourceCount":0,"hidden":true,"showMore":false,"fulltext":false,"publicationDownloadCount":{"data":{"value":2,"valueFormatted":"2","widgetId":"rgw27_56aba1f479f68"},"id":"rgw27_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDownloadCount.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDownloadCount.html?publicationUid=261065571","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"widgetId":"rgw25_56aba1f479f68"},"id":"rgw25_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationResources.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationResources.html?publicationUid=261065571&type=attachmentFile","viewClass":"views.publicliterature.PublicationResourcesView","yuiModules":["rg.views.publicliterature.PublicationResourcesView"],"stylesheets":[],"_isYUI":true},"publicationText":{"data":{"hide":false,"pages":[{"page":1,"text":"Bayesian Optimization with Unknown Constraints\nMichael A. Gelbart, Jasper Snoek, and Ryan P. Adams\nSchool of Engineering and Applied Sciences, Harvard University\n{mgelbart, jsnoek, rpa} @ seas.harvard.edu\nAbstract\nRecent work on Bayesian optimization has shown its effectiveness in global optimization\nof difficult black-box objective functions. Many real-world optimization problems of interest\nalso have constraints which are unknown a priori. In this paper, we study Bayesian optimiza-\ntion for constrained problems in the general case that noise may be present in the constraint\nfunctions, and the objective and constraints may be evaluated independently. We provide\nmotivating practical examples, and present a general framework to solve such problems. We\ndemonstrate the effectiveness of our approach on optimizing the performance of online la-\ntent Dirichlet allocation subject to topic sparsity constraints, tuning a neural network given\ntest-time memory constraints, and optimizing Hamiltonian Monte Carlo to achieve maximal\neffectiveness in a fixed time, subject to passing standard convergence diagnostics.\n1 Introduction\nBayesian optimization (Mockus et al., 1978) is a method for performing global optimization of\nunknown \u201cblack box\u201d objectives that is particularly appropriate when objective function evalua-\ntions are expensive (in any sense, such as time or money). For example, consider a food company\ntrying to design a low-calorie variant of a popular cookie. In this case, the design space is the\nspace of possible recipes and might include several key parameters such as quantities of various\ningredients and baking times. Each evaluation of a recipe entails computing (or perhaps actually\nmeasuring) the number of calories in the proposed cookie. Bayesian optimization can be used to\npropose new candidate recipes such that good results are found with few evaluations.\nNow suppose the company also wants to ensure the taste of the cookie is not compromised\nwhen calories are reduced. Therefore, for each proposed low-calorie recipe, they perform a taste\ntest with sample customers. Because different people, or the same people at different times, have\ndiffering opinions about the taste of cookies, the company decides to require that at least 95% of\ntest subjects must like the new cookie. This is a constrained optimization problem:\nmin\nx\nc(x) s.t. \u03c1(x) \u2265 1 \u2212 ? ,\nwhere x is a real-valued vector representing a recipe, c(x) is the number of calories in recipe\nx, \u03c1(x) is the fraction of test subjects that like recipe x, and 1 \u2212 ? is the minimum acceptable\nfraction, i.e., 95%.\nThis paper presents a general formulation of constrained Bayesian optimization that is suitable\nfor a large class of problems such as this one.\nrecognition performance on a smart phone such that the user\u2019s speech is transcribed within some\nacceptable time limit, or minimizing the cost of materials for a new bridge, subject to the constraint\nthat all safety margins are met.\nAnother use of constraints arises when the search space is known a priori but occupies a\ncomplicated volume that cannot be expressed as simple coordinate-wise bounds on the search\nvariables. For example, in a chemical synthesis experiment, it may be known that certain com-\nbinations of reagents cause an explosion to occur. This constraint is not unknown in the sense\nOther examples might include tuning speech\n1\narXiv:1403.5607v1  [stat.ML]  22 Mar 2014"},{"page":2,"text":"of being a discovered property of the environment as in the examples above\u2014we do not want to\ndiscover the constraint boundary by trial and error explosions of our laboratory. Rather, we would\nlike to specify this constraint using a boolean noise-free oracle function that declares input vectors\nas valid or invalid. Our formulation of constrained Bayesian optimization naturally encapsulates\nsuch constraints.\n1.1 Bayesian Optimization\nBayesian optimization proceeds by iteratively developing a global statistical model of the unknown\nobjective function. Starting with a prior over functions and a likelihood, at each iteration a\nposterior distribution is computed by conditioning on the previous evaluations of the objective\nfunction, treating them as observations in a Bayesian nonlinear regression. An acquisition function\nis used to map beliefs about the objective function to a measure of how promising each location in\ninput space is, if it were to be evaluated next. The goal is then to find the input that maximizes\nthe acquisition function, and submit it for function evaluation.\nMaximizing the acquisition function is ideally a relatively easy proxy optimization problem:\nevaluations of the acquisition function are often inexpensive, do not require the objective to be\nqueried, and may have gradient information available. Under the assumption that evaluating the\nobjective function is expensive, the time spent computing the best next evaluation via this inner\noptimization problem is well spent. Once a new result is obtained, the model is updated, the\nacquisition function is recomputed, and a new input is chosen for evaluation. This completes one\niteration of the Bayesian optimization loop.\nFor an in-depth discussion of Bayesian optimization, see Brochu et al. (2010b) or Lizotte (2008).\nRecent work has extended Bayesian optimization to multiple tasks and objectives (Krause and\nOng, 2011; Swersky et al., 2013; Zuluaga et al., 2013) and high dimensional problems (Wang et al.,\n2013; Djolonga et al., 2013). Strong theoretical results have also been developed (Srinivas et al.,\n2010; Bull, 2011; de Freitas et al., 2012). Bayesian optimization has been shown to be a powerful\nmethod for the meta-optimization of machine learning algorithms (Snoek et al., 2012; Bergstra\net al., 2011) and algorithm configuration (Hutter et al., 2011).\n1.2 Expected Improvement\nAn acquisition function for Bayesian optimization should address the exploitation vs. exploration\ntradeoff: the idea that we are interested both in regions where the model believes the objective\nfunction is low (\u201cexploitation\u201d) and regions where uncertainty is high (\u201cexploration\u201d). One such\nchoice is the Expected Improvement (EI) criterion (Mockus et al., 1978), an acquisition function\nshown to have strong theoretical guarantees (Bull, 2011) and empirical effectiveness (e.g., Snoek\net al., 2012). The expected improvement, EI(x), is defined as the expected amount of improvement\nover some target t, if we were to evaluate the objective function at x:\n?\u221e\nwhere p(y |x) is the predictive marginal density of the objective function at x, and (t \u2212 y)+\u2261 max(0,t \u2212 y)\nis the improvement (in the case of minimization) over the target t. EI encourages both exploitation\nand exploration because it is large for inputs with a low predictive mean (exploitation) and\/or a\nhigh predictive variance (exploration). Often, t is set to be the minimum over previous observa-\ntions (e.g., Snoek et al., 2012), or the minimum of the expected value of the objective (Brochu\net al., 2010a). Following our formulation of the problem, we use the minimum expected value of\nthe objective such that the probabilistic constraints are satisfied (see Section 1.5, Eq., 6).\nWhen the predictive distribution under the model is Gaussian, EI has a closed-form expression\n(Jones, 2001):\nEI(x) = E[(t \u2212 y)+] =\n\u2212\u221e\n(t \u2212 y)+p(y |x)dy ,(1)\nEI(x) = \u03c3(x)(z(x)\u03a6(z(x)) + \u03c6(z(x))) (2)\nwhere z(x) \u2261t\u2212\u00b5(x)\nis the standard normal CDF, and \u03c6(\u00b7) is the standard normal PDF. This function is differentiable\n\u03c3(x), \u00b5(x) is the predictive mean at x, \u03c32(x) is the predictive variance at x, \u03a6(\u00b7)\n2"},{"page":3,"text":"and fast to compute, and can therefore be maximized with a standard gradient-based optimizer.\nIn Section 3 we present an acquisition function for constrained Bayesian optimization based on\nEI.\n1.3 Our Contributions\nThe main contribution of this paper is a general formulation for constrained Bayesian optimiza-\ntion, along with an acquisition function that enables efficient optimization of such problems. Our\nformulation is suitable for addressing a large class of constrained problems, including those con-\nsidered in previous work. The specific improvements are enumerated below.\nFirst, our formulation allows the user to manage uncertainty when constraint observations are\nnoisy. By reformulating the problem with probabilistic constraints, the user can directly address\nthis uncertainty by specifying the required confidence that constraints are satisfied.\nSecond, we consider the class of problems for which the objective function and constraint\nfunction need not be evaluated jointly. In the cookie example, the number of calories might be\npredicted very cheaply with a simple calculation, while evaluating the taste is a large under-\ntaking requiring human trials. Previous methods, which assume joint evaluations, might query a\nparticular recipe only to discover that the objective (calorie) function for that recipe is highly unfa-\nvorable. The resources spent simultaneously evaluating the constraint (taste) function would then\nbe very poorly spent. We present an acquisition function for such problems, which incorporates\nthis user-specified cost information.\nThird, our framework, which supports an arbitrary number of constraints, provides an expres-\nsive language for specifying arbitrarily complicated restrictions on the parameter search spaces.\nFor example if the total memory usage of a neural network must be within some bound, this\nrestriction could be encoded as a separate, noise-free constraint with very low cost. As described\nabove, evaluating this low-cost constraint would take priority over the more expensive constraints\nand\/or objective function.\n1.4 Prior Work\nThere has been some previous work on constrained Bayesian optimization. Gramacy and Lee\n(2010) propose an acquisition function called the integrated expected conditional improvement\n(IECI), defined as\n?\nIn the above, EI(x?) is the expected improvement at x?, EI(x?|x) is the expected improvement at\nx?given that the objective has been observed at x (but without making any assumptions about\nthe observed value), and h(x?) is an arbitrary density over x?. In words, the IECI at x is the\nexpected reduction in EI at x?, under the density h(x?), caused by observing the objective at\nx. Gramacy and Lee use IECI for constrained Bayesian optimization by setting h(x?) to the\nprobability of satisfying the constraint. This formulation encourages evaluations that inform the\nmodel in places that are likely to satisfy the constraint.\nZuluaga et al. (2013) propose the Pareto Active Learning (PAL) method for finding Pareto-\noptimal solutions when multiple objective functions are present and the input space is a discrete\nset. Their algorithm classifies each design candidate as either Pareto-optimal or not, and proceeds\niteratively until all inputs are classified. The user may specify a confidence parameter determining\nthe tradeoff between the number of function evaluations and prediction accuracy. Constrained\noptimization can be considered a special case of multi-objective optimization in which the user\u2019s\nutility function for the \u201cconstraint objectives\u201d is an infinite step function: constant over the\nfeasible region and negative infinity elsewhere. However, PAL solves different problems than those\nwe intend to solve, because it is limited to discrete sets and aims to classify each point in the set\nversus finding a single optimal solution.\nSnoek (2013) discusses constrained Bayesian optimization for cases in which constraint viola-\ntions arise from a failure mode of the objective function, such as a simulation crashing or failing to\nterminate. The thesis introduces the weighted expected improvement acquisition function, namely\nIECI(x) =\nX\n[EI(x?) \u2212 EI(x?|x)]h(x?)dx?\n(3)\n3"},{"page":4,"text":"expected improvement weighted by the predictive probability that the constraint is satisfied at\nthat input.\n1.5 Formalizing the Problem\nIn Bayesian optimization, the objective and constraint functions are in general unknown for two\nreasons. First, the functions have not been observed everywhere, and therefore we must interpolate\nor extrapolate their values to new inputs. Second, our observations may be noisy; even after\nmultiple observations at the same input, the true function is not known. Accounting for this\nuncertainty is the role of the model, see Section 2.\nHowever, before solving the problem, we must first define it. Returning to the cookie example,\neach taste test yields an estimate of \u03c1(x), the fraction of test subjects that like recipe x. But\nuncertainty is always present, even after many measurements. Therefore, it is impossible to be\ncertain that the constraint \u03c1(x) \u2265 1 \u2212 ? is satisfied for any x. Likewise, the objective function can\nonly be evaluated point-wise and, if noise is present, it may never be determined with certainty.\nThis is a stochastic programming problem: namely, an optimization problem in which the ob-\njective and\/or constraints contain uncertain quantities whose probability distributions are known\nor can be estimated (see e.g., Shapiro et al., 2009). A natural formulation of these problems is to\nminimize the objective function in expectation, while satisfying the constraints with high probabil-\nity. The condition that the constraint be satisfied with high probability is called a probabilistic\nconstraint. This concept is formalized below.\nLet f(x) represent the objective function. Let C(x) represent the the constraint condition,\nnamely the boolean function indicating whether or not the constraint is satisfied for input x. For\nexample, in the cookie problem, C(x) \u21d0\u21d2 \u03c1(x) \u2265 1 \u2212 ?. Then, our probabilistic constraint is\nPr(C(x)) \u2265 1 \u2212 \u03b4 ,\nfor some user-specified minimum confidence 1 \u2212 \u03b4.\nIf K constraints are present, for each constraint k \u2208 (1,...,K) we define Ck(x) to be the\nconstraint condition for constraint k. Each constraint may also have its own tolerance \u03b4k, so we\nhave K probabilistic constraints of the form\n(4)\nPr(Ck(x)) \u2265 1 \u2212 \u03b4k. (5)\nAll K probabilistic constraints must ultimately be satisfied at a solution to the optimization\nproblem.1\nGiven these definitions, a general class of constrained Bayesian optimization problems can be\nformulated as\nmin\nx\nE[f(x)] s.t. \u2200k Pr(Ck(x)) \u2265 1 \u2212 \u03b4k. (6)\nThe remainder of this paper proposes methods for solving problems in this class using Bayesian\noptimization. Two key ingredients are needed: a model of the objective and constraint functions\n(Section 2), and an acquisition function that determines which input x would be most beneficial\nto observe next (Section 3).\n2 Modeling the Constraints\n2.1 Gaussian Processes\nWe use Gaussian processes (GPs) to model both the objective function f(x) and the constraint\nfunctions. A GP is a generalization of the multivariate normal distribution to arbitrary index sets,\nincluding infinite length vectors or functions, and is specified by its positive definite covariance\nkernel function K(x,x?). GPs allow us to condition on observed data and tractably compute the\n1Note: this formulation is based on individual constraint satisfaction for all constraints. Another reasonable\nformulation requires the (joint) probability that all constraints are satisfied to be above some single threshold.\n4"},{"page":5,"text":"posterior distribution of the model for any finite number of query points. A consequence of this\nproperty is that the marginal distribution at any single point is univariate Gaussian with a known\nmean and variance. See Rasmussen and Williams (2006) for an in-depth treatment of GPs for\nmachine learning.\nWe assume the objective and all constraints are independent and model them with independent\nGPs. Note that since the objective and constraints are all modeled independently, they need not\nall be modeled with GPs or even with the same types of models as each other. Any combination\nof models suffices, so long as each one represents its uncertainty about the true function values.\n2.2 The latent constraint function, g(x)\nIn order to model constraint conditions Ck(x), we introduce real-valued latent constraint functions\ngk(x) such that for each constraint k, the constraint condition Ck(x) is satisfied if and only\nif gk(x) \u2265 0.2\nbelow. By computing the posterior distribution of gk(x) for each constraint, we can then compute\nPr(Ck(x)) = Pr(gk(x) \u2265 0) by simply evaluating the Gaussian CDF using the predictive marginal\nmean and variance of the GP at x.\nDifferent constraints require different definitions of the constraint function g(x). When the\nnature of the problem permits constraint observations to be modeled with a Gaussian likelihood,\nthe posterior distribution of g(x) can be computed in closed form. If not, approximations or\nsampling methods are needed (see Rasmussen and Williams, 2006, p. 41-75). We discuss two\nexamples below, one of each type, respectively.\nDifferent observation models lead to different likelihoods on g(x), as discussed\n2.3 Example I: bounded running time\nConsider optimizing some property of a computer program such that its running time \u03c4(x) must\nnot exceed some value \u03c4max. Because \u03c4(x) is a measure of time, it is nonnegative for all x\nand thus not well-modeled by a GP prior. We therefore choose to model time in logarithmic\nunits. In particular, we define g(x) = log\u03c4max\u2212 log\u03c4, so that the condition g(x) \u2265 0 corresponds\nto our constraint condition \u03c4 \u2264 \u03c4max, and place a GP prior on g(x). For every problem, this\ntransformation implies a particular prior on the original variables; in this case, the implied prior\non \u03c4(x) is the log-normal distribution. In this problem we may also posit a Gaussian likelihood\nfor observations of g(x). This corresponds to the generative model that constraint observations\nare generated by some true latent function corrupted with i.i.d. Gaussian noise. As with the\nprior, this choice implies something about the original function \u03c4(x), in this case a log-normal\nlikelihood. The basis for these choices is their computational convenience. Given a Gaussian prior\nand likelihood, the posterior distribution is also Gaussian and can be computed in closed form\nusing the standard GP predictive equations.\n2.4Example II: modeling cookie tastiness\nRecall the cookie optimization, and let us assume that constraint observations arrive as a set of\ncounts indicating the numbers of people who did and did not like the cookies. We call these\nbinomial constraint observations. Because these observations are discrete, they are not modeled\nwell by a GP prior. Instead, we model the (unknown) binomial probability \u03c1(x) that a test subject\nlikes cookie x, which is linked to the observations through a binomial likelihood.3\n1.5, we selected the constraint condition \u03c1(x) \u2265 1 \u2212 ?, where 1 \u2212 ? is the user-specified threshold\nrepresenting the minimum allowable probability that a test subject likes the new cookie. Because\n\u03c1(x) \u2208 (0,1) and g(x) \u2208 R, we define g(x) = s\u22121(\u03c1(x)), where s(\u00b7) is a monotonically increasing\nsigmoid function mapping R \u2192 (0,1) as in logistic or probit regression.4In our implementation,\nwe use s(z) = \u03a6(z), the Gaussian CDF. The likelihood of g(x) given the binomial observations\nIn Section\n2Any inequality constraint g(x) \u2264 g0or g(x) \u2265 g1can be represented this way by transforming to a new variable\n\u02c6 g(x) \u2261 g0\u2212 g(x) \u2265 0 or \u02c6 g(x) \u2261 g(x) \u2212 g1\u2265 0, respectively, so we set the right-hand side to zero without loss of\ngenerality.\n3We use the notation \u03c1(x) both for the fraction of test subjects who like recipe x and for its generative inter-\npretation as the probability that a subject likes recipe x.\n4When the number of binomial trials is one, this model is called Gaussian Process Classification.\n5"},{"page":6,"text":"is then the binomial likelihood composed with s\u22121. Because this likelihood is non-Gaussian, the\nposterior distribution cannot be computed in closed form, and therefore approximation or sampling\nmethods are needed.\n2.5 Integrating out the GP hyperparameters\nFollowing Snoek et al. (2012), we use the Mat\u00b4 ern 5\/2 kernel for the Gaussian process prior, which\ncorresponds to the assumption that the function being modeled is twice differentiable. This kernel\nhas D + 1 hyperparameters in D dimensions: one characteristic length scale per dimension, and\nan overall amplitude. Again following Snoek et al. (2012), we perform a fully-Bayesian treatment\nby integrating out these kernel hyperparameters with Markov chain Monte Carlo (MCMC) via\nslice sampling (Neal, 2000).\nWhen the posterior distribution cannot be computed in closed form due to a non-Gaussian\nlikelihood, we use elliptical slice sampling (Murray et al., 2010) to sample g(x). We also use the\nprior whitening procedure described in Murray and Adams (2010) to avoid poor mixing due to\nthe strong coupling of the latent values and the kernel hyperparameters.\n3 Acquisition Functions\n3.1 Constraint weighted expected improvement\nGiven the probabilistic constraints and the model for a particular problem, it remains to specify an\nacquisition function that leads to efficient optimization. Here, we present an acquisition function\nfor constrained Bayesian optimization under the Expected Improvement (EI) criterion (Section\n1.2). However, the general framework presented here does not depend on this specific choice and\ncan be used in conjunction with any improvement criterion.\nBecause improvement is not possible when the constraint is violated, we can define an ac-\nquisition function for constrained Bayesian optimization by extending the expectation in Eq. 1\nto include the additional constraint uncertainty. This results in a constraint-weighted expected\nimprovement criterion, a(x):\na(x) = EI(x)Pr(C(x))(7)\n= EI(x)\nK\n?\nk=1\nPr(Ck(x))(8)\nwhere the second line follows from the assumed independence of the constraints.\nThen, the full acquisition function a(x), after integrating out the GP hyperparameters, is given\nby\n?\nwhere \u03b8 is the set of GP hyperparameters for the objective function model, \u03c9 is the set of GP\nhyperparameters for the constraint model(s), D = {xn,yn}N\nobservations, and D?are the constraint function observations.\na(x) =EI(x|\u03b8)p(\u03b8|D)p(C(x)|x,D?,\u03c9)p(\u03c9|D?)d\u03b8d\u03c9,\nn=1are the previous objective function\n3.2 Finding the feasible region\nThe acquisition function given above is not defined when at least one probabilistic constraint is\nviolated for all x, because in this case the EI target does not exist and therefore EI cannot be\ncomputed. In this case we take the acquisition function to include only the second factor,\na(x) =\nK\n?\nk=1\nPr(gk(x) \u2265 0) (9)\nIntuitively, if the probabilistic constraint is violated everywhere, we ignore the objective function\nand try to satisfy the probabilistic constraint until it is satisfied somewhere. This acquisition\n6"},{"page":7,"text":"-50510\n0\n5\n10\n15\n(a) True objective\n-50510\n0\n5\n10\n15\n(b) True constraint\n-50510\n0\n5\n10\n15\n(c) Objective GP mean\n-50510\n0\n5\n10\n15\n(d) Objective GP variance\n-50510\n0\n5\n10\n15\n0.00\n0.25\n0.50\n0.75\n1.00\n(e) Pr(g(x) \u2265 0)\n-5 0510\n0\n5\n10\n15\n(f) Pr(g(x) \u2265 0) \u2265 0.99\n-5 0510\n0\n5\n10\n15\n(g) a(x)\n-50510\n0\n5\n10\n15\n(h) pmin(x)\nFigure 1: Constrained Bayesian optimization on the 2D Branin-Hoo function with a disk con-\nstraint, after 50 iterations (33 objective evaluations and 17 constraint evaluations): (a) Branin-Hoo\nfunction, (b) true constraint, (c) mean of objective function GP, (d) variance of objective function\nGP, (e) probability of constraint satisfaction, (f) probabilistic constraint, Pr(g(x) \u2265 0) \u2265 0.99,\n(g) acquisition function, a(x), and (h) probability distribution over the location of the mini-\nmum, pmin(x). Lighter colors indicate lower values. Objective function observations are indicated\nwith black circles in (c) and (d). Constraint observations are indicated with black \u00d7\u2019s (violations)\nand o\u2019s (satisfactions) in (e). Orange stars: (a) unique true minimum of the constrained problem,\n(c) best solution found by Bayesian optimization, (g) input chosen for the next evaluation, in this\ncase an objective evaluation because \u2206So(x) > \u2206Sc(x) at the next observation location x.\nfunction may also be used if no objective function exists, i.e., if the problem is just to search for\nany feasible input. This feasibility search is purely exploitative: it searches where the probability\nof satisfying the constraints is highest. This is possible because the true probability of constraint\nsatisfaction is either zero or one. Therefore, as the algorithm continues to probe a particular\nregion, it will either discover that the region is feasible, or the probability will drop and it will\nmove on to a more promising region.\n3.3Acquisition function for decoupled observations\nIn some problems, the objective and constraint functions may be evaluated independently. We\ncall this property the decoupling of the objective and constraint functions. In decoupled problems,\nwe must choose to evaluate either the objective function or one of the constraint functions at each\niteration of Bayesian optimization. As discussed in Section 1.3, it is important to identify problems\nwith this decoupled structure, because often some of the functions are much more expensive to\nevaluate than others. Bayesian optimization with decoupled constraints is a form of multi-task\nBayesian optimization (Swersky et al., 2013), in which the different black-boxes or tasks are\nthe objective and decoupled constraint(s), represented by the set {objective,1,2,...,K} for K\nconstraints.\n3.3.1Chicken and Egg Pathology\nOne possible acquisition function for decoupled constraints is the expected improvement of indi-\nvidually evaluating each task. However, the myopic nature of the EI criterion causes a pathology\nin this formulation that prevents exploration of the design space. Consider a situation, with a\nsingle constraint, in which some feasible region has been identified and thus the current best input\nis defined, but a large unexplored region remains. Evaluating only the objective in this region\ncould not cause improvement as our belief about Pr(g(x) \u2265 0) will follow the prior and not ex-\nceed the threshold 1 \u2212 \u03b4. Likewise, evaluating only the constraint would not cause improvement\nbecause our belief about the objective will follow the prior and is unlikely to become the new\n7"},{"page":8,"text":"best. This is a causality dilemma: we must learn that both the objective and the constraint are\nfavorable for improvement to occur, but this is not possible when only a single task is observed.\nThis difficulty suggests a non-myopic aquisition function which assesses the improvement after a\nsequence of objective and constraint observations. However, such a multi-step acquisition function\nis intractable in general (Ginsbourger and Riche, 2010).\nInstead, to address this pathology, we propose to use the coupled acquisition function (Eq. 7)\nto select an input x for observation, followed by a second step to determine which task will be\nevaluated at x. Following Swersky et al. (2013), we use the entropy search criterion (Hennig and\nSchuler, 2012) to select a task. However, our framework does not depend on this choice.\n3.3.2Entropy Search Criterion\nEntropy search works by considering pmin(x), the probability distribution over the location of the\nminimum of the objective function. Here, we extend the definition of pminto be the probability\ndistribution over the location of the solution to the constrained problem. Entropy search seeks\nthe action that, in expectation, most reduces the relative entropy between pmin(x) and an un-\ninformative base distribution such as the uniform distribution. Intuitively speaking, we want to\nreduce our uncertainty about pminas much as possible at each step, or, in other words, maximize\nour information gain at each step. Following Hennig and Schuler (2012), we choose b(x) to be\nthe uniform distribution on the input space. Given this choice, the relative entropy of pminand\nb is the differential entropy of pminup to a constant that does not affect the choice of task. Our\ndecision criterion is then\n?\nwhere T is one of the tasks in {objective,1,2,...,K}, T\u2217is the selected task, S(\u00b7) is the differential\nentropy functional, and p(yT)\nminis pmin conditioned on observing the value yT for task T. When\nintegrating out the GP covariance hyperparameters, the full form is\n?\nwhere yT is a possible observed outcome of selecting task T and \u03b8 and \u03c9 are the objective and\nconstraint GP hyperparameters respectively.5\nT\u2217= argmin\nT\nEy\nS\n?\np(yT)\nmin\n?\n\u2212 S(pmin)\n?\n, (10)\nT\u2217= argmin\nT\nS\n?\np(yT)\nmin\n?\np(yT|\u03b8,\u03c9)dyTd\u03b8d\u03c9(11)\n3.3.3Entropy Search in Practice\nSolving Eq. 11 poses several practical difficulties, which we address here in turn. First, estimat-\ning pmin(x) requires a discretization of the space. In the spirit of Hennig and Schuler (2012),\nwe form a discretization of Nd points by taking the top Nd points according to the weighted\nexpected improvement criterion. Second, pmincannot be computed in closed form and must be\neither estimated or approximated. Swersky et al. (2013) use Monte Carlo sampling to estimate\npminby drawing samples from the GP on the discretization set and finding the minimum. We use\nthe analogous method for constrained optimization: we sample from the objective function GP\nand all K constraint GPs, and then find the minimum of the objective for which the constraint is\nsatisfied for all K constraint samples.\n3.3.4 Incorporating cost information\nFollowing Swersky et al. (2013), we incorporate information about the relative cost of the tasks\nby simply scaling the acquisition functions by these costs (provided by the user). In doing so, we\npick the task with the most information gain per unit cost. If \u03bbAis the cost of observing task A,\nthen Eq. 10 becomes\n?\nA\u2217= argmin\nA\n1\n\u03bbAEy\nS\n?\np(yA)\nmin\n?\n\u2212 S(pmin)\n?\n. (12)\n5For brevity, we have omitted the base entropy term (which does not affect the decision T\u2217) and the explicit\ndependence of pminon \u03b8 and \u03c9.\n8"},{"page":9,"text":"1020304050\n7.5\n8\n8.5\n9\n9.5\nFunction Evaluations\nMin Function Value\n \n \nConstrained GP EI MCMC\nGP EI MCMC\n(a) Online LDA\n2040 6080100\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFunction Evaluations\nMin Function Value\n \n \nConstrained GP EI MCMC\nGP EI MCMC\n(b) Neural Network\nFigure 2: Empirical performance of constrained Bayesian optimization for (a) Online Latent\nDirichlet Allocation and (b) turning a deep neural network.\ncurve: unconstrained Bayesian optimization with constraint violations as large values. Errors\nbars indicate standard error from 5 independent runs.\nBlue curve: our method.Red\n01234\nlog10\u03c4\n-3\n-2\n-1\n0\nlog10?\n-5.4\n-3\n-4.8\n-4.2\n-3.6\n-2\n-3.0\nlog10?\n-2.4\n-1\n-1.8\n-1.2\n(a) Objective\ntion\nFunc-\n01234\nlog10\u03c4\n0\n0.00\n-3\n0.25\n0.50\n-2\n0.75\n-1\nlog10?\n1.00\n(b) Geweke\n01234\nlog10\u03c4\n0\n0.00\n-3\n0.25\n0.50\n-2\n0.75\n-1\nlog10?\n1.00\n(c) Gelman-Rubin\n01234\nlog10\u03c4\n0\n0.00\n-3\n0.25\n0.50\n-2\n0.75\n-1\nlog10?\n1.00\n(d) Stability\n01234\nlog10\u03c4\n0\n0.00\n0.25\n0.50\n0.75\n1.00\n(e) Overall\nFigure 3: Tuning Hamiltonian Monte Carlo with constrained Bayesian optimization: (a) objective\nfunction model, (b-e) constraint satisfaction probability surfaces for (b) Geweke test, (c) Gelman-\nRubin test, (d) stability of the numerical integration, (d) overall, which is the product of the\npreceding three probability surfaces. In (a), lighter colors correspond to more effective samples,\ncircles indicate function evaluations, and the orange star indicates the best solution. In (b-e),\nconstraint observations are indicated with black \u00d7\u2019s (violations) and o\u2019s (satisfactions). Vertical\naxis label at left is for all subplots. Probability colormap at right is for (b-d).\n4 Experiments\n4.1Branin-Hoo function\nWe first illustrate constrained Bayesian optimization on the Branin-Hoo function, a 2D function\nwith three global minima (Fig. 1(a)). We add a decoupled disk constraint (x1\u2212 2.5)2+ (x2\u2212 7.5)2) \u2264 50,\nshown in Fig. 1(b). This constraint eliminates the upper-left and lower-right solutions, leaving a\nunique global minimum at x = (\u03c0,2.275), indicated by the orange star in Fig. 1(a). After 33 ob-\njective function evaluations and 17 constraint evaluations, the best solution is (3.01,2.36), which\nsatisfies the constraint and has value 0.48 (true best value = 0.40).\n4.2Online LDA with sparse topics\nOnline Latent Dirichlet Allocation (LDA, Hoffman et al., 2010) is an efficient variational formu-\nlation of a popular topic model for learning topics and corresponding word distributions given\na corpus of documents. In order for topics to have meaningful semantic interpretations, it is\ndesirable for the word distributions to exhibit sparsity. In this experiment we optimize the hy-\nperparameters of online LDA subject to the constraint that the entropy of the per-topic word\ndistribution averaged over topics is less than log2200 bits, which is achieved, for example by al-\nlocating uniform density over 200 words. We used the online LDA implementation from Agarwal\net al. (2011) and optimized five hyperparameters corresponding to the number of topics (from\n9"},{"page":10,"text":"2 to 100), two Dirichlet distribution prior base measures (from 0 to 2), and two learning rate pa-\nrameters (rate from 0.1 to 1, decay from 10\u22125to 1). As a baseline, we compare with unconstrained\nBayesian optimization in which constraint violations are set to the worst possible value for this\nLDA problem. Fig. 2(a) shows that constrained Bayesian optimization significantly outperforms\nthe baseline. Intuitively, the baseline is poor because the GP has difficulty modeling the sharp\ndiscontinuities caused by the large values.\nTable 1: Tuning Hamiltonian Monte Carlo.\nExperimentburn-in\u03c4?mass# samples accepteffective samples\nBaseline\nBayesOpt\n10%\n3.8%\n100\n2\n0.047\n0.048\n1\n1.55\n8.3 \u00d7 103\n3.3 \u00d7 105\n85%\n70%\n1.1 \u00d7 103\n9.7 \u00d7 104\n4.3Memory-limited neural net\nIn the final experiment, we optimize the hyperparameters of a deep neural network on the MNIST\nhandwritten digit classification task in a memory-constrained scenario.\nparameters: 1 learning rate, 2 momentum parameters (initial and final), the number of hidden\nunits per layer (2 layers), the maximum norm on model weights (for 3 sets of weights), and\nthe dropout regularization probabilities (for the inputs and 2 hidden layers). We optimize the\nclassification error on a withheld validation set under the constraint that the total number of\nmodel parameters (weights in the network) must be less than one million. This constraint is\ndecoupled from the objective and inexpensive to evaluate, because the number of weights can\nbe calculated directly from the parameters, without training the network. We train the neural\nnetwork using momentum-based stochastic gradient descent which is notoriously difficult to tune\nas training can diverge under various combinations of the momentum and learning rate. When\ntraining diverges, the objective function cannot be measured. Reporting the constraint violation\nas a large objective value performs poorly because it introduces sharp discontinuities that are\nhard to model (Fig. 2). This necessitates a second noisy, binary constraint which is violated when\ntraining diverges, for example when the both the learning rate and momentum are too large. The\nnetwork is trained6for 25,000 weight updates and the objective is reported as classification error\non the standard validation set. Our Bayesian optimization routine can thus choose between two\ndecoupled tasks, evaluating the memory constraint or the validation error after a full training\nrun.Evaluating the validation error can still cause a constraint violation when the training\ndiverges, which is treated as a binary constraint in our model. Fig. 2(b) shows a comparison of\nour constrained Bayesian optimization against a baseline standard Bayesian optimization where\nconstraint violations are treated as resulting in a random classifier (90% error). Only the objective\nevaluations are presented, since constraint evaluations are extremely inexpensive compared to an\nentire training run. In the event that training diverges on an objective evaluation, we report 90%\nerror. The optimized net has a learning rate of 0.1, dropout probabilities of 0.17 (inputs), 0.30\n(first layer), and 0 (second layer), initial momentum 0.86, and final momentum 0.81. Interestingly,\nthe optimization chooses a small first layer (size 312) and a large second layer (size 1772).\nWe optimize over 11\n4.4 Tuning Markov chain Monte Carlo\nHamiltonian Monte Carlo (HMC) is a popular MCMC sampling technique that takes advantage\nof gradient information for rapid mixing. However, HMC contains several parameters that re-\nquire careful tuning. The two basic parameters are the number of leapfrog steps \u03c4, and the step\nsize ?. HMC may also include a mass matrix which introduces O(D2) additional parameters in\nD dimensions, although the matrix is often chosen to be diagonal (D parameters) or a multiple\nof the identity matrix (1 parameter) (Neal, 2011). In this experiment, we optimize the perfor-\nmance of HMC using Bayesian optimization; see Mahendran et al. (2012) for a similar approach.\n6We use the Deepnet package: https:\/\/github.com\/nitishsrivastava\/deepnet.\n10"},{"page":11,"text":"We optimize the following parameters: \u03c4, ?, a mass parameter, and the fraction of the allotted\ncomputation time spent burning in the chain.\nOur experiment measures the number of effective samples (ES) in a fixed computation time;\nthis corresponds to finding chains that minimize estimator variance. We impose the constraints\nthat the generated samples must pass the Geweke (Geweke, 1992) and Gelman-Rubin (Gelman\nand Rubin, 1992) convergence diagnostics. In particular, we require the worst (largest absolute\nvalue) Geweke test score across all variables and chains to be at most 2.0, and the worst (largest)\nGelman-Rubin score between chains and across all variables to be at most 1.2. We use PyMC\n(Patil et al., 2010) for the convergence diagnostics and the LaplacesDemon R package to compute\neffective sample size. The chosen thresholds for the convergence diagnostics are based on the\nPyMC and LaplacesDemon documentation. The HMC integration may also diverge for large\nvalues of ?; we treat this as an additional constraint, and set \u03b4 = 0.05 for all constraints. We\noptimize HMC sampling from the posterior of a logistic regression binary classification problem\nusing the German credit data set from the UCI repository (Frank and Asuncion, 2010). The data\nset contains 1000 data points, and is normalized to have unit variance. We initialize each chain\nrandomly with D independent draws from a Gaussian distribution with mean zero and standard\ndeviation 10\u22123. For each set of inputs, we compute two chains, each with 5 minutes of computation\ntime on a single core of a compute node.\nFig. 3 shows the constraint surfaces discovered by Bayesian optimization for a simpler ex-\nperiment in which only \u03c4 and ? are varied; burn-in is fixed at 10% and the mass is fixed at 1.\nThese diagrams yield interpretations of the feasible region; for example, Fig. 3(d) shows that\nthe numerical integration diverges for values of ? above \u2248 10\u22121. Table 1 shows the results of\nour 4-parameter optimization after 50 iterations, compared with a baseline that is reflective of a\ntypical HMC configuration: 10% burn in, 100 leapfrog steps, and the step size chosen to yield an\n85% proposal accept rate. Each row in the table was produced by averaging 5 independent runs\nwith the given parameters. The optimization chooses to perform very few (\u03c4 = 2) leapfrog steps\nand spend relatively little time (3.8%) burning in the chain, and chooses an acceptance rate of\n70%. In contrast, the baseline spends much more time generating each proposal (\u03c4 = 100), which\nproduces many fewer total samples and, correspondingly, significantly fewer effective samples.\n5Conclusion\nIn this paper we extended Bayesian optimization to constrained optimization problems. Because\nconstraint observations may be noisy, we formulate the problem using probabilistic constraints,\nallowing the user to directly express the tradeoff between cost and risk by specifying the confidence\nparameter \u03b4. We then propose an acquisition function to perform constrained Bayesian optimiza-\ntion, including the case where the objective and constraint(s) may be observed independently. We\ndemonstrate the effectiveness of our system on the meta-optimization of machine learning algo-\nrithms and sampling techniques. Constrained optimization is a ubiquitous problem and we believe\nthis work has applications in areas such as product design (e.g. designing a low-calorie cookie),\nmachine learning meta-optimization (as in our experiments), real-time systems (such as a speech\nrecognition system on a mobile device with speed, memory, and\/or energy usage constraints),\nor any optimization problem in which the objective function and\/or constraints are expensive to\nevaluate and possibly noisy.\nAcknowledgements\nThe authors would like to thank Geoffrey Hinton, George Dahl, and Oren Rippel for helpful\ndiscussions, and Robert Nishihara for help with the experiments. This work was partially funded\nby DARPA Young Faculty Award N66001-12-1-4219. Jasper Snoek is a fellow in the Harvard\nCenter for Research on Computation and Society.\n11"},{"page":12,"text":"References\nAlekh Agarwal, Olivier Chapelle, Miroslav Dud\u00b4 \u0131k, and John Langford. A reliable effective terascale\nlinear learning system, 2011. arXiv: 1110.4198 [cs.LG].\nJames S. Bergstra, R\u00b4 emi Bardenet, Yoshua Bengio, and B\u00b4 al\u00b4 azs K\u00b4 egl. Algorithms for hyper-\nparameter optimization. In NIPS. 2011.\nEric Brochu, Tyson Brochu, and Nando de Freitas. A Bayesian interactive optimization approach\nto procedural animation design. In ACM SIGGRAPH\/Eurographics Symposium on Computer\nAnimation, 2010a.\nEric Brochu, Vlad M. Cora, and Nando de Freitas. A tutorial on Bayesian optimization of expen-\nsive cost functions, 2010b. arXiv:1012.2599 [cs.LG].\nAdam D. Bull. Convergence rates of efficient global optimization algorithms. JMLR, (3-4):2879\u2013\n2904, 2011.\nNando de Freitas, Alex Smola, and Masrour Zoghi. Exponential regret bounds for Gaussian\nprocess bandits with deterministic observations. In ICML, 2012.\nJosip Djolonga, Andreas Krause, and Volkan Cevher. High dimensional Gaussian Process bandits.\nIn NIPS, 2013.\nAndrew Frank and Arthur Asuncion. UCI machine learning repository, 2010.\nAndrew Gelman and Donald R. Rubin. A single series from the Gibbs sampler provides a false\nsense of security. In Bayesian Statistics, pages 625\u201332. Oxford University Press, 1992.\nJohn Geweke. Evaluating the accuracy of sampling-based approaches to the calculation of posterior\nmoments. In Bayesian Statistics, pages 169\u2013193. University Press, 1992.\nDavid Ginsbourger and Rodolphe Riche. Towards Gaussian process-based optimization with finite\ntime horizon. In Advances in Model-Oriented Design and Analysis. Physica-Verlag HD, 2010.\nRobert B. Gramacy and Herbert K. H. Lee. Optimization under unknown constraints, 2010.\narXiv:1004.4027 [stat.ME].\nPhilipp Hennig and Christian J. Schuler. Entropy search for information-efficient global optimiza-\ntion. JMLR, 13, 2012.\nMatthew Hoffman, David M. Blei, and Francis Bach. Online learning for latent Dirichlet allocation.\nIn NIPS, 2010.\nFrank Hutter, Holger H. Hoos, and Kevin Leyton-Brown. Sequential model-based optimization\nfor general algorithm configuration. In LION, 2011.\nDonald R. Jones. A taxonomy of global optimization methods based on response surfaces. Journal\nof Global Optimization, 21, 2001.\nAndreas Krause and Cheng Soon Ong. Contextual Gaussian Process bandit optimization. In\nNIPS, 2011.\nDan Lizotte. Practical Bayesian Optimization. PhD thesis, University of Alberta, Edmonton,\nAlberta, 2008.\nNimalan Mahendran, Ziyu Wang, Firas Hamze, and Nando de Freitas. Adaptive MCMC with\nBayesian optimization. In AISTATS, 2012.\nJonas Mockus, Vytautas Tiesis, and Antanas Zilinskas. The application of Bayesian methods for\nseeking the extremum. Towards Global Optimization, 2, 1978.\n12"},{"page":13,"text":"Iain Murray and Ryan P. Adams. Slice sampling covariance hyperparameters of latent Gaussian\nmodels. In NIPS, 2010.\nIain Murray, Ryan P. Adams, and David J.C. MacKay. Elliptical slice sampling. JMLR, 9:541\u2013548,\n2010.\nRadford Neal. Slice sampling. Annals of Statistics, 31:705\u2013767, 2000.\nRadford Neal. MCMC using Hamiltonian dynamics. In Handbook of Markov Chain Monte Carlo.\nChapman and Hall\/CRC, 2011.\nAnand Patil, David Huard, and Christopher Fonnesbeck. PyMC: Bayesian stochastic modelling\nin Python. Journal of Statistical Software, 2010.\nCarl Rasmussen and Christopher Williams. Gaussian Processes for Machine Learning. MIT Press,\n2006.\nA. Shapiro, D. Dentcheva, and A. Ruszczynski. Lectures on stochastic programming: modeling\nand theory. MPS-SIAM Series on Optimization, Philadelphia, USA, 2009.\nJasper Snoek. Bayesian Optimization and Semiparametric Models with Applications to Assistive\nTechnology. PhD thesis, University of Toronto, Toronto, Canada, 2013.\nJasper Snoek, Hugo Larochelle, and Ryan P. Adams. Practical Bayesian optimization of machine\nlearning algorithms. In NIPS, 2012.\nNiranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian process opti-\nmization in the bandit setting: no regret and experimental design. In ICML, 2010.\nKevin Swersky, Jasper Snoek, and Ryan P. Adams. Multi-task Bayesian optimization. In NIPS,\n2013.\nZiyu Wang, Masrour Zoghi, Frank Hutter, David Matheson, and Nando de Freitas. Bayesian\noptimization in high dimensions via random embeddings. In IJCAI, 2013.\nMarcela Zuluaga, Andreas Krause, Guillaume Sergent, and Markus P\u00a8 uschel. Active learning for\nmulti-objective optimization. In ICML, 2013.\n13"}],"widgetId":"rgw28_56aba1f479f68"},"id":"rgw28_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationText.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationText.html?publicationUid=261065571&hide=0","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationDetailAd":{"data":{"googleDfpSlot":null,"widgetId":"rgw29_56aba1f479f68"},"id":"rgw29_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicationDetailAd.html","templateExtensions":["generalHelpers"],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicationDetailAd.html?publicationUid=261065571&slotId=336x280_Publications_ATF_Right&collapseSlotMode=never&fallbackContainerEnabled=1","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true},"publicationRelations":null,"publicationRelationsReverse":null,"publicationUid":261065571,"showSignUpDialog":false,"selectNewSignUpDialog":false,"publicationQuestions":null,"publicationCitations":null,"publicationReviewPromo":null,"publicationUsedInReviews":null,"publicationPdfJsReader":null,"useFulltextOptimizedLayout":false,"publicationActions":null,"requestFulltextPromo":null,"currentUrl":"publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints","isLeaderboardAd":false,"enableStickyBox":null,"googleDfpSlotMobileBottom":null,"fullTextExitPopup":null,"showExitPopupDialog":false,"widgetId":"rgw2_56aba1f479f68"},"id":"rgw2_56aba1f479f68","partials":{"romeo_legal_notice":"publicliterature\/stubs\/partials\/romeo_legal_notice.html"},"templateName":"publicliterature\/stubs\/PublicPublicationDetails_NewLayout.html","templateExtensions":["generalHelpers"],"attrs":{"publicationUid":261065571},"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetailsOld.html?publicationUid=261065571&isTestOldDesign=0","viewClass":"views.publicliterature.PublicPublicationDetailsOldView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsOldView"],"stylesheets":[],"_isYUI":true},"widgetId":"rgw1_56aba1f479f68"},"id":"rgw1_56aba1f479f68","partials":[],"templateName":"publicliterature\/stubs\/PublicPublicationDetails.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/publicliterature.PublicPublicationDetails.html","viewClass":"views.publicliterature.PublicPublicationDetailsView","yuiModules":["rg.views.publicliterature.PublicPublicationDetailsView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"uaClass":"","headPrefix":[],"rootUrl":"https:\/\/www.researchgate.net\/","requestToken":"4nvqRNb6r5f44lJYzzbRqOi9kCiUq6tYPxKk0wmXzJ56bbDL1lldZN6BnKMdWZrfjucrkHTLDbQahxx7c7kw5aEyZQ06Y49jofnxc6lsu1qZhzNxE5F5qq7ta\/5ej3NGWBWFos0UPVlD9XESpE1ZwXaIZiAhyHGS8IV1VSvjgRPXZwl8dSGQMEUSDG7NTpLQINArFyUly7xi0sClVzDa\/TYy2Dbr7ttuOKjb\/5OA1wiPHJaUbsP\/eptgB4ovwec9iD8JurSvume5BuFTgQUn5rv2kzwMk\/TQmmBSlQjgBeI=","faviconCdnUrl":"https:\/\/c5.rgstatic.net\/m\/2390829798215018\/images\/favicon.ico","headerOutput":"<noscript><\/noscript><link rel=\"canonical\" href=\"https:\/\/www.researchgate.net\/publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/c5.rgstatic.net\" \/>\n<link rel=\"dns-prefetch\" href=\"\/\/i1.rgstatic.net\" \/>\n<meta property=\"twitter:card\" content=\"summary\" \/>\n<meta property=\"twitter:site\" content=\"@ResearchGate\" \/>\n<meta property=\"og:title\" content=\"Bayesian Optimization with Unknown Constraints\" \/>\n<meta property=\"og:description\" content=\"Recent work on Bayesian optimization has shown its effectiveness in global\noptimization of difficult black-box objective functions. Many real-world\noptimization problems of interest also have...\" \/>\n<meta property=\"og:site_name\" content=\"ResearchGate\" \/>\n<meta property=\"og:image\" content=\"https:\/\/i1.rgstatic.net\/publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints\/links\/03353e7e0cf216bc595c8d9c\/smallpreview.png\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.researchgate.net\/publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints\" \/>\n<meta property=\"rg:id\" content=\"PB:261065571\" \/>\n<meta name=\"DC.identifier\" scheme=\"DCTERMS.URI\" content=\"http:\/\/dx.doi.org\/\" \/>\n<meta name=\"gs_meta_revision\" content=\"1.1\" \/>\n<meta name=\"citation_title\" content=\"Bayesian Optimization with Unknown Constraints\" \/>\n<meta name=\"citation_author\" content=\"Michael A. Gelbart\" \/>\n<meta name=\"citation_author\" content=\"Jasper Snoek\" \/>\n<meta name=\"citation_author\" content=\"Ryan P. Adams\" \/>\n<meta name=\"citation_publication_date\" content=\"2014\/03\/21\" \/>\n<meta name=\"citation_abstract_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints\" \/>\n<meta name=\"citation_fulltext_html_url\" content=\"https:\/\/www.researchgate.net\/publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints\" \/>\n<meta name=\"viewport\" content=\"width=device-width,initial-scale=1\" \/>\n<link href=\"\/\/c5.rgstatic.net\/m\/22664197317151888\/styles\/rg.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<link href=\"\/\/c5.rgstatic.net\/m\/21004998181197492\/styles\/rg2.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<!--[if lt IE 9]><link href=\"\/\/c5.rgstatic.net\/m\/238176252723686\/styles\/ie.css\" type=\"text\/css\" rel=\"stylesheet\"\/><![endif]-->\n<link href=\"\/\/c5.rgstatic.net\/m\/217752362214895\/styles\/modules\/publicprofile.css\" type=\"text\/css\" rel=\"stylesheet\"\/>\n<script src=\"\/\/c5.rgstatic.net\/m\/2321000301012716\/javascript\/vendor\/webfontloader\/webfontloader.js\" type=\"text\/javascript\"><\/script>\n <script>(function(i,s,o,g,r,a,m){i[\"GoogleAnalyticsObject\"]=r;i[r]=i[r]||function(){ (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o), m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m) })(window,document,\"script\",\"\/\/www.google-analytics.com\/analytics.js\",\"ga\");\n ga(\"create\",\"UA-58591210-1\");ga(\"set\",\"anonymizeIp\",true);ga('set', 'dimension1', 'publication slurped');ga('set', 'dimension2', 'Article');ga('set', 'dimension3', 'Logged out');ga('set', 'dimension4', 'https');ga(\"send\",\"pageview\");<\/script>\n","webfont":"<noscript><\/noscript><script> WebFontConfig = { custom: { families: ['source_sans_pro_italic', 'source_sans_pro_bold', 'source_sans_pro', 'martel'], urls : ['\/\/c5.rgstatic.net\/m\/231392577336386\/styles\/fonts.css'] } }; WebFont.load(WebFontConfig); <\/script>\n","correlationId":"rgreq-237ad02b-41ed-47ad-8792-b39f65bfca40","accountId":null,"module":"publicliterature","action":"publicliterature.PublicPublicationDetails","page":"publicationDetail","product":"publications","backendTime":387,"continent":"Asia","stylesHome":"\/\/c5.rgstatic.net\/m\/","staticHost":"c5.rgstatic.net","useEarlyFlush":false,"longRunningRequestIdentifier":"LongRunningRequest.publicliterature.PublicPublicationDetails","longRunningRequestFp":"e9e5bbe1366c7ee8c8e41d02f793c884ac9be19b","widgetId":"rgw30_56aba1f479f68"},"id":"rgw30_56aba1f479f68","partials":[],"templateName":"application\/stubs\/StaticHeader.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.StaticHeader.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicliterature.PublicPublicationDetails.run.html.loggedOut.get", "59de9668d07aeb77ab4de5f24d382b5d199d2437", "rgreq-237ad02b-41ed-47ad-8792-b39f65bfca40", "e0d09ae7e4f0d6935bb2884cbe56d526b0daaf7e");
        
            Y.rg.core.pagespeed.Monitoring.monitorPage("\/\/glassmoni.researchgate.net", "publicationDetail.loggedOut", "ed1993d9e20f6cefa83edacaa24401a18c071aea", "rgreq-237ad02b-41ed-47ad-8792-b39f65bfca40", "e0d09ae7e4f0d6935bb2884cbe56d526b0daaf7e");
        })();
(function(){Y.rg.createInitialWidget({"data":{"year":"2016","inlinePromo":null,"isAdmin":false,"contactUrl":"https:\/\/www.researchgate.net\/contact","aboutUsUrl":"https:\/\/www.researchgate.net\/about","widgetId":"rgw31_56aba1f479f68"},"id":"rgw31_56aba1f479f68","partials":[],"templateName":"application\/stubs\/DefaultFooter.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.DefaultFooter.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"headerLogin":{"data":{"urlAfterLogin":"publication\/261065571_Bayesian_Optimization_with_Unknown_Constraints","requestToken":"RilBMm9dtoMY34doR46E8za0vGML06V5qhHM\/reN7BMqn\/ZQdEIq3tWEBIZe4BK1yn7M6AVNJkGnLk5eZzNcff0aNWwKHFim9YAjrgnMKTfLezFwwm\/Dff+wlPylZEF9bS+4T9dpt23Dm+726y2HMXBgXxhWTJQ3JVYnYiNOyLo0MiebecYdNNNtBBlclYhUYVAHGY0Z1BPpmDu9Ku7M+u10Ib1S1ijalf3hUt9\/H8HiWRapRSI7d\/O+btjG8TsBsqWaR6XoA8NnycXP04g7+QM+sIJVuKJtuijz5ZuToiU=","loginUrl":"https:\/\/www.researchgate.net\/application.Login.html","signupUrl":"https:\/\/www.researchgate.net\/signup.SignUp.html?ev=su_chnl_index&hdrsu=1&_sg=X1C_axkLrbX-2A-ELv22LpvCWNdaTVyvp-Yau5ZPoWkQrZHLKfnAcjWKUOV1F5g7","encodedUrlAfterLogin":"cHVibGljYXRpb24vMjYxMDY1NTcxX0JheWVzaWFuX09wdGltaXphdGlvbl93aXRoX1Vua25vd25fQ29uc3RyYWludHM%3D","signupCallToAction":"Join for free","widgetId":"rgw33_56aba1f479f68"},"id":"rgw33_56aba1f479f68","partials":{"partial":"application\/stubs\/partials\/headerLoginDefault.html"},"templateName":"application\/stubs\/HeaderLogin.html","templateExtensions":[],"attrs":{"goal":"milestoneHeaderLoginSeen"},"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLogin.html","viewClass":"views.application.HeaderLoginView","yuiModules":["rg.views.application.HeaderLoginView"],"stylesheets":[],"_isYUI":true},"cookieConsent":null,"logoSvgSrc":"https:\/\/c5.rgstatic.net\/m\/235107188705592\/images\/template\/brand-header-logo.svg","logoFallbackSrc":"https:\/\/c5.rgstatic.net\/m\/238113351022438\/images\/template\/brand-header-logo.png","widgetId":"rgw32_56aba1f479f68"},"id":"rgw32_56aba1f479f68","partials":{"schemaSocialProfiles":"application\/stubs\/partials\/schemaSocialProfiles.html"},"templateName":"application\/stubs\/HeaderLoggedOut.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.HeaderLoggedOut.html","viewClass":null,"yuiModules":[],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.createInitialWidget({"data":{"logEvent":"su_banner","specialCopy":false,"widgetId":"rgw34_56aba1f479f68"},"id":"rgw34_56aba1f479f68","partials":[],"templateName":"application\/stubs\/LoggedOutBanner.html","templateExtensions":[],"attrs":[],"widgetUrl":"https:\/\/www.researchgate.net\/application.LoggedOutBanner.html","viewClass":"views.application.LoggedOutBannerView","yuiModules":["rg.views.application.LoggedOutBannerView"],"stylesheets":[],"_isYUI":true,"initState":[]});})();
(function(){Y.rg.core.util.ParameterFilter.filter(["ev","cp","ch","ref","dbw","pli","loginT","uid","claimChannel","enrichId","enrichSource","utm_source","utm_medium","utm_campaign","el","ci"]);})();
});}); } else { throw 'YRG was not loaded when attaching widgets'; }</script><script> dataLayer = [{"pageCategory":"publication slurped","publicationType":"Article","eventCategory":"Publication page"}]; </script> <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-MKVKH7" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-MKVKH7');</script><script>(function(e){function n(n,t,o,a){"use strict";var i=e.document.createElement("link"),r=t||e.document.getElementsByTagName("script")[0],d=e.document.styleSheets;return i.rel="stylesheet",i.href=n,i.media="only x",a&&(i.onload=a),r.parentNode.insertBefore(i,r),i.onloadcssdefined=function(e){for(var t,o=0;d.length>o;o++)d[o].href&&d[o].href.indexOf(n)>-1&&(t=!0);t?e():setTimeout(function(){i.onloadcssdefined(e)})},i.onloadcssdefined(function(){i.media=o||"all"}),i}function t(e,n){e.onload=function(){e.onload=null,n&&n.call(e)},"isApplicationInstalled"in navigator&&"onloadcssdefined"in e&&e.onloadcssdefined(n)}var o=function(a,i){"use strict";if(a&&3===a.length){var r=e.Image,d=!(!document.createElementNS||!document.createElementNS("http://www.w3.org/2000/svg","svg").createSVGRect||!document.implementation.hasFeature("http://www.w3.org/TR/SVG11/feature#Image","1.1")||e.opera&&-1===navigator.userAgent.indexOf("Chrome")||-1!==navigator.userAgent.indexOf("Series40")),c=new r;c.onerror=function(){o.method="png",o.href=a[2],n(a[2])},c.onload=function(){var e=1===c.width&&1===c.height,r=a[e&&d?0:e?1:2];o.method=e&&d?"svg":e?"datapng":"png",o.href=r,t(n(r),i)},c.src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///ywAAAAAAQABAAACAUwAOw==",document.documentElement.className+=" grunticon"}};o.loadCSS=n,o.onloadCSS=t,e.grunticon=o})(this);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_header-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_admin-ico.fallback.scss"]);grunticon(["https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.svg.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.png.css", "https://c5.rgstatic.net/c/o1q2er/styles/icons/_ico.fallback.scss"]);</script></body>
</html>
